{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Since j m,n increases monotonically with n, we must take n = 1 to minimize T . Thus the optimal streamlines pattern displays a single cell in the radial direction. It is then a simple matter of enumerating the zeros j m,1 to find that the integrated mean exit time is minimized for (m, n) = (2, 1), independent of the Péclet number (see Figure 1). The streamline and mean exit time patterns are illustrated in Figure 2.\n\nMentions:\n- It is then a simple matter of enumerating the zeros j m,1 to find that the integrated mean exit time is minimized for (m, n) = (2, 1), independent of the Péclet number (see Figure 1).\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1. The coefficient of Pe2 in the small-Pe optimal enhancement (32) with n = 1 (first zero). The optimal enhancement is achieved for m = 2, then drops off slowly."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- 1) Implications of Filtered Window: As shown in Fig. 9, we can mitigate the possible outliers by preprocessing the data. Note that Fig. 9 is obtained by applying the moving average with window size equals to 10. We further examine the effect of window size on the risk classification performance, and the results are shown in Fig. 11\n- Previously, we have verified that the accuracy in distance estimation did affect the classification performance. Hence, we examine the distance estimation performance for all the five cases above. In particular, we used the mean absolute error  (MAE) to compute the error between the estimated distance and the ground truth distance. The CDF of the distance estimation errors for all the six cases is plotted, as shown in Fig. 15. It is obvious that the filtered data achieves a better performance. The window size 100 is selected based on the justification provided in Fig. 11. From the CDF, we can see that, for 80% of the time, the error is less than 1.27 m for HH case, 0.76 m HP case, 3.93 m HB case, 14.94 m PB case, 1.79 m PP case, and 1.48 m BB case. We observe that PB has the worst performance. This can be explained by the fact that the signals from both smartphones were suffered through different paths of attenuation. Hence, even though we tried to calibrate the model based on the environmental factor, the model is unable to capture such variations. For HH, PP, and BB cases, the signals from both sides might suffer similar path of attenuation. Take the BB case for example, the smartphone observed a signal blocked by a human body since the smartphone on the other side was located inside the backpack. Similarly, the smartphone on the other side also observed similar signal blocked by another human body. Hence, as long as both smartphones measure the signal from similar position, it is most likely to produce a good estimation.\n\nMentions:\n- We further examine the effect of window size on the risk classification performance, and the results are shown in Fig. 11\n- The window size 100 is selected based on the justification provided in Fig. 11.\n\nOCR Text: \"Wing w Sue\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 11: The accuracy score for different window size."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Beyond the J 0 approximation, ω ′ 0 can be determined numerically as discussed above or from the Monte Carlo simulation. We illustrate the onset of the breakdown of the J 0 approximation in Fig. 1 where it is shown that the precession frequency shift determined from the simulation is best predicted by the numerical diagonalization of a higher order perturbation theory where the shift due to the electric field is included in the Hamiltonian used to determine the perturbed Zeeman splitting. From this result we might assume that this numerical method can be used to predict the relaxation and frequency shifts that are introduced with inhomogeneities in the holding fields. However this technique will not provide an accurate prediction, because the frequency shift is an extrinsic effect, it is determined from the strength of the spectrum for the correlation function at the Larmor frequency. Since the frequency has been changed by the spin dressing we may also expect the contribution from the correlation function to change as well. Therefore a more detailed investigation of the relaxation and frequency shifts due to inhomogeneities of the field is desired; in the following section we present a model for this purpose.\n- We assume here that equation 16 is the solution to the Hamiltonian; for ω 0 &lt;&lt; ω the J 0 approximation is valid. When that condition is broken the exact expansion must be used, or alternately, computed to high accuracy numerically, this is discussed in section 2.1. We use this solution to add additional perturbations in the Hamiltonian, these perturbations take the form of an electric field E = Eẑ, and inhomogeneous field functions for the spin dressing B 1 oscillating at a frequency ω, and DC holding field B 0 . We will discuss solutions in the particle rest frame such that spatial B-field inhomogeneities couple with the particle motion Figure 1: Frequency shift due to a neutron EDM of d n = 1 × 10 −25 e cm and 75 kV/cm electric field in a critically dressed system of 3 He and neutrons. Excellent agreement is observed between the shift predicted from the simulation (∆ω sim ) and the higher order perturbation theory (∆ω pt ). The deviation from the simulation and the shift predicted from the J 0 approximation (∆ω J ) is apparent at lower frequencies. The deviation from a constant for the J 0 approximation is due to deviation of x c from that given by Eq. 17. For these calculations B 0 = 3 µT giving ω ′ 0 ∼ 380 rad/s to become time-dependent variations. The perturbation is thus time varying n = λ(t). However, this will be represented by a change in the field B 1 → B 1 + δB 1 (t), where for now δB 1 (t) is some arbitrary function of time, but is small compared to B 1 . A similar term for the Hamiltonian occurs in reference [18], where they also model the gradient of a dressing field. Recall that this change is a negligible shift in the total energy, shifting all levels by the same amount at any instant, thus the overall dynamics can be well described if we include the term γδB1(t) λ 1 2 σx 4 a † + a , this will result in a time dependent energy, but does not introduce significant error as long as γδB 1 is small compared to ω and γB 1 . Interestingly, γδB 1 need not be small compared to γB 0 . Furthermore we require ω &gt;&gt; ω 0 if we wish to use the approximation in equation 15.\n\nMentions:\n- We illustrate the onset of the breakdown of the J 0 approximation in Fig. 1 where it is shown that the precession frequency shift determined from the simulation is best predicted by the numerical diagonalization of a higher order perturbation theory where the shift due to the electric field is included in the Hamiltonian used to determine the perturbed Zeeman splitting.\n- We will discuss solutions in the particle rest frame such that spatial B-field inhomogeneities couple with the particle motion Figure 1: Frequency shift due to a neutron EDM of d n = 1 × 10 −25 e cm and 75 kV/cm electric field in a critically dressed system of 3 He and neutrons.\n\nOCR Text: \"1z; (rac 1 D\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Frequency shift due to a neutron EDM of dn = 1 × 10−25 e cm and 75 kV/cm electric field in a critically dressed system of 3He and neutrons. Excellent agreement is observed between the shift predicted from the simulation (∆ωsim) and the higher order perturbation theory (∆ωpt). The deviation from the simulation and the shift predicted from the J0 approximation (∆ωJ) is apparent at lower frequencies. The deviation from a constant for the J0 approximation is due to deviation of xc from that given by Eq. 17. For these calculations B0 = 3 µT giving ω ′ 0 ∼ 380 rad/s"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Fig. 12, one can see that the t 2g bands are fitted very well by the Wannier functions. The t 2g manifold is separated in energy space from the e g bands above 1.5 eV and oxygen bands below −2 eV. Since we constructed the t 2g model, the e g bands and oxygen bands are not taken into account in the present DMFT calculation. Figure 13 shows A(k, ω) computed by DMFT Compared to the DFT band structure in Fig. 12, one can see that the t 2g band is substantially renormalized by strong correlation effects as expected.\n\nMentions:\n- In Fig. 12, one can see that the t 2g bands are fitted very well by the Wannier functions.\n- Figure 13 shows A(k, ω) computed by DMFT Compared to the DFT band structure in Fig. 12, one can see that the t 2g band is substantially renormalized by strong correlation effects as expected.\n\nOCR Text: \"08:-*000: I8e4 See] W Pa;-*000: J0j8; 80*_ 40\"%| Neb:o 80f0;0. 8%1e;978388177 Jlaaitdn $ [oixki 357 L\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 12: Comparison of LDA band structure and fit by the Wannier functions. The circles denote the energies computed by LDA and the solid lines denote the Wannier fit."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We show this fit in Figure 22. If we translate this relation and exclude the sparse star forming regions, the ∆β-distance relation is well-described by the model, as shown in Figure 23.\n\nMentions:\n- If we translate this relation and exclude the sparse star forming regions, the ∆β-distance relation is well-described by the model, as shown in Figure 23.\n\nOCR Text: \"log( ZHa) 1.0 38.4 log(>ua) < 39 3 0,5 0.0 0.5 1.0 Distance from Nucleus (kpc\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 23. The difference between the measured β values for the individual regions and galaxy light vs. distance from the nucleus, colour-coded by ΣHα , which is presented in units of erg s −1 kpc−2. The error bar in the top left corner represents the characteristic uncertainty. The second order polynomial shown in Figure 19 is translated from ∆τlB to ∆β using the B16 relation (black line), when the sparse star forming regions are excluded. The resulting dispersion associated with the fit is σdisp = 0.28."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Let H(t) denote the state of the hub at time t. Due to the symmetry of the considered graph, it is not difficult to observe that the dynamics is fully described by a continuous-time Markov process (H(t), |S 0 (t)|, |S 1 (t)|, |S e 0 (t)|, |S e 1 (t)|) t≥0 . We need to compute the expected value of the smallest time t such that |S 1 (t)| = 0, i.e. the time when the state 1 becomes depleted. To this end, it suffices to consider (H(t), |S 0 (t)|, |S 1 (t)|, |S e (t)|) where S e (t) = S e 0 (t) ∪ S e 1 (t), i.e. the system states e 0 and e 1 are lumped into one state, which we denote with e. The system will be said to be in mode i at time t, whenever the number of depleted state 1 nodes before time t is equal to i, for i = 0, 1, . . . , |S 1 (0)| − 1. Notice that if the system is in mode i at time t, then |S 0 (t)| = |S 0 (0)| − i, |S 1 (t)| = |S 1 (0)| − i, and |S e (t)| = |S e (0)| + 2i. For simplicity, we will use the following notation x i 0 = |S 0 (0)| − i, x i 1 = |S 1 (0)| − i, and x i e = |S e (0)| + 2i, for every given mode i. We will compute the expected sojourn time in each of the modes by analyzing a discretetime Markov chain φ i = (φ i k ) k≥0 , for given mode i, defined as follows. This Markov chain is embedded at time instances at which the hub node interacts with a leaf node. The state space of φ i consists of the states 0, 1, e, e * with the transition probabilities given in Figure 6. The state of the Markov chain φ i indicates the state of the hub at contact instances of the hub with the leaf nodes, where we introduced an extra state e * to encode the event where the hub is in state e and that this state was reached by the hub from either state 0 or 1, thus indicating a depletion of state 1. Note that the expected duration of mode 0 is equal to the mean hitting time of state e * for the Markov chain φ 0 started at either state 0 or 1, while the expected duration of mode i, for 0 &lt; i &lt; |S 1 (0)|, is equal to the mean hitting time of state e * for the Markov chain φ i started at state e. We compute these mean hitting times in the following. Fix an arbitrary mode i, and then let ϕ s (i) be the mean hitting time of state e * for the Markov chain φ i started at state s = 0, 1, and e. By the first-step analysis, we have that the latter mean hitting times are the solution of the following system of linear equations ϕ 0 (i) =\n- Figure 6: Star-shaped network: the transition probabilities of φ i .\n\nMentions:\n- The state space of φ i consists of the states 0, 1, e, e * with the transition probabilities given in Figure 6.\n- Figure 6: Star-shaped network: the transition probabilities of φ i .\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6: Star-shaped network: the transition probabilities of φi."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Besides, we check that the proposed fixes do not break the code. For example, the model can remove a whitespace character in the middle of an expression and thus change the AST, as shown in Figure 4. We parse a new UAST for each changed line and compare it with the original UAST. If any difference is detected, we drop the suggestions on the matching line. However, this check is costly if there are many suggestions, and we leverage a parsing optimization: we avoid parsing a complete file each time. Instead, we take the common parent UAST node of the checked line and parse the code block it spans over. The parsing may fail -some nodes are not parsable in isolation; we then continue to climb up the AST hierarchy until we succeed. On average, this optimization brings a 1.5x speedup. The AST checks are independent of each other, so they can be efficiently parallelized to leverage multiple CPU cores. However, they execute in Python which has a Global Interpreter Lock, so the parallelization yields only a moderate 50% run time decrease on a 32-core machinedue to the large increase in computing power needed to obtain this reduction, we turn it off by default. Similarly to training, we limit the size of changed files that we analyze in a pull request. Only the first 2 MB are analyzed.\n\nMentions:\n- For example, the model can remove a whitespace character in the middle of an expression and thus change the AST, as shown in Figure 4.\n\nOCR Text: \"Jiunccion _ classes okrravz walze {: if :slrray valu? _ 'IJ recui? @wa_wel; _ [e LIL 0;\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4: (a) Original example of valid JavaScript code (b) examples of predictions of formatting elements that are changing the AST (in blue) and thus rejected during the AST filtering."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The single galaxy candidate observed at z ∼ 10 poorly constrains L 10 from below. However, the lack of detections at brighter magnitudes places interesting limits from above. Figure 1  On the other hand, the solid and short-dashed lines in Figure 1 assume no evolution in the mass-to-light ratio from z = 8.0 and log 10 M supp = 9.4 and 8, respectively. While ML11 showed that the larger value of M supp is slightly preferred at z ≈ 8, neither is ruled out. Both also match the z ∼ 10 data well. Given that the model LFs for these values are near the two observed upper limits at M UV = −18.9 and -19.6, we conclude that the average galaxy brightness for a given halo mass cannot increase as significantly from z ≈ 8-10 as between z ≈ 6-8.\n- Here we use the ML11 LF model, with log 10 (M supp , L 10 ) = (9.4, 27.7) (see §4), along with the exact redshift selection function generated by the B11 J-dropout photometric selection criteria, to more carefully consider this effect. We calculate the difference between the abundance of galaxies at the mean selection redshift of z = 10.2 and that in a survey along the line-of-sight light-cone taking into account the evolving LF through the photometric band. We find that the density of observed J-dropouts with magnitude M UV = −18.4 is about 17% higher than the abundance at z = 10.2 owing to the fact that the mean redshift of the observed galaxies is actually z = 9.8 (see Fig. 1). For reference, the star formation rate density required to keep the universe ionized is 12% higher at z = 9.8 than at 10.2%. The effect increases with increasing brightness causing a slight flatting of the LF (for M UV = −20.4 the observed abundance is 35% higher than at z = 10.2 and the mean redshift sampled is z = 9.7). As discussed in ML08, we ignore the slightly varying differential volume element of the survey with redshift as a negligible contribution.\n\nMentions:\n- Figure 1  On the other hand, the solid and short-dashed lines in Figure 1 assume no evolution in the mass-to-light ratio from z = 8.0 and log 10 M supp = 9.4 and 8, respectively.\n- We find that the density of observed J-dropouts with magnitude M UV = −18.4 is about 17% higher than the abundance at z = 10.2 owing to the fact that the mean redshift of the observed galaxies is actually z = 9.8 (see Fig. 1).\n\nOCR Text: \"{My_pp' Li}=(9. 2B2) (8,27,6) (9.4,27.7) 4,27.7) LC 1 0 2 1\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1. The luminosity function of J-dropouts. The point and upper limits show results from Ref. [8], while the curves show the LF generated with the ML11 model for different combinations of the free parameters Msupp and L10. The solid, blue and short-dashed, magenta lines have log10 Msupp = 9.4 and 8, respectively, with log10L10 ≈ 27.7. The long-dashed, green curve assumes log10 (Msupp, L10) = (9.4, 28.2) with the value of L10 extrapolated to z ≈ 10 based on the 0.24 dex increase per redshift interval found from z ≈ 6–8 (see Fig. 2). The thick solid curve is the same as the thin one except that the evolution in the LF along the light-cone (LC) through the B11 redshift selection function has been taken into account."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In order to achieve a resolution on the order of the mode bandwidth (or lower), as can be understood from Fig. 5.7, the required sensitivity in term of tuning angle has to be much smaller than 17.5 µrad. If we want to achieve a better sensitivity in the resonant frequency tuning, we have to add another, less sensitive, tuning system. One possibility is to insert a small cylinder of dielectric material(alumina or sapphire) as sketched in Fig. 5.8. With a cylinder of 60 mm of radius, in sapphire, we achieved, as an example, a sensitivity of 200 Hz/mm that can be further reduced decreasing the diameter of the cylinder itself. The quality factor and of the TM 010 mode as a function of frequency and for different number of rods is given in Fig. 5.9. In the plot we supposed that the rods and the cavity are made of copper with a value of RRR equal to 20 and, as a consequence, with a conductivity at 4 K of 1.16 × 10 9 S/m [141]. We also calculated the form factor [139] and the results are given in Fig. 5.10. We also As already pointed out, to cover the frequency range up to 250 MHz, we will substitute the cavity with another one with a smaller radius. To increase as much as possible, the frequency range to be explored we decided to choose this cavity radius in order to have a starting frequency higher than the highest frequency reachable with the cavity with larger radius. The frequency gap of about 20 MHz (see Table 5.1) between the two systems can be covered with a modification of the cavity with larger radius. As represented in Fig. 5.12, the cavity can be divided in three sectors through Figure 5.12: Modification of the cavity with insertion of metallic plates to cover the range up to 150 MHz. metallic plates. A similar solution was proposed in [142]. The metallic rods, also in this case, can be moved on the same circular trajectory detuning the TM 010 mode. In this condition the mode will cover a range up to 150 MHz as represented in the mode chart of Fig. 5.13. In this case, due to the Figure 5.13: Frequency of the TM 010 mode and of all other identified modes, as a function of the angle of rotation α in the case of cavity with metallic plates. presence of the metallic plates the mode ensemble is more complex but, in any case, the working mode is still very well identified. Fig. 5.14 reports the quality factor and form factor as a function of frequency for the same case.\n\nMentions:\n- Fig. 5.14 reports the quality factor and form factor as a function of frequency for the same case.\n\nOCR Text: \"{ceh\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5.14: Quality factor and form factor as a function of frequency in the case of cavity with metallic plates."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Importance of hierarchical latent variables. Here, we investigate the effect of using hierarchical latent variable models for representation learning. In this analysis, the hidden layers were eliminated one by one from the proposed model with 5 hidden layers. The elimination was conducted in order of lowest layer to highest. Figure 7 shows the results on the ABCD dataset. In the figure, the models with 4 and 5 hidden layers perform better. This result shows the importance of extracting hierarchical latent variables in rare event detection tasks. A pair of scenes includes various types of events, which range from low-level to high-level. By utilizing hierarchical latent variables, the events of each level can be represented by each level of latent variables. As a result, detection models can easily distinguish high-level events from low-level ones. The other merit is that we can choose which level of features to use according to target events.\n\nMentions:\n- Figure 7 shows the results on the ABCD dataset.\n\nOCR Text: \"2 85,04 Number of hidden Iavers\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 7. Sensitivity analysis of the number of hidden layers. The error bar shows the standard deviations of the accuracies for 10 runs with different training subsets."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In this section, we describe how to separate PAST and FUTURE functions from decoding states. We introduce two additional RNN layers (Figure 2):\n- Formally, the PAST layer is another recurrent neural network (the second gray layer in Figure 2), and its state at time step t is calculated by\n- We integrate PAST and FUTURE layers together in our final model (Figure 2):\n\nMentions:\n- We introduce two additional RNN layers (Figure 2):\n- Formally, the PAST layer is another recurrent neural network (the second gray layer in Figure 2), and its state at time step t is calculated by\n- We integrate PAST and FUTURE layers together in our final model (Figure 2):\n\nOCR Text: \"De\"uder [xwr Fauwpe Laner Layer Shnt\" Autenho ( FcSpw SJiffvakc Layer\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: NMT decoder augmented with PAST and FUTURE layers."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- • Neural-ICO algorithm: It is similar to the DQN algorithm used in [23], [25], except that the output from the neural network is the value functions for the post-decision states instead of the Q factors for the state and action pairs. The function approximation architecture of this algorithm is given in Fig. 11 in Appendix C. We developed a discrete-event system-level simulator for the NB-IoT MEC system, where the the simulation parameters are given in Table II. We simulate a circular cell of radius R with a BS in the center. The IoT devices are uniformly distributed in the cell area in random. We divide the circular area of the considered cell into K disjunct zones by K − 1 concentric circles around the BS, where the zone k ∈ {1, . . . , K} is the region between two concentric circles with radius d k−1 and d k . We consider that all the IoT devices in zone k, k ∈ {1, . . . , K} select the same MCS level from the 13 MCS levels of the NB-IoT uplink data channel, so that their transmission data rates r k can be determined according to the Table V in [38]. Then, the mean transmission rate in terms of packets per second for IoT device n can be derived as µ k = r k /l p , where l p is the mean packet size. Note that the MCS level selection schemes are not the focus of this paper, and our proposed offloading and scheduling algorithm can be used for any link adaptation schemes. The transmission power of the IoT device n is derived according to the NB-IoT uplink open loop power control formulas given in [39]. The CPU cycles f loc n of the IoT devices are considered to be uniformly distributed. Therefore, the mean local processing rate µ loc n and local power consumption P loc n can be derived according to (1) in [8] and ( 4) in [11], respectively. Without loss of generality, we set the weights of delay and power consumption to be ω ′ n = γ ′ n = 0.5 in the reward function. For both the proposed Neural-ICFMO algorithm and the Neural-ICO algorithm, we use the ε-greedy algorithm to explore the actions. Specifically, the optimal action derived by Algorithm 1 is selected with probability (1 − p(k)) at the k-th decision epoch, where p(k) = G1\n- The function approximation architecture for the Neural-ICO algorithm is given in Fig. 11. Note that we have one convolutional layer between the input layer and output layer. We have tried to add a fully connected layer after the convolutional layer, but it turns out that this additional layer does not contribute to improving the performance.\n\nMentions:\n- The function approximation architecture of this algorithm is given in Fig. 11 in Appendix C.\n- The function approximation architecture for the Neural-ICO algorithm is given in Fig. 11.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 11. Function Approximation Architecture for neural-ICO algorithm."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 10 shows the mobility in the direction parallel and perpendicular to the low-angle grain boundary defects in sample G100c. Parallel to the defects, the mobility is only slightly   lower than that of a comparable sample without these defects, G100a, whereas it is drastically reduced perpendicular to the defects, especially at low temperatures. Above room temperature, however, the mobilities in both direction coincide and decrease with increasing temperature as expected for phononlimited transport, indicating a negligible effect of the grain boundaries. Assuming the mobility µ within the grains to be isotropic the total mobility perpendicular to the defects ([001]direction), µ ⊥ , is reduced due to the activated, barrier-related mobility µ barrier :\n\nMentions:\n- Figure 10 shows the mobility in the direction parallel and perpendicular to the low-angle grain boundary defects in sample G100c.\n\nOCR Text: \"300 GIODa G100a GIOOc 250 GIOOc [001] E GlOOc [010] 200 2 150 G1OOc [010] 100 G1Dc I G1oOc [001] 100 150 200 250 30} 350 Temperature\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 10: Temperature-dependent Hall mobility for sample G100c containing low-angle grain boundaries along the b-direction. Cyan rhombs: average mobility and mobility along and perpendicular to the defects. Black squares: mobility of the sample G100a without these defects for comparison."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Interference Link Forward Link Fig. 1. Secondary communication via an interference-aided EH relay in spectrum sharing.\n- As shown in Fig. 1, we consider a primary network that consists of L pairs of primary transmitters (PTs) and primary destinations (PDs). A spectrum of bandwidth B Hz is divided equally among L primary links, i.e., each primary link is allocated a bandwidth of B/L Hz. The secondary network consists of a secondary transmitter (ST) which communicates with a secondary destination (SD) via an energy harvesting decode-and-forward (DF) secondary relay (SR). All nodes have a single antenna. We focus on the underlay mode, where PUs are active during the operation of SUs and share the spectrum with the secondary network provided that the QoS of each primary link is maintained above a given threshold.\n\nMentions:\n- Interference Link Forward Link Fig. 1.\n- As shown in Fig. 1, we consider a primary network that consists of L pairs of primary transmitters (PTs) and primary destinations (PDs).\n\nOCR Text: \"L Sectonm proposc - (IIc bascd cncrg} SR for interfererce plus woise and noise dominant scenarlos. In Sectionly we detive analytical expressions Tor th? a8: erag? SU thzoughput in intetfetenc? plus noise scenario. Section WL we deriv? unalyLical expressions for Uhe average SU throughput In nise domnant senario, In Sectionb w? extend our Unroughpul Am./ysIs Tor Uhe case where ST is also an EH wode: In  Section [ we discuss  analytical and simulation Tesulls Icr bouh nD EH and EH ST cases; Lu SectlonL we provide conclusions and future dirac ticns Tswihung ' Karveslu? pocy\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1. Secondary communication via an interference-aided EH relay in spectrum sharing."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Here we use their results to numerically solve the Bloch equations. We then compute the force F(v) to arbitrary order of Ω 1 and extract α by means of a linear fit to the small-v region. We then compute F(v) and the photon scattering rate Γ averaged over the phase between the two standing waves. Fig. 6 compares α(δ) obtained through the continued fractions approach with the results of the perturbative expansion for the φ = 0 case. The continued fractions approach has removed the divergence at δ = 0 and α crosses zero linearly. The overall friction coefficient is reduced but the two methods show qualitative agreement in the range of δ considered. At the Raman condition the interaction with light is canceled due to the presence of |NC , thus the diffusion coefficient D p in momentum space also cancels. To lowest order, the diffusion and friction coefficients scale as\n\nMentions:\n- Fig. 6 compares α(δ) obtained through the continued fractions approach with the results of the perturbative expansion for the φ = 0 case.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 6: Comparison of results using the perturbative calculation (dashed), and the continued fractions (solid) for the φ = 0 case, with the same parameters as in Fig. (4) and Ω2 = 2.1Γ."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Thistlethwaite [Thi88] uses a decomposition of a link diagram into maximal alternating pieces to compute a lower bound on crossing number similar to Inequality (1.1). Consider a link diagram D as 4-valent plane graph with over/under decorations at the vertices. An edge or face of D should be understood to refer to an edge or face of the 4-valent plane graph. An edge of D is called non-alternating if both of its endpoints are over-strands or both of its endpoints are understrands. An edge is called alternating if one of its endpoints is an over-strand and the other is an under-strand. Mark each non-alternating edge of D with two distinct points, and in each face of D connect those marked points with arcs as depicted in Figure 1. This process results in a collection of pairwise disjoint simple closed curves {γ 1 , . . . , γ k }. The pair (D, {γ 1 , . . . , γ k }) is called the alternating decomposition of D.\n- The plane embedding of D induces an embedding of each component of G onto a sphere, as described in Section 3. Since each component of G can be embedded on a sphere, the graph G is Figure 1: Each non-alternating edge is marked with two points. Inside of each face, draw arcs that connect marked points that are adjacent on the boundary but do not lie on the same edge of D.\n\nMentions:\n- Mark each non-alternating edge of D with two distinct points, and in each face of D connect those marked points with arcs as depicted in Figure 1.\n- Since each component of G can be embedded on a sphere, the graph G is Figure 1: Each non-alternating edge is marked with two points.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Each non-alternating edge is marked with two points. Inside of each face, draw arcs that connect marked points that are adjacent on the boundary but do not lie on the same edge of D."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- because of the presence of correction terms in Eq. (22b). This issue is investigated numerically in the next section, see Figures 4 and 5. The relation Eq. ( 27) was indeed hypothesized in [7], but [24] showed through analytical and numerical studies that while it seems qualitatively correct, and is quantitatively correct in the three limits F → 0, F → ∞, and β → ∞, it is not quantitatively accurate for general parameter values. Our results in Proposition 3.1 give quantitative formulas for this discrepancy, for example, through third order:\n- b. D as a function of F for γ = 0.5, 5 and 10.   4: U for γ = 1 (blue lines) and γ = 50 (red lines). Solid lines: U computed from (15). Markers: Monte Carlo estimates. Broken lines: Expansion for U in terms of F by solving numerically for the coefficients V j given by (23a). γ = 1: Dots: linear approximation, Dash: 5th-order expansion, Dash-Dash: 9th-order expansion (overlapped with solid line). γ = 50: Dots: linear approximation, Dash-dot: 3rd-order expansion, Dash-Dash: 5th-order expansion (overlapped with solid line). Parameters of the simulations are V 0 = 1, β = 5, and L = 1. For the Monte Carlo simulations we have integrated 5000 trajectories using a time step ∆t = 0.01 over 1000000 time steps for γ = 1, while ∆t = 0.005 over 4000000 time steps for γ = 50. Now we turn our attention to the numerical study of formulas ( 21) and (27). In Figure 4 we have calculated numerically the effective drift U using ( 15), and we have also calculated numerically the coefficients V n in (21). For this we need to solve the Poisson equations (24a), where the generator of the unperturbed dynamics, i.e. with F = 0, appears. We can see that as we increase the number of terms in the power series expansion, the series converges to the value of u computed from solving the stationary Fokker-Planck equation ( 14) and computing the integral in (15). We stress that, using the expansion ( 21) we can calculate the nonequilibrium drift for arbitrary values of the external forcing F using only information from the equilibrium dynamics.\n- In Figure 5 we plot the diffusion coefficient D, as a function of the forcing F , using (16a). In addition, we plot the power series expansions of different orders according to assumption (27) that linear response relationships between drift and diffusion extend to the higher order coefficients. The drift U is computed as in Figure 4, using the expansion (21). While the power series expansion does match the value of D for F = 0, as it should according to linear response theory, the series does not converge to the values computed numerically for F = 0 using the spectral method described in Appendix A. This shows in particular that the correction terms Ξ k in Eq. (23c) are nontrivial, as also evidenced by the results of Figure 1 in [24]. To emphasize this point, we also compare in Figure 6 the effective diffusion coefficient with formula (27), where U is computed using (15) and the derivative with respect to F is approximated numerically using a centered finite difference scheme. See Figure 6. The linear response relations, however, do perform better for larger values of γ.\n\nMentions:\n- This issue is investigated numerically in the next section, see Figures 4 and 5.\n- In Figure 4 we have calculated numerically the effective drift U using ( 15), and we have also calculated numerically the coefficients V n in (21).\n- The drift U is computed as in Figure 4, using the expansion (21).\n- Parameters of the simulation are as in Figure 4.\n- Parameters of the simulation as in Figure 4.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4: U for γ = 1 (blue lines) and γ = 50 (red lines). Solid lines: U computed from (15). Markers: Monte Carlo estimates. Broken lines: Expansion for U in terms of F by solving numerically for the coefficients Vj given by (23a). γ = 1: Dots: linear approximation, Dash: 5th-order expansion, Dash-Dash: 9th-order expansion (overlapped with solid line). γ = 50: Dots: linear approximation, Dash-dot: 3rd-order expansion, Dash-Dash: 5th-order expansion (overlapped with solid line). Parameters of the simulations are V0 = 1, β = 5, and L = 1. For the Monte Carlo simulations we have integrated 5000 trajectories using a time step ∆t = 0.01 over 1000000 time steps for γ = 1, while ∆t = 0.005 over 4000000 time steps for γ = 50."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 4 shows the comparison of our convolution implementation with its implementation in the Intel IPP, Nvidia NPP and cuDNN, ArrayFire libraries. Time measurements were made for the first CNN 1 convolution layer calculation (Fig. 1) on the image with a 4K Ultra HD resolution and were averaged over 10 3 launches. When implementing the convolution for GPU, we used the method proposed by F. N. Iandola et al. [7]. Due to the fine code optimization for each CNNs architecture, the speed of layers calculation is higher than when using more universal functions from the respective libraries. In order to provide continuous GPU load, scanning of several levels of the image pyramid (3 by default) is executed simultaneously in different streams (concurrent kernels). However, a kernel computation start is synchronized between streams, as they use common pointer to texture memory.\n\nMentions:\n- Figure 4 shows the comparison of our convolution implementation with its implementation in the Intel IPP, Nvidia NPP and cuDNN, ArrayFire libraries.\n\nOCR Text: \"9' 08 ICFU PCL CPU PCZ CPU FCI GPU PC? 1 8 8 8 1 1 8\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4: Comparison of the calculation speed of the first convolution layer for CNN1 for an image with a 4K Ultra HD resolution (equipment specifications are listed in section 4)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Four different permutation matrices U Per are tried. For each U Per , the optimal power allocation is calculated and the resulted weighted MSE is calculated and shown in Fig. 1. Notice that the actually effect of the unitary matrix U Per is to realize the allocation of the weighting factors, i.e., the diagonal elements of Λ w , over different eigenchannels. Thus to save the space, instead of the adopted U Per matrices, we provide their corresponding orderings of the weighting factors in the legend, while the amplitudes of the eigenchannels are in a decreasing order. It can be clearly seen from the figure that permutation ambiguous effect significantly affects the system performance. Improper choice of permutation matrix U Per can result in serious performance degradation. The corresponding solutions are named joint transceiver (Diagonal) and the joint transceiver (DFT), respectively. As shown in Fig. 2, the joint solutions have better performance than the equalizer only solution. Furthermore, the two joint solutions have the same sum-MSE performance, verifying the conclusion that U w can be an arbitrary unitary matrix.\n\nMentions:\n- As shown in Fig. 2, the joint solutions have better performance than the equalizer only solution.\n\nOCR Text: \"Equulizc Umky Joini Transceive Diagonal) Jpint TTansceive SNR (UF\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2. The sum-MSEs of a4× 4 MIMO system with different transceiver designs when the generalized matrix-field weighted MSE model is used and Conclusion 5 is adopted for the solution."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where the length ∆L is defined by the half-value distance of the peak evolution on the background, and the width ∆W stands for the distance between the two zero-amplitude points. As shown in Fig. 3, the curve (solid line) of the aspect ratio is symmetric about q = q s line. As q → q s , the value of ∆L ∆W increases rapidly, and eventually becomes infinity when q = q s . Namely, the emergence of w-shaped wave is associated with delocalization of propagation direction in presence of the higher-order effects.\n- We highlight that, the interesting finding is that the size characteristic of the waves turns out to be positively associated with the reciprocal of the MI zero-frequency growth rate, i.e., ∆L ∆W ∼ 1 G0 , as shown in Fig. 3. Therefore, the link between the transition characteristic and the non-uniform distribution of MI growth rate is established. We also perform direct numerical simulations of Eq. (1) to verify the robustness of transition. As shown in Fig. 4, the numerical results agree well with the exact case (illustrated in Fig. 2) for the first six propagation Recently, it is demonstrated that the Peregrine rogue wave spectrum in the standard NLSE features a triangular shape and gets dramatically broadened at the maximally-compressed peak, which is corresponding to the onset of supercontinuum generation [27][28][29][30]. Thus, it is meaningful to study the spectral property of the transition between the Peregrine rogue wave and the w-shaped traveling wave. Here the analytic spectrum of solution (4) can be written with a = 1 as:\n\nMentions:\n- As shown in Fig. 3, the curve (solid line) of the aspect ratio is symmetric about q = q s line.\n- We highlight that, the interesting finding is that the size characteristic of the waves turns out to be positively associated with the reciprocal of the MI zero-frequency growth rate, i.e., ∆L ∆W ∼ 1 G0 , as shown in Fig. 3.\n\nOCR Text: \"JVJ -VGo 1.66667\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 3: Characteristics of the wave size ∆L/∆W (blue solid line) and the reciprocal of the zero-frequency growth rate of modulation instability 1/G0 (red dashed line)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Figures 8, 9, and 10 are presented the results of the kNN regression, employing optimized weights and parameters mentioned above. A summary of the 68.3% central intervals of predicted energy as a function of true energy for kNN and energy-sum NN model is presented in Fig. 11, showing that the spatial information summarized in the additional 15 features discussed in Sec. 3 may indeed be fruitfully exploited. Indeed, the combined kNN regressor outperforms the energy sum model by a large amount. Fig. 12 shows the relative uncertainty in predicted muon energy as a function of true muon energy for the two models, further proving the point.\n\nMentions:\n- Fig. 12 shows the relative uncertainty in predicted muon energy as a function of true muon energy for the two models, further proving the point.\n\nOCR Text: \"Erec;SuT Ets [GeVi\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 12: Relative RMS (σ(Epred)/Epred) of the predicted muon energy as a function of true muon energy for the kNN (blue) and the energy sum (red) models."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In this setting, the incoming Z 2 brane is denoted by \"c\" and the normal incoming brane is denoted by \"b\". The outgoing brane \"a\" is also considered as Z 2 symmetric (see Figure 9). The equations for the conservation laws are (45) and (46). They give ρ c = ρ a i.e. the effective matter density for the two Z 2 branes are identical. It has been shown previously (see equation ( 6)) that the \"gravitational\" density ρ gravity of a Z 2 brane is entirely determined by the parameters at the collision point. In particular, when collisions of only Z 2 branes are considered, ρ gravity has to be identical for every Z 2 brane at the collision point. Thus when collisions including Z 2 branes are considered, all the Z 2 branes have necessarily the same matter density. In the case studied here with one incoming and one outgoing Z 2 brane, there will necessarily be conservation of matter between those two branes 6 . Equations ( 45) and ( 46) imply: ρ b = ρ matter b − ρ gravity b = 0. Nothing constraints the value of the \"gravitational\" density of the normal brane. In particular, ρ matter b can take any value. This constitutes an instant decay of matter disappearing into the vacuum 7 (see Figure 9). Note that if another \"normal\" brane is added, the conclusion would still hold i.e. no instant creation of matter between the two Z 2 (a strict conservation) but possibly for the two (or more) non-Z 2 branes.\n\nMentions:\n- The outgoing brane \"a\" is also considered as Z 2 symmetric (see Figure 9).\n- This constitutes an instant decay of matter disappearing into the vacuum 7 (see Figure 9).\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 9: A Z2 symmetric nil-brane (c) collides with a brane (b) with null effective energy density but with non zero matter density to give rise to a Z2 symmetric nil-brane (a)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- A typical plot of P (V ) versus UAV speed V is shown in Fig. 1, together with the three individual power components and the convex approximation given in (7).\n- MR speed: On the other hand, the MR speed V mr is the optimal UAV speed that maximizes the total traveling distance with any given onboard energy E. For any given E, the range with Fig. 1: Propulsion power consumption versus speed V for rotary-wing UAV. constant traveling speed V can be expressed as EV P (V ) . Define the function\n- expression for V mr is difficult to obtain, it can be efficiently found numerically. Alternatively, V mr can also be obtained graphically based on the power-speed curve P (V ), by drawing the tangential line from the origin to the power curve that corresponds to the minimum slope (and hence power/speed ratio) [34], as illustrated in Fig. 1. In practice, we usually have V me ≤ V mr ≤ V max .\n\nMentions:\n- A typical plot of P (V ) versus UAV speed V is shown in Fig. 1, together with the three individual power components and the convex approximation given in (7).\n- For any given E, the range with Fig. 1: Propulsion power consumption versus speed V for rotary-wing UAV. constant traveling speed V can be expressed as EV P (V ) .\n- Alternatively, V mr can also be obtained graphically based on the power-speed curve P (V ), by drawing the tangential line from the origin to the power curve that corresponds to the minimum slope (and hence power/speed ratio) [34], as illustrated in Fig. 1.\n- This is due to the fact that hovering is not the most power-conserving UAV status, as shown in Fig. 1.\n\nOCR Text: \"TIniei R\"-4 P\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1: Propulsion power consumption versus speed V for rotary-wing UAV."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We now compare the ASAP accelerator (configured in LV mode) when used in an end-to-end aligner (that uses Algorithm 1), SNAP [9] 13 . We ensure that the maximum permissible LD in both the ASAP and SNAP implementations of the LV algorithm are identical across all individual alignments. The baseline SNAP aligner exploits parallelism in the alignment problem by dividing the work of aligning a set of reads among all of the 192 threads available on the system. Since our current implementation of ASAP allows for only one calling context on the host-side, we use ASAP in SNAP by maintaining a pool of memory shared among all threads to communicate with the accelerator. The procedure for each thread communicating with the accelerator is as follows: 1) picks a read from the set it was assigned; 2) queries the reference index for candidate locations for the read; 3) contends for a lock, then writes nucleotides for the read and the candidate locations into shared memory; 4) at this point, the accelerator reads from the shared memory and writes out the results to another shared segment of memory; and 5) polls for results from the accelerator using a test and test-and-set based locking protocol [40], then consumes the 13. We used version 1.0 of the SNAP tool. output. This algorithm is another example scenario where CAPI is very beneficial, and we can make use of cache coherence between the CPUs and FPGA to easily implement mutual exclusion. Fig. 15 shows the time taken per read by the baseline and the ASAP accelerator for all LD computations. We see that there is a large spread for total time spent in computing LD because some reads map to more regions of the reference than others. This variation is an artifact of both the nature of the human genome and the read simulator's practice of picking reads at random from the genome. The ASAP accelerator was used in SW mode in these experiments (a quadratic complexity algorithm), but is compared to a CPU implementation of the LV algorithm (a linear complexity algorithm). This comparison represents the most challenging acceleration use-case for ASAP. We demonstrate how to configure ASAP to a LV accelerator in Section 3.2. Overall, we see that the aligner is accelerated by 2× (i.e., 1.85 hr /0.92 hr). This is close to the Amdahl's law limit of the SNAP algorithm based on our measurements presented in Table 2.\n\nMentions:\n- Fig. 15 shows the time taken per read by the baseline and the ASAP accelerator for all LD computations.\n\nOCR Text: \"1 2 17 SNAP) ASAP 1:5 TimeisiRead\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 15. Comparing performance of SNAP using the default LV implementation and ASAP using the SW algorithm."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Thus, the free bosonic theory in the ferromagnetic regime is meaningful if the proper mean-field theory is used. Fig. 8 shows that the previous considerations are not special to the magic filling: for all fillings of the edge state, u s /K s is non-negative on both sides of the Stoner point, if the proper mean-field theory is used.\n\nMentions:\n- Thus, the free bosonic theory in the ferromagnetic regime is meaningful if the proper mean-field theory is used. Fig. 8 shows that the previous considerations are not special to the magic filling: for all fillings of the edge state, u s /K s is non-negative on both sides of the Stoner point, if the proper mean-field theory is used.\n\nOCR Text: \"V VU Ialo IzUSi Zeli ' ^\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 8: The prefactor us/Ks of the term (∂xφs) 2 in the bosonized Hamiltonian near the Stoner point, as a function of te/U for varying fillings near half-filling. The red curve stands for n̄ = 0.4 and the blue line stands for n̄ = 0.6."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- A small example goes as follows. Assume the syntax says that the structure of the sentence \"Some club defeated Leeds.\" is (some (club)) (defeated Leeds) where the function is always the term on the left. If the semantic terms are as in the lexicon in figure 1, placing the semantical terms in place of the words yields a large λ-term that can be reduced:\n\nMentions:\n- If the semantic terms are as in the lexicon in figure 1, placing the semantical terms in place of the words yields a large λ-term that can be reduced:\n\nOCR Text: \"Uant] (uaw | Vyjr 6+/+/ 1/+/+6 (uac | Iye J | e-ip+# JM, Kxw !h\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1. A simple semantic lexicon"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Table 2 and Figure 3 shows the summary of usability results of all the snippets. A total of 3M code snippets were analyzed. Python and JavaScript proved to be the languages for which the most code snippets are usable: 537,767 (65.88%) JavaScript snippets are parsable and 163,247 (20.00%) of them are runnable; for Python, 402,249 (76.22%) are parsable and 135,147 (25.61%) are runnable. Conversely, Java and C# proved to be the languages with the lowest usability rate: 129,727 (16.00%) C# snippets are parsable but only 986 (0.12%) of them are compilable; for Java, only 35,619 (3.89%) are parsable and 9,177 (1.00%) compile.\n\nMentions:\n- Table 2 and Figure 3 shows the summary of usability results of all the snippets.\n\nOCR Text: \"~ujluiJ WeMmt GicuD) n Ficmt SIcUD) Pzcszele 4CCD) Ucnmnne-prurnacle 3IC0) 3IC0D) L0Dj Jzns  Ipt   ? lhcn\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Parsable and compilable/runnable rates histogram"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The polarization-controlled 2D spectra were phased to pump-probe data where pump and probe pulses were polarized in parallel. This procedure is not rigorously correct because in polarization-controlled 2D-ES the first two pulses have different polarization directions while in pump-probe the first two interactions derive from the same pump pulse which naturally means parallel interactions. In other words, the projection slice theorem is strictly speaking not valid for the experiments presented here 10 . Despite this discrepancy, one can still satisfactorily phase polarization-controlled 2D spectra to all-parallel pump-probe as shown in Supplementary Figure 4. One explanation of this is leakage of the much stronger all-parallel signals through the crossed polarizers, meaning that the all-parallel signal still dominates the non-oscillatory part of the (90,0,90,0) 2D-signal. In this work, we decided to phase polarization-controlled 2D data to parallel pump-probe data. We note that the imperfection in phasing parameters only affects the lineshapes of the real and imaginary part of ω 2 maps, but preserves their amplitude-maps in both lineshape and magnitude. Hence, the difficulties in phasing polarization-controlled 2D spectra discussed above do not affect the conclusions drawn in the main part of the main text, which were based on ω 2 amplitude-maps. To this end, we found that arbitrary and large changes of the phasing parameters do not alter ω 2 amplitude-maps shown in Figure 2 of the main text (results not presented). It is noted that sophisticated phasing techniques based on heterodyned transient grating instead of pump-probe offer a correct method to phase crossed-polarization 2D signals 11 .\n\nMentions:\n- Despite this discrepancy, one can still satisfactorily phase polarization-controlled 2D spectra to all-parallel pump-probe as shown in Supplementary Figure 4.\n\nOCR Text: \"proj: 2D Detection frequency Wz / 10\" cm\"\"\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 4. Pump-probe and projected 2D signal. Projection of phased polarization-controlled 2D spectra(blue) to all-parallel pump-probe (light blue) at t2 = 180 fs."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In simulations, we set mean UAV speed as 40 kilometers per hour, mean coverage radius as 2.0 kilometers, mean hovering altitude as 5.0 kilometers. In Figure 3, our sustainable UAV deployment problem is solved by both min-sum and min-max approach. As the number of UAVs increases, the energy consumption of both approaches decreases due to the UAV diversity gain, as more UAVs have larger coverage radius, smaller hovering altitudes and higher flying speeds. Figure 3 tells that minimizing the total deployment energy consumption can imply a significant increase in maximum deployment energy consumption in the bottleneck UAV.\n\nMentions:\n- In Figure 3, our sustainable UAV deployment problem is solved by both min-sum and min-max approach.\n- Figure 3 tells that minimizing the total deployment energy consumption can imply a significant increase in maximum deployment energy consumption in the bottleneck UAV.\n\nOCR Text: \"Ckit-Sur Xiin:h M #M: = Cmfec chwc c%;h\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3: Comparison between min-max and min-sum approaches in terms of the maximum deployment energy consumption."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- As mentioned previously and shown on Fig. 2(a), the recorded images present a shape radically different from the expected circular rings. To get better understanding on the observed anisotropy, we solved numerically the coupled Eqs. (4) and (2). The results shown in Fig. 2(b) for I = 100 mW.cm −2 is, at least qualitatively, in very good accordance with the corresponding experimental image. To compare with, we present in Fig. 2(c), the numerics obtained at the same input intensity in the isotropic approximation, solving directly Eq. (5). This allows us to clearly attribute the non-symmetric shape of the spectrum to the anisotropic response of the crystal, described by Eqs. (1) and (2). To be more quantitative, we apply the same experimental procedure on profiles obtained along k y to extract the nonlinear refractive index for each input intensity. Although not shown here, a saturation is also observed. We present on Fig. 5 the ratios of |∆n max | along k x over along k y extracted from the fits for all the different sets of parameters E ext and I sat (blue-filled circles). Those values are compared to the aspect ratios extracted from the images (Fig. 2(a)), defined as the ratio between the diameter of the outer ring along the k x -direction and along the perpendicular direction and averaged over the 10 images having the highest intensity (red-filled diamonds). We observe a very good agreement between the two methods used to estimate the anisotropy of the nonlinear crystal, and the value of 1.6 is coherent with the literature [17]. Moreover, our simulations indicate a slightly higher anisotropy of 2.0. This measurement suggests that one can take benefit of the spatial self-phase modulation to access and characterise properly the anisotropy of a photorefractive crystal such as SBN. If we want to describe entirely the diffracted pattern in the far-field, the anisotropic description of the PR effect is required. However, if we are only interested in measuring the nonlinear refractive index of the medium along the c-axis (k x conjugated), the isotropic approximation is quantitatively pertinent.\n\nMentions:\n- We present on Fig. 5 the ratios of |∆n max | along k x over along k y extracted from the fits for all the different sets of parameters E ext and I sat (blue-filled circles).\n\nOCR Text: \"1 Er5 'r) 1.0 , t05 1.5 . 105 15 . \"d5 15 . 105 4mehcm-\" Z.0\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 5. Measurement of the anisotropy of SBN. Blue circles: ratio of the fitted |∆nmax| along the kx direction and the orthogonal direction, errorbars: uncertainty given by the fitting procedure. Blue solid line: mean value of the data, light-blue area: standard deviation of the data. Red diamonds: aspect ratios extracted directly from the images, averaged over the 10 images having the highest intensity. Error bars correspond to the standard deviation."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where the dynamic pressure amplitude P 1 and its phase φ are directly calculated from the (amplitude, phase) outputs of the LIA or of the FFT analyzer. In order to exploit the amplification factor of the acoustic modes, all the measurements have been performed in correspondence of the first cavity radial modes, having resonance frequencies ν n0 . The acoustic modes of  the cavity are probed exciting sound waves either using a loudspeaker external to the sphere or through electric polarization, by placing an oscillating high voltage on the inner electrode. The frequency of the resonant modes is determined maximizing the output of the LIA and monitored through its phase. The associated quality factor Q is obtained from the decay time of the LIA output when the exciting source is switched off (see Fig. 6). By scanning the exciting frequency, the lineshape of the resonance mode can also be traced directly from the output of the LIA or of the FFT (Fig. 7). The quality factor of a mode depends on pressure, increasing with increas-   ing pressure (Fig. 8). The measured frequency spectrum of the cavity (Fig. 9) shows an excellent agreement with the calculated spectrum. Frequency shifts due to finite admittance of the cavity walls or to geometrical imperfections [38] are beyond the experimental sensitivity and the scope of the present work. The frequencies of the acoustic modes are seen to depend on the position of the central electrode. In Fig. 10 we have an example of this behavior: the resonance frequency shows a maximum which corresponds to a minimum in the eccentricity of the electrode [39].\n\nMentions:\n- In Fig. 10 we have an example of this behavior: the resonance frequency shows a maximum which corresponds to a minimum in the eccentricity of the electrode [39].\n\nOCR Text: \"Inner electrode positou (arb , olfseti\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 10. Resonance frequency of the (1, 0) mode of the r1 = 4 cm cavity filled with 1 atm of N2 as a function of the vertical position of the central electrode. Following Ref. [39] the data are fitted with a parabolic curve."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- After narrowing down the possible solutions from ∼120 self-consistent CMD realizations to 5 solutions, one for each age, we compare these 5 solutions to the 7 average CMD solutions for the other ages in the grid. The diagnostics for these final 12 solutions are shown in Figure 22. We find that the case for an age of ∼1 Gyr is a little stronger than it was from Figure 18, due to an improvement in the σ Fe , and that the other diagnostics are more closely correlated with this improvement. However, while the 1 to 1.25 Gyr solutions are the most stable overall, the differences between the solutions with ages between 1 to 2.5 Gyr are still small. Therefore, our final age constraint for NGC 1718 is 1 to 2.5 Gyr. This is a narrower range in age than we are generally able to constrain for old clusters, but because NGC 1718 is young, the stellar population changes considerably over this 2.5 Gyr timescale. Accordingly, the [Fe/H] solution is less constrained than for older clusters in the training set, although the solutions for the older clusters spanned a larger range in age. Explicitly, the 1.0 Gyr solution has [Fe/H]=−0.39, and the 2.5 Gyr solution has [Fe/H]=−0.89. To convey this range of possible solutions in age and corresponding abundance, we average the two [Fe/H] results and quote an uncertainty due to the age solution, σ age , of ±0.25 dex.\n\nMentions:\n- The diagnostics for these final 12 solutions are shown in Figure 22.\n\nOCR Text: \"NSC Ew Iscchrone Ace (Syrs}\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 22. Same as Figure 18, except the 1.25-2.5 Gyr solutions have been replaced by the solutions for the best-fitting CMD realization at these ages. The best solutions are found for ages of 1−2.5 Gyr."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Being in a low energy state suggests that the wavefunction is strongly limited in the motion it can exhibit. If we have a cloud in a spherical harmonic trap and make small quasi-static deformations of it through a one-parameter set of quadrupolar deformations, we can obtain a set of wavefunctions ΨpX; αq where α gives the deformation. A deformation that is then released in the spherical trap can then evolve over this set to a good approximation so long as E j ! E s for all time. This gives a solution ΨpX; αptqq. The many body Ψ can be described in terms of a limited set of variables like the one-body density ρpxq, a one-coordinate projection ψpxq \" ψpx 1 q \" ΨppXq| x2\"c2...x N \"c N q, the best fit product expansion as in Eqn. 15, or even the single parameter α. Any equation of motion in terms of these variables must eventually fail since the Ψ has current terms and so is not going to stay well describable by such a limited class of function forever. Since the Schrödinger equation can be written as a hydrodynamic equation with a quantum pressure term it is not surprising that we can get a linearized Euler equation for the dynamics in terms of ψpxq induced ρpxq, vpxq valid for some finite time. This is expressed in Fig. 1. The extent to which such a situation holds for more general gases the subject of the following sections.\n- Another approach to such abbreviated \"order parameters\" to describe Ψ is to look at it along a particular subslice. For bosons we could use the value of Ψ near the many body diagonal x 1 \" x 2 \" . . . x N \" x. The function must near vanish there due to interactions but we can look at an displacement from it ψ d pxq \" Ψpx` , . . . x` q. This is also possible for a fermionic wavefunction ΨpX, tq U H pt,t 1 q P / / ρpx, tq, vpx, tq Euler Eqns. ΨpX, t 1 qP / / ρpx, t 1 q, vpx, t 1 q Figure 1: For a near ground state of an interacting Bose gas the kinds of adiabatic deformations and release in a trap and the stiffness of such a wavefunction implies near commutativity of the diagram. In contrast, a noninteracting gas makes this exact since the induced many-body currents cannot transfer energy to its transverse directions.\n- Discussions of the GP equation often utilize that one way to think of it is as the product of many one-body wavefunctions (with some Jastrow-like correction that is not rapidly changing as the function evolves). If this reasoning is also valid for the wavefunction of a gas in the range of higher temperatures then we would instead have to consider the commutation of actions as in Fig. 1 where we assume that dynamics enforce a coarse grained view of\n\nMentions:\n- This is expressed in Fig. 1.\n- This is also possible for a fermionic wavefunction ΨpX, tq U H pt,t 1 q P / / ρpx, tq, vpx, tq Euler Eqns. ΨpX, t 1 qP / / ρpx, t 1 q, vpx, t 1 q Figure 1: For a near ground state of an interacting Bose gas the kinds of adiabatic deformations and release in a trap and the stiffness of such a wavefunction implies near commutativity of the diagram.\n- If this reasoning is also valid for the wavefunction of a gas in the range of higher temperatures then we would instead have to consider the commutation of actions as in Fig. 1 where we assume that dynamics enforce a coarse grained view of\n\nOCR Text: \"VK,0)_ Pls,t) &s,) Uxu_ Euler Equ3 WKO)_ 0,81, es,81\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: For a near ground state of an interacting Bose gas the kinds of adiabatic deformations and release in a trap and the stiffness of such a wavefunction implies near commutativity of the diagram. In contrast, a noninteracting gas makes this exact since the induced many-body currents cannot transfer energy to its transverse directions."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To demonstrate the effectiveness of virtual vehicle parameterization, a simulation of two quadrotors flying pass each other is presented. The trajectories of two quadrotors are illustrated in Fig. 5.\n\nMentions:\n- The trajectories of two quadrotors are illustrated in Fig. 5.\n\nOCR Text: \"Aean\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 5: Trajectories of two quadrotors flying pass each other plotted in X-Y plane. Control efforts for performing this task are illustrated in Fig. 7."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Training data window To decide on how far back in time we need to look for data to train a good model we conduct an experiment where samples of an equivalent size are taken from each of the previous N days, for increasing values of N (Fig. 7). We observe that the performance of the model is initially increasing as we add more days into the training window, but slows down for N between [3..7] days and the performance even drops as we keep increasing the size of the window further. We attribute this observation  Model staleness To decide how frequently to retrain the model in production we measure its staleness in an experiment (Fig. 8). We consider a six day long period with two variants: when the model is trained once before the start of the experiment and when the model is retrained every single day. The results suggest, that the one-off trained model quickly stales by an average of 0.3% in AUC with every day of the experiment. The model retrained every single day, although also affected by daily fluctuations, outperforms the one-off trained model. This result motivates our decision to retrain the model every day. share of total rules, % dropped added Fig. 9: Model stability, daily changes of (origin, destination, airline) rules inferred from the random forest model.\n\nMentions:\n- We attribute this observation  Model staleness To decide how frequently to retrain the model in production we measure its staleness in an experiment (Fig. 8).\n\nOCR Text: \"UDJL Oncc r-train cvcC' dmy 044 1 dar ` alict (ramning\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 8: Model staleness of the one-off trained model vs. the model retrained every day."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- -The energy resolution of the recoil detector should be better than 30 keV due to the fine granularity of detectors along the beam axis. -The recoil detector should be able to measure the recoil protons with energy down to about 600 keV, which corresponds to |t|=0.001 (GeV/c) 2 . Considering all these requirements, the recoil detector assembly has been designed as presented in Fig. 3. With a movable plate the detector can cover various ranges of recoil angle as required for the beam momenta being investigated. The kinetic energy of recoil protons as a function of the recoil angle at beam momenta P=1.5 and 15 GeV/c is plotted in Fig. 4. In order to detect the recoil protons with kinetic energy in the range of 0.6-60 MeV, corresponding to |t| = 0.001-0.1 (GeV/c) 2 , 2 silicon strip detectors with 1 mm thickness and 1.2 mm pitch and 2 germanium strip detectors with 5 and 11 mm thickness and 1.2 mm pitch have been proposed. Thus, not only the energy but also the relevant angular information of the recoil protons will be obtained. As shown in Fig. 5, the silicon and germanium detectors are arranged in two overlapping rows. It is noted that the different size of the overlapping areas between the two rows is because of the limited thickness of the silicon detectors. The 1 mm thick silicon detectors will measure the recoil protons with energy up to 12 MeV. The protons with higher energy up to 60 MeV will be measured by 5 and 11 mm thick germanium detectors. Those detectors will be positioned in a dedicated vacuum chamber and can be placed at a distance of 70 cm to 120 cm from the interaction point, depending upon the momentum of the HESR beam.\n\nMentions:\n- The kinetic energy of recoil protons as a function of the recoil angle at beam momenta P=1.5 and 15 GeV/c is plotted in Fig. 4.\n\nOCR Text: \"P-15G-Vlc P =15 Gevic Recoil anple (Degreel\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4. The kinetic energy of the recoil protons as a function of the recoil angle at beam momenta P=1.5 and 15 GeV/c, blue and red, respectively."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 8 shows skewness profiles over the smooth wall and porous foams. Consistent with previous measurements at comparable Reynolds number (e.g., Mathis et al. 2011), the skewness over the smooth wall is negative or close to zero across much of the boundary layer (0.004 &lt; y/δ &lt; 1). In contrast, over the foam substrates, the sign of the skewness is positive until y/δ ≈ 0.1. Interestingly, the location of this change in sign for the skewness corresponds to the location of the outer peak in streamwise intensity profiles. Further, the magnitude of the skewness generally increases with pore size, which is consistent with the intensity measurements (the 10 and 20 ppi cases again show non-monotonic behaviour). These observations are particularly interesting given the intrinsic link between skewness and amplitude modulation (Schlatter &amp;Örlü 2010;Mathis et al. 2011), and suggest that structures responsible for the outer peak in streamwise intensity over the foams may have a modulating effect on the interfacial turbulence. This possibility is discussed in greater detail in §4.1 below.\n- As noted earlier, the skewness profiles shown in figure 8 strongly suggest that the amplitude modulation phenomenon that has received significant attention in recent smooth and rough wall literature (e.g., Mathis et al. 2009;Marusic et al. 2010;Mathis et al. 2013;Pathikonda &amp; Christensen 2017) may also be prevalent in turbulent flows over permeable walls.\n- Subsequent studies have shown rigorously that the correlation coefficient R in (4.1) is intrinsically linked to the skewness of the velocity (Schlatter &amp;Örlü 2010;Mathis et al. 2011;Duvvuri &amp; McKeon 2015), whereby positive correlations between scales (R &gt; 0) translate into increased skewness. Thus, the increase in skewness observed in figure 8 near the porous interface suggests that R &gt; 0 in this region.\n\nMentions:\n- Figure 8 shows skewness profiles over the smooth wall and porous foams.\n- As noted earlier, the skewness profiles shown in figure 8 strongly suggest that the amplitude modulation phenomenon that has received significant attention in recent smooth and rough wall literature (e.g., Mathis et al. 2009;Marusic et al. 2010;Mathis et al. 2013;Pathikonda &amp; Christensen 2017) may also be prevalent in turbulent flows over permeable walls.\n- Thus, the increase in skewness observed in figure 8 near the porous interface suggests that R &gt; 0 in this region.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 8: Wall-normal profiles of skewness (Sk) for the smooth wall and for all the porous foams at x/h = 42."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 1 The result of test-particle calculation for a parallel propagating wave with ω = 5Ω ci = 2.72 × 10 −3 Ωce, k = kx = 2.044Ω ci /V A , and |Bw|/B 0 = 10 −4 . The variance of µ, the cosine of the pitch angle, is plotted against the average velocity Vex , where the horizontal axis is normalized by the Alfvén velocity V A . The peak at Vex /V A at ∼ −892.2 shows the effect of the fundamental cyclotron resonance condition.\n- In Figure 1 δµδµ is plotted against Vex , where we see a peak of δµδµ at Vex = −892.2V A ≡ V R , where V R is the resonance velocity satisfying\n- Fig. 2 The same as Figure 1 except that it is for a oblique propagating wave with ω = 5Ω ci = 2.72 × 10 −3 Ωce, k = 2.417Ω ci /V A , kx = 1.709Ω ci /V A , θ = 45 o , and |Bw|/B 0 = 10 −4 . Integers given above the peaks (−5, −4, ..., , 5) are harmonic numbers in the resonance condition, ω − kxV R = nΩce. Half integers given below the curve (−5/2, −3/2, ..., 5/2) are for the subharmonic resonance (see text).\n\nMentions:\n- Fig. 1 The result of test-particle calculation for a parallel propagating wave with ω = 5Ω ci = 2.72 × 10 −3 Ωce, k = kx = 2.044Ω ci /V A , and |Bw|/B 0 = 10 −4 .\n- In Figure 1 δµδµ is plotted against Vex , where we see a peak of δµδµ at Vex = −892.2V A ≡ V R , where V R is the resonance velocity satisfying\n- Fig. 2 The same as Figure 1 except that it is for a oblique propagating wave with ω = 5Ω ci = 2.72 × 10 −3 Ωce, k = 2.417Ω ci /V A , kx = 1.709Ω ci /V A , θ = 45 o , and |Bw|/B 0 = 10 −4 .\n- In Figure 1 there is a dip at Vex = V R , which is the manifestation of the trapping effect (Appendix B).\n- Again these dips are manifestation of the trapping effect already seen in Figure 1.\n\nOCR Text: \"{ouc N) 4.0. J605 <>\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1 The result of test-particle calculation for a parallel propagating wave with ω = 5Ωci = 2.72 × 10−3Ωce, k = kx = 2.044Ωci/VA, and |Bw|/B0 = 10 −4. The variance of µ, the cosine of the pitch angle, is plotted against the average velocity 〈Vex〉, where the horizontal axis is normalized by the Alfvén velocity VA. The peak at 〈Vex〉/VA at ∼ −892.2 shows the effect of the fundamental cyclotron resonance condition."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The recoil mass distributions for the SM background and signal for various values of m φ and κ = 1 are shown in Fig. 6 after applying all cuts except the m reco requirement. The plot on the right (left) shows the distributions with (without) an ECAL. Note that we only consider scalar masses down to 30 GeV, as going below this requires looser cuts than those assumed above. The signal peak is wider at lower masses, corresponding to the decreased p T resolution for more boosted muons, reflected in Eq. (4). In the tracker-only case, the background is significantly larger below m W , due to the 2µ + ISR contribution. This degrades the sensitivity to scalars with m φ m W , as shown in Fig. 7, which illustrates the corresponding discovery potential in the Z(→ µ + µ − )φ recoil search with and without calorimetry. Also shown for reference are the current limits from the analogous decay modeindependent search at OPAL [28]. Although the sensitivity to light scalars below ∼ m W is reduced without a photon veto, the alternative photon ID method discussed in Sec. 3.3 could be used to improve the tracker-only reach so that it is comparable to that afforded by calorimetry. Therefore, we conclude that tracking detectors at a Higgs factory could still be capable of providing excellent sensitivity to new light scalars beyond the Standard Model coupling to the Z.\n\nMentions:\n- This degrades the sensitivity to scalars with m φ m W , as shown in Fig. 7, which illustrates the corresponding discovery potential in the Z(→ µ + µ − )φ recoil search with and without calorimetry.\n\nOCR Text: \"250 Cel\" Raldu-O Lrackt Trmckt ECAL (GeV\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 7: Discovery reach for new light scalars in a Z(→ µ+µ−)φ recoil search at a future Higgs factory with 200 fb−1 and 5 ab−1 of integrated luminosity. The dashed (solid) curves show the sensitivity to the effective φ−Z coupling with (without) calorimetry. In the trackeronly case, the reach is degraded at low masses due to the significant 2µ + ISR background, which can be eliminated by a photon veto or the alternative photon ID method described in Sec. 3.3. Also shown for reference are the analogous decay mode–independent bounds from OPAL [28]."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Note that unpacking can be valuable even without further inflation. For instance, unpacking the instrumental scenario of Fig. 4 leads to an associated correlation scenario which is trivial, such as depicted in Fig. 12. Any distribution over the four variables {A X=0 , A X=1 , B A=0 , B A=1 } is compatible with Fig. 12. Nevertheless, demanding that the distributions P (A, B|X) admit such an unpacking leads to nontrivial constraints. We can formulate the admission of an unpacked distribution as a linear program, via Eq. (39). Explicitly formulating this linear program for the instrumental scenario looks like P original (A=a, B=b|X=x) = P unpacked (A x =a, B a =b),\n\nMentions:\n- For instance, unpacking the instrumental scenario of Fig. 4 leads to an associated correlation scenario which is trivial, such as depicted in Fig. 12.\n- Any distribution over the four variables {A X=0 , A X=1 , B A=0 , B A=1 } is compatible with Fig. 12.\n\nOCR Text: \"4X-0 BA-v AI-I BA-1\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 12. The unpacking of the instrumental scenario."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- FIG. 7: The ωD − ω ′ D diagram for non-interacting NTADE in HL cosmology. Here, we have taken ΩD(z = 0) = 0.73, H(z = 0) = 67, Ω k (z = 0) = 0.01, λ = 1.6 and B = 2.4\n\nMentions:\n- FIG. 7: The ωD − ω ′ D diagram for non-interacting NTADE in HL cosmology.\n\nOCR Text: \"62 6-3 64 J   48 J W 0 4 W5\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 7: The ωD − ω ′ D diagram for non-interacting NTADE in HL cosmology. Here, we have taken ΩD(z = 0) = 0.73, H(z = 0) = 67, Ωk(z = 0) = 0.01, λ = 1.6 and B = 2.4"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To see this clearly, in Fig. 3, we plot the average displacement q(t) /σ of the mirror versus the time ωmt with k = 0.005 for different angles α [α = 0.996 × π/2 (red-solid), 0.9995 × π/2 (orange-dashed) and π/2 (blue-dotted-dashed)]. It can be seen clearly from Fig. 2 that when α = 0.996 × π/2 the amplification (red-solid) is a time function of period 2π and at time ωmt = (2n + 1)π (n = 0, 1, 2, • • • ) the amplifications can reach strong coupling limits (the level of the ground state fluctuation) q = −σ [26]. However, when α → π/2, such as α = 0.9995 × π/2, the amplification around time ωmt = 2nπ (n = 1, 2, • • • ) appears, but the rest of the amplifications (orange-dashed) is decreasing. Until α = π/2 which indicates that the quantum beam splitter corresponds to a beam splitter, the amplification (blue-dotted-dashed) around time ωmt = 2nπ (n = 1, 2, • • • ) is very prominent and reaches the level of the ground state fluctuation q = ±σ. The result is caused by the kerr phase which can't be explained by the standard weak measurement [1,19] and has been discussed in detail in Ref. [9]. Note that in the optomechanical cavity A (see Fig. 2) the maximal displacement of the mirror caused by one photon is 4kσ and the displacement −σ can be obtained using quantum beam splitter in weak measurement, hence the amplification factor can be Q = 1/4k which is −50 when k = 0.005. Moreover, the negative amplification is a counterintuitive result since intuitively there cannot exist negative displacement relative to the direction of the photon propagation.\n- To explain the amplification phenomenon shown in Fig. 3, we only give an analysis about the special case that the maximal amplification appearing at the time T = (2n + 1)π (n = . Fig. 4. p(t) /h(2σ) −1 as a function of ωmt with k = 0.005 for different α, i.e., α = 0.996 × π/2 (red), 0.999 × π/2 (green) and π/2 (blue). 0, 1, 2, • • • ). In the same way as the Eq. ( 12), for Eq. ( 23) we can perform a small quantity expansion about time T till the second order. Suppose that |ωmt − T | ≪ 1, and k ≪ 1, then\n- It can be seen clearly that if the decay rate of the cavity which is considered in [11] is small, i.e., ωm = 4κ (red line), the photon arrival rate is almost distributed from 0 to 2π. Combined with the amplifications from Fig. 3 within time ωmt = 2π, the implementation of our scheme is feasible in experiment.\n\nMentions:\n- To see this clearly, in Fig. 3, we plot the average displacement q(t) /σ of the mirror versus the time ωmt with k = 0.005 for different angles α [α = 0.996 × π/2 (red-solid), 0.9995 × π/2 (orange-dashed) and π/2 (blue-dotted-dashed)].\n- To explain the amplification phenomenon shown in Fig. 3, we only give an analysis about the special case that the maximal amplification appearing at the time T = (2n + 1)π (n = .\n- Combined with the amplifications from Fig. 3 within time ωmt = 2π, the implementation of our scheme is feasible in experiment.\n\nOCR Text: \"3 ~IOL Ont\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3. 〈q(t)〉/σ as a function of ωmt with k = 0.005 for different α, i.e, α = 0.996 × π/2 (red-solid), 0.9995 × π/2 (orangedashed) and π/2 (blue-dotted-dashed)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In this section we describe a hierarchical model for fitting noisy photometry and estimating galaxy redshifts. The model is schematically summarized in Fig. 1 and has three main components: the spectral energy distributions, their prior distributions, and a noise recalibration module. Those are characterized by sets of hyperparameters denoted by α, β, and γ, respectively. We will apply this model to a set of objects with known redshifts and solve for the hyperparameters by maximizing the full posterior distribution. Note that our modelling choices are relatively simple, partly because this paper is a demonstration of the approach, and these models are sufficient for the data we consider. In what follows, we describe the various components of the model, then write the full posterior distribution and discuss the optimization procedure.\n\nMentions:\n- The model is schematically summarized in Fig. 1 and has three main components: the spectral energy distributions, their prior distributions, and a noise recalibration module.\n\nOCR Text: \"Kerisruiin Air;f SHI: Eena #hLR Ca TE {xSHZ M Kon; Xk EOMEc Prue i CIa -e Hrc \"ncCF} #\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1. Graphical representation of our hierarchical probabilistic model. Plates indicate independent, identically distributed variables: the observed galaxies and the model SED templates. Squares indicate fixed parameters or model components: the analytic form of the priors and the noise recalibration, as well as the linear SED bases used in the templates, their corrections and variances. Circles or round boxes refer to random variables. We optimize for the hyperparameters of the model, α, β, and γ, and marginalize over the intrinsic true properties of the objects (true photometry, redshift, type, luminosity). The shaded rounded box refer to observed random variables, i.e., the measured fluxes and their noise. Standard template fitting typically only include the left part of the model, without SED or photometric corrections, or with simple corrections not optimized simultaneously with the priors and other parameters of the model. The novelty of our approach lies in the greater flexibility of the model, its causal structure, and the joint optimization of all the parameters for obtaining photometric redshifts."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In the toy examples, the data were simulated from y i = sinc(x i − 6) + ε i , ε i ∼ N(0, 0.1) (see Fig. 1 for a visualization). The same function with thresholding was used in the classification examples in the Appendix. Table 1 shows comparisons for different log-concave likelihoods over a simulated data set with n = 1000. Example functions can be seen in Fig. 1 and Appendix E. The results are shown for a Matérn (ν = 3 /2) with a full GP (naïve handling of latent, full EP as in [24]), state space (SS, exact state space model, ADF as in [22]), and IHGP. With m only 2, IHGP is not faster than SS, but approximation errors remain small. Fig. 3 shows experimental results for the computational benefits in a regression study, with state dimensionality m = 2, . . . , 100. Experiments run in Mathworks MATLAB (R2017b) on an Apple MacBook Pro (2.3 GHz Intel Core i5, 16 Gb RAM). Both methods have linear time complexity in the number of data points, so the number of data points is fixed to n = 10,000. The GP prior is set up as an increasing-length sum of Matérn (ν = 3 /2) kernels with different characteristic length-scales. The state space scheme follows O(m 3 ) and IHGP is O(m 2 ).\n\nMentions:\n- Fig. 3 shows experimental results for the computational benefits in a regression study, with state dimensionality m = 2, . . . , 100.\n\nOCR Text: \"Sualz' Spacc\" IHCP Suate diuensionality ma\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Empirical running time comparison for GP regression on n = 10,000 data points. Maximum RMSE in IHGP E[f(t∗)] < 0.001."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In principle, the use of the MS template could cause only an underestimate of the extrapolated LIR from 24 µm fluxes, in particular at high redshift or for off-sequence sources (L 24 IR &gt; 10 11.7 L ). This is due to the relatively higher PAHs emission of the MS template (see Elbaz et al. 2011 for more details). However, the LIR estimated with the best-fitting templates based on PACS and 24 µm data agrees very well with the LIR extrapolated from the 24 µm flux (L 24 IR ) using the MS template. Fig. 3 shows such a comparison for the GOODS fields where we have the deepest PACS coverage. In larger fields such as COSMOS and ECDFS there is a larger probability to find rare strong star-forming off-sequence galaxies even at low redshift. However those sources should be detected by the Herschel observations given their very high luminosity, above the PACS detection threshold. Thus, even for these rare cases we correctly estimate the LIR. Our estimated luminosities are also consistent with those computed using an alternative set of templates from Rodighiero et al. (2010).\n\nMentions:\n- Fig. 3 shows such a comparison for the GOODS fields where we have the deepest PACS coverage.\n\nOCR Text: \"Icg,0l-R?Aum}\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3. Comparison between the extrapolation of LtotIR from PACS (100 and 160µm) versus that from 24µm for the GOODS fields as a function of redshift (colour bar). The dashed line represents a one to one relation while the black stars with error bars represent the median infrared luminosity based on Herschel plus 24 µm fluxes for each bin of L24IR. The luminosities are expressed in solar units."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In light of the discussion in sections III A 1 and III A 2, as well as our earlier work [75], it is clear that Se-atoms play a significant role in transport behavior of these glasses. We observe that the states in the band edges are derived mainly from Se p-orbitals as shown from the projected density of states in Fig 7 . Se atoms exhibit a wide range of local environments, bonding with Ge to form host network, bonding with Ag atoms to form a Ag 2 Se phase, and attaching to other Se atoms to form Se-filaments. These bonding environments lead to different electronic activity around the Fermi level. To discuss this in familiar terms, we use the terminology commonly used to describe O atoms in common network-former/ network-modifier glasses (e.g. NaSiO 3 ). We categorize Se-atoms bonded with 2 Ge atoms as 'bridging' Se atoms. Similarly, Se-atoms bonded to one or no Ge-atom are referred here as 'non-bridging' Se-atoms. The later group of Se-atoms either terminates the GeSe network (one-fold Se) or is more or less mobile and form AgSe phase/short Se-chains. With this classification in mind, it is interesting to observe that the non-bridging Se atoms are highly electronically active around Fermi-energy and are transport significant. We make this point in Fig. 8 where known observation that states deeper into the band are more extended as can be noted from the higher number of atom projections used to amass top 50% of the charge for those states. We also note the asymmetrical localization on the two sides of Fermi level. Conduction edge states appear to be more localized than the valence edge states. The anomalous origin of LUMO+1 and LUMO+2 comes from two strained tetrahedral Ge sites.\n- The story of Cu in alumina is different than that of Ag in GeSe 3 . The Cu atoms segregate and form clusters in alumina, whereas Ag atoms organize themselves more or less uniformly in GeSe 3 . The highly ionic character of the alumina host (contrasted with the relatively covalent selenide) is the origin of the difference. Indeed, the tendency to cluster is associated with the host rather than the transition metal, since we have shown separately that Ag clusters in the alumina host much like Cu does. As Cu is introduced into the alumina host, levels appear in the alumina gap. Quite interestingly, the gap fills up rather uniformly as Cu concentration increases, one does not see an \"impurity band\" form, as one might suppose from experience with doped semiconductors. Because the Cu clusters, Cu regions eventually become space filling (with periodic boundary conditions) and these connected filaments become pathways for metallic conduction. The models with 20% and 30% Cu are conducting. Here, we focus our study on the 30% Cu model. Left plot in Fig. 9 shows the total density of states and IPR. The states near the Fermi level are fairly extended, and there is a continuous filling of states in the fundamental electronic (optical) gap of the oxide host. The right plot in Fig. 9 shows the density of states projected on atomic sites for this model which reveals that the states near Fermi level are mostly derived from Cu atoms. This case is quite different from the Ag in GeSe 3 systems, where most of the contribution to the total density of states come from Se atoms as shown in Fig. 7.\n\nMentions:\n- We observe that the states in the band edges are derived mainly from Se p-orbitals as shown from the projected density of states in Fig 7 .\n- This case is quite different from the Ag in GeSe 3 systems, where most of the contribution to the total density of states come from Se atoms as shown in Fig. 7.\n\nOCR Text: \"Total 300 Ge projected Se-projected Ag-projccted Energy [eV]\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 7. Projected density of states (DOS) on atomic sites for (GeSe3)0.85Ag0.15 models The Fermi level is shifted to zero."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The JSMA attack shows that modifying one bit in the feature vector can bypass the real-world ML-based malware detector. The attack can be done by modifying the malware source code by adding one API call. We show some preliminary results on building attack resiliency to API additions for mitigation. It is an open challenge to design a defense against a powerful adaptive attack. For future work, we are building the real-world black-box testing framework as proposed in Figure 2 using open source data with different features and models. We will study the intepretability of adversarial examples to develop more effective defenses.\n\nMentions:\n- For future work, we are building the real-world black-box testing framework as proposed in Figure 2 using open source data with different features and models.\n\nOCR Text: \"4eeo Urane  Ineen Aadtt Iroana Malware Sample Substitute Model ` Quldy r c\" Target Model Adversarial Example IMen MAalsere Suruc)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2. A framework for grey-box and black-box attacks in real-world setting"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Les quelques irrégularités observées sur les courbes des figure 41 et 43 semblent confirmer l'hypothèse avancée lors de l'étude de la courbure du pli : la ligne brisée considérée doit «sauter» d'un filament à l'autre, deux filaments côte à côte n'ayant pas précisément la même courbure. Filament plié À t = 2 (i.e. sur le filament plié T 2 (x 1 ), . . . T 2 (x 1000 )), les deux plus grandes valeurs propres de la différentielle de T 2 sont représentées figure 43. Elle est légèrement supérieure à 2 au voisinage de T 2 (x 690 ), mais il n'y pas d'expansion tout au long du filament. Celle-ci est en effet de module inférieur à 1 entre T 2 (x 1 ) et T 2 (x 444 ) où elle traverse le cercle unité, puis croît lorsqu'on se déplace le long du filament plié vers x 1000 où elle dépasse 4. Il n'y a donc pas uniforme hyperbolicité sur l'attracteur.\n\nMentions:\n- Les quelques irrégularités observées sur les courbes des figure 41 et 43 semblent confirmer l'hypothèse avancée lors de l'étude de la courbure du pli : la ligne brisée considérée doit «sauter» d'un filament à l'autre, deux filaments côte à côte n'ayant pas précisément la même courbure.\n\nOCR Text: \"Eamk #IkI KhITZ\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 41 – Différentielle de T 2 sur le filament non-plié : première valeur propre."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where R c is the distance of the galaxy's center from the CNO center, and R 1 is the distance from the CNO center to the rotational arm of the galaxy. The notation | R C means the centripetal speed is relative to the galaxy's center. The M33 data requires this to be   is the CNO enclosed mass at radial coordinate R. It can only be computed numerically [1] by solving the hydro-static equation of equilibrium using the neutrino equation of state. To work out &lt; v 2 &gt;, we use Figure 2.\n\nMentions:\n- To work out &lt; v 2 &gt;, we use Figure 2.\n\nOCR Text: \"15 kpc 740.423 kpc 740.575 kpc\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: Working out the M33 rotation speed at 15 kpc."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In this section, simulations are performed in order to visualize and validate the theoretical findings. Observations are assumed to be real valued. The formulations are general, therefore, the observation space can be any discrete, continuous, finite or infinite set, with slight modifications for the discrete case. It can also be extended to the multidimensional case, but for large n, Monte-Carlo simulations may be required in order to solve the non-linear equations, c.f., [10]. In the first simulation, the composite uncertainty model (39) with mean and variance shifted nominal distributions, F 0 ∼ N (−1, 1) and F 1 ∼ N (1, 2), and the uncertainty parameters {ε 0 = 0.15, ε 1 = 0.05, 0 = 0.02, 1 = 0.02} is considered. Note that for this choice of nominal distributions, neither ∂F 1 /∂F 0 is monotone nor they are symmetric with respect to any point on their domain or codomain. In addition to this, ε 0 = ε 1 is chosen so that the given example is general enough for the solution of Equations ( 16) and (17). Regarding the −contamination part of the composite model, 0 = 1 is chosen to be consistent with ρ = 1 for the uncertainty model based on relative entropy. Accordingly, in Fig. 1 the LFDs together with their nominal distributions are shown, whereas in Fig. 2 the log-likelihood ratios of the nominal distributions, the least favorable densities (ĝ 0 ,ĝ 1 ) when {ε 0 = 0.15, ε 1 = 0.05, 0 = 1 = 0} and the least favorable densities (q 0 ,q 1 ) when {ε 0 = 0.15, ε 1 = 0.05, 0 = 0.02, 1 = 0.02} are shown. In the second simulation, the mean shifted Gaussian distributions F 0 ∼ N (−1, 1) and F 1 ∼ N (1, 1) are considered when the closed balls are formed with respect to the symmetrized χ 2 distance with (ε 0 = ε 1 = 0.08) and a relative entropy distance D with (ε 0 = ε 1 ≈ 0.0087). The parameters are  chosen such, such that the LFDs resulting from both distances have equal relative entropy relative to the nominal density functions. Figure 3 illustratesl/l, the ratio of the likelihood ratios. It can be seen that there is a significant difference when the χ 2 distance is considered instead of the KL-divergence. While this ratio tends to 1 asδ → 0 andδ → 1 for the symmetrized χ 2 distance, meaning that the tails of the density functions are preserved, it is a constant l l &lt; 1 whenδ = 0 and another constant l u &gt; 1 whenδ = 1 for the KL-divergence.\n\nMentions:\n- Figure 3 illustratesl/l, the ratio of the likelihood ratios.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3. The ratio of the likelihood ratio of the least favorable densities l̂ = ĝ1/ĝ0 to the likelihood ratio of the nominal distributions l = f1/f0 for F0 ∼ N (−1, 1) and F1 ∼ N (1, 1), when the LFDs are based on the symmetrized χ2 distance with ε = ε0 = ε1 ≈ 0.08 and when the LFDs are based on the KL-divergence with ε ≈ 0.0087."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In conclusion, we have observed a doubling of the ionization energy of a single As donor because of dielectric confinement near an oxide interface. This measurement on a single isolated dopant requires extremely small samples, a condition achieved in ultra-scaled microelectronics devices. This shift in ionization energy is responsible for the very large off-state current up to room temperature, and is therefore a major source of variability for the off-state current in ultra scaled CMOS devices. Since dopants closer to the gate are much less affected by this shift, gate-all-around structures are the most suitable choice to avoid unacceptable sample to sample variations. On the other hand the next step for taking advantage of this effect is to manipulate single donor electronic orbitals to implement new quantum functionalities in silicon CMOS devices. FIG. 6: figure S1: Room temperature characteristics (drain-source current versus gate voltage) for a set of nominally identical samples from the same wafer. Sample to sample variations are very important, especially below the threshold voltage (-0.5 V for the best device, in red). In the most extreme case (blue curve) the many decades of excess current compared to the best device are attributed to direct transport through individual arsenic dopants which have diffused into the channel after doping and annealing steps. The source-drain voltage is 10 mV for these data. FIG. 7: figure S2: Sketch for negative or positive differential conductance lines arising from the spectroscopy of the source and drain local density of states (LDOS) probed by a single dopant level. A single dopant level (red line) has a slightly unbalanced tunneling rate to source and drain: the barrier to drain is less transmissive at zero drain bias, represented by a thicker barrier. The drain source differential conductance is dominated by the less transmissive barrier (red arrow, green arrow for the more transmissive ones) and proportional to the LDOS in the corresponding reservoir. At finite bias around point B the lines are due to LDOS fluctuations in the drain, because the drain barrier is less transmissive. On the opposite, at finite bias in A the drain barrier becomes more transmissive than the source barrier due to the electric field (Fowler-Nordheim tunneling): differential conductance lines are then proportional to the LDOS fluctuations in the source.\n\nMentions:\n- FIG. 6: figure S1: Room temperature characteristics (drain-source current versus gate voltage) for a set of nominally identical samples from the same wafer.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 6: figure S1: Room temperature characteristics (drain-source current versus gate voltage) for a set of nominally identical samples from the same wafer. Sample to sample variations are very important, especially below the threshold voltage (-0.5 V for the best device, in red). In the most extreme case (blue curve) the many decades of excess current compared to the best device are attributed to direct transport through individual arsenic dopants which have diffused into the channel after doping and annealing steps. The source-drain voltage is 10mV for these data."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Results are shown in Figure 3. The leftmost bars are the 30-day averages of MAPE for all time zones, and the others are the 30-day averages of MAPE for each time zone (1-3, 4-7, 8-11, 12-15, 16-19, 20-22 o'clock, respectively). Performance improvements are attained by the proposed methods in total score and almost all time zones. MAPEs of WB ( = 0.2) are large in all time zones. This is caused by its large entropic regularization term, which blurs the estimation result excessively. MAPEs of WB ( = 0.1) are small in time zones 1-3 and 12-15, but large in other time zones. This is caused by the difference in intensity of crowd movements in the targeted area. In time zones 1-3 and 12-15, the histogram shape does not change so much because people do not move around, but in other time zones the histograms change greatly because many people move around with commuting. WB ( = 0.1) does not seem to be able to deal with the drastic histogram changes. We tried to calculate WB also for = 0.01, but the algorithm does not converge because is too small. The proposed methods achieve small MAPEs in all time zones, regardless of the intensity in crowd movement.\n\nMentions:\n- Results are shown in Figure 3.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Interpolation error of each method on population histogram data from Tokyo and Kanagawa prefectures. We used MAPE as the evaluation metric for evaluating the difference between interpolated histogram and true histogram."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 2(c) shows the lateral optical field distributions of the produced PJ and PH in the points of I max . It is observed that the FWHM of PH (~0.29λ) is far below the diffraction limit and less than the FWHM of PJ (~0.35λ) as well. The lines connected by the specific points on the right end of each contour line, which are marked in green and red dotted lines in Figs. 2(a) and 2(b), clearly show the intensity distributions along a straight trajectory for PJ and a curve trajectory for PH. This situation can also be seen from the 2D intensity distributions in Fig. 2(d). All the points representing the green dotted line are on the x-axis, while the points on the red dotted line move away from the x-axis in the beginning and then slowly approach towards the x-axis. With regard to I max , the value for PH is ~10.2 which is a little less than the value of ~10.9 for the case of PJ. The reason is probably because some total internal reflection occurs near the output end of Janus microcylinder. When the light interacts with the interface between upper and lower half-cylinders, there is a small fraction of the light propagating from optically dense medium (n 2 = 1.46) into optically thinner medium (n 3 = 1.4) and thus causing total internal reflection. This little bit of light, for the focused light beam on the rear side of the Janus microcylinder, is equivalent to loss in the ways of optical reflection or scattering. As can be seen from the comparison of optical field distributions around the region of light output end in Figs. 2(a  According to classical electromagnetic theory and our previous research, the energy flow diagram denoted by the field lines of time-averaged Poynting vectors can be used to compare and analyze the formation mechanism of PJs and PHs [26,27]. As shown in Fig. 3(a), the Poynting vector fields (conical arrows), in the case of conventional microcylinder, are always symmetric relative to the axis of symmetry (black dotted line) in the whole computational domain. On the rear side of the microcylinder, the Poynting vector fields and energy flow lines within the auxiliary green-dotted-line converged into a focused beam of typical PJ. While in Fig. 3(b), the intensity of Poynting vector fields in the region near the emergent points and the configuration of energy flow streamlines on the back surface are respective higher and denser above the x-axis. The difference of dielectric properties between two half-cylinders resulted in the broken of microparticle's symmetry in material composition and eventually led to the formation of curved beam profile in the case of Janus microcylinder. For the 2D full-wave simulation model in this work, the electrical and magnetic vector flux is on the x-y plane. Referring to the practice in Ref. 7, we studied the variations of the bending angle (δ) with RI contrast (n3/n2) for illustrating the curvatures of the PHs. When the connecting line between the start point and the inflection point turned clockwise to the connecting line between the inflection point and the end point, the bending angle δ was defined as positive. Conversely, when the deflection was in the opposite direction, δ was defined as negative. The RI of lower half-cylinder was fixed as n2 = 1.46, the RIs of upper half-cylinder were changing from 0.91n2 to 1.05n 2 . As shown in Fig. 4, the bending angles δ first increase and then decrease as the ratio n3/n2 increases from 0.91 to 0.99. All the bending angles are greater than zero and the curved strong field areas are all located above the x-axis. The illustration shows δ = 15.18° in the case of n 3 = 0.95n 2 . The maximum bending angle is over 20 degrees around n 3 /n 2 ~ 0.93. If the ratio n 3 /n 2 increases to 1, the PH turns into PJ due to the Janus microcylinder recovering its symmetry of material compositions for two half-cylinders. When n3/n2 &gt; 1, the focused light beams began to bending below the x-axis and reforming PHs with negative δ. The inset in the right panel of Fig. 4 shows the bending angle δ equaling -3.73° at n 3 = 1.02n 2 . Obviously, researches in this area indicate that the degree and direction of deflections for PHs are connected with the RI contrasts between upper and lower half-cylinders of the investigated Janus microcylinder. The tunability of PH's bending characteristics is significant for moving objects along specific curved trajectories in the areas of optical trapping or manipulating nanoparticles, biomolecules, cells and viruses. Benefitting from the properties of infinite length in one direction and structural symmetry of Janus microcylinder, it is easy to rotate such microparticle around the central axis of symmetry. In this way, the incident plane-waves hit the boundary profiles of the half-cylinders of which involving two different dielectric materials with different component ratios, and form PHs of different shapes and curvatures. As shown in Fig. 5(a), the Janus microcylinder was rotated clockwise (red arrow) and counterclockwise (blue arrow) about the transmission axis, respectively. The material properties of Janus microcylinder, the surrounding medium and the illumination source are the same with Fig. 2. Here, the geometric center of the structure was defined as the origin, φ as the rotation angle where the value obtained by rotating clockwise is positive and negative in the opposite direction. In view of the axial symmetry in geometry of a Janus microcylinder, the rotation angle ranging from -90° to 90° was considered. The step size of rotation angle for each adjustment is 10°. As can be seen from Fig. 5(b), the maximum bending angle (δ = 14.6°) of the PHs happens at the rotation angle of φ =0°. With the increase φ, no matter in a clockwise or counterclockwise direction, the bending angles φ are both getting smaller. Figure 5(c-1) to 5(c-4) show the electric field distributions and the calculated bending angles for the following four cases: φ = -40°, -90°, 40° and 90°. The values of bending angles, which are also corresponding to the indicated four cases in Fig. 5(b), are δ = 11.87°, 0°, 9° and 0°. If we extend the parts of the PHs that are not shown, the bending angles δ can be circularly restructured back and forth within ±15°. If one half-cylinder of the Janus microcylinder is an elastic flexible material, the resultant PHs can be adjusted by another method of mechanical stretch or compression. Such tuning mode is discussed with numerical experiments in the following content. Figure 6(a) illustrates the schematic of the stretching and compression tuning. The polydimethylsiloxane (PDMS) was chosen as the stretchable half-cylinder because of its optical transparency and good viscoelasticity [28]. Solid silica was adopted as the other fixed half-cylinder. As shown in Fig. 6(b), the shape profiles were assumed to remain constant for silica half-cylinder (solid bluegrey line) and deform in a superellipse way for PDMS half-cylinder (solid green line) during the process of stretching or compression. The curves indicated by dashed blue-grey line and dashed green line are the other halves of the circle and superellipse, respectively. In the Cartesian coordinate system, the origin O is located at the geometric center of the Janus microcylinder, the radius of the circle is 2λ. The set of all points (x, y) on the superellipse satisfies the equation [29]:\n\nMentions:\n- As shown in Fig. 4, the bending angles δ first increase and then decrease as the ratio n3/n2 increases from 0.91 to 0.99.\n- The inset in the right panel of Fig. 4 shows the bending angle δ equaling -3.73° at n 3 = 1.02n 2 .\n\nOCR Text: \"';< n2 AMa > Fz 0 =..73\" 6 = 15.18\" 0X,FMi 092 0.96 L.00 1.04 nslnz\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4. Bending angle δ with RI contrasts n3/n2 from 0.91 to 1.05. Insets: the electrical field distributions and marked bending angles for the cases of n3/n2 equaling 0.95 and 1.02."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The tasks of automatic argument generation and debate require the ability to present a diverse and comprehensive set of supporting and opposing arguments given a controversial topic. Two critical components of such systems are an ability to determine the stance and the specificity of any claims employed in the proposed argument. Consider, for example, the argument thesis (i.e., the topic) of Figure 1: (THESIS) Would we like to live in the world of Harry Potter? Construction of an argument in support or in opposition to this thesis necessarily requires knowing the stance of the claims that comprise it: the claim Magic opens a lot of interesting possibilities should be identified as a claim in support of the THESIS, and The capacity of harm is greater when magic is involved (HARM), as a claim in opposition. Indeed, pre-vious work has studied this task (e.g., Bar-Haim et al. (2017); Faulkner (2014)).\n- It is not sufficient, however, to determine claim stance only with respect to the argument thesis. Debate and argument generation systems, in general, should also be able to determine whether two claims that address the same line of reasoning represent the same, or the opposing stance: using Defense is also made easier through magic to refute the HARM claim in Figure 1, for example, requires recognizing that it represents the opposite stance.\n- The issue of claim specificity in argumentation has been much less addressed. Existing work, however, suggests that a high degree of specificity is correlated with argument quality and persuasiveness (Carlile et al., 2018;Swanson et al., 2015). In terms of argument quality though, it is entirely possible for the presented claims to be coherent and meaningful, yet be too specific within the given discourse, and therefore be logically irrelevant (Dessalles, 2016). As a concrete example, suppose we wanted to assert a claim in support of the argument THESIS of Figure 1. While The Unforgivable Curses are illegal...and their use is grounds for immediate life imprisonment supports the THESIS, it is too specific a claim to introduce at this point in the argument. Namely, it doesn't flow naturally without first introducing the concept of Unforgivable Curses.\n\nMentions:\n- Consider, for example, the argument thesis (i.e., the topic) of Figure 1: (THESIS) Would we like to live in the world of Harry Potter?\n- Debate and argument generation systems, in general, should also be able to determine whether two claims that address the same line of reasoning represent the same, or the opposing stance: using Defense is also made easier through magic to refute the HARM claim in Figure 1, for example, requires recognizing that it represents the opposite stance.\n- As a concrete example, suppose we wanted to assert a claim in support of the argument THESIS of Figure 1.\n- To support the generation of diverse and potentially complex arguments on a topic of choice, we present here a dataset of manually curated Figure 1: Partial tree for the controversial topic \"Would we like to live in the world of Harry Potter?\".\n- Figure 1 shows a partial argument tree for the thesis \"Would we like to live in the world of Harry Potter?\".\n\nOCR Text: \"Ancaer exakht #hizde me77 770Fmt #inbj D 4r: Jatin rvahoi [~3bp\"%zyohr Xaiun \" Duittrormg 70t732tn 1kas hiatat60 J ocadti # ct Octetnkn \"Tiloc\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Partial tree for the controversial topic “Would we like to live in the world of Harry Potter?”. Each claim’s position towards its parent argument is indicated in the box on the edge between the claim and its parent. The full argument tree for this topic can be found at https://www.kialo.com/is-the-world-of-harry-potter-really-the-placeto-be-2415/2415.0=2415.1."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The stellar mass (M ) distributions of BSFs and RQEs are different compared with the distribution of CLEs, as shown in the top panel of Fig. 2. CLEs are typically more massive with a median M of 10 11.1 M , whereas the median M is 10 10.1 M for BSFs and 10 9.9 M for RQEs. The latter presents a nearly flat mass distribution below 10 10 M . As mentioned above, the percentage of E galaxies with different properties than CLEs increases for lower masses (see also Lacerna et al. 2016). The CLEs correspond to 86% of the galaxies with M ≥ 10 11 M , whereas RQEs are only ∼4%, and none BSF is in this massive bin. For the range 10 10.4 ≤ M &lt; 10 11 M , CLEs are 78%, RQEs are ∼4% and BSFs are ∼5%. The change in the fractions is more important for lower masses, M &lt; 10 10.4 M , where the CLEs are only 44%, RQEs are 29%, and BSFs are 12%. At M ≤ 10 9.4 M there are only RQE galaxies. The overall fractions reported in Table 1   sample of Es is not from a complete volume survey. The fractions reported for each stellar mass bin are less affected by the volume incompleteness. The g-i color is also different between CLEs and BSFs by definition. The median g-i color is 1.26 for the former and 0.90 for the latter. The RQEs are in the middle (median g-i color of 1.05), with an overlap of the BSF and CLE color distributions. The definition of McIntosh et al. (2014) strongly motivates our classification of RQE galaxies. Following, we mention some differences. First, they use light-weighted median stellar ages ≤ 3 Gyr derived by Gallazzi et al. (2005), whereas this upper limit is slightly relaxed to 4 Gyr in our case using integrated values of age lw within 1.5 R e f f as described in Sect. 2.2. Second, they follow the method of Peek &amp; Graves (2010) to define spectroscopically quiescent galaxies within an ellipse of EW(Hα) and EW([OII]) using fluxes from the Max Plank Institute for Astrophysics and Johns Hopkins University (MPA-JHU, Kauffmann et al. 2003;Brinchmann et al. 2004;Tremonti et al. 2004;Salim et al. 2007) catalog. Additionally, they consider quiescent galaxies those with S/N &lt; 3 in Hα flux or with EW(Hα) ≤ 2 Å. In our case, we use the definition of RGs, i.e., EW(Hα) &lt; 3 Å integrated within 1.5 R e f f , as the criterion for quiescent galaxies. Finally, RQE candidates are inside the non-star-forming region of the urz diagram according to McIntosh et al. (2014). We do not use this condition because we consider that our condition for RGs is good enough to select non-star-forming galaxies. Fig. 3 shows the urz diagram for our sample of E galaxies. Nearly all the RQEs using our selection criteria are located inside the modified non-star-forming region of McIntosh et al. (2014). We also include the empirical selection of RQE galaxies proposed by them using only the urz diagram, which is shown as a green shaded region. We find 25 (71%) of the RQEs using our selection criteria are located within this region. This fraction is smaller A&amp;A proofs: manuscript no. EMaNGA  RQEs and includes mild pollution of other Es, especially from blue RGs and UNDs. One of our aims is to explore whether the RQE galaxies, as defined here, are a distinct class of Es with respect to the CLEs, or they are just the expected tails in the color or age distribution of the CLE galaxies. Based on our results, we will discuss it in Sect. 6.3.1.\n- Do RQEs, as defined here, have different properties and evolutionary paths with respect to CLEs? Our results suggest that, indeed, this is the case, reassuring that our selection criteria were able to pick a class of physically different E objects. The RQEs are significantly younger than the CLEs on average, even at a given M , see Fig. 4; the differences are on average by ∼ 5 Gyr using age lw as expected given the selection criterion with age lw , and by 3-4 Gyr using age mw . At low masses of 9.4 &lt; log(M /M ) &lt; 10, the RQEs are younger than CLEs with on average differences of age mw ∼ 2 Gyr. The gradients in age mw of the RQEs are on average flat but there is a large scatter around 0, see Fig. 11. The gradients in age lw are even more scattered and clearly differ between RQE galaxies more and less massive than ∼ 10 11 M ; for the former, the gradients tend to be positive while for the latter, they tend to be negative (see Fig. 11, and Tables 2 and 3). We recovered 71% of the RQEs within the empirical region of the urz diagram that can capture the youngest portion of quiescent blue elliptical galaxies according to McIntosh et al. (2014; see Fig. 3). In fact, only 2% of the CLEs are contained inside this region. At low masses of 9.4 &lt; log(M /M ) &lt; 10, we recover 92% of the RQE galaxies, whereas 28% of the CLEs are located within this region. Furthermore, the progenitor(s) of RQEs were most of the time in an active phase of SF (at least one of them if they suffered a late merger), opposite to the progenitor of CLEs, see Fig. 16. Finally, the RQEs of all masses not only quenched recently (t q = 1.2 ± 0.9 Gyr ago) but notably faster than the CLEs, even comparing to those CLEs that also have late long-term quenching times (see Fig. 18). For instance, low-mass RQE galaxies show a median t q ∼ 1 Gyr and ∆t q = 2.5 Gyr, whereas those are 2.9 Gyr and 3.5 Gyr, respectively, for CLE galaxies of the same mass. All these facts suggest that RQEs are not a simple statistical extension of the properties of CLEs, including the low-mass range in which they overlap, and may support the distinction of RQEs and CLEs as two different classes of E galaxies. The definition of RQE galaxies is strongly motivated by McIntosh et al. (2014), but we admit it is sort of arbitrary, and the numbers reported above might change with other definitions to classify them. There exists the possibility that the RQE galaxies actually correspond to a continuous transition of CLE galaxies at low masses, in which the boundary is not totally clear.\n\nMentions:\n- Fig. 3 shows the urz diagram for our sample of E galaxies.\n- We recovered 71% of the RQEs within the empirical region of the urz diagram that can capture the youngest portion of quiescent blue elliptical galaxies according to McIntosh et al. (2014; see Fig. 3).\n\nOCR Text: \"non-SF 2 Tcd SFC HlucPl Z48 0.9r\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3. Color–color diagram. The colors are in AB magnitudes and Kcorrected at z = 0. The symbols for the galaxies are the same as Fig. 1. The thick black lines enclose the non-star-forming region of McIntosh et al. (2014) based on Holden et al. (2012). The green shaded region corresponds to an empirical relation to select RQE galaxies defined by McIntosh et al. (2014)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We train the proposed RNN model using the Tensorflow application running on a Windows environment. We set the number of hidden layers as D h = 10 and use the Adam  optimizer with the learning rate of η = 0.0005 [30]. As mentioned in Section IV, we split the total measurement data into training, validation, and test sets and prepare the input and output pairs from each set separately. Also, the optimal parameter in the RNN model may vary depending on the environment, and the proposed model needs to be retrained if the propagation environment is significantly changed. Every training process in this study takes much less than one hour using a workstation equipped with dual Intel(R) Xeon CPU E5-2680 v2 processors. At first, we verify the performance of the proposed RNN model depending on the number of data. This number is expressed in an exponential expression (e.g., 1e4=10000) and includes training, validation, and test data. We select certain amounts of data from these three data sets so that the portion of the selected data becomes 70 %, 15 %, and 15 %, respectively. Furthermore, we create each input data from P = 10 packet transmissions and run the optimization algorithm up to 1000 iterations. One iteration is also called epoch, which implies that the parameter of the proposed RNN model is updated using the entire training data. Fig. 5 illustrates the variation in cost function with respect to training, validation, and test sets. As the figure shows, the cost function for training data tends to decrease with the iteration, whereas those for validation and test data decrease to a certain iteration and then increase again. In addition, the more data are used for training, the less is achieved cost can be achieved. The minimum cost achieved for validation data is presented in the figure as x-mark. The performance for different numbers of data is summarized in Table II. The fields denoted by Minimum validation cost and Epochs to stop imply the minimum cost function with respect to the validation set and the number of training epochs to achieve this cost, respectively. The decision threshold is selected to maximize the average LOS/NLOS detection rate. The ROC curves of the proposed RNN model are depicted in Fig. 6. This figure indicates that the more number of data is used for training, the more accurate the results. However, the performance gap between the cases using 5e4 data and 1e5 data is not large. Regardless of the number of data used, the performance of the proposed LOS identification method exceeds the best conventional scheme depicted in Fig. 4(b), even though the proposed method considers only P = 10 packet transmissions. Fig. 7 depicts the ROC curves of the proposed model depending on the number of packet transmissions. The ROC curve approaches the perfect decision point as the number of monitored packet transmissions increases.\n\nMentions:\n- One iteration is also called epoch, which implies that the parameter of the proposed RNN model is updated using the entire training data. Fig. 5 illustrates the variation in cost function with respect to training, validation, and test sets.\n\nOCR Text: \"czinrg Jaaliani les. 38, K eozlincus ? Jblb; Foocn\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 5. Cost function with respect to training, validation, and test set. The minimum cost for validation set is marked as x."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- This property is illustrated in Fig. 4, where the incident angle dependence of the minus first diffraction order intensity, 1 ρ − , is shown for the geometry of Fig. 1. A ρ = − ∑ , is nothing else than the active losses per unit area. The inequality for the solution presented is to be true under rather general conditions, specifically for such β and κ values that are far from anomalies related to all diffraction orders except the specular one. If the active losses are absent, then the inequality transforms into the equality. In the specific case of short-period gratings, such that 2 κ &gt; , all diffracted orders except zeroth one with necessity correspond to evanescent waves. Under such conditions the strong specular reflectivity suppression is accompanied by maximal absorption. The energy redistribution between outgoing waves and the dissipation strongly depends on the parameters of the problem, as one can see from the explicit solution. Specifically, in the geometry shown in Fig. 1 the absorption, ( ) cf. point L in Fig. 6. Evidently, the absorption vanishes if the medium is dissipation free, 0 ξ ′ = . Under rather specific conditions max A can be of order unity, that does not describe general case contrary to the statement in [16].\n\nMentions:\n- This property is illustrated in Fig. 4, where the incident angle dependence of the minus first diffraction order intensity, 1 ρ − , is shown for the geometry of Fig. 1.\n\nOCR Text: \"Jac FL U.UZ 0.04 0.06 Aldd\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4. The 1ρ− plot versus β for three harmonic gratings differing by depth with parameters indicated in"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Assume that R ε z consists of m path-connected components called valid regions and denoted by {Z i sys } m i=1 . In general, if the points from R z are sufficiently far away from each other, m is larger than 1. Note that each valid region is an equivalence class with respect to the equivalence relation (1). To connect them, we define a set of transitions between them: Definition 2: A transition function f i,j z : Z i sys × U → Z j sys maps any point z ∈ Z i sys to a class representative z j sys ∈ Z j sys , where i, j ∈ {1, 2, ..., m} and i = j. Given a set of valid regions R ε z in Z sys and a set of transition functions connecting them we can approximate the global transitions of Z sys as shown in Fig. 2. To this end, we define a Latent Space Roadmap: Definition 3: A Latent Space Roadmap is a directed graph LSR = (V LSR , E LSR ) where each vertex v i ∈ V LSR ⊂ Z sys for i ∈ {1, 2, ..., m} is an equivalence class representative of the valid region Z i sys ⊂ Z sys , and an edge e i,j = (v i , v j ) ∈ E LSR represents a transition function f i,j z between the corresponding valid regions Z i sys and Z j sys for i = j.\n\nMentions:\n- Given a set of valid regions R ε z in Z sys and a set of transition functions connecting them we can approximate the global transitions of Z sys as shown in Fig. 2.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2: A visualisation of the structure of the latent state space showing valid regions Zisys and transition functions f j,iz between them."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- This defines the likelihood function as a Gaussian noise with variance σ 2 . Since this variance is unknown, we keep it as a model parameter. As we have no specific prior information about plausible values of α and β, we define the following vague prior distributions:   The first two distributions represent our initial -conservative -belief that the two implementations are equivalent in terms of scaling behavior and constants. We model the variance of the observation noise σ 2 as an inverse gamma distribution instead of a normal distribution, since a variance cannot be negative. Figure 8 shows a listing to compute and show the posterior distribution of these three parameters using SciPy [91] and PyMC3 [97]. Results are listed in Table 4. The interval of Highest Probability Density (HPD) is constructed to contain 95% of the probability mass of the respective posterior distribution. It is the Bayesian equivalent of the confidence interval (Section 7.3.2) and also called credible interval. The most probable values for α A/B and β A/B are -5.22 and 1.01, respectively. Taking measurement uncertainty into account, the true values are within the intervals [−6.87, −3.58] respective [0.70, 1.29] with 95% probability. This shows that KADABRA is faster on average, but results about the relative scaling behavior are inconclusive. While the average for β A/B is 1.01 and suggests similar scaling, the interval [0.7, 1.29] neither excludes the hypothesis that KADABRA scales better, nor the hypothesis that it scales worse.\n\nMentions:\n- Figure 8 shows a listing to compute and show the posterior distribution of these three parameters using SciPy [91] and PyMC3 [97].\n\nOCR Text: \"\"port 5c LN :C4pI \"rort %pticize \"asic_wd2] Pm2l4) vith b1si: Jfolo ; H[n2 'oriel \" elpla [LA0; Jd-L3} Merwal( \"eta [IF0 , id-|W8 sipa 'Jnnrsekec9('sige)' , slphee],bixe]} 1he bataeLoglitear Ls PVorralk V,ots\" IUII :bierved: ozl telddz_) vilh besi: role ; eproxicate ;o 4 roc #UsEr buAJoE Vica \"0*O? gcles (ze path  0.05, suwr (truce' 74,\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 8. Example listing for inference of parameters αA/B , βA/B and σA/B ."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- First we looked to address the performance of the SRC approach given limited training set sizes. Figure 3 provides a comparison between the SRC ans SIFT SVM approaches over various training. Note that by 20 elements from each classwhich represents less than a third of even the smallest class -our SRC dictionaries were able to obtain better than 95% accuracy with their 40 test images. On the other hand, the SIFT SVM approach was unable to yield an average accuracy rate above 90% until 35 samples from each class and was vastly outperformed by the SRC method for the smaller training sizes.\n\nMentions:\n- Figure 3 provides a comparison between the SRC ans SIFT SVM approaches over various training.\n\nOCR Text: \"SIFT S8HI Oatr }lknLt Troiuily 1>A Fzik:t Af Exh !Jhjcat = Tvinint\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3: Classification rates over various training set sizes."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Phase II: This phase corresponds to Step 5 and Step 6, which conduct a weight restoration operation and trains the entire network, respectively. Step 5 removes the cardinality constraints over parameters which are set to be 0 in Step 3 so that all parameters are updated in Step 6 freely. This step is critical in SDNN for the following reason: by updating all parameters including those set to be 0 in Step 3, the SDNN is able to restore some connections that are beneficial for learning feature representation with strong discriminative power. As can be seen in Figure 2, at the initial training stage, a large proportion of connections which are truncated in the last round of hard thresholding operation become significant (with parameters in large magnitudes) at this phase. Thus SDNN has a strong capability to search among a large parameter space for seeking a better local optimum. Besides, we observe that the ratio decrease as the training progresses due to the deep model converges to a good optimum.\n\nMentions:\n- As can be seen in Figure 2, at the initial training stage, a large proportion of connections which are truncated in the last round of hard thresholding operation become significant (with parameters in large magnitudes) at this phase.\n\nOCR Text: \"Multi-Step Updlare Single-Step Update 3 Hard thresholding times\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: The change ratio of connection truncated by neighboring hard thresholding operations. The multi-step update adopted by SDNN has large change ratio in initial training phase, allowing the deep model to learn features with strong discriminative capability, while the single-step update would result ∼ 0.01% change ratios during all training phase, thus undermines the representation capability of deep models."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- With our more balanced dataset, training on 5000 random samples (less than 1/4) and validating against the remaining 3/4 gives a precision of 98.36% and with φ = 0.96. The learning curve of the first 5% to 20% training size, validated against the remaining 95% to 80% are shown in Figure 2. Thus now we can indeed say with confidence that the AI has \"learnt\" which Cayley tables are those of simple finite groups.\n\nMentions:\n- The learning curve of the first 5% to 20% training size, validated against the remaining 95% to 80% are shown in Figure 2.\n\nOCR Text: \"7e3m6\" a34 Ibrres; I\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: The learning curve for recognizing whether a group is simple. The data consists of (permutations, with enhanced frequencies for the simples, which are relatively rare) all Cayley tables of all first 602 finite groups, up to size 70. We are nearing complete agreement by seeing up to only 20% of total data."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Likewise, the particle stress in figure 7a is further decomposed into its components in figure 7c, which include the IBM, viscous, and convective stresses, given by σ PIBM ,x , σ Pvisc,x , and σ Pconv ,x , respectively, in (4.12). The IBM term dominates, the convective term is negligible, and the viscous term is detectable only near the lower wall. After omitting the negligible terms, (4.12) yields the following balance between the dominant terms in the fluid momentum equation Figure 8 shows the momentum balance for (4.20), in which the sum of the hydrodynamic and collision stresses is zero. Thus, the hydrodynamic force driving the particle in the positive x-direction is balanced by the collision forces between the particle and ). The sum of the fluid and collision stresses is not in equilibrium with the external stress where the particle is present (y/L y &lt; 0.5), except at the lower wall (y/L y = 0).\n- the lower wall acting in the negative x-direction, indicating that the particle is not accelerating. For a single particle, this figure shows only the net hydrodynamic force, F I,p + F IBM ,p , and net collision force, F c,p at the lower wall, smeared by the coarsegraining method. Thus, there exist stresses above the particle diameter (y/L y &gt; 0.5) because the coarse-graining width we chose (w = 0.67D p ) spreads values beyond the particle radius. For each stress component, the value at the lower wall represents the entire stress (e.g., F c,p /(L x L z )) acting on the particle. At the lower wall, the particle stress in figure 7a matches the hydrodynamic stress in figure 8, which is balanced by the collision stress. Therefore, the particle stress at the lower wall in figure 7a represents the stress between the particle and wall due to collision forces.\n- Figure 11 shows the stress balance for the particle phase given by (4.21), in which the particle weight is in equilibrium with the sum of the hydrodynamic stress and the collision stress. In this case, the particle weight represents the gravitational force, F g,p acting on the single particle, smeared by the coarse-graining method. The fluid exerts a positive lift force on the particle, but the vast majority of the particle's weight is supported by the collision force with the lower wall. Comparing figure 8 to figure 11, we can see that the lift force (related to the y-momentum hydrodynamic stress) is a fraction of the drag force (related to the x-momentum hydrodynamic stress), but the y-momentum collision force is six times larger than the drag force. In fact, the particle weight and collision stress are the only terms in the y-momentum balance that are comparable in magnitude to the stresses in the x-momentum balance. Having analyzed the momentum balances for the simple test case of a rolling particle in a pressure-driven channel flow, we now proceed to the more complex case of a sediment bed involving many particles. 5.2. Stress balance of a sheared particle bed\n\nMentions:\n- After omitting the negligible terms, (4.12) yields the following balance between the dominant terms in the fluid momentum equation Figure 8 shows the momentum balance for (4.20), in which the sum of the hydrodynamic and collision stresses is zero.\n- At the lower wall, the particle stress in figure 7a matches the hydrodynamic stress in figure 8, which is balanced by the collision stress.\n- Comparing figure 8 to figure 11, we can see that the lift force (related to the y-momentum hydrodynamic stress) is a fraction of the drag force (related to the x-momentum hydrodynamic stress), but the y-momentum collision force is six times larger than the drag force.\n\nOCR Text: \"Hrahodyuatuic CallisJoni Hralo Collision ~U 0 ,} |onf\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 8: Single rolling particle: stress balance of the particle phase in the x-direction according to (4.20). The hydrodynamic stress, which propels the particle in the positive x-direction, is in equilibrium with the stress due to the collision between the particle and the lower wall, which slows the particle down."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The 3D velocity is better correlated with the surrounding density fluctuations. This is mathematically evident by the expressions for ξ v v vδ and ξ uδ in §3. Figure 4 illustrates this point. The solid and dashed curves are the dipole moments of the v v v − δ correlation from mocks, respectively, for volume and mass weighting schemes. The shaded areas are the corresponding cosmic variance and shot noise scatter. The curves are obtained in a similar way to the calculation of ζ for u − δ, as described in §4.3, with two differences: µ β is replaced withŷ y y • v v v α/vα in (23) and uα is replaced with vα in both ( 24) and (25). Comparison with the u − δ correlations in the top panels in figure 2, clearly reveals a stronger v v v − δ signal for a similar width of the shaded areas.\n- Using v v v WF , we compute the dipole moment of the velocity density correlation. Volume and mass weighted dipole moments of v v v WF −δ are shown in the bottom panels of figure 2. The amplitude of the correlations, both in the data and the mocks, is substantially lower than the amplitude of the unfiltered v v v − δ correlations of in figure 4. The shaded areas representing shot-noise and cosmic variances corresponding to the v v v − δ correlation and shown in the bottom panels of Figure 2 are broader than in the top panels for the u − δ. This may seem surprising since a stronger correlation is expected for the 3D velocity. The reason for that is that the derivation of the 3D velocity is associated with an interpolation of the radial velocity data to all spaces. This effectively introduces additional source of error, as also seen in figures 6&amp;7.\n\nMentions:\n- Figure 4 illustrates this point.\n- The amplitude of the correlations, both in the data and the mocks, is substantially lower than the amplitude of the unfiltered v v v − δ correlations of in figure 4.\n\nOCR Text: \"no filtering mocks volumiz-Icizhting GT 488-' himILtid Srnaradich Moch\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4. Dipole (l = 1) moments of v − δ correlations derived from unfiltered peculiar velocities in the mocks."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- which yields the four coefficients of the peak flux f (t), the oscillation amplitude a(t), Gaussian width σ s (t), and mean background flux b(t) for each time t. The 4-parameter fits The cross-sectional flux profiles F (s, t) and the Gaussian fits F f it (s, t) are shown in Fig. 4 for each time in the interval between t 1 =19:05 UT and t 150 =19:35 UT, while a corresponding time-slice with Gaussian fits is rendered in color scale in Fig. 3 (second panel). The average Gaussian loop width during the oscillation period is σ s = 2.1 Mm, which corresponds to a FWHM loop width of w = σ s 2 √ 2 ln 2 = 4.9 ± 0.6 Mm.\n\nMentions:\n- The 4-parameter fits The cross-sectional flux profiles F (s, t) and the Gaussian fits F f it (s, t) are shown in Fig. 4 for each time in the interval between t 1 =19:05 UT and t 150 =19:35 UT, while a corresponding time-slice with Gaussian fits is rendered in color scale in Fig. 3 (second panel).\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4.— Cross-sectional flux profiles F (x, t) (blue) obtained from the running-minimum difference technique (Fig. 3, top panel) and Gaussian fits (red)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The shape of the curves for the general scenario and the mean load scenario obtained from Corollary 2 are consistent. The small gap under some parameter settings is caused due to the fact that the approximation error of the mean load is aggravated with the operation of summation in (25). Fig. 2 shows the evolution of the SCDP with regard to the helper density for different helper cache capacity. Hybrid caching policy outperforms the most popular caching policy and the performance gain increases with a growing helper density. The reason is that numerous users are offloaded to the helpers tier and obtain the content from the local cache as the helper density increases, which is different form the most popular caching policy whose load distribution is independent of the helper density. By avoiding the influence of the limited MBS backhaul link as much as possible, dense helpers deployment can increase the delivery rate dramatically and improve the SCDP. Comparing the curves of different helper cache capacity, caching at helpers also helps to improve the SCDP. Fig. 3 illustrates the evolution of SCDP with respect to the helper density for different content library size N . For the fixed content library size, the SCDP increases with the increase of the helper density. The performance gain of the hybrid caching policy over the most popular caching policy increases with the increase of the helper density as well. As content library size increases, the SCDP decreases. When the content library contains a large number of contents, the span of the content popularity distribution is large and the popularity of each content is relatively small given the fixed δ. On the one hand, the cache hit ratio of the first content group decreases. On the other hand, the cache probability of the second content group decreases. As a result, UEs requesting content from the second group prefer to associate with the MBSs tier, the SBDP decreases accordingly. SADP C Lp,2 decreases due to the low helper density caching the requested content. Increasing the helper density can effectively cope with the expansion of the content library to achieve the fixed SDCP requirements. Fig. 4 depicts the impact of the helper cache capacity on the SCDP for different content popularity distribution parameter δ. The cache probability of the content in the second group increases as cache capacity increases. For a small parameter indicating the more even content popularity distribution, the SCDP gain of the proposed caching policy over the most popularity caching policy increases with the increasing helper cache capacity. For instance, setting δ = 0.5, the performance gain ranges from 10% to 19%, when N 2 varies from 25 to 400. The reason for this trend is that the increasement of cache capacity helps to make full use of the content diversity to improve the SCDP. For a big parameter indicating the more skewed content popularity distribution, a large number of users requests focus on the popular contents in the first group and MBS caching gives a large SCDP. The gap between two caching policy shrinks. However, as cache capacity increases, more users requesting the content from the second group associate with the helpers tier and active probability of the helpers increases accordingly. The SCDP experiences a slight decline under the influence of increasing inter-tier interference.\n\nMentions:\n- Comparing the curves of different helper cache capacity, caching at helpers also helps to improve the SCDP. Fig. 3 illustrates the evolution of SCDP with respect to the helper density for different content library size N .\n\nOCR Text: \"Gulau \"rall Mamide*ahu Men Imd*91r7n Euudn 6esmn Mbu MeEm Heloer density' A\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3: SCDP versus helper density with different content library size, Cb= 2.5×10 6 bps, δ = 0.5."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- When learning the decomposition of a global property, such as the total energy, one can hope to lift the degeneracy by using features centred on other atoms in the structure. For the construction in Fig. 1b, there is always at least one atom outside the bisecting A plane that breaks the indistinguishability of X + and X − . Indeed, a model that combines C and H-centred non-linear kernels can approximate the molecular energy to excellent accuracy, also along the degenerate manifold (see Fig. 3, middle panels). In general, however, one cannot rely on such a mechanism. For the sake of simplicity, we demonstrate this for the case of 2-body descriptors |X (1) . It is well-known that the list of distances from the centre of an environment, or even the list of distances in a structure [23], are not complete representations. It has, however, been speculated [28] that simultaneous knowledge of all atom-centred lists of distances in a structure would provide a complete representation of the configuration, and that one could use this representation to predict arbitrary potentials using an additive model based on nonlinear functions of X (1) . Both conjectures are false. We present a counter-example to the first conjecture in the SI. The counterexample to the second statement, cf. Figure 4a, is far more concerning though: even if, in a training set, all configurations can be uniquely identified by the collection of the atom-centered 2-body histograms, it does not follow that a total energy represented in terms of these histograms can be learned.\n- Using the higher-body order features to differentiate between X + and X − does indeed lead to a more efficient model (Fig. 3, bottom panel), that predicts the energy along the degenerate manifold with an error that is roughly a third of that obtained by a multi-center, power-spectrum-based model. Substantial improvements are also seen for the random CH 4 configurations. A NN based on |X (3) reduces the full-train-set error by 40%, down to ≈ 0.5 kcal/mol. Similar to what was observed for X (2) -based models that combine multiple cutoff distances [29], there is a data/complexity tradeoff. For small training set sizes a simpler powerspectrum model can outperform one based on the bispectrum, and linear regression outperforms a deep neural network. The best balance between data efficiency, computational cost and ultimate accuracy might involve a combination of different kinds of features, as demonstrated by the hybrid model in Fig. 4. Approaches such as the moment tensor potentials [22], permutationally invariant polynomials [30,31] and the atomic cluster expansion [20] allow, if necessary, to further resolve degeneracies by including arbitrary body-orders of correlation. We show in the SI that similar considerations apply also to a database of bulk silicon structures [32]. The cutoff distance, however, complicates the picture, because the number of neighbors included in the environments influences the proximity of structures to the degenerate manifold, and because the model accuracy is also affected by the truncation of long-range interactions [33]. Descriptors such as eigenspectra of matrices constructed from the atomic configuration (distance matrix, Laplacian, orbital overlap, etc.) [34] also contain information on high body order correlations, and as such are not expected to be degenerate for the present examples. Their completeness properties are not understood at present.\n- Still, since atomic environments can be thought of as a very restrictive class of signals, the invariant theory perspective may shed additional light on our questions. The perspective of Boutin and Kemper [23] appears to be particularly useful, establishing conditions under which a points cloud can be reconstructed from the histogram of distances. The problem we tackle here is closely related: degeneracy of two centred environments with re-spect to n-body correlations implies degeneracy of the point clouds consisting of the neighbors with respect to n − 1 body correlations. For example, Fig. 1a, implies that the length-histogram of the neighbours lying on the circle are degenerate (indeed, this is the example given in Fig. 4 in Ref. [23] and in Fig 2 of [28]). Similarly, Fig 2f, shows environments that are degenerate with respect to the 4-body correlation (tetrahedron histograms) are also degenerate with respect to the 3-body correlations (triangle histograms) of the entire structure. A similar approach may therefore help determine tight bounds on the codimension of the degenerate manifold although, as far as we are aware, there are no rigorous results in this direction. The problem of formulating a complete feature map is of fundamental importance -particularly when considering the use for generative models that require inverting the relation between a representation and the underlying structure -and has practical implications, particularly when one wants to achieve high accuracy with the minimum amount of data. The presence of many neighbors or of different species (that provide distinct \"labels\" to associate groups of distances and angles to specific atoms), and the possibility of using representations centred on nearby atoms to lift the degeneracy of environments reduces the detrimental effects of the lack of uniqueness of the power spectrum when learning extensive properties such as the energy. We show, however, that the learning rate of this kind of models reduces dramatically in the high accuracy regime, revealing the limitations of a description based on 3-body features. Diagnostic tools such as the joint distance histogram that we introduce here can help identify problematic parts of datasets, give more confidence in the reliability of simpleto-compute low-order invariants, and guide the choice of a small number of higher-order features to improve the accuracy and efficiency of models.\n\nMentions:\n- The counterexample to the second statement, cf. Figure 4a, is far more concerning though: even if, in a training set, all configurations can be uniquely identified by the collection of the atom-centered 2-body histograms, it does not follow that a total energy represented in terms of these histograms can be learned.\n- The best balance between data efficiency, computational cost and ultimate accuracy might involve a combination of different kinds of features, as demonstrated by the hybrid model in Fig. 4.\n- For example, Fig. 1a, implies that the length-histogram of the neighbours lying on the circle are degenerate (indeed, this is the example given in Fig. 4 in Ref. [23] and in Fig 2 of [28]).\n- Other examples are shown in panels B and C of Fig. S4.\n\nOCR Text: \"Ee Ceeen Mier Acla IO ?\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 4. (a) Four configurations distinguishable by their atomcentered 2-body histograms. Only three different site energies occur in these configurations, hence fitting four total energies leads to overdetermined regression. (b) Correlation plot of powerspectrum and bispectrum distances between C environments in a database of random CH4 configurations. (c) Learning curves for the atomization energy of random CH4 configurations."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We then study the z evolution of the M * /M h relation in different DM cosmologies, shown in Fig. 11. As seen from this plot, CDM shows the lowest M * /M h value at any z -this is the result of stronger feedback decreasing the total stellar mass of low-mass halos. As expected, the M * /M h value increases with decreasing m x at any redshift. Indeed, integrating down to a limit of M U V −17.7, CDM galaxies show a M * /M h − z slope proportional to 0.003z 2 (0.002z 2 ) including (excluding) UV feedback. The corresponding, steeper, 1.5 keV WDM slopes have a value of about 0.004z 2 , given the minimal effect of UV feedback on galaxies in this model. As expected, the slopes steepen in all cases integrating down to a fainter magnitude limit of M U V = −16.5. In this case, the CDM slopes correspond to 0.003z 2 (0.0015z 2 ) with (without) UV feedback which steepen to 0.005z 2 for the 1.5 keV model as tabulated in Table 3. Current estimates on the stellar mass have an error of about 0.3 dex, with abundance matching techniques, to infer halo masses, resulting in errors of the order of 0.5 dex. While current stellar mass-halo mass observations are probably unable to distinguish between these different slopes, more precise estimates, of both stellar and halo masses, in the future might offer a useful tool to distinguish between CDM and 1.5 keV WDM.\n\nMentions:\n- We then study the z evolution of the M * /M h relation in different DM cosmologies, shown in Fig. 11.\n\nOCR Text: \"004, < *R WU; <-445 166X M\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 11.— The M∗/Mh relation as a function of z for CDM (black lines), 3 keV WDM (blue lines) and 1.5 keV WDM (red lines) integrating down to a UV magnitude corresponding to −17.7 (observed galaxies) and −16.5 in the eft and right panels, respectively. In each panel, the top and bottom lines show the relation ignoring and including UV feedback, for each DM model. As seen, the steeper slope of the M∗/Mh − z relation could be used to obtain constraints on the WDM particle mass."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Topics in the content of extremist and non-extremist users can play a critical role in determining intra-class similarity and inter-class differences among users. We used Latent Dirichlet Allocation (LDA) over n-grams (n=1-3) to assess topical similarity of content of extremist users to characteristics of Islamist extremism. As LDA is a parametric probabilistic approach for retrieving topics, the number of topics should be carefully selected to capture the themes optimally. Hence, we used perplexity measure to obtain the optimal number of topics that best represents the content. A higher perplexity score implies higher representativeness and semantic integrity among the topics. Researchers [32] have applied LDA over various combinations of unigrams (U), bigrams (B), and trigrams (T) to obtain informative topics. We apply this procedure creating three different topic models covering the following combinations: (i) U, (ii) U+B, (iii) U+B+T (see Section 3.2). Using the perplexity score depicted in Figure 3, we identified 90 as the threshold for the optimal number of topics for each user for each of the four topic models. We have also used the same number (90) of topics for non-extremist users. That is, we did not compute perplexity scores for non-extremist users separately as they were created using Hierarchical Dirichlet Process (HDP), which is a non-parametric form of LDA. In the content of extremist users, topics related to the Islamist ideology are prevalent compared to topics related to hate and religion. For instance, Islamist extremist users frequently make use of ideology-related words/phrases to promote their organization (e.g., \"islamic state\", \"isis\", \"join isis\"), its activities (e.g., \"martyrdom operation syrian opposition\") or attacks on non-muslim people (e.g., \"kaffir\") (see Table 3). On the other hand, topics related to religion are prevalent in the content of nonextremist users. For instance, non-extremist users invoke religious concepts such as \"allah\", \"prophet muhammad\", \"quran\", \"alhamdulillah\" and \"jannat\", whereas there is only one reference to hate, \"masjid job kill\". We observe that the prevalence of these topics related to different contextual dimensions in the content of extremist and non-extremist users varies.\n\nMentions:\n- Using the perplexity score depicted in Figure 3, we identified 90 as the threshold for the optimal number of topics for each user for each of the four topic models.\n\nOCR Text: \"cnntent xttemIST I3er5 Tolcs latcd thc [slamist idcology prevalent com topics related hal religion For MSACF Isarst prtust Wacn frequently mnnakeS ideology-related words Dnrase: promot their organization (e g.. \"islamic state \"join jcuvlc: mafYI operanon svran opposition attacks nan-muslim people (B; \"kaffir' (see the olher hand, lupiC ? relaed religion prevalent in the content non extremst uScTS instancc, non cxtrcmst users Invoke rchglous concepls such 4s allah prophel [IluHaIIad\" quiul \"alhiundulillii\" \"jannat ercas DnT reter CICC hatc, Iaso job kill\" Wc obscrvc that the prevalence these topics relaled dif- Ieren conexM; dumensione the content ol Pued 71915 dom there\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3. Identification of optimal number of relevant topics based on perplexity scores. From these graphs, we identify 90 as an optimal number of topics that best represent the content of users."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Initial conditions (i.e. the nuclei fractions at E 0 = 100 TeV) have to be given by direct measurements on satellites or stratosphere balloons. The most recent resume of such measurements is presented in [8]  As is seen from the comparison (V.20) with (V.13), the values of fitted parameters (corresponding to models with different initial conditions) are consistent with each other within the limits of statistical uncertainties of the fit. The energy evolution of nuclei fractions is shown in Fig. 8. In Fig. 9 the fraction of light nuclei and the mass number average logarithm are presented for two variants of initial conditions (V.4) and (V.18). As is seen, the mass composition characteristics (p + α) and lnA are practically the same for initial condition variants discussed above. Let us remark here that the mass composition evolution obtained in this work is in good agreement with averaged results reported in [6].\n\nMentions:\n- In Fig. 9 the fraction of light nuclei and the mass number average logarithm are presented for two variants of initial conditions (V.4) and (V.18).\n\nOCR Text: \"An47 scenment Pev {0+\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 9: The summary fraction of light nuclei (p + α) and the mass number average logarithm vs energy per a nucleus obtained by means of experimental data processing shown in Fig.1. The reconstruction is performed with the quadratic logarithmic model (V.11), (V.5): (1) — initial conditions (V.4) with parameters (V.17), (2) — initial conditions (V.18) with parameters (V.19)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To select a basis for 1 -minimization we employ a four step iterative procedure involving restriction, expansion, identification and selection. The iterative basis selection 3,1 . For clarity we only depict one connection per node, but in d dimensions a node of a given degree p will be a child of up to d nodes of degree p´1. For example, not only is the basis φ r1,1s a child of φ r1,0s (as depicted) but it is also a child of φ r0,1s procedure is outlined in Algorithm 1. A graphical version of the algorithm is also presented in Figure 6. The latter emphasizes the four stages of basis selection, that is restriction, growth, identification and selection. These four stages are also highlighted in Algorithm 1 using the corresponding colors in Figure 6.\n\nMentions:\n- A graphical version of the algorithm is also presented in Figure 6.\n- These four stages are also highlighted in Algorithm 1 using the corresponding colors in Figure 6.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6: Graphical depiction of the basis adaptation algorithm."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Figure 4, we show the observed fluence of FRB 200428 as a function of frequency, which suggests a fluence dependence of approximately F ∝ ν 0.46 . In addition, Figure 4 shows the observed fluences of the Xray bursts reported within our observing session along with our NIR fluence limits. While the spectrum of the X-ray burst for FRB 200428 remains unconstrained below ∼ 1 keV, Figure 4 shows that our observations had the sensitivity to detect (at ∼ 30 σ confidence) a NIR counterpart of FRB 200428 if PGIR was observing at the time of the burst and the emission was characterized by a continuous power law extending from X-ray to radio frequencies. We note that the BOOTES limit from Lin et al. 2020b does not rule this out. However, we caution that the observed radio spectrum of FRB 200428 shows signatures of narrow bandwidth fluctuations (The CHIME/FRB Collaboration et al. 2020;Bochenek et al. 2020). Hence, the SED of FRB 200428 may not be consistent with a single power law extending from X-ray to radio frequencies, although propagation effects may affect this interpretation.\n\nMentions:\n- In Figure 4, we show the observed fluence of FRB 200428 as a function of frequency, which suggests a fluence dependence of approximately F ∝ ν 0.46 .\n- In addition, Figure 4 shows the observed fluences of the Xray bursts reported within our observing session along with our NIR fluence limits.\n- While the spectrum of the X-ray burst for FRB 200428 remains unconstrained below ∼ 1 keV, Figure 4 shows that our observations had the sensitivity to detect (at ∼ 30 σ confidence) a NIR counterpart of FRB 200428 if PGIR was observing at the time of the burst and the emission was characterized by a continuous power law extending from X-ray to radio frequencies.\n\nOCR Text: \"148 FF CHFE FFE Ai;i' ST?FE? -FP ECIE E x\"m6 1 Frequency {Hzj\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4. Constraints on the NIR fluence of X-ray bursts from SGR 1935+2154 based on limits from Palomar Gattini-IR. For comparison, we show the observed X-ray and radio fluence of FRB 200428 (in red) detected by CHIME and STARE2 in coincidence with a hard X-ray burst detected by HXMT, INTEGRAL, AGILE and Konus-Wind. Optical limit from the BOOTES telescope for FRB 200428 is also shown. Limits from the PGIR campaign are shown in orange, with transparent circles showing raw limits, while solid squares show extinction corrected limits for AJ = 2.0 mag. The four X-ray bursts detected by HXMT and NuSTAR during the PGIR observing sessions (labels indicated as per Table 2) are shown with yellow dashed lines connecting the corresponding NIR limits. For each X-ray burst, we place the fluence at the frequency corresponding to the peak of the fluence spectrum (= νfν) observed in the X-rays (and its uncertainty). The estimated fluence peak for bursts A and B are consistent with the observed spectrum of the bursts from NuSTAR observations (Borghese et al. 2020), where the fluence is observed to rise up to at least ∼ 20 keV. In the case of burst C and D, the fluence spectrum was not well constrained and hence are placed nominally at 10 keV with error-bars spanning the full HXMT sensitivity range. The observed fluence index for FRB 200428, and constraints derived for the X-ray bursts are shown."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- with E in V/cm and R s , ∆R, R + and R − in km. In Fig. 2 we plot Eq. ( 14) for a strange star with 10 km radius and with the surface electric fields indicated. Figure 2 indicates that the increase in gravitational mass of strange stars, resulting from the energy associated with the electric dipole layer on the surface, is very small. We can conclude then that only when the star possess a net charge one might find a significant contribution to the gravitational mass, due to a macroscopic extension of the electric field outside of the star.\n\nMentions:\n- In Fig. 2 we plot Eq.\n- Figure 2 indicates that the increase in gravitational mass of strange stars, resulting from the energy associated with the electric dipole layer on the surface, is very small.\n\nOCR Text: \"#R ikmi\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 2. (color online) Increase of gravitational mass due to electric dipole energy, as a function of dipole width. The calculations are performed for a strange star with radius of 10 km and for the surface electric fields indicated."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The experiment was performed at the HIRFL-CSR in Lanzhou, China [8]. In this experiment, we aimed at studying the moderated neutron spectra of the spallation reaction in water bath. For this purpose, a proton beam with energy of 250 MeV was used to bombard a cylindrical lead target. The beam was collimated on the centre of front surface of the target and the beam spot was less than 2 cm in diameter. The irradiation of the beam lasted about 24 hours to get sufficient fluence. In front of the target, an air ionization chamber was placed to monitor the beam current. The ionization chamber which wasn't calibrated could only show the relative amount of the beam current as function of time. The total number of protons in the process of irradiation was given by activation analysis via the yield of the reaction 27 Al(p, 3p1n) 24 Na. The Al-foil with highpurity and thin 800 µm was placed immediately in front of the ionization chamber. The cross section had been measured to be 10.6 mb in reference [9]. The total number of protons was determined to be (1.69±0.05) × 10 12 . With the aid of activity analysis of Al foil, the ionization chamber was calibrated and the absolute beam current I beam (t) was obtained, see Fig. 1. The Pb target with 10 cm long by 10 cm diameter was surrounded by a tank of water. The dimensions of the container were 120 cm (L)×100 cm (W)×100 cm (H), where this size effectively contained most of the neutrons emitted from the target. Along the surface of the target, the activation foils of Au, Mn and In were located in the water in order to measure the moderated neutrons. The detailed description for the foils is summarized in Table . 1. The foils were covered with Cd (0.8 mm thickness) to shield the thermal neutrons. All the foils were divided into three groups, and each group consisted of a piece of Au, Mn, and In foils, respectively. Three groups of foils were placed at a radial distance of 8 cm from the target axis and at the longitudinal distances of -10, 0 and 10 cm from the center of the target, see Fig. 2. The neutrons emitted from target were slow down in the water and measured by the resonance detectors. Via capture reaction (n,γ), the stable isotopes composing of the detector foils were transmuted into radioactive ones, which were identified by observing the characteristic γ rays. The measurements were performed by using high-purity germanium (HPGe) detectors. The HPGe detector relative efficiency was about 65% and energy resolutions was 1.90 keV at 1.33 MeV. The absolute efficiency was calibrated with the standard sources 60 Co, 133 Ba, 137 Cs, and 152 Eu.\n\nMentions:\n- With the aid of activity analysis of Al foil, the ionization chamber was calibrated and the absolute beam current I beam (t) was obtained, see Fig. 1.\n\nOCR Text: \"Eelo ae 6 6\"\" 8 neIn\" Time 51\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1. Course of irradiation with 250 MeV protons."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Actually, since (1 − t)η 1 + tη 2 = ((1 − 2t)α, α), the set GC p (S) consists of the points exp p ((1 − 2t)α, α), with t ∈ [0, 1], see Figure 1. Note that the above fact also shows that the curve\n\nMentions:\n- Actually, since (1 − t)η 1 + tη 2 = ((1 − 2t)α, α), the set GC p (S) consists of the points exp p ((1 − 2t)α, α), with t ∈ [0, 1], see Figure 1.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Difference between convex hull and geodesic convex hull."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In appendix A, we present the minimum separation ξ min as a function of the initial offset b in , which is consistent with the attenuation of particle inertia and the presence of lift forces (figures 14, 15 and the accompanying discussion). Figure 7 shows that the minimum separation decreases for smaller apertures of the pinching gap for a constant magnitude of inertia. Therefore, similar to the discussion in §4.1, we can infer that the critical initial-offset b c,in increases with pinching. Further, the minimum separation also decreases with an increasing magnitude of inertia, for a fixed aperture of the pinching gap. Thus, the critical initial-offset increases with increasing particle inertia (see figures 11 and 14-16 in appendix A). Since b c,out is a monotonically increasing function of b c,in , one might expect that b c,out also increases with pinching and inertia. On the other hand, figure 8 shows that for a constant b in , the final offset decreases due to the effect of inertia. Due to such competing effects, the behavior of the critical final-offset as a function of the aperture of the pinching gap and inertia is not obvious. Our simulations indicate, as shown in figure 9, that the critical final-offset b c,out indeed increases with inertia, but the change becomes substantially smaller in the presence of pinching, provided that the effect of lift forces generated due to the pinching wall are negligible (see also ξ min as a function of b out in figures 20 and 21 in appendix A).\n\nMentions:\n- Our simulations indicate, as shown in figure 9, that the critical final-offset b c,out indeed increases with inertia, but the change becomes substantially smaller in the presence of pinching, provided that the effect of lift forces generated due to the pinching wall are negligible (see also ξ min as a function of b out in figures 20 and 21 in appendix A).\n\nOCR Text: \"Cm;Ai 033} C=03\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 9. The critical final-offset bc,out corresponding to = 0.3, as a function of Reynolds and Stokes numbers, in the presence and the absence of a pinching wall."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The proposed approach greatly improves the hit rate. Our key finding from the simulation shows that we predict a reduced rate of costly unnecessary replacements visits from 18.8% (actual) to 2.0% (proposed). Figure 9 illustrates the direct comparison of hit rates for our proposed approach, IWAL(0.7), based on our ActualFlint simulation, compared to Flint FAST Start.\n\nMentions:\n- Figure 9 illustrates the direct comparison of hit rates for our proposed approach, IWAL(0.7), based on our ActualFlint simulation, compared to Flint FAST Start.\n\nOCR Text: \"1 2 1 1\"Q0 2010 M0Q 43D0 Number af homes Ms ted aclual iwalc70)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 9: Tracking hit rates over time, the proposed IWAL algorithm (blue; mean = 98.0%) outperform actual (green; mean = 81.2%; thick line is smoothed plot)"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 1 shows the calculated densities of states (DOS) in the ferromagnetic Fe. Thermal spin fluctuations in the static approximation broaden the band width for each spin band. The DOS with the 4th-order dynamical charge and spin fluctuations shifts the main peak towards the Fermi level and causes a small hump around ω = −0.5 Ry which corresponds to the '6 eV' satellite as found in Ni at 6 eV below the Fermi level. [59][60][61][62][63] The present calculations with dynamical effects yields the ground-state magnetization 2.58 µ B , which is considerably larger than the experimental value 2.22 µ B . The exchange splitting in the present calculations is therefore somewhat overestimated. When the exchange splitting is reduced, the main peak in the up spin band shifts up and the second peak of the down spin band shifts down, so that one expects that the main peak around ω = −0.1 Ry in the total DOS becomes sharper and shifts towards the Fermi level.\n\nMentions:\n- Figure 1 shows the calculated densities of states (DOS) in the ferromagnetic Fe.\n\nOCR Text: \"dun ~Tc-0.7 enfrt\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1. Up and down d partial densities of states (DOS) of Fe in the static approximation (dotted curves) and in the 4th-order dynamical CPA (solid curves). The total DOS in the 4th-order dynamical CPA is shown by dashed curve."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The systematic error is given by the mean absolute deviation from the central value and should reflect the uncertainties from residual cutoff and strange quark mass effects. In Fig. 3 we show a compilation of the results from a constant fit for all definitions of f 0 and the two methods for Z P /Z S . Within errors we find good agreement with the result from pure Yang-Mills theory, which is also included in the plot. Another possibility to deal with the residual effects of quark mass dependence and the lattice spacing is to include additional, higher order terms in the fit function\n\nMentions:\n- In Fig. 3 we show a compilation of the results from a constant fit for all definitions of f 0 and the two methods for Z P /Z S .\n\nOCR Text: \"Ie:\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Results from pure Yang-Mills theory (YM) and dynamical simulations. Open and closed symbols correspond to the two sets of values forZP/ZS (M1, M2). The three definitions D1, D2 and D3 forf0 are indicated by circle, triangle and diamond symbols, respectiv ly. The solid black line represent the final, p–value weighted average from dynamical simulations and itsstatistical error is given by the dotted lines. The gray band represents its systematic error; see text."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 3, where we plot the susceptibilities for a system of N = 4 ceRNAs. One sees that different interactions are switched on in different ranges of values for the miRNA transcription rate, leading to a gradual modification of the structure of the interaction network as β changes. Notice that heterogeneity in the quantities µ 0,i leads to interaction asymmetry. A schematic summary of the cross-talk in this system is given in Figure 4.\n\nMentions:\n- Figure 3, where we plot the susceptibilities for a system of N = 4 ceRNAs.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 3. Susceptibilities χij in a system of N = 4 ceRNAs, as a function of miRNA transcription rate β (all other parameters being fixed). In this example ceRNAs are cast in two groups: group A, formed by ceRNAs m1 and m3, and group B, formed by ceRNAs m2 and m4. CeRNAs belonging to the same group share identical kinetic parameters. In particular, µ0,1 = µ0,3 µ0,2 = µ0,4. While for β smaller than about 500 no cross-talk is observed, as β increases a symmetric interaction between ceRNAs in group A (of magnitude comparable to the self-susceptibilities) appears: χ31 = χ13 ' χ11 = χ33. As β increases further, this interaction is switched off, and ceRNAs in group B begin to crosstalk instead: χ42 = χ24 ' χ22 = χ44. In this region, a change of transcription of a ceRNA in group A can affect the level of ceRNAs in group B, but not viceversa (asymmetric crosstalk): χ21 = χ23 = χ41 = χ43 >> χ12 = χ32 = χ14 = χ34. Finally, for sufficiently large β all no cross-talk takes place."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Table 5: The point transformations between difference equations from Tables 3 and 4 Figure 3: Intersecting lines initial value problem\n\nMentions:\n- Table 5: The point transformations between difference equations from Tables 3 and 4 Figure 3: Intersecting lines initial value problem\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Intersecting lines initial value problem"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The minimize function for the six orbital elements as a function of the mass of SgrA* returns the residuals squared of the (initial guess) parameters. The likelihood function can be interpreted as an indicator for the goodness of the fit (Parsa et al. 2017) since it calculates the sum of the squared residuals. From the minimized and optimized orbital fit parameters, we are introducing a variation of the SMBH mass. Based on the analysis and the fit, the resulting plot of the likelihood function  (2019). The uncertainties of the mass is derived from the range of mass values for which the variation of the value of the χ 2 value is below unity. Hence, we find for the central mass (4.15 ± 0.6) × 10 6 M (see Fig. 7). This value is in good agreement with the mass derived by Gravity Collaboration et al. (2018aCollaboration et al. ( ,b, 2019.\n\nMentions:\n- Hence, we find for the central mass (4.15 ± 0.6) × 10 6 M (see Fig. 7).\n\nOCR Text: \"0~6# ;un.\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 7. Likelihood as a function of mass. We normalized the resulting likelihood fit values to the minimum value of the mass variation analysis."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- minosity outshines the galaxy. In Fig. 8, we compare these results with those in our simulation, shown as filled circles.\n- • Initially, the BH growth lags behind the galaxy growth, in relative terms, while later the BH grows faster than the galaxy, also in relative terms (Fig. 8).\n\nMentions:\n- In Fig. 8, we compare these results with those in our simulation, shown as filled circles.\n- • Initially, the BH growth lags behind the galaxy growth, in relative terms, while later the BH grows faster than the galaxy, also in relative terms (Fig. 8).\n\nOCR Text: \"J= -j= cMijer_i 717 Fjet\" ii Wnt GB-MlM:e Rlet Ta Sjne'\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 8. MBH accretion rate versus SFR for the main galaxy. The empty symbols correspond to the observations by Wang et al. (2011), Mullaney et al. (2012), Netzer et al. (2014), and Willott et al. (2015), while the filled circles correspond to our simulation, with the colour representing the redshift and the size the MBH mass. At earlier times, the MBH does not grow significantly, and the simulation shows properties similar to those of galaxies at z ∼ 1 − 2. At later times, instead, when the MBH starts to grow at the Eddington limit, the MBH accretion rate moves upward towards the data of high redshift quasars (z = 6 and z = 4.8)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The approximation is valid for p close the one. This means that in the weak noise regime and in this subspace, the effective noise parameter is suppressed with the third power. We numerically find that the effective noise parameters for a cluster-ring encoding in the logical subspace is again white noise for all m. Compared to the physical noise, the effective error rates are reduced in the weak noise regime. The same is true for the mean noise channel ( 9), which is numerically calculated. However, the averaging diminishes the positive effect of the encoding. For both methods, see Fig. 3 for an illustration. Interestingly, the mean error rates µ i&gt;0 are close to a simple estimate: If p is the probability for no error on one such qubit, the overall probability to have at most one error equals p eff = p 5 + 5p 4 (1 − p). This can be seen on the logical level of having no error [19]). One finds that µ i&gt;0 ≈ (1 − p eff )/4.\n\nMentions:\n- For both methods, see Fig. 3 for an illustration.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Effective error rates λ̃(0)i>0 and µi>0 for the clusterring code with m = 5 and physical white noise. Since the effective noise is again of white noise structure, they equal for all i. Both effective models give rise to a reduced effective error rate for small physical noise rates. The mean channel is compared to a simple estimate on the effective error rate peff (see text)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- gain compared with the respective C2D baseline on Jester in Figure 8 and Something-Something in Figure 10.\n\nMentions:\n- gain compared with the respective C2D baseline on Jester in Figure 8 and Something-Something in Figure 10.\n\nOCR Text: \"Mlirt Rakef | aan Tam ?A4 X Tcltt Amene diuh 7a SN Mimact CClAeelcsliac Et Toeati ad [ea A EATe Fineea dilen- Ading Fincee Salc Ei Sedi AECCn-M2 Uih Fu= tafming Hend Gcatel juin Juo hncor; Ra4t SaLflea 4a1 7 Dr Cun\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 8: Per-class top-1 accuracy gain in percentage on Jester v1 dataset due to CP module."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The dataset contains Tweets from 3,764,507 unique users. The dataset was collected using the Twitter API via the Spark framework for data analytics, as explained in Appendix C. It is interesting to first consider what fraction of geotagged tweets contain words or sequences of words that can be interpreted (using a knowledge base) as locations within a reasonable distance of the location the tweet was sent from. In particular, we considered a 10% sample of the February 2015 tweets we collected, consisting of 3,466,922 geotagged tweets from our larger dataset. For each tweet we considered all word sequences with up to 5 words (no POS or NER was used; we just considered every possible sequence of 5 or less consecutive words). We then did a case-insensitive search through all locations in the GeoNames knowledge base (http://www.geonames.org). This search included \"alternative names\" for locations with more than one name. We measured the distance between each match and the coordinates of the tweet, and kept the shortest distance for each tweet. Figure 7 shows a cumulative plot of the distance between the tweet coordinates and the closest mention in the tweet text, as a function of the fraction of tweets. We see immediately that few tweets contain location references in the tweet text that are close to the tweet coordinates. In particular, we find that • 82.5% of tweets contain a word sequence that matches a location in GeoNames • 2.2% of tweets mention a location within 10 km • 5.4% of tweets mention a location within 100 km • 8.3% of tweets mention a location within 161 km (100 miles) This shows that we cannot expect high accuracy when attempting to geolocate tweets and comparing to the location the tweet was sent from. Nevertheless, it is interesting to see whether our approach manages to correctly geolocate tweets that do contain relevant location information in the text. (Note that we used GeoNames instead of OpenStreetMap for this particular test, since the GeoNames data was in a more practical format for our current purposes. The general points we made here do not depend on whether one uses GeoNames or OpenStreetMap.)\n\nMentions:\n- Figure 7 shows a cumulative plot of the distance between the tweet coordinates and the closest mention in the tweet text, as a function of the fraction of tweets.\n\nOCR Text: \"Mateeat\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 7. Twitter tests. Cumulative plot of the distance between the coordinates of a sample of geotagged tweets, and the closest mention of a location in the tweet text (that is present in the GeoNames knowledge base), as a function of the fraction of tweets."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- If we increase the value of A (which corresponds to increase the initial concentration of H + ions), the \"asymptotic value\" of s -i.e. the thickness of the slab whenṡ vanishes -becomes smaller, as shown in Fig. 6. This is physically consistent since a more acid solution is expected to erode a larger part of the slab.\n\nMentions:\n- If we increase the value of A (which corresponds to increase the initial concentration of H + ions), the \"asymptotic value\" of s -i.e. the thickness of the slab whenṡ vanishes -becomes smaller, as shown in Fig. 6.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6: Free boundary x = s(t) for non constant g and A = 1, 2, 3, 4, 5."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The largest contribution to the combinatorial background, originating from the sequential pairings between oppositely charged dielectron pairs (see Sec. 3.4), is given by conversion electrons. These are more than 70% of all electrons in the selected track sample. Since the signal-to-background ratio is of the order of 10 −3 − 10 −2 in the low-mass region (0.2 &lt; m ee &lt; 1 GeV/c 2 ), the suppression of the largest background contributor is a crucial aspect for the dielectron measurement. Two methods for rejecting electrons from photon conversion in the detector material are applied. A single-track conversion rejection is complemented by a pair-rejection method which exploits the orientation of the plane spanned by the electron-positron pair produced by a photon conversion with respect to the magnetic field. Conversion rejection on a single-track level is based on the properties of the track reconstruction algorithm. All tracks are propagated down to the point of closest approach to the primary vertex. In the case of secondary tracks, this inward propagation results in wrong cluster associations in the ITS causing some distortions in the reconstructed track and in the measured momentum. Secondary tracks have shared clusters in the ITS with other primary tracks. The average fraction of shared clusters for secondary tracks increases for larger distances between the true production vertex of conversion electrons and the primary vertex, as shown in Fig. 3. A maximum 40% fraction of shared clusters in the ITS is allowed for all reconstructed tracks. This requirement represents the best compromise between the maximization of the photon conversion rejection and the minimization of signal loss. The single-track rejection method, which includes the requirement on the maximum fraction of shared ITS clusters, the requirement of having a cluster in the first ITS layer and the selection of tracks based on their DCA (see Sec. 3.1), is very efficient in rejecting single electrons from conversion in the case of a non-reconstructed partner. A large suppression of conversions is achieved by this technique, amounting to about 80% (60%) at low (high) p T,e , with a loss of signal tracks of around 15% approximately constant with p T,e (Fig. 5-B).\n\nMentions:\n- The average fraction of shared clusters for secondary tracks increases for larger distances between the true production vertex of conversion electrons and the primary vertex, as shown in Fig. 3.\n\nOCR Text: \"CE Sirvaor,Pr-P: 278 Tev;c-1O 30   35 ich `\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3: Average fraction of shared clusters in the ITS for secondary tracks as a function of the radial distance (R) between the true production vertex and the primary vertex obtained from MC simulations."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Vehicle flow, velocity, and density are related by Q = v eq ρ which results in the diagram in Fig. 4 relating the traffic flow Q to the density ρ. This graph is also one of the fundamental As is expected from a traffic flow model, after a peak density matching the available capacity is reached, traffic flow starts deteriorating in the sense that any more vehicles only serves to slow down every vehicle. Before this peak, the traffic is in the free flow regime and then switches to congested.\n\nMentions:\n- Vehicle flow, velocity, and density are related by Q = v eq ρ which results in the diagram in Fig. 4 relating the traffic flow Q to the density ρ.\n\nOCR Text: \"Wansit ol echiles ivert '\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4. Vehicle flow versus density: For our microscopic model, this graph shows the macroscopic relationship between the number of identical vehicles passing a fixed point on a ring road per unit of time and the density of vehicles. As is expected from a traffic flow model, after a peak density matching the available capacity is reached, traffic flow starts deteriorating in the sense that any more vehicles only serves to slow down every vehicle. Before this peak, the traffic is in the free flow regime and then switches to congested."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Initial antenna testing were done by directly connecting it to the VNA and placing it in the hand just to test that the whole fabrication process goes well. In the next step, minced pork meat placed in a plastic container ( r is roughly equal to 1.5). Since the r of the antenna substrate is much higher and the distance between the boundary of container is higher compared to the antenna size, the thin plastic container with low r was not taken into consideration while simulations. The antenna was carefully placed in the middle half, and the was fully covered with minced meat to form a block where the antenna is approximately in the middle. The cable connecting the antenna to the VNA was carefully attached to a stand to ensure proper isolation between the AUT and the measurement equipment. Figure 4.8 shows the pictures taken during the measurement process. The results presented in Figure4.9 indicates a good agreement between the simulated and the measurement results. This ensures the proper fabrication and measurement process in which even minor details like milling bit selection and antenna layering selection was carefully monitored. Some of the non-radiating poor resonances occur in between MICS and ISM band. These resonances appear commonly in wideband electrically small antennas [97] but due to their non-radiating nature, they do not have a major impact in radiation efficiency of the antenna for the desired bands. The fabricated antenna retains the predicted impedance bandwidth, both for ISM and MICS bands.\n\nMentions:\n- The results presented in Figure4.9 indicates a good agreement between the simulated and the measurement results.\n\nOCR Text: \"\"b] H21ta6\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4.9: Comparison between simulated and measured results for the antenna prepared for minced pork meat implant."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We define the NB921 emitters as those satisfying z ′ −NB921&gt;0.2 and z ′ −NB921&gt;3σ (see the magenta crosses above the z ′ −NB921=0.2 line and z ′ −NB921=+3σ curve in Fig. 2). We detect 724 NB921 emitters in total, but a part of these emitters are not necessarily Hα emitters at z = 0.4 since Hβ/[Oiii] emitters at z ∼ 0.8 and [Oii] emitters at z ∼ 1.45 can also be detected as NB921 emitters. However, these contaminations can be relatively easily eliminated based on their broad-band colors. In Fig. 3, we show the B − R v.s. R − z ′ color-color diagram. We here plot galaxies with 0.30 &lt; z phot &lt; 0.45 (gray dots) and all the NB921 emitters selected above (magenta crosses). We also show the model prediction of colors for galaxies at z =0.4, 0.8 and  Kodama et al. (1999). This color combination clearly separates the Hα emitters at z ∼ 0.4 from other major line emitters at other redshifts, and in fact, these colors are also used in Kodama et al. (2004) in order to identify Hα emitters at a similar redshift (z = 0.39). We define the NB921 emitters distributed in the closed box in Fig. 3 as Hα emitters at z = 0.4. We note that this boundary is set by eye, and we may miss a small fraction of real members near the boundary. However, a small change of this boundary does not change our results at all (see similar selection method in Kodama et al. 2004 andKoyama et al. 2010). Also, this color selection can be applied only for the galaxies that are detected in all BRz ′ -bands. Therefore, we do not include any Hα emitter if it is not detected in any of these bands. However, the quantitative analyses presented in this paper are mainly based on bright galaxies with z ′ &lt; 23 mag, and are not affected by the faint undetected objects. After application of all these color cuts, we are left with 445 Hα emitters in total in and around the Abell 851 cluster throughout the observed field. This is one of the largest samples of Hα emitters currently available for distant clusters.\n- where N Hα , N member and N cont represent the numbers of Hα emitters, all member galaxies (phot-z members or Hα emitters) and the expected contaminations (i.e. N cont = ρ cont × S, where S is the surface area of each environment), respectively. In this calculation, we limit our sample to the galaxies with z ′ ≤ 23 mag, since the selection of Hα emitters becomes incomplete below this magnitude as can be seen in Fig. 3. In Fig. 6, filled circles with error bars indicate the values after the statistical subtraction, while open triangles show those be- fore subtraction. It can be seen that the Hα fractions are almost unchanged after statistical subtraction in the dense cluster central region, while the contribution of contaminant galaxies gets larger in poorer environments. Note that, since we can not calculate the Hα fraction for the field environment by definition (see equation 4), we present instead the blue galaxy fraction calculated based on the phot-z selected galaxies in the field environment as a rough estimate of the fraction of star forming galaxies. This assumption would be reasonable because we find that the fraction of Hα emitter among blue galaxies is ∼ 1 at all environments after statistical correction. However, as will be discussed in the next section, there must be Hα emitters with red colors as well. Therefore, the blue galaxy fraction shown here for field environment may be a lower limit of the Hα fraction. We also note that, although our \"Field\" environment avoids any prominent structures, it is still located near the rich clus-ter. This may lead to an overestimation of contaminant galaxies in the above calculation. However, our results including the \"relative\" trends seen in Fig. 6 would not change even if we do not apply any correction for the contamination.\n- Note that we include all galaxies with z ′ −NB921&gt;3σ that satisfy the same color selection criteria as in Fig. 3. If we use only the secure Hα emitters as defined in this study (i.e. z ′ −NB921&gt;3σ and z ′ −NB921&gt;0.2), these values would become smaller by a factor of ∼2 (lower limit), and if we use all galaxies with z ′ −NB921&gt;0, these values would increase by ∼25% (upper limit). Although this uncertainty is large, this does not affect our main conclusion. We expect that the uncertainty regarding the dust extinction may also be very large. In fact, we derive ΣSFR(IR)∼ 370M ⊙ /yr by just summing up the IRderived SFRs of the 16 MIPS-detected spec-z members listed in Table 2, which is clearly larger than ΣSFR(Hα) derived above. Although we use a constant 1 mag extinction correction in this paper to make a fair comparison with the values in other works in the literature, it is also essential to study such obscured/hidden activities more in detail with the MIR-FIR observations (see some examples of such IR studies of the global evolution of star forming activities in clusters by e.g. Geach et al. 2006;Bai et al. 2007;Koyama et al. 2010;Haines et al. 2009a;Chung et al. 2011).\n\nMentions:\n- In Fig. 3, we show the B − R v.s. R − z ′ color-color diagram.\n- We define the NB921 emitters distributed in the closed box in Fig. 3 as Hα emitters at z = 0.4.\n- In this calculation, we limit our sample to the galaxies with z ′ ≤ 23 mag, since the selection of Hα emitters becomes incomplete below this magnitude as can be seen in Fig. 3.\n- Note that we include all galaxies with z ′ −NB921&gt;3σ that satisfy the same color selection criteria as in Fig. 3.\n\nOCR Text: \"HG / [OIII] E(B-V)=D2 (on] 2=1.46 -0.5 05 2.5 R-2' Inal\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3.— The color–color diagram to separate Hα emitters at z = 0.4 from other major line contaminations at other redshifts (Hβ/[Oiii] emitters at z ∼ 0.8 and [OII] emitters at z ∼ 1.45). Gray dots represent phot-z selected member galaxies and magenta crosses indicate all NB921 emitters selected in Fig. 2. Three solidline curves indicate color tracks of galaxies at z =0.4, 0.8 and 1.45, based on the model prediction of Kodama et al. (1999). Along each track, we change the fraction of bulge contribution to the total light from 0.0 to 1.0 (from blue side to red side). We define the NB921 emitters (i.e. magenta crosses) within the blue-line closed box as Hα emitters at z = 0.4, while emitters outside the box as contaminant line emitters. We also show the reddening vector corresponding to E(B − V ) = 0.2 for z = 0.4 galaxies, calculated from the extinction law of Calzetti et al. (2000)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- it is found that (22) and (24) are surprisingly similar. In both cases, the α s uncertainty is substantially larger than the higher-order one -especially given the recent increase in the α s uncertainty provided by the PDG [17] -which underlines the good potential of α s extractions from hadronic τ decays. In figure 5, we again provide partial sums up to order n of the perturbative series for δ (0) FO (â M τ ) at different values of C. The colour coding is equivalent to figure 3: C = 1 (violet); C = 0 (black); CONF12 C = −0.5 (blue); C = −0.882 (red); C = −1 (green); C = −1.5 (cyan). The C = 0 series is close to the corresponding one in the MS scheme. Although the general perturbative behaviour is similar to the Adler function, one observes that for δ (0) FO (â M τ ) it is substantially smoother. This is due to a suppression of the leading IR renormalon pole related to the gluon condensate. For larger C, the onset of the asymptotic behaviour is delayed, however the series needs more terms to approach the resummed result. For smaller C, and this is the case in particular for C ≈ −1, the onset of the asymptotic behaviour is earlier, but also the series requires less terms to come close to the \"true\" value. In the region around C = −0.882 (red and green curves), a satisfactory approach to the Borel sum is achieved with just the four analytically available orders. If C is still taken to be smaller, the behaviour of the series deteriorates, and error estimates from small terms become larger.\n\nMentions:\n- In figure 5, we again provide partial sums up to order n of the perturbative series for δ (0) FO (â M τ ) at different values of C.\n\nOCR Text: \"KdX;' WG n\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5. Partial sums up to order n of the perturbative series for δ(0)FO(âMτ ) for different values of C: C = 1 (violet); C = 0 (black); C = −0.5 (blue); C = −0.882 (red); C = −1 (green); C = −1.5 (cyan). The grey band corresponds to the Borel sum according to the central model of ref. [18]. For further discussion see the text."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- It is well known that σ 0 is a function of the broadening mechanisms present in the atmosphere of the star, except rotation (Melo et al. 2004). Since the broadening mechanisms are a function of the temperature and gravity, we may expect the colour index, B-V to depend on σ 0 , for stars in the main sequence. To determine this dependence, we created morphed spectra with no rotational velocity using the ATLAS9 code by Kurucz (1993) adapted to operate on a linux platform by Sbordone et al. (2004) and Sbordone (2005). Temperatures vary from 3500 K to 6500 K (in intervals of 250 K). Since the stars were selected to be on the main sequence, log g was fixed to be 4.5. Solar metallicity was assumed. The morphed spectra were broadened to match the instrumental profile of the real spectra using the FWHM of the calibration arc lines, which is a good approximation of the broadening for the instruments used. Given that each spectrograph has a different resolving power, different instrumental broadenings were applied to obtain a calibration curve for each instrument. These curves are shown in Fig. 1.\n\nMentions:\n- These curves are shown in Fig. 1.\n\nOCR Text: \"SwMurvsaSA uekwhhyi WM IUsysMo Yp Fun WAb #: Rya HePeu Sab Uzsp Hhhubuh R Za kal UUU Temuhehhre peheyih: ywa M6aHeNayebadha_whaGODh TBL Hbpes\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1. Calibration betweenσ0 and colour indexB–V , whereσ0 represents the “natural” broadening of the spectrum lines and wasobtained from synthetic spectra covering the spectral range of the obs rved stars. Given that the instrumental profile also contributes as a broadening mechanism and considering that each spectrograph has a different one,"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The performance of the proposed white-box attacks is compared in Fig. 3 under the same simulation environment used in Section IV-D. As we discussed in Section IV-B, γ can be optimized with line search for the targeted MMSE attack, e.g., it performs better with γ = 1.2 compared to γ = 1. It is observed that the naive non-targeted attack performs poorly compared to other attacks and the non-targeted MRPP attack outperforms all other attacks in most of the PNR region. This can be explained by the freedom of the direction that the non-targeted adversarial attack can take. For targeted attacks, we can only have 10 different directions since we have 11 modulation types. However, the non-targeted attack does not have such a restriction. Thus, it is more likely that the non-targeted attack chooses a better direction to enforce misclassification. Moreover, the computation complexity for non-targeted attacks is lower compared to the targeted attacks that involve iterations to reach the desired accuracy.\n\nMentions:\n- The performance of the proposed white-box attacks is compared in Fig. 3 under the same simulation environment used in Section IV-D.\n\nOCR Text: \"MMASE wrhx = MpSE #ty=] MRPF MpS # hy=12 MRFP FhPb\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3. Classifier accuracy under different white-box adversarial attacks when SNR = 10 dB."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To model the GaBi bandstructure, we used the GGA-WC exchange correlation functional as explained above. In the first step, the lattice constant of zinc-blend GaBi is optimized taking into account spin-orbit coupling. The result found here of a 0 = 6.368Å is consistent with previous DFT calculations: 6.324, 6.28 and 6.47Å [54,57]. This gives a lattice mismatch of 12% relative to GaAs (Tabs. II and III), which hints that Bi atoms embedded in GaAs host lattices will give rise to large strains. This will be confirmed in the next section. Here, we used this a 0 to calculate the GaBi band structure shown in Fig. 1. Relativistic effects play an important role in the electronic structure of GaBi that is evident from a large spin-orbit splitting of E SO = 1.80 eV. We find an inverted band structure with the Γ 6c state positioned energetically below the Γ 8v state, in agreement with Janotti et al. [54]. This arrangement results in a negative band gap of E g = −1.65 eV at Γ  b The strain is calculated with respect to the equilibrium bond length (r 0 ) in GaAs.\n\nMentions:\n- Here, we used this a 0 to calculate the GaBi band structure shown in Fig. 1.\n\nOCR Text: \"3 E Wave vector\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 1. Band structure of zinc-blend GaBi obtained with GGA-WC-TBmBJ. Energies are plotted relative to the Fermi energy."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To observe the diffusion process of weights, we calculate Euclidean distances of weight vectors from the initial weight vector for each iteration. We also used the best results from large batch method and small batch method as initial weights to train model for validating the randomness level around local minimum. As illustrated in Figure 3 and 4, our experiments got similar generalization performance gap with Keskar et al. in [23] which leads to high training accuracy but low test accuracy respectively, particularly by using large batch method. The gap between training accuracy and test accuracy becomes wider as the batch size becomes larger. In Figure 1 and 2, the small batch method's weights go further than large batch method's weights at the same iteration of updated weights. This result is similar with Hoffer et al. in [18]. Different batch size methods have different diffusion rates and the distances between initial weights and weights at the end of training process are different. According to our theory, this phenomenon illustrates that small batch size method has high level of omnidirectional noise which enables weights to random walk with large range of surface of nonconvex loss function. Essentially, the Ghost Batch Normalization [18] which is to acquire statistics on partial batch rather than the full batch statistic increases the randomness level. Therefore, we infer that at the end of training process large batch method tends to converge to sharper minimizers which has lower test accuracy [23] because of low diffusivity. As shown in Figure 5 and 6, we can confirm small batch method has larger diffusion rate than large batch method and by vibrating test accuracy one could get a better result. As illustrated in Figure 5, when the weights come to a flat area relatively, large batch method tends to find the global minimizers or flat local minimizers rather than small batch method with random walk. The only way to find a better result than large batch method by using small batch method is training longer. It increases the probability of getting a good result during a training process. On the contrary, in Figure 6 when the weights come to a sharp area relatively, small batch method make weights escape from sharp minimizers.\n\nMentions:\n- As shown in Figure 5 and 6, we can confirm small batch method has larger diffusion rate than large batch method and by vibrating test accuracy one could get a better result.\n- As illustrated in Figure 5, when the weights come to a flat area relatively, large batch method tends to find the global minimizers or flat local minimizers rather than small batch method with random walk.\n\nOCR Text: \"Ba.nf@2 12 Fceneorkr @a: n9@x 2016#uiWge\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: The accuracy of the test set evaluated as the number of epoch. The best result of small batch method as the initial weight and set η = 0.1"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Different experiments have been conducted in the literature comparing LOS with non-LOS (NLOS) Links. In [27], they have carried out an experimental studies where the measurements have been done in an empty room for different scenarios including LOS and NLOS cases. They have considered frequency sweeping technique in their measurements by the using of vector network analyser (VNA) in an effort to obtain CIR. While the reflected rays can still be detected in NLOS scenarios, the channel gain severely dropped (see [27,Fig. 6]). Similar observations can be seen in [28] where a commercially optical and illumination design software is used for obtaining CIR. Moreover, uplink in VLC systems can be considered as one of the major challenges. Furthermore, the advancements in the LED industry are still hard to support the user equipment (UE)'s side with a VLC transmitter. Moreover, VLC is sensitive to interference caused by other light sources which can be clearer in daylight or outdoor scenarios. Also, the dimming mode for using VLC in low light scenarios can be an issue although an advanced modulation technique (enhanced unipolar orthogonal frequency division multiple access (eU-OFDM) [29]) has been proposed for Li-Fi systems to operate at very low light intensity scenarios while keeping a reasonable data rate. An important design consideration for VLC networks is the LED connectivity to the internet source. Typically, VLC networks present a large number of APs of LED arrays which make internet connectivity challenging. Most of these limitations have been recognized in literature and solutions have been proposed to resolve them [30]. We will outline the main solutions later in this survey.\n- 1) Dual-Hop Hybrid RF/VLC System: In this type of hybrid system, the user access link is either VLC or RF. The network model where the user access link is RF can be observed in Fig. 6.\n\nMentions:\n- While the reflected rays can still be detected in NLOS scenarios, the channel gain severely dropped (see [27,Fig. 6]).\n- The network model where the user access link is RF can be observed in Fig. 6.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 6. Hybrid RF/VLC Dual-hop systems."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We evaluate performance of the AC algorithm (without tactile exploration) on a large dataset comprising of 8 different obstacles (some of them depicted in Fig. 5) in 3 types of environment (forest, stairs, hallway) with the robot driven by 3 different operators in both MC and AC modes 5 . Each of the traversals is repeated 3-10 times to allow for statistical evaluation. The operators driving the robot are denoted as E (Experienced), IE (InExperienced) and IE2 (InExperienced #2). The experiments cover more than 115 minutes of robot time during which the robot traveled over 775 meters.\n- Each obstacle was traversed multiple times with both manual (MC) and autonomous flipper control (AC) using RFs following Equation 5, and the sensed states contained naturally missing DEM features. We emphasize that the complexity of testing obstacles was selected in order to challenge robot hardware capabilities. See the examples in Fig. 5 and the elevation maps (DEM) of testing obstacles computed online by the robot in Table IIIb.\n\nMentions:\n- We evaluate performance of the AC algorithm (without tactile exploration) on a large dataset comprising of 8 different obstacles (some of them depicted in Fig. 5) in 3 types of environment (forest, stairs, hallway) with the robot driven by 3 different operators in both MC and AC modes 5 .\n- See the examples in Fig. 5 and the elevation maps (DEM) of testing obstacles computed online by the robot in Table IIIb.\n\nOCR Text: \"H WuKHH\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 5. Top left: Forest obstacle. Top right: Rubble obstacle. Bottom left: Stairs obstacle. Bottom right: Operator controlling the robot using only sensor data."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 10 further annotates the cost-risk relationship with different values of Risk max in the RB-OPF. The result shows that the relationship between cascading outage risk and Risk max is non-smooth, which will cause difficulties in managing the cascading outage risk. By comparison, the RM proposed in this paper achieves a better performance in reducing risk, and the amount of risk reduction is more controllable. The mechanisms of RB-OPF and RM are further analyzed and compared. In RB-OPF, as Risk max reduces, the RB-OPF optimization model adjusts the loading of lines to reduce the severity of post-outage states of all the lines in the system. However, as RB-OPF does not have accurate knowledge of the most risky cascading outage patterns, it is not as effective as RM for cascading outages. The risk in RB-OPF is described as the outage probability times post-contingency severity, but the severity does not very well reflect the actual loss (load or economic loss) of the potential subsequent cascading outages, so the RB-OPF tends to lower the loading of lines systemwide, disregarding that the risk under many subsequent out-ages are actually very low. As Fig. 11 shows, the distribution of subsequent cascading outage risk in terms of level-1 line outage is very uneven: most risk is concentrated on the level-1 outages of lines 21, 25, 49, 87 and 121 (note that there are only 120 lines in RTS-96 system, and the \"line 121\" stands for no outage in the studied time interval). Particularly, the risk after line 25 outage accounts for about 95% of the total risk. We select the lines whose loading rates are significantly changed by these methods, and the relationship between line loading rates and control cost. As shown in Fig. 12(a), the RB-OPF reduces the risk by suppressing the loading level on many lines even if the actual risk of the following cascading outages is relatively trivial. In contrast, Fig. 12(b) shows the line loading rates under RM. Since the risk assessment identifies that the outage of line 25 is the most risky, the RM mainly reduces the loading of line 25, and thus more effectively reduces the overall subsequent cascading outage risk with lower control cost. Fig. 13 compares the distribution of risk before control, after RB-OPF and after RM. The results show that RM identifies the \"bottleneck\" of cascading outage risk and better concentrates the control effort on the risky cascading outage patterns. As a result, the overall risk under RM is lower at the same level of control cost. This is attributed to the feature of the RM optimization model that makes use of the information of risk distribution obtained from the risk assessment and risk gradient. 2) US-Canada Northeast system: Next, the RM is tested on a real-world system model, i.e. the US-Canada Northeast power grid model [19] retaining all generator buses and the buses at 230 kV or above. The system contains 410 buses (287 load buses and 233 generator buses, note that a bus may be both load bus and generator bus) and 882 branches. In this case, increase the system load to 1.5 times of the base load and set initial outages on branches 320 and 321. Fig. 14 compares the cost-risk profiles obtained by RM and RB-OPF, respectively. It can be seen that the RM reduces the risk by over 70%, while the RB-OPF only reduces the risk by about 10%. The effect of RB-OPF is limited due to its mechanism. According to the definition of severity function in RB-OPF, if the loading rates of a line under all N-1 states are below 90%, then the risk of the line is 0. So when setting the Risk max = 0, the RB-OPF will guarantee the loading rates of all the lines under all N-1 states under 90%. However, in this case, substantial risk may still exist under some cascading outage patterns, which cannot be reduced by RB-OPF. In contrast, since RM uses the information obtained from the risk assessment of cascading outages, it more effectively reduces the risk of subsequent cascading outages. The computational efficiency of RB-OPF and RM is also tested. Table I shows the average time of RM and RB-OPF in US-Canada northeast system. The methods are developed with MATLAB and are tested on a PC with 2.6GHz CPU and 32GB RAM. Similar with the computational strategies proposed by [10], the RB-OPF is realized with the technique of benders decomposition to handle large-system case. Since the RB-OPF considers the severity function of all the lines under all the N-1 outages, the scale of the optimization problem grows in proportion to the square of the number of branches in the system. In the US-Canada northeast system with 882 branches, the average computation time of RB-OPF is 72.29s. While the optimization model of RM has much smaller scale than RB-OPF, and the average computation time is less than 1s. However, since the optimization model of RM requires risk gradient, the computation time of risk assessment and risk gradient calculation should also be included. In this case, each risk assessment uses 200 Markovian tree search attempts, and the average computation time is longer than that of RB-OPF. The performance of the RM is limited only within a range where linearization is effective. Fig. 9 shows that with the RM, the risk stops decreasing at around $2.5×10 5 . IRM keeps updating risk gradient at new operating points and the risk is further reduced. Table II and Fig. 15 demonstrates the cost-risk relationship derived by IRM. The results indicate that the IRM can effectively overcome the limitation of linearization with the RM and further reduce risk of cascading outages. After 4 rounds of iterations, the subsequent risk of cascading outages has reduced by 97.3%. The results indicate that the total risk at the 6th iteration reaches the lowest among all iterations, where the total risk is expected to decrease by 54.5%, and the subsequent cascade risk drops significantly by 93.6%. The subsequent cascade risk is even lower in the 7th iteration, but the drop in subsequent cascade risk is offset by a substantially high cost, which causes a higher total risk than that of the 6th iteration. In practice, the adopted strategy for risk management may vary depending on the risk preference. Therefore, the strategies in Fig. 16 can also be regarded as the results of multi-objective optimization of risk and cost, and the control strategy for risk management can be selected depending on the risk preference.\n\nMentions:\n- Since the risk assessment identifies that the outage of line 25 is the most risky, the RM mainly reduces the loading of line 25, and thus more effectively reduces the overall subsequent cascading outage risk with lower control cost. Fig. 13 compares the distribution of risk before control, after RB-OPF and after RM.\n\nOCR Text: \"Txio' _ Before control RB OPF @ 6x1o' - RM 9 Sx10\" _ 6 4x10' _ 8 Jxlo\" 1 2x10' _ Ixio' _ Line nimbar\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 13. Cascading outage risks of selected level-1 outages."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- This function (shown in Fig. 2) is a scaled version of the PGS 3.2 impact-parameter efficiency for a c-jet to fake a b-jet [8]. In this paper we vary k c between the Run I value of k c = 1, and an enhanced value of k c = 5 to extract the net reach with different charm tagging efficiencies. The algorithm above is predominantly a fit to distributions of events in impact-parameter vs. track invariant-mass [14], and has the most room for adaptation to a dedicated charm tagger.\n- As a check of our main results, we have reproduced all numbers using a CDF Run II-like algorithm ǫ 2 c from PGS 4. The E T dependence of the charm tagging efficiency ǫ 2 c is shown in Fig. 2 (scaled by a factor of 4). Despite the different E T dependence, we find exactly the same significances after cuts as the backgrounds tend to have harder jets. Since the dominance of charm-initiated processes is insensitive to the details of the charm tagging algorithm, we present numerical results using ǫ 1 c .¯¾ An important consideration evident in Fig. 2 is that the typical charm E T is about 20-40 GeV. Therefore, the effective charm tagging efficiency used in this analysis is ǫ c ∼ 12-48%. We emphasise an algorithm that improves charm tagging acceptance at such characteristic E T 's is the relevant efficiency and not the asymptotic value. Neither 100% acceptance of b jets as \"fakes\" of charms, nor a factor of 3 change to the light-jet fake rate, materially change our results. However, we maintain a separate list of backgrounds so that our predictions may be rescaled to whatever efficiencies a dedicated charm tagger eventually obtains.\n\nMentions:\n- This function (shown in Fig. 2) is a scaled version of the PGS 3.2 impact-parameter efficiency for a c-jet to fake a b-jet [8].\n- The E T dependence of the charm tagging efficiency ǫ 2 c is shown in Fig. 2 (scaled by a factor of 4).\n- Since the dominance of charm-initiated processes is insensitive to the details of the charm tagging algorithm, we present numerical results using ǫ 1 c .¯¾ An important consideration evident in Fig. 2 is that the typical charm E T is about 20-40 GeV.\n\nOCR Text: \"8s { E7 {Gev}\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 2: Charm tagging efficiency curves and characteristic charm jet ETc from H → WW → lνcj decay. ǫ 1 c(ǫ 2 c) are CDF Run I(II)-like algorithms scaled up by a factor of 4."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Since SNRs are the short-lived end-points of young stars we would expect a linear relation between the number of X-ray selected SNRs and SFR (e.g. Condon &amp; Yin 1990). To verify this connection, we plot the number of SNRs above the completeness limit of our sample (5×10 36 ) against the integrated FIR luminosity of each galaxy (Fig. 13). For comparison, in the same plot we include the X-ray selected sample of SNRs in the MCs from the XMM-Newton study of Ghavamian et al. (2005). Although in the case of SMC this study covers a fraction of the area of the galaxy, a comparison with the ASCA-selected SNR sample of Yokogawa et al. (2000) which covers almost the whole galaxy show that the sample of Ghavamian et al. is complete, and the MC SNR census extends down to a luminosity of 10 35 erg s −1 , much lower than our completeness threshold.\n- We find a linear relation between the number of X-ray selected SNRs and the FIR luminosity (Fig. 13), but the small number of objects does not allow us to quantify their scaling relation. However, a linear correlation coefficient of 0.72 shows that this is a significant correlation. Even if we remove NGC 2403, which seems to drive the correlation, or NGC 4449 which has the greatest distance uncertainties and hence FIR luminosity uncertainties, we measure a correlation coefficient of 0.53 and 0.90 respectively.\n\nMentions:\n- To verify this connection, we plot the number of SNRs above the completeness limit of our sample (5×10 36 ) against the integrated FIR luminosity of each galaxy (Fig. 13).\n- We find a linear relation between the number of X-ray selected SNRs and the FIR luminosity (Fig. 13), but the small number of objects does not allow us to quantify their scaling relation.\n\nOCR Text: \"00) {6J\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 13.— Number of SNRs above the completeness limit of this study against the integrated Far Infrared (42–122µm) luminosity (see text for details)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- OBU → CSP A : h n (P S i OBU ), X OBU , Cert OBU b. At the time of authentication and request for charging, the vehicle must provide the CP with a member Fig. 5: Authentication process in PHA scenario hash from the registered hash chain h n−1 (P S i OBU ). OBU → CP : h n−1 (P S i OBU ), X OBU c. CP forwards this value to the CSPA.\n- If the authentication is successful then charging plate will transfer the electric power to the vehicle, otherwise the process halts. It is to be noted that one hash chain is long enough to use it for the whole day. For the next day, the vehicles can register another hash chain. This process will still preserve the conditional privacy of the OBU. The protocol for pure hash chain based authentication mechanism is given in Fig 5.\n\nMentions:\n- At the time of authentication and request for charging, the vehicle must provide the CP with a member Fig. 5: Authentication process in PHA scenario hash from the registered hash chain h n−1 (P S i OBU ).\n- The protocol for pure hash chain based authentication mechanism is given in Fig 5.\n\nOCR Text: \"4'UP84,} \"o56; {eop\" #cci Ack: Pop\" Eertwov rlencteo Wilgi N-!ieskw) oxz_ tisestamp; k'-] Paax Milrs tthctal RLeL Kwk, SLALMS , {Skgu\"-Pessp 3-CE IS5 /\" - Poosd\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 5: Authentication process in PHA scenario"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Given an algorithm WinningCore(G, j) that computes the winning core A j (G) for player j in G we can compute winning regions in parity games using the algorithm in Figure 6.\n- Fig. 6. Solving parity games using winning core computation\n\nMentions:\n- Given an algorithm WinningCore(G, j) that computes the winning core A j (G) for player j in G we can compute winning regions in parity games using the algorithm in Figure 6.\n- Fig. 6.\n\nOCR Text: \"ParIvGaMESOLER G}; A+ IlwtoCoreg,a If eU then returu |6,S1 A= Amtll;, (IV6 W + PartvGaHzSorvir/G |(|4V refuu ( |ul WH\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 6. Solving parity games using winning core computation"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Pressure distribution for cases #000 and #111 at t = 5, 30, 50 and 180 days is shown in Figure 8. One can see that the depression zone spreads faster in case of the higher rock permeability. The Dirichlet outer boundary condition in case #111 allows a fluid inflow to the reservoir from the surrounding media. Together with the high permeability this brings the flow to the stationary regime starting at t ≈ 40 h as it follows from the comparison of the bottom-right column of pictures in Figure 8 and from the corresponding productivity curve in Figure 7. In contrast, the Neumann boundary condition in case #000 leads to the leveling of pressure for higher t.\n\nMentions:\n- Together with the high permeability this brings the flow to the stationary regime starting at t ≈ 40 h as it follows from the comparison of the bottom-right column of pictures in Figure 8 and from the corresponding productivity curve in Figure 7.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 7: Well production (103 m3) versus time (days) for an “arbitrary” set of fractures. Values of the instrument parameters are coded by a 3-digits binary number according to Table 3"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- On our finest lattice, we find S c ≃ 0.1. The same measure with t 0 gives about 0.2. The formal two-loop expression for the running of g R in eq. ( 6) yields S c ≃ 0.3. Since the renormalized couplings obtained for these c are large, the two-loop beta function does not run the coupling reliably, so one should take the last number only as indicating that such large changes in scale are natural when changing c. It is more interesting to ask whether the ratio of lattice spacings at two different bare couplings and quark masses is independent of the choice of c, when each of these is given in units of w 0 . Ideally, of course, such a ratio should not change with c. In Figure 7 we show that, in fact, there is some residual dependence on the parameter c. Take the case where this ratio is close to 1.175 for c = 0.3. The change in this ratio of lattice spacings for variation of ∆c = 0.2 around c = 0.3 is 3% of the central value. While not ideal, this change is rather small. Presumably this uncertainty in the scale setting is due to remaining lattice spacing corrections. It would be interesting in the future to perform this comparison at smaller lattice spacings.\n\nMentions:\n- In Figure 7 we show that, in fact, there is some residual dependence on the parameter c.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 7: The ratio of the lattice spacings for two different sets of bare parameters depends on the renormalization scheme, i.e., the choice of c, through w0(c)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The PV sources deliver variable power which depends on solar radiation which varies from 200-1000W/m 2 during sun hour period. In order to extract/draw maximum power from PV source, solar power regulator has been incorporated which regulates the voltage to respective Maximum Power Point (MPP) i.e. the intersection point between load line and V-I curve at different insolation(radiation) level as shown in fig. 3. This is achieved through intelligent controller unit of solar (PV) converter system.\n\nMentions:\n- In order to extract/draw maximum power from PV source, solar power regulator has been incorporated which regulates the voltage to respective Maximum Power Point (MPP) i.e. the intersection point between load line and V-I curve at different insolation(radiation) level as shown in fig. 3.\n\nOCR Text: \"I0O0 Wim\" Load Lute; 600 Wim? J 200 Wlm? 0,1 0,2 0,3 0,4 0,5 Voltage in V\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3. V - I Characteristic of a Solar (PV) Cell"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Figure 12 we show the q PAH map overlayed with contours of 12 CO (J= 1 − 0) emission from the NAN-TEN survey (Mizuno et al. 2001). There is a strong correlation between the presence of molecular gas and the regions with higher PAH fraction. In Figure 13 we show a histogram of q PAH for lines-of-sight with detected CO emission in the NANTEN map of the SMC and those without. The mean value of q PAH for a line of sight with CO is twice that for a line of sight without CO. In addition, there are no lines of sight through only atomic gas which have q PAH higher than ∼ 1%. We note that the CO map has much lower resolution than our map of q PAH , so the association of PAHs with CO is most likely stronger than the evidence we present here.\n\nMentions:\n- In Figure 13 we show a histogram of q PAH for lines-of-sight with detected CO emission in the NANTEN map of the SMC and those without.\n\nOCR Text: \"0,10F\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 13.— Histogram of qPAH in lines-of-sight with and without detected CO emission from the NANTEN survey."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Since we do not deal with separate photons, but just with intensities of their radiation, we provide all necessary averaging of corresponding trigonometric functions over the photon multiplicities (at given intervals in p T ) in different azimuthal angular sectors covering thus the whole 2π interval. This allows us to find the angle ψ n already in event plane, and provide the standard procedure to present v n as, in fact, the mean value of v The total photon spectra for all centralities are shown in Fig. 2. As it was discussed above, we compare the two scenarios: chemically equilibrated (iHKM) and chemically frozen (HKM). The resulting spectra show that there is an insignificant difference between two scenarios. The reason is that an appearance of additional chemical potentials of hadrons in a chemically frozen scenario changes the equation of state (EoS) compared to chemically equilibrated hydrodynamics, see Fig. 1 in Ref. [37] and Fig. 6 in Ref. [36]. This results in difference of the expansion rates at the different scenarios. As a result of the interplay between expansion rates at different EoS and chemical composition of the matter, the temperatures at the same energy densities are lower in the case of chemically frozen expansion compared to the chemically equilibrated one, as it is demonstrated in Fig. 7 in Ref. [36].\n\nMentions:\n- The reason is that an appearance of additional chemical potentials of hadrons in a chemically frozen scenario changes the equation of state (EoS) compared to chemically equilibrated hydrodynamics, see Fig. 1 in Ref. [37] and Fig. 6 in Ref. [36].\n\nOCR Text: \"iHKI: , n*-0,08 ,#o HE) iHKN' ,r\"s-0.C8 'chHE) HKW' , n*-0,0 Sith HE) PHENIX\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 6. Elliptic flow for 0-20% centrality for the different models: iHKM chemically equilibrated with contribution, iHKM chemically equilibrated without hadronization emission (HE) contribution, HKM chemically frozen at the hadron stage with continuous transition from hydrodynamics to hadron gas (with HE contribution). Experimental results are taken from [56]."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We study the following ten methods on their ability to estimate the population mean µ of INTRDVX accurately and quickly. AME. This is AMELIA (Honaker, King and Blackwell, 2011) with default parameters except that the empirical prior level is set at 5. According to the manual, the prior shrinks the covariances of the data but keeps the means and variances the same. It helps when there are many missing values, small sample sizes, large correlations among the variables, or categorical variables with many levels. MICE. This is the R software (van Buuren and Groothuis-Oudshoorn, 2011) with default options, including five multiple imputations. Problems due to multi-collinearity in linear regression and quasi-complete separation in logistic regression severely limits the number of variables it can employ. With the help of previous analyses with similar types of data, we identified 19 variables that do not cause the software to fail. Their names and numbers of missing values are given in Table 4. None are income variables. Only 5 of the 19 have missing values (and only 1 with a significant amount). This set of 19 is quite easy for all imputation methods. SIM. This is the simple method that estimates µ with the meanȳ of the non-missing Y values, ignoring the values of the X variables. GCT. This uses a GUIDE classification tree (Loh, 2009) to form adjustment cells for conditional mean imputation. Figure 1\n- In addition, note that the nodes in the tree models as displayed in Figures 1-4 can be interpreted as nonresponse adjustment cells. For example, Little and Vartivarian (2003) extended the ideas of Little (1986) to outline two strategies for reducing the number of adjustment cells: (a) choosing cells that are homogeneous with respect to the probability of response, and (b) choosing cells that are homogeneous with respect to outcome variable Y . They noted that weighting based on either of these methods of grouping removes non-response bias in estimating population means: The nodes produced by the classification tree method constitute cells that are homogeneous with respect to the estimated probability of response as shown in Figures 1 and 2, while the regression tree method produces cells that are homogeneous with respect to the predicted outcome variable, INTRDVX, as shown in Figures 3 and 4 This work here can be extended in several directions. First, it would be useful to consider design-adjusted versions of the procedures proposed here. This potentially would include the use of weights in the growth of individual trees or forests; in estimation of tree parameters; and in the corresponding estimation of (sub)population means. Of special interest would be evaluation of the extent to which a given procedure may be sensitive to specified patterns of heterogeneity in the weights. For example, as noted in Section 3, all tree and forest algorithms use approximations, and the properties of the resulting procedures can be sensitive to the extent to which a given dataset is consistent with the approximations used for that procedure; and some weight-heterogeneity patterns may exacerbate that sensitivity. In addition, several analyses here used some predictor variables that are equal to membership indicators for certain large primary sample units. It would be of interest to extend previous literature on the use of stratum and PSU labels in regression to the current case.\n\nMentions:\n- Figure 1\n- They noted that weighting based on either of these methods of grouping removes non-response bias in estimating population means: The nodes produced by the classification tree method constitute cells that are homogeneous with respect to the estimated probability of response as shown in Figures 1 and 2, while the regression tree method produces cells that are homogeneous with respect to the predicted outcome variable, INTRDVX, as shown in Figures 3 and 4 This work here can be extended in several directions.\n\nOCR Text: \"--m\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig 1. GUIDE classification tree for predicting missingness in INTRDVX (INTRDVX = C) from 4609 observations and 630 X variables, with minimum node size 50. At each split, an observation goes to the left branch if and only if the condition is satisfied. The symbol ‘≤∗’ stands for ‘≤ or missing’. Set S1 = {CO, DE, FL, HI, IL, LA, MA, MO, NJ, NY, OH, PA, SC, TN, WA, WI}; set S2 = {1102, 1110, 1423}. PSU codes are given in Table 9. Numbers beside each terminal node are the proportion missing INTRDVX (top) and the mean INTRDVX of the non-missing values; the sample size is beneath the node. Yellow and green nodes have proportions of INTRDVX missing greater and less, respectively, than 0.50."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The vertical error bars reflect the combined uncertainties in the flowmeter tube diameter (± 0.5 mm), transit time measurement precision (± 100ns) and transducer foil spacing (± 0.1 mm after distance calibration in an ideal static gas). The rms deviation of the ultrasonic flowmeter relative to the fit (shown as red bands) is ± 4.9 lmin -1 , around 2 % of the limiting flow of 230 lmin -1 . Figure 5 compares the sound velocity measured in the instrument in varying C 2 F 6 /C 3 F 8 molar mixing ratios at a temperature of 19.2 ºC and a pressure of 1.14 bar abs with predictions from two different theoretical models. The range of mixtures of C 3 F 8 and C 2 F 6 spans the region of thermodynamic interest to the ATLAS silicon tracker cooling application.\n- • where m is the local slope of the sound velocity/ concentration curve (ms -1 % -1 ). From figure 5 it can be seen that the 0.05 ms -1 sound velocity uncertainty yields a concentration uncertainty ~0.3 % at 20 %C 2 F 6 in C 3 F 8 , where the slope of the velocity/concentration curve is ~ -0.18 ms -1 % -1 .\n- Our sound velocity measurements agree to within ± 0.05ms -1 of the predictions of the NIST-REFPROP theoretical package [6], which are shown with ±1% variation in figure 5 for reference.\n\nMentions:\n- Figure 5 compares the sound velocity measured in the instrument in varying C 2 F 6 /C 3 F 8 molar mixing ratios at a temperature of 19.2 ºC and a pressure of 1.14 bar abs with predictions from two different theoretical models.\n- From figure 5 it can be seen that the 0.05 ms -1 sound velocity uncertainty yields a concentration uncertainty ~0.3 % at 20 %C 2 F 6 in C 3 F 8 , where the slope of the velocity/concentration curve is ~ -0.18 ms -1 % -1 .\n- Our sound velocity measurements agree to within ± 0.05ms -1 of the predictions of the NIST-REFPROP theoretical package [6], which are shown with ±1% variation in figure 5 for reference.\n\nOCR Text: \"L F=JoJEL PAuTAIKh MEIREMGCMA-4a CaiceItaalioi € C,F C F rilxturel\"\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5. Comparison between measured sound velocity data and theoretical predictions in molar C3F8/C2F6 mixtures of thermodynamic interest. NIST-REFPROP predictions shown within a ± 1 % band."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We explore the uncertainties of our predictions on r multib and N multib in Fig. 8, where we vary the strength of the kicks imparted at BH formation from σ = 0 km/s to 265 km/s [59] (see Appendix A for additional details). As σ increases, more and more binaries are unbound by supernova explosions, and the detection rate consequently decreases. We report variations of a factor ∼ 30. It is worth noting that only moderately extreme cases (σ 150 km/s, T obs = 4 yr) present N multib &lt; 1. Therefore, we predict that the LISA space mission will detect a few -but possibly dozens -sources which are also observable from the ground. These events (with ρ LISA &gt; 8) are going provide forewarnings for ground-based operations, but about O(100) more events with ρ LISA &gt; 4 could potentially be extracted from the LISA data stream in coincidence with sources that have already been observed from Earth.\n\nMentions:\n- We explore the uncertainties of our predictions on r multib and N multib in Fig. 8, where we vary the strength of the kicks imparted at BH formation from σ = 0 km/s to 265 km/s [59] (see Appendix A for additional details).\n\nOCR Text: \"Rratne De c FAATTTX Ncrzuni-u arkr. #u] tc: cllzct hjgh pus: Atgtutlicu \"ruct ucgligbk Ucuueuc The kyyelle Wel# (HW _e Mlirs Mau\" 4uacL' licls (llzd \"0o-Coni plinl 62, 63. u O_uid Jalc \"\"olulicu:ry is culer ~TEull Ahe Niuat} 4Ali CG= ua Fn\"el s shich dlissijiatez hrding TAs shrinkirg the Garin Ke . |N: dirde rae \"exulta #mmi ti.ux Irto eig Mli-ta excsix  XuASa Aenela frefit fIrP Th MOrF (TSTTI; ATid] (BIT?; Wclx vcl Zr4C LOMAU-CORL VA {(Ei. Ti JulC I[li' M: OM: Mu Mui' Maly?Alin \"BHF\" UJIl:t W:ri = BHZ \"? MJJLUMI 3stM: Tit Iausin- migl FuitS {uar C 143ar MMSITS hesendar KH icm tirzr (i. WIT ?\" Kace \"M Comon Fve Opf TXIc 4Iv Mcr1r $ Jy cl T #In8; hnar} ILr`\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 8. Multiband detection rates (top) and number of events (bottom) predicted by population synthesis simulations of binary stars. We present results from 7 different simulations with varying supernova kicks, where the 1D dispersion σ ranges from 0 km/s (no kicks) to 265 km/s (BHs receives full kicks as inferred from galactic pulsars). Blue (red) curves show results assuming a LISA mission duration of 4 yr (10 yr); solid (dashed) lines assumes a LISA SNR threshold of 8 (4). The vertical dotted line at σ = 50 km/s marks the model used in Figs. 5-7. Results appear identical if LIGO or a third-generation ground-based detector is considered. We set Twait = 5 × Tobs, i.e. either 20 or 50 yr. Data to reproduce this figure are reported in Table II."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The radius R of the isolation cone is fixed at R = 0.4. We study the isolation parameter dependence by considering values of E T max in the range between 2 GeV and 10 GeV (in this section we mainly show results at the extreme values in this range). In the case of smooth cone isolation, we use the isolation function χ(r; R) in Eq. (4) and we examine the cross section dependence on the power n that controls the shape of χ(r; R). In Fig. 1 we show the r dependence of the isolation function for some selected values of the power n and the fixed value R = 0.4 of the radius of the isolation cone. We note that the value R = 0.4 is sufficiently small so that the two isolation functions in Eqs. (3) and (4) quantitatively coincide at the percent level in the case with n = 1.\n\nMentions:\n- In Fig. 1 we show the r dependence of the isolation function for some selected values of the power n and the fixed value R = 0.4 of the radius of the isolation cone.\n\nOCR Text: \"(r/R;\"\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: The different shapes of the isolation functions χ(r;R) for selected values of the power n and with R = 0.4. The functions χ(r;R) in Eqs. (3) and (4) are labelled as (r/R)2n (dashed lines) and Cosn (solid lines), respectively."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To conclude the results section, we present, in Fig. 6, the quenching function for the turbulent diffusivity η T (β). We note that due to background shear, the η tensor becomes anisotropic. We however cannot address this issue with the current data. For practical purposes, we restrict our analysis to η φφ . Unlike for the two preceding quantities related to the α tensor, we now find a scaling exponent closer to one, i.e. p η = 0.96±0.06. This is in good agreement with p η = 1, as predicted by Rogachevskii &amp; Kleeorin (2000), and coincides with the scaling suggested 4 by Blackman &amp; Brandenburg (2002). Our result also agrees with the effective diffusivity (in the one-dimensional case) quoted in the line after eqn. (46) of Kitchatinov et al. (1994). Yousef et al. (2003) have derived diffusivity quenching from simulations of forced turbulence. Their results appear consistent with an assumed p η = 2, based on which they state a best-fit value of q η = 8.\n\nMentions:\n- To conclude the results section, we present, in Fig. 6, the quenching function for the turbulent diffusivity η T (β).\n\nOCR Text: \"61/; +6,34_ 0001 a210 0C relc; # fi:d srarj?\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6. Same as Fig. 4, but for the turbulent diffusivity ηφφ(β)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where N is the number of vehicles performing the formation flight. T k is the number of timestamps during the duration of the formation flight. P D i (t) and P R i (t) denote the desired and real position of UAV i at timestamp t, respectively. Fig. 10 shows the average mean position error for formation flights with 3, 7, 10, 14, 21 UAVs using the proposed architecture. It is clear that as the number of the vehicles increases, the average mean position error for formation flights hovers around 10 meters. As far as we know, many existing work are evaluated though field experiments outdoor, however, most of them are small scale (e.g. two to five). Therefore, it can be seen that the proposed architecture shows decent scalability while increasing the scale of the swarm system. Admittedly, the scalability of the architecture relies heavily on the communication infrastructure. And many solutions such as hierarchical design and multi-cast can improve the scalability of the communication. However, this is out of the scope of this paper. In this paper, we focus on the architecture design in the level of the swarm system.\n\nMentions:\n- Fig. 10 shows the average mean position error for formation flights with 3, 7, 10, 14, 21 UAVs using the proposed architecture.\n\nOCR Text: \"'nz Mas\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 10. The average mean position error for formation flights with increasing number of UAVs using the proposed architecture."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- High-speed access to mobile data has become one of the most important demands of smart phone subscribers. The increasing availability of intelligent equipment and ever-growing demand for multi-media streaming services have considerably increased the volume of mobile data traffic [10]. In 2016, the total generated mobile data was 8.8 Exabytes 50% of which belonged to video contents. It is expected that this amount will reach 71 Exabytes in 2020 75% of which will belong to video contents [12]. Network carriers have spent lots of R&amp;D costs on providing better data. Therefore, it has led to significant expansion of mobile broadband. Statistics and figures indicate that mobile broadband subscriber growth rate has been steadily higher than that of fixed broadband in the world. Fixed broadband penetration rate has reached 11.9% in 2016 from 0.9% in 2012 while mobile broadband penetration rate has reached 49.4% in 2016 from 21.7% in 2012 (Figure 1). In I.R. Iran, mobile broadband penetration rate has been much higher than that of fixed broadband as the growth rate of fixed broadband subscribers has moved from 2.81% in the summer 2012 only to 12% in the spring 2017 while the growth rate of mobile broadband subscribers has shifted from 1.92% in summer 2012 to 42.5% in spring 2017 (Figure 2). Cellular technologies such as the third generation (3G) and fourth generation (4G) of mobile networks have majorly contributed to the development of mobile broadband. 3G Technology was developed during the 90's and entered global market in 2002. This technology was getting widespread in 2007. Actually, in December 2007, 190 operators in 40 countries provided 3G services to their customers. In I.R. Iran, Rightel operator which was established in 2007, received the 3G service license in January 2010 but the operator started its work in June 2011. In I.R. Iran, 3G mobile networks entered the market with a delay of 9 years. In fact, the commercialization of 3G in I.R. Iran coincided with the commercialization of 4G in the world. In I.R. Iran, the fourth generation of mobile networks was launched in September 2014 by the I.R. Irancell operator. Subsequently, mobile operators such as Mobile Telecommunication Company of Iran (MCI) and Rightel also launched 4G mobile networks. Limitations such as low-rate of transmission, mobility and bandwidth constraints were among the most important reasons leading mobile operators to go to launch 4G mobile networks. -Data transmission rate should be 100 megabits per second (Mbit/s) for high mobility communications (such as from trains and cars) and 1 gigabit per second (Gbits) for low mobility communications (such as pedestrians and stationary users)\n\nMentions:\n- In I.R. Iran, mobile broadband penetration rate has been much higher than that of fixed broadband as the growth rate of fixed broadband subscribers has moved from 2.81% in the summer 2012 only to 12% in the spring 2017 while the growth rate of mobile broadband subscribers has shifted from 1.92% in summer 2012 to 42.5% in spring 2017 (Figure 2).\n\nOCR Text: \"R Us Aak MEC Cau Aauc Ihtitou dedji\" dennlt 0ijz] tmectece $jnc\" cemoeteo ujimiz\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: Comparison of penetration rate of mobile broadband subscribers with that of fixed broadband subscribers in the I.R. Iran from 2013 Q3 – 2017 Q2 ( Source: Ministry of I.C.T of I.R. Iran)"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 3 illustrates the problem formulation. Each candidate set for a keyword forms a cluster in the graph. The weight of each edge in this graph is given by the cost function, which includes the distance between the nodes in the subdivision graph of the input knowledge graph as well as the confidence scores of the candidates. The GTSP requires the solution to visit one element per cluster and minimises the overall distance.\n\nMentions:\n- Figure 3 illustrates the problem formulation.\n\nOCR Text: \"Sa Men Gie Kepazr Kepaio = 'Tcsh\" Kepau: = Sputer' Kofiad-'kIe' ati Gilau(\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3. Using GTSP for disambiguation : The bold line represents the solution offered by the GTSP solver. Each edge represents an existing connection in the knowledge graph. The edge weight is equal to the number of hops between the two nodes in the knowledge graph. We also add the index search ranks of the two nodes the edges connect to the edge weight when solving for GTSP."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We have performed bulk measurements using 2-coil ac susceptibility and bulk magnetization measurements (SQUID). The samples used in this study consist of pure and Cointercalated NbSe2 single crystals 17 grown through iodine vapor transport in sealed quartz ampules. The random intercalation of Co provides us a handle to control the degree of pinning 18 . For the first crystal on which all the STS measurements were carried out, we started with a nominal composition Co0.0075NbSe2 and the growth was continued for 5 days. We obtained single crystals with narrow distribution of Tc, in the range 5.3 -5.93 K. The crystal chosen for our studies had a Tc ~ 5.88 K (sample A). The second crystal was also grown starting from the same nominal composition (in a different ampule), but the growth was continued for 10 days. Here we obtained crystals with Tc varying in the range 5.3 -6.2 K. We conjecture that this larger variation of Tc results from Co gradually depleting from the source such that crystal grown in later periods of the growth run have a lower Co concentration. However, over the 2mm × 2mm crystal chosen from this growth run (Tc ~ 6.18 K) (sample B), we did not see significant compositional variation. The third sample was a pure NbSe2 single crystal with Tc ~ 7.25 K (sample C). Compositional analysis of the three crystals was performed using energy dispersive x-ray analysis (EDX). We obtained a Co concentration of 0.45 atomic % for A and 0.31 atomic % for B. While these Co concentrations are marginally higher than the ones reported in ref. 18, we note that these measurements are close to the resolution limit of our EDX machine, where precise determination of the absolute value is difficult. However, measurements at various points on the crystals revealed the composition to be uniform, which is also validated by the sharp superconducting transitions observed from a.c. susceptibility in these crystals (Fig. 4.1).\n\nMentions:\n- However, measurements at various points on the crystals revealed the composition to be uniform, which is also validated by the sharp superconducting transitions observed from a.c. susceptibility in these crystals (Fig. 4.1).\n\nOCR Text: \"45  50 3 60 65 7 75 IE TK\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4.1. Temperature variation of χ' in zero applied d.c magnetic field for samples A, B and C."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The couplings are again chosen from a normal distribution with mean zero and variance one, and the rescaling factor N −1/2 ensures a sensible thermodynamic limit of the energy and free energy per spin. Figure 5 is a plot of q ∞ (N ) as a function of N . While it is clear that q ∞ (N ) is decreasing with N , it again is not obvious whether q ∞ (N → ∞) is zero or greater than zero. The best fit implies that q ∞ (N → ∞) → 0, although slowly. However, a different fit -not as good, but still reasonable -implies a nonzero limit. For details, see [26]. Nevertheless, a heuristic argument presented in [26] suggests that it is most reasonable to expect that indeed q ∞ (N → ∞) → 0. The main idea is the following: it is known that the SK model has exponentially many (in N ) one-spin-flip metastable states [29], and moreover numerics indicate that the number of spin flips grows linearly with N [26]. Given the O(N ) distance traveled by the SK model on the state space hypercube to find an absorbing state, and given that these states have been shown to be uncorrelated [29], the decay of q ∞ → 0 as N → ∞ appears to be its most likely behavior.\n\nMentions:\n- Figure 5 is a plot of q ∞ (N ) as a function of N .\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 5: Simulation results for q∞(N) vs.!N for the SK model. The curve is the highest quality fit: q∞(N) = a (logN)1/3 + b N , with a and b both close to 0.4. From [26]."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The case of k = 5 in Fig. 6 is similar with a few minor differences. There exist two paramagnetic phases, denoted as QP + and QP − . The former has θ = 0 with magnetization vector m = (1, 0, 0) and the latter θ = π with m = (−1, 0, 0). The QP − phase appears at the top left corner of the phase diagram, where the antiferromagnetic multiple-X term s(1 − λ)(m x ) k dominates. The transition is of second order only between QP + and F phases. The first-order transition line within the ferromagnetic phase between F and F extends toward QP − at the upper left part of the phase diagram. For larger p, this line reaches the phase boundary of the QP − phase as seen for p = 21 in Fig. 6. The two ferromagnetic phases, F and F , are then completely separated by a line of first-order transitions. It is therefore concluded for k = 5 that a proper choice of annealing path makes it possible to reduce the first-order phase transition for the stoquastic case (λ = 1) to second order (smaller λ) as long as p is not too large. These features are shared by other values of k, even k similar to Fig. 5 and odd k to Fig. 6.\n\nMentions:\n- The case of k = 5 in Fig. 6 is similar with a few minor differences.\n- For larger p, this line reaches the phase boundary of the QP − phase as seen for p = 21 in Fig. 6.\n- These features are shared by other values of k, even k similar to Fig. 5 and odd k to Fig. 6.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 6: λ-s phase diagram of the non-stoquastic Hamiltonian with k = 5 (antiferromagnetic XXXXX interactions). The boundary between QP+ (quantum paramagnetic) and F′ (ferromagnetic) phases is for second-order phase transitions and all other boundaries represent first-order transitions. A new phase QP− exists for k odd, where spins point to the −x direction whereas they point to the +x direction in the QP+ phase. The two ferromagnetic phases F and F′ are clearly separated for p = 21 but not for smaller p."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 6 shows the performance of our proposed model on a reduced dataset with a sample rate of 5 Hz. We remove k data points in the middle of a time window of length n, as shown in Fig. 5. From Fig. 6, we can see that the prediction accuracy varies little as the reduced data length increases from 0 to 3. This implies that we can save computation memory and upload bandwidth by reducing the input data and achieve the same performance as before. Fig. 7 shows how our model's performance changes as the prediction length vary. From Fig. 7, we observe that it is difficult to predict long-term movement. This is due to the fact that future movements over an in long period are less relevant to the current movement, and, hence the prediction length should be limited for better performance.\n\nMentions:\n- Fig. 7 shows how our model's performance changes as the prediction length vary.\n- From Fig. 7, we observe that it is difficult to predict long-term movement.\n\nOCR Text: \"Pali P14z Pra.d35 Aqe eller\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 7: Effect of input duration on the angle error."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- with N &gt; 1, and by setting T 0 = 50 fs (the respective FWHM width is ≈ 90 fs). With the above-mentioned value of the second-order GVD coefficient β 2 = −0.0021 ps 2 /m, the peak power of the fundamental soliton corresponding to this width is P 0 = 10.5 W. While the second-order dispersion parameter is kept constant, its third-order counterpart (the TOD coefficient) will be varied. In the case of weak TOD, the analysis of the N -soliton splitting, based on the perturbation theory, was developed in Ref. [14]. However, for larger values of β 3 , the dynamics is more complex, strongly deviating from the prediction of the perturbative analysis (see Fig. 4 below). The TOD distorts the fundamental pulse, making it asymmetric, with a periodic oscillatory structure emerging near one edge [20,27], which results from dispersive waves and is described by the Airy function [28]. Accordingly, the shape of the soliton experiences a strong deformation with the appearance of this oscillating structure at the front or rear side of the soliton [20], depending on the sign of β 3 . Here we consider the case of opposite signs of the TOD and second-order GVD (for identical signs, the results can be obtained by substitution T → −T , which reverses the sign of β 3 ). Despite the fact that the energy coupled into the radiation under the action of TOD is relatively small, it was demonstrated that the dispersive wave carries away a significant amount of momentum [29]. The periodic light structure at the edge of the pulse induces effective refractive index change, that attracts the main peak of the pulse. If more energy is injected into the fiber than it is required for the formation of the fundamental soliton, the interaction between the oscillatory structure and the peak of the pulse becomes stronger, causing acceleration or delay of the pulse. This situation is shown in Fig. 1(a), where input pulse (2) was taken with peak power 23.625 W, corresponding to N = 1.5 at T 0 = 50 fs.\n- results are plotted in Fig. 4 versus order N of the input.\n- where P 0 is the same as in Eq. ( 2). This result, if applied to the splitting of the 10-soliton, shows that the largest peak power in the set of the emerging fundamental solitons should be 3.6 times higher than that of the original Nsoliton, and the energy carried away by this tallest soliton is about 18% of the total energy of the parental N -soliton. However, the relatively strong TOD makes the actual peak power of the escaping soliton significantly lower, as seen in Fig. 4. For example, the fission of the 10-soliton displayed above produces the free soliton with the peak power of 1 kW, which is only 1.2 times higher than that of the parental 10-soliton. On the other hand, the energy of the single ejected soliton is about 30% of initial pulse's energy. The latter result is explained by the fact that, under the action of the strong TOD, the collisions are not exactly elastic [30], allowing the ejected soliton to collect energy donated by weaker solitons in the course of its travel through the chain.\n\nMentions:\n- However, for larger values of β 3 , the dynamics is more complex, strongly deviating from the prediction of the perturbative analysis (see Fig. 4 below).\n- results are plotted in Fig. 4 versus order N of the input.\n- However, the relatively strong TOD makes the actual peak power of the escaping soliton significantly lower, as seen in Fig. 4.\n- Due to inelastic effects of the collisions and interaction with the conspicuous radiative component of the field, the escaping soliton experiences additional acceleration (the wavelength shift, which is shown by the solid red curve in Fig. 4).\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 4: (Color online) The wavelength shift of the tallest fundamental soliton generated by the fission of the N -soliton input (2), relative to the input wavelength of 800 nm (the solid red curve), and the peak power of this soliton (the dashed blue curve), vs. the input peak power, for β3 = 6.98 · 10−5 ps3/m. For the sake of comparison, the dotted green curve shows the analytical result (3) for the peak power of the tallest soliton in the integrable NLS equation."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 2 shows a part of a logic control that protects a heat exchanger when the compression process is going to stop. In summary, the application must take control of the valve when the compressor stops. However, there is a switch that can deactivate the application and a variable that indicates if the system is under operation. Finally, a sensor that indicates an abnormal temperature condition. The classical logic verifies the state of the variables every 5 seconds. The effective control is performed when it is detected that the compressor has stopped, the controller is on, the system is under an operation condition, and the temperature is normal. In this example, we can verify the use of a polling method to check the condition of certain variables. It is also remarkable the presence of the verification abnormal T? which regards to a higher priority condition referring to abnormal temperatures. Along with many diagrams of different processes, we can find the same verification since this condition should trigger an overriding process.\n- To e↵ectively perform the plan, two conditions must be true: switch(open) and cond op(normal). In this sense, the approaches presented in Figure 2 and Figure 3 produce similar e↵ects. Besides the agent implementation being arguably shorter and simpler, it is often faster since agents reacts promptly to events, while the SCF may delay up to 5 seconds.\n- Control logic also makes frequent use of timers. Some of them are used to avoid too much stress on checking a condition as presented in Figure 2. Other timers are placed to give some timeout for an operation. Other ones are used for supporting some process that needs to filter outlier conditions, and so on. In the case of agents, the timers were also sensibly simplified since Jason supports scheduled operations such as .at(t,e) which executes e after t period.\n\nMentions:\n- Figure 2 shows a part of a logic control that protects a heat exchanger when the compression process is going to stop.\n- In this sense, the approaches presented in Figure 2 and Figure 3 produce similar e↵ects.\n- Some of them are used to avoid too much stress on checking a condition as presented in Figure 2.\n- In this sense, the approaches presented in Figure 2 and Figure 3 produce similar effects.\n- Some of them are used to avoid too much stress on checking a condition as presented in Figure 2.\n\nOCR Text: \"\"lis T\"ti' C;Er\"\" FILEC cFjll Jl\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2. Current implementation of part of stopping compression process."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- At  with other frameworks on VOC 2007. Figure 4 Further Experiments. We train YOLOv2 for detection on VOC 2012. Table 4 shows the comparative performance of YOLOv2 versus other state-of-the-art detection systems. YOLOv2 achieves 73.4 mAP while running far faster than competing methods. We also train on COCO and compare to other methods in Table 5. On the VOC metric (IOU = .5) YOLOv2 gets 44.0 mAP, comparable to SSD and Faster R-CNN.\n\nMentions:\n- Figure 4 Further Experiments.\n\nOCR Text: \"8 881 sttr 1-0 Faster A-0\" 5 78+ ; 8 Frames Per Secand YOLOvz\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4: Accuracy and speed on VOC 2007."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- • Fixed time spans. In training, we use the last month as the target span and previous months as the feature span. In predicting, we use all available months as the feature span. Figure 3 illustrates the details of fixed time spans in both local and online settings.\n\nMentions:\n- Figure 3 illustrates the details of fixed time spans in both local and online settings.\n\nOCR Text: \"Kwu khuwwwhw ma hphu hphuwahHyyu Fukuhbk KaHu yHH Mkiu\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Fixed time spans on local and online data."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- On the stage of environment design all the possible configurations were considered and the model of the Scenario 3 was chosen so as to provide the hybrid infrastructure were the corporate and public components were used (Fig. 2).\n\nMentions:\n- On the stage of environment design all the possible configurations were considered and the model of the Scenario 3 was chosen so as to provide the hybrid infrastructure were the corporate and public components were used (Fig. 2).\n\nOCR Text: \"Imernel ICZ VPC Fublic Subnet VPN Friwat Subrer P1ver\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2. The Hybrid Cloud configuration."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The Herbarium Challenge 2019 was conducted through Kaggle as part of FGVC6 at CVPR19, with 22 participating teams and 254 submissions. The final leaderboard from the held-out private test data can be seen in Figure 5. The winning method by Megvii Research Nanjing achieved a Their method involved an ensemble of 5 different models, using the SeResNext-50, SeResNext-101 [21] and ResNet-152 [10] architectures. They also used a combination of different losses: cross-entropy, focal loss, and classbalanced focal loss [7]. Their models were pretrained on the ImageNet [14] and iNaturalist Challenge 2018 datasets, with input image dimensions of 448x448 and standard data augmentation. Additional techniques used include deformable convolutions [8], iSQRT [12] and random erasing [23].\n\nMentions:\n- The final leaderboard from the held-out private test data can be seen in Figure 5.\n\nOCR Text: \"Heroa 2019 Cnallerj? Dadero emo 1 1 1 ' 1 1\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5. Final challenge leaderboard."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In MOG, each cluster is represented by a Gaussian distribution N (X|µ k , Σ k ) with its own parameters µ k (mean) and Σ k (covariance), as shown in Equation ( 5), c k is the mixing coefficient of each cluster. For each of the chosen datasets in Table 2, we model the data distribution as p(X) in Equation( 5) and approximately, each cluster resulted data site represent a different distribution. For simplicity, we assume all clusters are with different mean vectors but share the same covariance matrix to assemble a distribution shift. The T-SNE plot is done to the SRS scenario (Figure 3) and the DRC scenario. In DRC, we use PCA as dimension reduction, keeping 10% (Figure 4) and 50% (Figure 5) of the total variance to tell if the reduced dimension makes a big difference in generating an unevenly distributed data sites scenario). From these figures, we do not observe a big difference between different percentage of variance to reserve in PCA, but observe a big difference between SRS and DRC where SRS generates a more evenly distributed data sites, while DRC generates more uneven distributions across different clusters(data sites).\n\nMentions:\n- The T-SNE plot is done to the SRS scenario (Figure 3) and the DRC scenario.\n\nOCR Text: \"dala site s 8 ds| 055\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3: Stratified Random Split (SRS)"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- One of the most important issues of this experiment is the measurement of polarized status of the final output FEL radiations. In ref. [19], we have already proposed a method for polarization measurements. The proposal is based on the basic optic principle, i.e. to figure out the 4 Stokes parameters [23], thus calculate the polarization degrees. The optical diagnostics station has been already finished design and installation, now the workbench is in the drive-laser hutch for off-site testing and calibration (see Figure 6). The off-line calibrations of the optical workbench have already been finished by using a standard 523 nm laser, i.e., the 2 nd harmonic of the photocathode driven laser. By tuning the polarizer and the quarter-wave plate, several groups of intensity can be obtained by energy-meter, from which Stokes parameters can be calculated, then the total polarization and circular polarization degrees. Figure 7 shows us two groups of the measurement results, and the corresponding calibrated results can be found in Table 1.\n\nMentions:\n- Figure 7 shows us two groups of the measurement results, and the corresponding calibrated results can be found in Table 1.\n\nOCR Text: \"Crciapoliizinn Ellijucal plnzatitr (cegee)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 7: Two groups of off-line calibration results at the optical diagnostics station."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Figure 3 we show examples of configurations which minimize and maximize the entanglement harvested.\n\nMentions:\n- In Figure 3 we show examples of configurations which minimize and maximize the entanglement harvested.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 3. Examples of relative orientation of the atoms that (a) minimize and (b) maximize the entanglement harvested from the field when all the 1s→ 2p transitions are considered. The Euler angles for the configuration of maximal harvesting shown are (π/4, 1.231, π/4)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To illustrate the typical strength of these constraints, we compute the median z-and K s -band contrast curves used in this work, derived from speckle imaging and AO, respectively. We plot these median contrast curves along with their 16 th -84 th percentile ranges in Figure 4. We obtained high resolution optical spectra for most of the targets in this work using Keck/HIRES, described in detail in P18. From these spectra we derive stellar parameters using SpecMatch-syn (Petigura et al. 2017) for stars hotter than 4200 K, and SpecMatch-emp (Yee et al. 2017) for cooler stars. In addition, we refer the reader to Dressing et al. (2017a) and Martinez et al. (2017), in which spectroscopic analyses of many of the M dwarfs in the sample were presented. As an input to vespa, we adopt the constraints on secondary stars determined by ReaMatch (Kolbl et al. 2015) (presented in P18), which are typically ∆V ≤ 5 mag for v sin i ≥ 10 km s −1 . We show a plot illustrating this analysis in Figure 5. For a subset of 21 late-type stars in this work, we adopt the stellar parameters of Dressing et al. (2017a) and Martinez et al. (2017), who obtained medium resolution NIR spectra with IRTF/SpeX and NTT/SOFI. In total, 119 of the host stars we analyze here have spectroscopicallyderived parameters.\n- Several cases of low mass eclipsing stellar companions that were initially classified as planets via statistical validation have recently come to light (Shporer et al. 2017b); these stars have radii in the range 0.9-1.9 R Jup , consistent with planets in the Jovian size regime (see also Mordasini et al. 2012). Thus, to err on the side of caution, we do not validate any planet candidate with radius larger than 10 R ⊕ (0.89 R Jup ). This cautionary radius threshold can also be seen as an empirically sound choice based on the FPPs of the Kepler candidates presented by Morton et al. (2016), which rise quickly from ∼1% above 10 R ⊕ in aggregate (see e.g. Figure 4 of Morton et al. 2016). Candidates with radii larger than this in Table 2 are indicated by \"LR\" in the Note column. Many candidates with large radii are clear false positives based on their FPPs, but we do not validate several candidates with large radii in spite of their low FPPs. Future RV measurements of these unvalidated candidates are likely to reveal many of them to be giant planets.\n\nMentions:\n- We plot these median contrast curves along with their 16 th -84 th percentile ranges in Figure 4.\n- This cautionary radius threshold can also be seen as an empirically sound choice based on the FPPs of the Kepler candidates presented by Morton et al. (2016), which rise quickly from ∼1% above 10 R ⊕ in aggregate (see e.g. Figure 4 of Morton et al. 2016).\n\nOCR Text: \"4 5,0 101 Separatior [arcsec]\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4. The median and 16th – 84th percentile range of the z- and Ks-band contrast curves used in this work. The z-band contrast curves are derived from speckle imaging observations, which have a smaller field of view than the Ksband AO images."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To further evaluate the subjective quality of our method, we also conduct a mean opinion score (MOS) test at QP= 37. Specifically, we asked 15 subjects to rate an integral score from 1 to 100 following [31] on the enhanced videos. The higher score indicates better perceptual quality. The subjects are required to rate the scores for compressed video sequences, raw sequences, sequences enhanced by Li et al. [21], MFQE [42], MFQE 2.0 [12], our MW-GAN method. Table 3 presents ∆MOS which is calculated between enhanced and compressed sequences. As can be seen, our MW-GAN method obtains the highest ∆MOS score for each class of sequences, and it obtains an average increase of 4.55 in terms of MOS, considerably better than other methods.  [36]. Note that the channels of the first convolutional layer before the first RRDB are increased to keep the structure of RRDB unchanged. (2) MW-GAN-CNN: We directly replace the WPT and IWPT in WDRB by average pooling and upsampling layers. Note that the above two strategies all lead to parameter increasing. Figure 6 shows the results of the above two ablation strategies over different classes of test sequences at QP= 32. We can see from this figure that WDRB has a positive impact on the perceptual quality, leading to 0.170 and 0.126 decrease in ∆PI compared with MW-GAN-CNN and MW-GAN-RRDB, respectively. Therefore, we can conclude that WDRB plays an effective role in MW-GAN.\n- Effectiveness of WPT in the discriminator: To evaluate the effectiveness of the multi-level wavelet-based discriminator, we add a baseline without WPT in the discriminator, i.e., MW-GAN-D w/o WPT. Specifically, we replace WPT in the discriminator with an average pooling layer that extracts features with the same size as wavelet sub-bands and keeps other components unchanged for a fair comparison. As Figure 6 shows, without WPT in the discriminator, the ∆PI score increases by average 0.192 over the whole test set, which indicates the effectiveness of the proposed multi-level wavelet-based discriminator. Fig. 6. Comparison between our MW-GAN and corresponding baselines on the test sequences of JCT-VC [29] according to −∆PI between enhanced and compressed sequences. Effectiveness of the proposed MW-GAN can be verified.\n\nMentions:\n- Figure 6 shows the results of the above two ablation strategies over different classes of test sequences at QP= 32.\n- As Figure 6 shows, without WPT in the discriminator, the ∆PI score increases by average 0.192 over the whole test set, which indicates the effectiveness of the proposed multi-level wavelet-based discriminator.\n\nOCR Text: \"Na34 (raule Wooh (29\"Ghe Na3 4 ce Ummule H#e# JaLFT= Ineiaane\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 6. Comparison between our MW-GAN and corresponding baselines on the test sequences of JCT-VC [29] according to −∆PI between enhanced and compressed sequences. Effectiveness of the proposed MW-GAN can be verified."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The size-dependent a-C(:H) particle photo-processing timescales, τ UV,pd , can be derived as per Eq. (31) of paper II by inserting appropriate values for the particle absorption efficiency, Q abs (a, E), and the photo-darkening efficiency, ǫ (here we assume ǫ = 0.1 but the exact value is uncertain). Fig. 18 shows τ UV,pd , multiplied by the ISRF factor G 0 , as a function of particle radius for a-C(:H) materials (E g = −0.1 to 2.67 eV), where surface hydrogenation has been included. Presenting the a-C(:H) evolution in this way, Fig. 18 indicates that in the diffuse ISM Fig. 18. a-C(:H) particle photo-processing time-scales, τ UV,pd , multiplied by the ISRF factor G 0 , as a function of particle radius for increasing band gap from top (−0.1 eV, grey) to bottom (2.67 eV, purple). A photo-darkening efficiency, ǫ = 0.1, was assumed here.\n\nMentions:\n- Fig. 18 shows τ UV,pd , multiplied by the ISRF factor G 0 , as a function of particle radius for a-C(:H) materials (E g = −0.1 to 2.67 eV), where surface hydrogenation has been included.\n- Presenting the a-C(:H) evolution in this way, Fig. 18 indicates that in the diffuse ISM Fig. 18. a-C(:H) particle photo-processing time-scales, τ UV,pd , multiplied by the ISRF factor G 0 , as a function of particle radius for increasing band gap from top (−0.1 eV, grey) to bottom (2.67 eV, purple).\n\nOCR Text: \"Log radiu;\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 18. a-C(:H) particle photo-processing time-scales,τUV,pd, multiplied by the ISRF factorG0, as a function of particle radius for increasing band gap from top (−0.1 eV, grey) to bottom (2.67 eV, purple). A photo-darkening efficiency,ǫ = 0.1, was assumed here."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The measured nonadiabatic factor Q * (t) and mean work during the expansion stroke are shown in Fig. 6. For the reference driving, the Q * (t) of the strongly interacting Fermi gas monotonically increases, witnessing quantum friction induced by excitations that emerge from the nonadiabatic dynamics. At time t f = 800µs, Q * (t f ) approaches to 1.17 for the experimental parameters (brown dots in Fig. 6A). Quantum friction decreases the mean work of the stroke, κ(t) = W (t) / Ĥ (0) (see Fig. 6B). By contrast, quantum friction is greatly suppressed with the local counterdiabatic driving scheme. Indeed, Q * (t) reduces to 1 after completion of the superadiabatic stroke (blue dots in Fig. 6A), revealing that it is a friction-free process. Using STA by LCD thus allow reaching Q * (t f ) = 1, enhancing the work output. Its maximum value is reached at t f and is determined by the geometric mean frequency according to\n- The measured value of the mean work κ in the experiment is about −0.37 in units of Ĥ (0) (blue dots in Fig. 6B), which is in very good agreement with the theoretical prediction, (2 −2/3 − 1) = −0.370. Compared to the case of reference driving process, the mean work in the the local counterdiabatic driving process is increased by nearly 42.3%. As seen from Eq. 9, in a STA by LCD quantum friction is suppressed and the final mean work depends only on the ratio of geometric mean frequency between the initial state and the target state. For processes satisfying ν(t f ) = ν(0), such as a change of the anisotropy of the cloud, the mean work vanishes. We verify this prediction experimentally. The strongly interacting Fermi gas is first prepared in a quantum state with frequencies of ω x (0) = 2π × 1200 Hz and ω y (0) = ω z (0) = 2π × 300 Hz. Then, following by Eq. 62, the trap frequencies are changed to ω 22 Hz, modifying the aspect ratio of the cloud. The measured nonadiabatic factor Q * (t), mean work and cloud expansion images are presented in Fig. 7. It is apparent that there is no friction Q * (t f ) = 1 (blue dots in Fig. 7A) and that the mean work vanishes W (t f ) = 0 (blue dots in Fig. 7B) when manipulating the quantum state using STA by LCD. For the nonadiabatic reference driving, quantum friction is however induced and causes Q * (t f ) &gt; 1 (brown dots in Fig. 7A), signaling energy dispersion in the final state. The mean work is then increased to a positive value of 1.22 Ĥ (0) (brown dots in Fig. 7B).\n- where b j denotes the expansion scaling factor in j direction and ω j,0 is the initial frequency measured by the parametric resonance for the atomic cloud. During the local counterdiabatic driving of the system, the time-dependent frequencies are given by ω j (t) = Ω j (t), t 0 &lt;t ≤ t 0, t &lt;t ≤ t + t tof (51) Here t 0 is the time when we start the STA process, t = t 0 + t exp is the time at which we turn off the trap, and t tof is the duration of the time-of-flight expansion for imaging of the cloud. Using equations ( 50) and (51), we can numerically calculate the expansion factors at any time t using the boundary conditions b j (t 0 ) = 1 andḃ j (t 0 ) = 0 for all j = x, y, z. In Fig. 6, we show σ z and σ z obs of the unitary Fermi gas for a time-of-flight t tof = 600µs.\n\nMentions:\n- The measured nonadiabatic factor Q * (t) and mean work during the expansion stroke are shown in Fig. 6.\n- At time t f = 800µs, Q * (t f ) approaches to 1.17 for the experimental parameters (brown dots in Fig. 6A).\n- Quantum friction decreases the mean work of the stroke, κ(t) = W (t) / Ĥ (0) (see Fig. 6B).\n- Indeed, Q * (t) reduces to 1 after completion of the superadiabatic stroke (blue dots in Fig. 6A), revealing that it is a friction-free process.\n- The measured value of the mean work κ in the experiment is about −0.37 in units of Ĥ (0) (blue dots in Fig. 6B), which is in very good agreement with the theoretical prediction, (2 −2/3 − 1) = −0.370.\n\nOCR Text: \"Iood 1200 1400 Expanslon Tline (us)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 6: Determination of the cloud size 〈σz〉 obtained from the Gaussian fit of the density profile of the unitary Fermi gas for a time-of-flight ttof = 600µs. The blue dots are the data 〈σz〉obs and the red dots are 〈σz〉 = 〈σz〉obs/btof . The red solid line is the best fit based on theory curve. The blue solid line is the red solid line multiplied by btof . Error bars represent the standard deviation of the statistics."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 11 suggests an inverted U relation between an article's direct citation count and the depth of citation cascade. The depth is divided into seven segments. With increasing cascade depth, the median and quartile values of direct citation count for articles in each segment first increase and then decrease. This indicates that (1) the growth of citation cascade depth at first depends on the number of first-generation citations of the source article, since young cascades have more opportunities to grow when they have more direct citations; but (2) the dependence gets weaker and weaker when cascade depth reaches a threshold, after which the cascade's growth benefits more from indirect citations. Yet, as is shown in cascades with depth range (54, 63], extremely deep cascades still benefit from a high direct citation count. Figure 12 suggests a similar inverted U shape between direct citation count and structural virality of the corresponding cascade. The median and quantiles of direct citations first increase, then decrease with increases in structural virality. The result is reasonable, since we calculate structural virality by the average shortest path length between the root of a cascade and its children. Previously, we analyzed the relation between direct citation counts and topic relevance, finding that total relevance increases with direct citation counts while average relevance simultaneously decreases. Now, we investigate how direct citation impact changes with average topic relevance by diving into the first three citation generations.\n\nMentions:\n- Figure 11 suggests an inverted U relation between an article's direct citation count and the depth of citation cascade.\n\nOCR Text: \"{\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 11. Number of direct citations increases and then decreases with cascade depth"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In this paper we have presented a comparison of three classification methods in terms of accuracy and execution time. We reported their performance with and without input obfuscation. Overall, KNN provides the best trade-off in terms of accuracy and execution time with both the original and the obfuscated data sets. We tested each of the algorithms following the wrapper method by increasing the number of input features in each model in steps of one. The results, summarized in Tables 6, 7, and 8, show that the metadata can be effectively exploited to classify individuals inside a group of 10,000 randomly selected Twitter users. While both KNN and RF are similar in terms of accuracy and execution, MLR is consistently outperformed. Moreover, as shown in Figures 3, 4, and 5, as the number of output classes increases, the difference in performance becomes more pronounced. It is also important to note that, as shown in Table 8, the accuracy exhibited by MLR depends entirely on the six constituent features of the account creation time. Finally, the performance of MLR is the most sensitive to obfuscation. As shown in Figure 11, the rounding algorithm results in a monotonic drop in accuracy for the best performing combinations. One challenge in multi-class classification lies in the scalability of the algorithm. We found that while both the number of input features and the number of output classes have a detrimental impact on performance, the bottleneck of the algorithm is caused by the number of output classes. To address this, we implemented a divide and conquer algorithm that partitions the users and creates models with fewer output classes. The intermediate outputs of each of the smaller models are then combined to produce a single result. Figure 13 shows that this implementation does not affect the accuracy of the classification algorithms. As shown in the figure, varying the size of the partitions results in the same precision and recall for the same total number of users as compared to the results obtained from a single model. Having shown that the results are equivalent the implementation becomes a contribution from our work: partitioning the number of output classes in intermediate steps results in less memory consumption and faster execution times.\n\nMentions:\n- As shown in Figure 11, the rounding algorithm results in a monotonic drop in accuracy for the best performing combinations.\n\nOCR Text: \"feamne do; - minule MunutO Minuo Coco7 1 Fraction of Obluscated Training Data\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 11: MLR Change in predictive accuracy with obfuscated training data."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- A broad rectangular pulse of a unit amplitude with period = 10 s and duration /2 was used as an input disk signal ( ). A square step signal contains high-frequency components and therefore it is suitable to evaluate reconstruction quality of sharp signal details. Long pulse duration is needed to avoid overlapping of the rise and fall responses. Ring signal ( ) was produced by convolution (eq. 1) with reference IRF ℎ ( ) calculated for a reference diffusion coefficient value . Then IRF was corrected (or rather corrupted) according to eq. 4 for a range of diffusion coefficient values. The corrected ℎ ( ) were used to reconstruct disk signal. An example of reconstructed _ ( ) for = 1.5 is presented in Fig. 4. Mean square error was used as a measure of reconstruction accuracy. Reconstruction with reference IRF ℎ ( ) resulted in MSE ⁓ 10 -8 which is practically ideal. The resulted MSE values are plotted in Fig. 5 against diffusion coefficient values used to correct IRF. To evaluate effectiveness of deconvolution approach we compare it to the ring signal shift approach. Disk signal can be estimated simply by shifting back the delayed ring signal and scaling it in order to allow for the collection efficiency :\n- According to our calculation the best result in terms of mean square error (MSE) is achieved for ℎ = 1.58 . (The result of such scale/shift approach is presented in Fig. 4.) The shift time ℎ also depends on the diffusion coefficient value. The MSE calculated for the ring signal shift approach is presented in Fig. 5 as a function of value used to estimate ℎ . It should be noted, that the data represented if Fig. 5 for both approaches are independent of rotation speed Ω. The deconvolution approach significantly outperforms the ring signal shift approach if the IRF is accurately corrected for the diffusion coefficient value. Unfortunately, it loses its effectiveness if the estimated is 1.5 times under-/overestimated. Nevertheless, the acceptable error window is wide enough to apply deconvolution approach in practice.\n\nMentions:\n- An example of reconstructed _ ( ) for = 1.5 is presented in Fig. 4.\n- (The result of such scale/shift approach is presented in Fig. 4.)\n\nOCR Text: \"recun3tructed Iccushified\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4. Disk current: true signal (black), signal reconstructed by means of the deconvolution approach (green) and signal estimated by rescaling and shifting the ring signal (blue). An overestimated (x1.5) value of the diffusion coefficient was used to calculate IRF for the reconstruction procedure. Electrode rotation speed – 800 rpm."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The track path length is the main handle to study the muon response. Figure 25 shows the response of cells in the second layer as a function of the path length x. It includes cosmic events crossing the BC cells over the entire barrel and over all accepted angles. A clear edge at the path length of 840 mm is visible in the figure. This represents the radial depth ∆R of the BC layer cells. Since most cosmic rays are vertical, a large fraction of the muons crossing the central region have a reconstructed path length equal or slightly larger than the layer radius. This is very evident for all cells with a z-coordinate within the vertical coverage of the SCT detector |z 0 | &lt; 1 m. A linear fit to the corresponding distribution of mean values shows that the muon response scales approximately linearly with the path length, as expected. Figure 25 suggests that the ratio of the cell response with the track path length, i.e. the slope of dE/dx, is one of the quantities that can be used to study the cell/layers intercalibration. This will be discussed in more detail in Section 5.3.\n\nMentions:\n- Figure 25 shows the response of cells in the second layer as a function of the path length x.\n- Figure 25 suggests that the ratio of the cell response with the track path length, i.e. the slope of dE/dx, is one of the quantities that can be used to study the cell/layers intercalibration.\n\nOCR Text: \"4JCU 9 4oc] ATLAS L 35cok 30CO1 20C0 1500 1OcO1 50| BC0 doo Tack Patn Lennth Imm 25C0'\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 25: Mean response of the barrel module BC cells as a function of track path length for tracks with 10 < p < 30 GeV/c. A linear fit to the corresponding distribution of mean values is superimposed. The excess of events at around the track path length of 840 mm (radial size of the barrel module BC cells) is a purely statistical effect, since most of the cosmic ray muons enter the calorimeter at small zenith angle."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Given discrete random variables X d = (X 1 , . . . , X d ) with the ranges X d X 1 × • • • × X d and joint distribution P X d , we model the common structure shared by these random variables as a high-dimensional latent variable W , in which the random variables X 1 , . . . , X d are conditionally independent given W , i.e., P X d |W = Π d i=1 P X i |W , as depicted in Fig. 2. Our goal is to learn this common structure from i.i.d. sample vectors generated from P X d . Since the correlation between W and X i 's is generally complicated, it is difficult to directly identify and learn the Fig. 2: The random variables X 1 , . . . , X d are conditional independently generated from some hidden structure W . structures of W without the labels and assumptions on the generating models of X i 's as in the unsupervised learning scenarios. Therefore, instead of identifying the high-dimensional latent variable W , we focus on learning the low-dimensional random variable U that contains much common information shared between X i 's, which can be viewed as the informative attribute for the common structure.\n\nMentions:\n- , X d are conditionally independent given W , i.e., P X d |W = Π d i=1 P X i |W , as depicted in Fig. 2.\n- Since the correlation between W and X i 's is generally complicated, it is difficult to directly identify and learn the Fig. 2: The random variables X 1 , . . .\n\nOCR Text: \"27 2 W 2 2 7 2)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2: The random variables X1, . . . , Xd are conditional independently generated from some hidden structure W ."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- While a rigorous proof of the convergence of the scheme seems difficult and falls outside the scope of this work, in Figure 2 we show that the numerical error e k converges to zero as the number of iterations increases. Moreover, the convergence is monotone, which results in a good stability of the scheme.\n\nMentions:\n- While a rigorous proof of the convergence of the scheme seems difficult and falls outside the scope of this work, in Figure 2 we show that the numerical error e k converges to zero as the number of iterations increases.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2. The trajectory of the error ek for the 16 iterations of the algorithm when ε = 10−3."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Supply Chain Management (SCM) consists on the integration of companies' processes according the exchanges between the suppliers, the customers and the company. It includes, fundamentally, the purchasing, production and distribution processes. The figure 3 presents a summarized illustration of a company's supply chain based on user task. The concept of supply chain Management (SCM) has raised the prominences over the past ten years for many reasons [1]. The globalization of supply and the requirements of customers have encouraged companies to implement efficient solutions for the best coordination and automation of their process. This global orientation and the increased performance-based competition combined with the technology evolution and economic conditions have contributed sequentially on the marketplace. For that, availability and flexibility of the supply chain is required to keep their place in the market.\n- Based on Stuart Taylor from cisco has proposed in [40] some perspectives on the future of the mobile industry. And in order to enhance competitive advantage through effective SCM, and with the growing of the web technology and mobile communication, we propose a model for a multi agent based services, illustrated on figure 4, according to the company's schema management (Figure 3). The model consists on deploying for each process an m-service able to be interacted with the other. Under this model, we propose to integrate in the internal infrastructure a:\n\nMentions:\n- The figure 3 presents a summarized illustration of a company's supply chain based on user task.\n- And in order to enhance competitive advantage through effective SCM, and with the growing of the web technology and mobile communication, we propose a model for a multi agent based services, illustrated on figure 4, according to the company's schema management (Figure 3).\n\nOCR Text: \"S=\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3. An illustration of a company's supply chain management based on user task"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In particular, we propose to precompute reachability during different predefined time intervals. To this end, we break T into a set of disjoint intervals I1, I2, . . . , In with equal length L, and precompute reachability between vertices in Ct a and Ct b for each Ii=[ta, t b ]. Accordingly, DN is augmented with a new directed edge from every connected component c ∈ Ct a to every other connected component c ′ ∈ Ct b if there is a path of length L from c to c ′ . We call such edges the long edges and weight them by L which indicates the number of time instances that encompass. The resulted augmented hyper graph HN can be considered as the union of DN with a new graph consisting of long edges each with a weight L. We term the latter graph contact network at the L-th resolution and denote it by DN L . Accordingly, DN can be considered as the contact network at first resolution or DN 1 . One can extend this idea and precompute reachability at other time intervals to generate a multi-resolution graph HN =DN ∪ DN L 1 ∪ DN L 2 ∪ . . . ∪ DN Ln . However, this can significantly increase the number of edges if overdone and hence adversely reduce the efficiency of query expansion. In Section 6, we experimentally select the optimal resolutions for HN . Figure 6 depicts DN 3 where DN shown in Figure 5.\n\nMentions:\n- Figure 6 depicts DN 3 where DN shown in Figure 5.\n\nOCR Text: \"0,(317 0,3 0,(0| 0,(3\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6: DN3 for HN whose DN1 is the graph in Figure 5"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Openstate controller. To support Openstate features at control plane, the well known Ryu SDN software framework [18] has been extended in order to integrate the OFPT STATE MOD message and the OFPIT SET STATE instruction. The message structure for both messages reflects the ofp state mod and ofp instruction set state structures defined above. Figure 5 shows an excerpt of an OpenState application code.\n\nMentions:\n- Figure 5 shows an excerpt of an OpenState application code.\n\nOCR Text: \"sen_en_unxete(sel * datzpatr|: ~ehaecatifat\",orfrot: Vcalolac ?mc urzolem]U,d6e \"ulduiceckau -Earmle tatjralt,ol:.07*ec_JET_M ExThcTIR H, p,Mu%F_[ue_Socl} tatypatt, *\"ness| s4V_Vicat? ectra tc -| se1a_xfse e :r((sel : datepatt | Falch cataral\" ,olecol: pansen, CAmene\" 62Le0)61-5014 \"J2-O(NEO) Aroljel7,40_:st-rort_Ls: [steiel) jatjrat',ofarot:_parser,[FPrstcict:\"ngtizelstate+E] acticos Knitarinn.pgrota Farse - efpae- onut -al d4.al4,h, : Uprzro_re\"s:6 , OFFfliaoi; Fatlcatuedateoatt Mc en (oxhie_ra3te6, Idue_1j-6, (6etem]-@mocgto Ipt(_ad, ile_ iarut-u, \"d;ieut-e, preec L4p32968, Cuffe _ ceonptot: , 0Fp_Wa_BuFfeR, FLemeenaacole MTF EleFimjole.u7tuAul Iljs-} , Mtcherit:' , *ustn ctL:cs-Jest)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: Excerpt of a Openstate application written in python"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Fig. 3 we show one example for the three different types of electron trajectories 1HCNT, 1HCRT, and 2HCT with the same asymptotic momentum k z = 1.5 a.u. and k ⊥ = 0.17 (corresponding to the first minimum in the holographic structure with longitudinal momentum close to maximum emission according to the SMM). The two trajectories released during the first half cycle 1HCNT and 1HCRT have about the same ionization time (within the statistical uncertainty) t 0 = 28.02 a.u. This is a general characteristic for all 1HCNT and 1HCRT. The initial position after the first step (tunneling) depends only on the ionization time and is z 0 = − I P /F (t 0 ) = −6.76 a.u. for both types of trajectories. What makes the difference between the two trajectories 1HCNT and 1HCRT is the initial transverse momentum which is v ⊥0 = 0.265 a.u. for 1HCNT and v ⊥0 = −0.0594 for the 1HCRT. We can see an abrupt change of direction in the 1HCRT due to the collision with the nucleus when z 0, which changes the direction of the transversal velocity. This collision takes place at t = 149.76 a.u., which is about 25 a.u. ( 20 % of an optical cycle) after the end of the electric field (2π/ω = 125.66 a.u.). The trajectory released within the second half cycle 2HCT starts its trip in the continuum at t 0 = 90.69 a.u., which is close to the SMM prediction that the sum of the ionization times for 1HCNT and 2HCT is 2π/ω = 125.66 a.u. The difference stems from the action of the Coulomb potential on the electron trajectories (departing from the SFA). The initial position is z 0 = − I P /F (t 0 ) = 6.77, almost the same as the trajectories released during the first half cycle but with the opposite sign due to the inversion of the potential barrier. Finally, we plot an example of a GT with the same asymptotic longitudinal momentum k z = 1.5 and, by definition, null asymptotic transversal momentum since the collision is not enough to bend the trajectory so that the final transverse momentum has opposite sign to the initial transverse velocity, that in this case is v ⊥0 = −0.126. There are three different values of the initial perpendicular velocity v 0ρ contributing to the electron yield with a particular value of the final momentum k ρ corresponding to the three different types of trajectories.\n\nMentions:\n- In Fig. 3 we show one example for the three different types of electron trajectories 1HCNT, 1HCRT, and 2HCT with the same asymptotic momentum k z = 1.5 a.u. and k ⊥ = 0.17 (corresponding to the first minimum in the holographic structure with longitudinal momentum close to maximum emission according to the SMM).\n\nOCR Text: \"z(au,|\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 3. Examples of the three different types of trajectories present in the photoionization process: In blue 1HCNT, in red 1HCRT, and in gray 2HCT. All of these three trajectories have asymptotic momentum kz = 1.5 a.u. and k⊥ = 0.17. Besides, in dotted line the GT (transition between 1HCNT and 1HCRT, i.e., k⊥ = 0)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- • Update after G Membership Changes or Breakdown: Each new user joining the sensing just establishes a pairwise secret key with FC and GW . This has no impact on existing users. If some users leave the network, FC and GW remove their secret keys, which also has no impact on existing users. In both cases, and also in the case of a breakdown or failure, λ must be updated accordingly. Fig. 3:\n\nMentions:\n- Fig. 3:\n\nOCR Text: \"61\" \"PE €\"mpITS Ech R\"Yatn: C Illksc$ %co Selds dxitec Iu Jll SUs\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3: LP-3PSS protocol, θi ← EkFC ,GW (OPE .EkFC ,i(τ)), ςi ← EkGW ,i(OPE .EkFC ,i(RSS i)) and ζ ← EkFC ,GW ({bi}ni=1)"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The statistical uncertainties in the determination of fSDSS are very large in places; this is principally the result of large uncertainties in the true number of BAL troughs with lowdcor. For a number of datapoints the value plotted in Fig. 25 is dominated by just one or two BALQSOs that lie in regions with very low p det . The strong dependence of the uncertainties on the mean depth can be seen in Fig. 26, which shows fSDSS as a function ofdcor, averaged over all redshifts and luminosities. The results are binned according to the mean depths at which p det was measured. It is clear that the small number of troughs observed with lowd in the catalogue presented in Section 5 is due to the small detection probabilities, rather than an intrinsic lack of shallow absorbers.\n\nMentions:\n- The strong dependence of the uncertainties on the mean depth can be seen in Fig. 26, which shows fSDSS as a function ofdcor, averaged over all redshifts and luminosities.\n\nOCR Text: \"(ote led Me:\" deplh\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 26. Observed (circles) and estimated SDSS (squares) BALQSO fractions as a function of corrected mean depth, binned in the d̄cor ranges shown by the horizontal error bars. Symbols are plotted at the d̄cor values for which pdet was tested. Note that many of the vertical error bars are smaller than the plotted symbols."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 1, shows a plot of the dispersion relations of the AF layer given in Eq. (8), where the possible scattering processes are identified. In contrast to an N(F)-S junction, an AF-S junction permits both specular AR and retro normal reflection (NR) [56][57][58].\n- Here a = e iφ J 0 sin θ and b = J 0 cos θ. Unlike the eigenvectors of F in the F-S junctions the eigenvectors depend on the energy E, the chemical potential µ AF , and the exchange interaction J 0 [1-3]. In Fig. 1 we have plotted the branches q ± e,h , and the scattering processes we are considering. The wavefunction describing an incident electron from the AF is\n- where r ± i represents the reflection amplitude for a particle being reflected to the branch q ± i with i = e, h. For small temperatures T &lt; T c the Fermi-Dirac distribution makes sure that only the scattering processes with E µ AF − J 0 contributes to the electrical and thermal conductance. The sign in the exponents are determined by the z-component of the group velocity v z = ∂E/∂q z of the corresponding branch in Fig. 1.\n\nMentions:\n- Figure 1, shows a plot of the dispersion relations of the AF layer given in Eq. (8), where the possible scattering processes are identified.\n- In Fig. 1 we have plotted the branches q ± e,h , and the scattering processes we are considering.\n- The sign in the exponents are determined by the z-component of the group velocity v z = ∂E/∂q z of the corresponding branch in Fig. 1.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 1. The dispersion relation and the allowed scattering processes in the AF. Electrons (holes) are drawn as filled (empty) circles. The branches are parameterized as given in the figure. Incoming (reflected) particles are represented with a right (left) arrow. The new scattering processes not present in N(F)-S junctions are the ones labelled by q−e,h. The processes contributing to the conductances are the ones where E µAF − J0. Higher energies are suppressed by the Fermi-Dirac distribution."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Another way to analyze the total magnetization is by exploring the temperature dependence T /T c of the total magnetization. In Fig. 13 we plot the magnetization as a function of temperature for two values of J/T c = ±1 and J 1 = −1.0, where the magnetization of the saturated FM state and the FIM state is illustrated. For the case of J/T c = 1 we have the FM region with M = 1 the total magnetization at zero temperature. The total mag- netization vanishes at T = T c as temperature increases; therefore, a second-order phase transition occurs at the critical temperature.\n- For J/T c = −1, the total magnetization corresponds to the FIM region, with M = 1/3 at zero temperature, and it vanishes at T = T c as displayed in Fig. 13. Thus a second-order phase transition occurs again at the critical temperature, which is in agreement with the critical point curve displayed in Fig. 7.\n\nMentions:\n- In Fig. 13 we plot the magnetization as a function of temperature for two values of J/T c = ±1 and J 1 = −1.0, where the magnetization of the saturated FM state and the FIM state is illustrated.\n- For J/T c = −1, the total magnetization corresponds to the FIM region, with M = 1/3 at zero temperature, and it vanishes at T = T c as displayed in Fig. 13.\n\nOCR Text: \"FI 1.0 . JT 1.0 TT\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 13: Temperature dependence of the total magnetization of the pentagonal Ising model for two different values of J/Tc and a fixed value J1 = −1.0."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The proposed ontology-based essay grading system (On-toEG 1 ) consists of a set of integrated natural language tools (see Fig. 1). The system is structured on layers. The first layer contains the Text2Onto tool [3], used to obtain a consistent ontology from a corpus of high ranked or relevant essays in a given domain. The second layer exploits the OWLNatural service [9], to generate natural text from a selected ontology. We organise the text generated by OWLNatural as a set of hypotheses.\n- The system components and the main workflow appear in Fig. 1. The following four components are detailed: (1) developing the domain ontology, (2) generating hypothesis from ontology, (3) textual entailment methods, and (3) natural language processing of essays.\n\nMentions:\n- The proposed ontology-based essay grading system (On-toEG 1 ) consists of a set of integrated natural language tools (see Fig. 1).\n- The system components and the main workflow appear in Fig. 1.\n\nOCR Text: \"VA Iedurtr Pat J66_Tert Er Ah Jteetn Iceo An Kak E4|\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1. System architecture."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- -Route web traffic in site D using the default priority. To evaluate setup time, we initiate various numbers of TCP connections on port 80 between H1 and H40, and measure the response time for each. We define the response time as the amount of time which is needed to establish all of the connections between the two endpoints. This is the amount of time which is needed to read and parse a network policy, generate flow rules , and install generated rules on the network devices for all of the connections between the two end-points. By increasing the number of TCP connections established between two end-points, we increase the number of OpenFlow rules that must be installed in network switches along the path. Consequently, we measure the response time as a function of number of flows and compared the response time of OSDF with reactive forwarding approach implemented by ONOS, and compare the two in Fig. 12. As the results show, the response time increases linearly as number of connections increase between two end-points. The reason that our system outperforms the ONOS reactive approach arises from an optimization in which our system preinstalls OpenFlow rules in all switches across the entire end-to-end path when the first PACKET IN message arrives at a controller. The ONOS reactive forwarding approach waits until a PACKET IN message arrives from a switch before installing a forwarding rule in the switch. To validate the simulation results, we ran the same experiment on our testbed; Fig. 13 shows the results. The experimental measurement confirms that the simulation results are valid. Note that the TCAM table on network switches in our testbed can hold up to 2K entries when more than 2 match fields are used by flow rules, so the number of connections is limited.\n\nMentions:\n- To validate the simulation results, we ran the same experiment on our testbed; Fig. 13 shows the results.\n\nOCR Text: \"WictteLei\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 13. Response time, ONOS reactive forwarding vs OSDF for the testbed"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Fig. 9 we show the same oscillations in the later time, τ ≥ 115. We can confirm clearly the relative inphases behaviors between the boson and fermion oscillations and the positive shift of the mean values of ∆x T . Furthermore, we find the expansion of the fermion gases for the transverse direction, which has been shown also in the longitudinal oscillations. In Fig. 9 the fermion gas expansions appear moderately for In order to examine the oscillation behaviors, we show the transverse strength functions S T in Fig 10.\n\nMentions:\n- In Fig. 9 we show the same oscillations in the later time, τ ≥ 115.\n- In Fig. 9 the fermion gas expansions appear moderately for In order to examine the oscillation behaviors, we show the transverse strength functions S T in Fig 10.\n\nOCR Text: \"Ic HOSO au TOHO OSCHIOIS KS I VaE al Ihe Mtal le He OsHaloh sats €7 _ Hh he ITB mode; Whch IS HOTOOHOUS Bul HorHeouOFuhase ialcndtons; Hte\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 9: (Color online) Time evolution of∆xT (B) (boson oscillation, dotted lines) and∆xT (F) (fermion oscillation, solid lines) for the in-phase initial condition. The panel (a), (band (c) are forh ≡ hBF/gBB =−1.273,−2.0 and−2.5."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Transparency to electrons -The mesh's transparency to electrons traversing from the drift into the amplification region has been addressed in detail in the experimental and simulation studies presented in the first part of this thesis. As discussed in chapter 3 and shown in figures 8.2 and 8.3, meshes woven from thinner wires yield a slightly reduced electron transparency compared to thicker wire meshes with comparable open area, and are thus unfavorable (d-). Electrical field strength -Despite being primarily determined by the potential difference between the cathode, the mesh and the anode, the electrical fields are sensitive to small changes in the mesh's geometry and position. The impact of variation of the mesh parameters has been studied using a finite element study with COMSOL Multiphysics ® [63] in a 2-dimension cross-section of the Micromegas at the center of a mesh aperture. Compared to the ANSYS ® and Garfield++ Simulation utilized for the majority of the studies presented in part I, this simplification allows for the simulation of a large parameter range and features a high granularity of the finite elements. It neglects, however, the impact of the second wire direction and, therefore, yields slightly biased absolute values in the electrical field strength compared to a full 3-dimensional model. This approach is, therefore, well suited for qualitative argumentation, while requiring the comparison to the full simulation for quantitative statements. An exemplary field map is shown in figure 8.4. The streamlines indicate the most probable electron path, neglecting diffusion in the gas. Electrons created in the drift volume are strongly focused on a path in-between the mesh wires and, accordingly, the electrical field strength along these paths is the main determinant for the Micromegas' gain. : Electrical Field strength in the vicinity of the mesh wires. The mesh has a periodicity of 100 µm (diameter = 30 µm, aperture = 70 µm), the strip pitch is 450 µm (strip width = 300 µm). The position and orientation of two probe lines are indicated (dashed). (COMSOL Multiphysics ® [63] simulation -the streamline (grey) density is not proportional to the field strength.) [134] The impact of the wire diameter on the electrical field strength along an electron's most probable path through the amplification gap becomes visible in comparison of figure 8.5 and 8.6, each showing five mesh configurations with different mesh apertures and comparable open areas in the same color. The transition slope between amplification and drift region is systematically steeper for the finer mesh structure, yielding a better approximation to a parallel plate setup (a+). The increased field strength is only partially due to the higher wire density, since the reduced mesh thickness reduces as well the mean distance from the wires to the anode.\n- Transparency to electrons -The strong dependence of the mesh's electron transparency to its open area is shown in the figures 8. Field strength along the electron's path -With a decreased distance between the wires, the electrical field strength along the electron's most probable path through the amplification gap (vertical probe line) increases, as shown in figure 8.5. Consequentially, a smaller aperture results in a larger gas gain, as discussed in chapter 4. Simultaneously, the mesh's shielding capability increases with smaller apertures (c+), reducing the continuous offset between the effective drift field strength E D,e↵ and its nominal value E D,nom as well as the region of non-homogeneous field strength in the drift volume (figure 8.7). While a continuous offset can easily be taken into account in a TPC-like reconstruction algorithm, the increase of the non-uniformity within the drift volume is more difficult to cope with. Hence, an aperture not wider than 80 µm between neighboring 30 µm-wires is recommended, to limit the E D,e↵ uncertainty in the top 80 % of the drift gap to  1 % • E D,nom .\n\nMentions:\n- (COMSOL Multiphysics ® [63] simulation -the streamline (grey) density is not proportional to the field strength.) [134] The impact of the wire diameter on the electrical field strength along an electron's most probable path through the amplification gap becomes visible in comparison of figure 8.5 and 8.6, each showing five mesh configurations with different mesh apertures and comparable open areas in the same color.\n- Field strength along the electron's path -With a decreased distance between the wires, the electrical field strength along the electron's most probable path through the amplification gap (vertical probe line) increases, as shown in figure 8.5.\n\nOCR Text: \"Jdmit 5 Jare 3 'ire 0 Muttgl J wi7e >> VOSWJM 1 Mue/0 ] wittey) 4 Mice0 d MiczJ0 E 0 Mello < 7 1 Vcop dinale (nm]\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 8.5.: Electrical field strength along a vertical probe line in the amplification gap with different apertures of a 30 µm-wire mesh. [134]"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Given a DIW, Figure 7 illustrates the flowchart of our approach. Following the two questions introduced in Section 1, first, (i) it decides which IR to Fig. 7 Flowchart of our approach materialize using ReStore and, then, (ii) for each of them which storage layout to choose.\n\nMentions:\n- Given a DIW, Figure 7 illustrates the flowchart of our approach.\n- Following the two questions introduced in Section 1, first, (i) it decides which IR to Fig. 7 Flowchart of our approach materialize using ReStore and, then, (ii) for each of them which storage layout to choose.\n\nOCR Text: \"Fated VFate KaEt 4AUr\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 7 Flowchart of our approach"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- 1) Dataset and Settings: Dataset A is collected in a seminar room, as shown in Figure 8, for verifying if our method could recognize actions as well as locations. We define seven actions: Box, Clap, Wave hand, Kick, Quick squat, Jump, Stand still and six locations a, b, c, d, e, f. A single subject performs each action 10 times on each location, so in total we have 7 × 6 × 10 = 420 data. Dataset B is collected to compare with vision-based methods on video action recognition, and thus we define actions the same as the benchmark dataset, KTH [27]. These actions include: Box, Clap, Wave hand, Walk, Jog, Run, Stand still. Two subjects are asked to perform each action 10 times and in total we have 2 × 7 × 10 = 140 data.\n- All actions are performed in a 5-second period, each generating four sets of CSI with dimension 30 × 5000 (interpolated to 1000 samples/second). We then transform them into four images of size 576 × 432. Fig. 8: Subject performs actions in each circle.\n\nMentions:\n- 1) Dataset and Settings: Dataset A is collected in a seminar room, as shown in Figure 8, for verifying if our method could recognize actions as well as locations.\n- Fig. 8: Subject performs actions in each circle.\n\nOCR Text: \"57 m Blasknoard Ccnon 2.4m 12 m 22m 23 m R540\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 8: Subject performs actions in each circle."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Here we present simulations that give an interpretation of the gains achieved in the two final fields resulting from optimizations starting from the initial field. Figure 4 shows the initial field and the two final fields as computationally inferred from the magnet positions. There are noticeable differences between these three fields: the locally optimized field exhibits an unexpected plateau followed by a steeper overshoot and the genetically optimized field is shallower overall. These differences can be explained by using a phase portrait, which depicts the deceleration dynamics in the one-dimensional phase-space (z, v) according to the radiation pressure force\n\nMentions:\n- Figure 4 shows the initial field and the two final fields as computationally inferred from the magnet positions.\n\nOCR Text: \"G\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4. (Color online) Calculated magnetic field profiles along the ZS for the initial field, locally optimized field, and genetically optimized field. The fields were calculated using final magnet positions and modeling the slower with a corresponding array of magnetic dipoles."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- which ensures that the equations for f A , f B are of the form ( 24) and the linearized system is of the form (29). The form of the new potential is plotted in Fig. 2. We can now compute the Fourier transform of\n\nMentions:\n- The form of the new potential is plotted in Fig. 2.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: Form of the potential Φ̃ST (x) for R = 2 and ν ST c νSTd κST 2 = 1. The potential is repulsive on its support."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We have presented the integration of a hardware-based monitor into the super-ARTIS UAV. The distinguishing features of our approach are the high level of expressiveness of the RTLola specification language combined with the formal guarantees on the resource usage. The comprehensive tool framework facilitates the development of complex specifications, which can be validated on log data before they get translated into a hardware-based monitor. The automatic analysis of the specification derives the minimal requirements on the development board needed for safe operation. If they are met, the specification is realized on an FPGA and integrated into the superARTIS architecture. Our experience shows that the overall system works correctly and reliably, even without thorough system-level testing. This is due to the non-interfering instrumentation, the validated specification, and the formal guarantees on the absence of dynamic failures of the monitor.  39 ∧ intersect_lon_p0p1 &lt; max (lonp 1 , lonp 2 ))) 40 \"VIOLATION: line crossing between p1 and p2\" Fig. 9: Geo-fencing\n- In this section, we depict the detection of a single line crossing of a geo-fence. The specification can be seen in Fig. 9 where italic variables represent constant points of the geo-fence (lat, lon) and their respective slopes (m) and y-intercepts (b).\n\nMentions:\n- This is due to the non-interfering instrumentation, the validated specification, and the formal guarantees on the absence of dynamic failures of the monitor.  39 ∧ intersect_lon_p0p1 &lt; max (lonp 1 , lonp 2 ))) 40 \"VIOLATION: line crossing between p1 and p2\" Fig. 9: Geo-fencing\n- The specification can be seen in Fig. 9 where italic variables represent constant points of the geo-fence (lat, lon) and their respective slopes (m) and y-intercepts (b).\n\nOCR Text: \"(eJetane I 5i Z I Luneten Cc J' ll\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 9: Geo-fencing"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The overall distribution for the simulation of the dark signal was constructed as follows. First, the Gamma distribution was generated with parameters a = 914.6579 and b = 0.0611, which were estimated from the hardware CMOS photosensor data. Next, the product of two uniform distributions on the interval [45,440] was superimposed (the parameters were chosen to fit both the \"outliers\" and to widen the left part of the probability density). The results are shown in Fig. 10. One can see a better agreement of the superimposed distribution described above with the experimental estimation of the dark signal statistics. The dark signal statistics is getting more complicated with very long integration time. The dark current FPN is no longer the main contributor to the noise statistics: one would expect the noise measurements to be dominated by thermally-generated electron noise. Those are the two distributions (see Fig. 11) that underlie the dark signal measurements. The dark signal in these cases may require modelling based on the experimental data for a particular type of a photosensor. In our example, we were able to compose a distribution to describe the experimental dark signal probability distribution using the superimposed distributions as follows:  The composed distribution describes the dark current FPN ( symbol) with good accuracy, as seen in Fig. 11. One can see from the provided data that the approximation of dark current FPN using only a Gaussian distribution is poor. On the other hand, the superimposed distributions provide a better description of the complex structure of the dark current FPN.\n- At short integration times, the noise is the variability of dark current across pixels, that is, dark current FPN (or dark signal non-uniformity, DSNU). At long integration times, one would expect the noise measurements to be dominated by thermally-generated electron noise. Those are the two distributions (see Fig. 7 and Fig. 11) that presumably underlie the measurements. As the integration time increases further, the probability distribution of the resulting dark signal gets more complicated and harder to model.\n\nMentions:\n- Those are the two distributions (see Fig. 11) that underlie the dark signal measurements.\n- In our example, we were able to compose a distribution to describe the experimental dark signal probability distribution using the superimposed distributions as follows:  The composed distribution describes the dark current FPN ( symbol) with good accuracy, as seen in Fig. 11.\n- Those are the two distributions (see Fig. 7 and Fig. 11) that presumably underlie the measurements.\n\nOCR Text: \"\"aa En jubiurordanu bnidita= nsnnivlo\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 11: Normalised frequency of occurrences of dark signal values in the averaged dark image for long integration time for very long integration time (300 seconds) using a superimposed distribution."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- For a network with three or fewer species the corresponding linear vector space can be displayed geometrically. For example, Fig. 3 illustrates a network containing the isotopes { 4 He, 8 Be, 12 C} and four reaction vectors corresponding to the reaction pairs 2α ⇀ ↽ 8 Be and α + 8 Be ⇀ ↽ 12 C. Larger networks cannot be visualized so easily, but their algebraic properties remain completely analogous to those of a simple network like that illustrated in Fig. 3.\n\nMentions:\n- For example, Fig. 3 illustrates a network containing the isotopes { 4 He, 8 Be, 12 C} and four reaction vectors corresponding to the reaction pairs 2α ⇀ ↽ 8 Be and α + 8 Be ⇀ ↽ 12 C. Larger networks cannot be visualized so easily, but their algebraic properties remain completely analogous to those of a simple network like that illustrated in Fig. 3.\n\nOCR Text: \"28e +12C\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3. (a) Four reaction vectors for a simple 3-species network corresponding to the astrophysical triple-α process that converts4He to12C in red giant stars."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 12 shows the variation in EDP and wireless utilization observed with varying WI counts. The wireless utilization parameter represents the percentage of total messages that are using the wireless channels. As observed from this figure, the EDP initially reduces as the WI count increases: higher number of wireless shortcuts improves the wireless utilization and hence, lowers the overall network latency. However, beyond a WI count of 24, with more than six WIs allocated on a single wireless channel, the MAC overhead (and hence, the channel access latency) and, therefore, the network EDP starts to increase [44]. Hence, in our WiHetNoC, we employ 24 WIs for GPU-MC communication (four channels are used with six WIs operating on each channel). Fig. 13 shows the effects of adding wireless channels for GPU-MC communication on the performance of the WiHetNoC. With increasing number of wireless channels, the amount of data using wireless medium increases and the overall EDP improves. However, the enhancement in wireless utilization and subsequent improvement in EDP slows down beyond a certain number of wireless channels. For the 64-tile system size, increasing the number of wireless channels beyond 4 does not enhance the system performance noticeably as the opportunity for more wireless utilization diminishes.\n\nMentions:\n- Fig. 12 shows the variation in EDP and wireless utilization observed with varying WI counts.\n\nOCR Text: \"Mormalized Flx EDp Konmalize d WI Ullzap 0 075 02 E 16   2024   28 WNIGout\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 12. EDP and Wireless utilization for various WI counts."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Thus, we observe that, as the code rate increases, the optimal blocklength generally increases, while as the internal erasure probability δ increases, the optimal blocklength generally decreases. Fig. 7: FER for a polar code of length N = 1024 designed for the BEC(0.5) under faulty SC decoding with δ = 10 −6 and various numbers of protected decoding levels.\n- The effect of the partial protection for a finite length code is illustrated in Figure 7, where we present P UB e (A n ) for N = 2 10 = 1024 and δ = 10 −6 when n p = 0, . . . , 5, levels of the tree are protected. To improve readability, we intentionally omit P LB e (A n ) from the figure. However, we have already seen that the bounds are tight, especially for low rates, so using only the upper bound is sufficient to illustrate the effect of unequal error protection. We observe that protecting only the root node already improves the performance significantly, especially for the lower rates. When n p = 5, the performance of the faulty SC decoder is almost identical to the non-faulty decoder in the examined FER region and it is remarkable that this performance improvement is achieved by protecting only with n p = n − 5, so that the protected part for each N is fixed to approximately 1.5% of the decoder. We observe that, contrary to the results of Section VII, increasing the blocklength actually decreases P e (A n ) in the examined FER region, as in the case of the non-faulty decoder.\n\nMentions:\n- Fig. 7: FER for a polar code of length N = 1024 designed for the BEC(0.5) under faulty SC decoding with δ = 10 −6 and various numbers of protected decoding levels.\n- The effect of the partial protection for a finite length code is illustrated in Figure 7, where we present P UB e (A n ) for N = 2 10 = 1024 and δ = 10 −6 when n p = 0, . . . , 5, levels of the tree are protected.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 7: FER for a polar code of length N = 1024 designed for the BEC(0.5) under faulty SC decoding with δ = 10−6 and various numbers of protected decoding levels."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Using the error-normalized velocity difference plotted in Figure 6, we define a star to be variable if individual independent measurements are more than 3σ discrepant in this quantity. We acknowledge that the number of 'identified' binaries depends sensitively on this choice. We find that 5 out of 89 in our full member sample are velocity variables (6%). These 5 stars are also contained in our cleaned sample. Given the large velocity differences (a few to tens of km s −1 ), we assume these systems are unresolved binary stars, as single variable stars are unlikely to be in this region of color-magnitude space. As expected, the fraction of binary stars identified in our incomplete sample is far less than 30 ± 4% measured by Clark et al. (2004), but does provide spectroscopic confirmation that unresolved binary stars exist in Pal 13.\n- We next examined the C02 stars for evidence of variability. There are two stars in this sample which we suspect, ORS-38 and ORS-91, identified as orange symbols in Figure 4. ORS-38 is near the RGB and varies in velocity by 17 km s −1 between the DEIMOS and HIRES measurements. This is one of the five variables identified in § 3.1. While it does not inflate the HIRES velocity dispersion, ORS-38 is the primary star inflating our calculations. ORS-91 is on the horizontal branch and lies in the instability band identified in Ivezić et al. (2005). Although the DEIMOS and HIRES velocities agree, and including/removing this stars does not affect our results, we chose to exclude this and other horizontal branch stars from our sample. Removing ORS-38 and ORS-91 does not affect the HIRES-determined dispersion, but the DEIMOS-based velocity dispersion falls to 0.7 +0.8 −0.6 km s −1 . This dispersion is consistent with a normal stellar mass-to-light ratio, although our measurement uncertainties are too large to rule out a higher than expected mass-to-light ratio. We plot the errornormalized differences in velocity between the HIRES and DEIMOS samples in Figure 6, noting the position of these two rejected stars.\n\nMentions:\n- Using the error-normalized velocity difference plotted in Figure 6, we define a star to be variable if individual independent measurements are more than 3σ discrepant in this quantity.\n- We plot the errornormalized differences in velocity between the HIRES and DEIMOS samples in Figure 6, noting the position of these two rejected stars.\n\nOCR Text: \"Mcmbri- Hk){o T\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 6.— Data comparison to Coté et al. HIRES velocities. We plot the histogram for the velocity difference, normalized by observational error: (vDEIMOS−vHIRES)(σ 2 DEIMOS+σ 2 HIRES) −1/2. The two Coté et al. objects in question are identified in their respective bins (ORS-91 and ORS-38)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 3 shows the initial fitness f of minimal reliable networks for several combinations of N and l, averaged for 6 × 10 3 (for N &lt; 40) or 2 × 10 3 (for N &gt; 40) independent network realizations. A large proportion of networks with small N and l have f = 1. As was observed in [9], for these networks the reliable trajectory often has a basin of attraction which dominates the entire state space, which explains why f is close to 1. When N and l increase, this changes, and the basin of the reliable trajectory no longer dominates the state space, resulting in smaller values of f . The only trivial exception is when l is so large that the reliable trajectory occupies a large portion of the state space (i.e. N l ∼ 2 N ). This explains the positive slope of the curve for N = 5. In the more interesting case N ≫ 1 and N l ≪ 2 N , the fitness is far from the maximum value, and the optimization procedure can considerably increase the fitness.\n\nMentions:\n- Fig. 3 shows the initial fitness f of minimal reliable networks for several combinations of N and l, averaged for 6 × 10 3 (for N &lt; 40) or 2 × 10 3 (for N &gt; 40) independent network realizations.\n\nOCR Text: \"N=8 N=M N=l5\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Average fitness 〈f〉, for several parameter combinations."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- It is also shown that P(R 1 &gt; R 0 ) decreases with the SIR threshold θ as the channel becomes more prone to be in outage when increasing the SIR threshold θ. To show the effect of p on the caching probability, in Fig. 6, we plot the histogram of the optimal caching probability at different values of p, where p = p * in Fig. 6(a) and p &gt; p * in Fig. 6(b). It is clear from the histograms that the optimal caching probability b * tends to be more skewed when p &gt; p * , i.e., when P(R 1 &gt; R 0 ) decreases. This shows that file sharing is more difficult when p is larger than the optimal access probability. More precisely, for p &gt; p * , the outage probability is high due to the aggressive interference. In such a low coverage probability regime, each device tends to cache the most popular files leading to fewer opportunities of content transfer between the devices. Last but not least, Fig. 7 manifests the prominent effect of the files' popularity on the offloading gain. We compare the offloading gain of three different caching schemes, namely, the proposed PC, Zipf's caching (Zipf), and CPF. We can see that the offloading gain under the PC scheme attains the best performance as compared to other schemes. Also, we note that both PC and Zipf schemes encompass the same offloading gain when β = 0 owing to the uniformity of content popularity. The results in this part are given for the energy consumption. Fig. 8 shows the energy consumption, normalized to the mean number of devices per cluster, versus β under different caching schemes, namely, PC, Zipf, and CPF. We can see that the minimized energy consumption under the proposed PC scheme attains the best performance as compared to other schemes. Also, it is clear that the consumed energy decreases with β. This can be justified by the fact that as β increases, fewer files are frequently requested which are more likely to be cached among the devices under PC, CPF, and the Zipf schemes. These few files therefore are downloadable from the devices via low power D2D communication. We plot the normalized energy consumption versus the mean number of devices per cluster in Fig. 9. First, we see that energy consumption decreases with the mean number of devices per cluster. As the number of devices per cluster increases, it is more probable to obtain requested files via low power D2D communication. When the number of devices per cluster is relatively large, the normalized energy consumption tends to flatten as most of the content becomes cached at the cluster devices.\n\nMentions:\n- We plot the normalized energy consumption versus the mean number of devices per cluster in Fig. 9.\n\nOCR Text: \"2 Zipf 8 3\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 9. Normalized energy consumption versus the mean number of devices per cluster."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To exemplify this picture further, let us consider that spacetime was initially in a highly symmetric configuration, e.g. four dimensional de Sitter spacetime, with the fields sitting in a local minimum of the potential. From the semi-classical analysis, we know that even a tiny region of this initial configuration evolves into infinitely many different universes. On the other hand, the holographic principle, or de Sitter entropy, says that such a small region can have only finite degrees of freedom. This implies that in the quantum picture, the origin of various semi-classical universes cannot be attributed to the difference of initial conditions-they must arise as different \"outcomes\" of a quantum state Ψ, which is uniquely determined once an initial condition is given. This clearly answers one question raised above: how can an initial state having only O(1) information evolve into different universes? In fact, the initial state evolves into the unique future state, which however is a probabilistic mixture of different (semi-classical) universes. The meaning of the sampling procedure in (ii) also becomes clear-sending a geodesic corresponds to making a \"measurement\" on Ψ, and the sampling corresponds to defining probabilities through \"repeated measurements.\" In particular, we need not take too seriously the sub-Planckian distances appearing in the procedure-the semi-classical picture of the multiverse is simply a pictorial way of representing probabilistic processes, e.g. bubble nucleation processes, occurring in the quantum universe. 1 The remaining task to define a complete, quantum mechanical framework for prediction is to come up with the explicit formalism of implementing procedure (ii), given originally within the semi-classical picture. To do so, we first define Ψ more carefully. For simplicity, we take it to be a pure state |Ψ (although an extension to the mixed state case is straightforward). A crucial point is that we describe the system from a single observer's point of view. This allows us to consider the state |Ψ(t) , where the time parameter t is taken as a proper time along the observer, which can be assigned a simple, invariant meaning. (We take the Schrödinger picture throughout.) We consider that |Ψ(t) is defined on observer's past light cones bounded by the (stretched) apparent horizons, as viewed from the observer. This restriction on spacetime regions comes from consistency of quantum descriptions for systems with gravity. If we provide an initial condition on a space-like hypersurface Σ, then the state is given on past light cones and Σ at early times (as in Fig. 2), and later, on past light cones inside and at the horizons (see Figs. 4, 5 and Eq. (21) in Section 4.4).\n- Figure 2: The entire multiverse can be described purely from the viewpoint of a single \"observer\" in terms of a quantum state |Ψ(t) , which is defined on the observer's past light cones (and on the initial hypersurface Σ at early times; later, the light cones are bounded by the apparent horizons).\n- Suppose that an initial condition for the multiverse is given on a space-like hypersurface Σ (which need not be the case, as we will see shortly). In this case, the multiverse state |Ψ(t) is defined at early times on the past light cone within C(t) and on Σ outside C(t), where C(t) is the intersection between the light cone and Σ (see Fig. 2 in Section 1). This situation, however, does not last long. After a brief initial period, specifically for ∆t &gt; ∼ −H −1 ln(l P H) after the initial moment, where H is the Hubble parameter, the state |Ψ(t) is given entirely on the past light cones bounded by the stretched horizon, as depicted in Fig. 4.\n\nMentions:\n- If we provide an initial condition on a space-like hypersurface Σ, then the state is given on past light cones and Σ at early times (as in Fig. 2), and later, on past light cones inside and at the horizons (see Figs. 4, 5 and Eq.\n- Figure 2: The entire multiverse can be described purely from the viewpoint of a single \"observer\" in terms of a quantum state |Ψ(t) , which is defined on the observer's past light cones (and on the initial hypersurface Σ at early times; later, the light cones are bounded by the apparent horizons).\n- In this case, the multiverse state |Ψ(t) is defined at early times on the past light cone within C(t) and on Σ outside C(t), where C(t) is the intersection between the light cone and Σ (see Fig. 2 in Section 1).\n\nOCR Text: \"ra \"ILWF pGIc_ tico Kco- Eica - hlox \"LIE MN |; 7o\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: The entire multiverse can be described purely from the viewpoint of a single “observer” in terms of a quantum state |Ψ(t)〉, which is defined on the observer’s past light cones (and on the initial hypersurface Σ at early times; later, the light cones are bounded by the apparent horizons). Once the initial condition is given, the state is uniquely determined according to unitary, quantum mechanical evolution."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The rotation curve of the galactic model when M n = 400 and c n = 0.25, is shown as the black line in Figure 1. In the same plot, the red line is the contribution from the spherical nucleus, while the blue line is the contribution from the diskhalo component. We observe, that at small distances from the galactic center r ≤ 2 kpc, dominates the contribution from the spherical nucleus, while at larger distances r &gt; 2 kpc the diskhalo contribution is the dominant factor.\n- Remember that due to the axial symmetry the third term in Eq. (5) is zero. Figure 2a shows the contours ρ(r, z) = const, for our model. The values of the contours are: (0.010, 0.017, 0.030, 0.052, 0.150). In Figure 2b, we see a 3D plot of the values of the density ρ(r, z) on the r − z plane. We can observe the spherical nucleus at the central region of the galaxy and also the well formed disk structure. This plot assures that the mass density has positive value everywhere. For large values of r and z the mass density varies like 1/r 5 and 1/z 5 respectively. This means that the total mass M(R), enclosed in a sphere of radius R increases linearly with distance. This explains why circular velocity profiles, shown in Fig. 1 remains almost flat for large values of r. Here we must point out, that our gravitational po- tential is truncated at R max = 25 kpc, otherwise the total mass of the galaxy modeled by this potential becomes infinite, which is obviously not physical (see Binney &amp; Tremaine, 2008).\n\nMentions:\n- The rotation curve of the galactic model when M n = 400 and c n = 0.25, is shown as the black line in Figure 1.\n- This explains why circular velocity profiles, shown in Fig. 1 remains almost flat for large values of r.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: The rotation curve of the galactic model is shown as the black line. The red line is the contribution from the spherical nucleus, while the blue line is the contribution from the disk-halo potential."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The model based RL method, as described in the Introduction Section, is a RL method that combines the strengths of both RL and optimal control. The main idea, as shown in Fig. 1, is to iteratively updates the belief of the system dynamics model and to optimize the control policy under the current dynamics approximation function. Concretely, the RL agent first explores the environment, collect trajectory Fig. 1. The framework overflow of our proposed guided policy search (GPS) model based RL method. For each iteration, we first collect trajectories in the simulator using the current policy, then the trajectory samples are used to update the system dynamics approximator, which is further used as the model within the model based policy optimization.\n\nMentions:\n- The main idea, as shown in Fig. 1, is to iteratively updates the belief of the system dynamics model and to optimize the control policy under the current dynamics approximation function.\n- Concretely, the RL agent first explores the environment, collect trajectory Fig. 1.\n\nOCR Text: \"Skn Dyrarks Lenlu @alr Hk Ilattiu IQH JDl Hlnd MRnla VLLnMukI Mirr Wamm mar CHTuar\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1. The framework overflow of our proposed guided policy search (GPS) model based RL method. For each iteration, we first collect trajectories in the simulator using the current policy, then the trajectory samples are used to update the system dynamics approximator, which is further used as the model within the model based policy optimization."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 2 shows the neutral stability curve for the spatially homogeneous equilibrium, which is the unique attractor of the model at small values of r. The primary transition is a Hopf bifurcation with spatial wave numbers that depend on the system size. For systems smaller than 2 × 2 cm 2 , the emerging periodic orbit is spatially homogeneous. For larger systems, space dependent orbits emerge, and their typical length scale converges to about 9.3 cm for large system sizes. These stability curves were computed by solving small eigenvalue problems for each combination of wavenumbers, independent from the PETSc implementation. The eigenvalues computed by Krylov-Schur iteration in SLEPc, presented in Sec. 7.2, are in good agreement.\n- Figure 2: Neutral stability curve for the spatially homogeneous equilibrium of the Liley model with parameters set according to Table 1. Shown is the scaling parameter, r, versus the linear domain size, L, and wave numbers k = (k x , k y ) are shown in parenthesis. When varying r, only for very small domains the primary instability is spatially homogeneous. For domain sizes over 12.5 × 12.5cm 2 the location of the primary instability approaches r = 1.04 and the length scale of the leading instability approaches L/ k = 9.3 cm.\n\nMentions:\n- Fig. 2 shows the neutral stability curve for the spatially homogeneous equilibrium, which is the unique attractor of the model at small values of r.\n- Figure 2: Neutral stability curve for the spatially homogeneous equilibrium of the Liley model with parameters set according to Table 1.\n\nOCR Text: \"'a i*9.0 :m TYn\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: Neutral stability curve for the spatially homogeneous equilibrium of the Liley model with parameters set according to Table 1. Shown is the scaling parameter, r, versus the linear domain size, L, and wave numbers k = (kx, ky) are shown in parenthesis. When varying r, only for very small domains the primary instability is spatially homogeneous. For domain sizes over 12.5 × 12.5cm2 the location of the primary instability approaches r = 1.04 and the length scale of the leading instability approaches L/‖k‖ = 9.3 cm."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- R 6 (π/2) = {0} but R 6 (π/2 + ε) (for any 0 &lt; ε ≤ π/2) being a non-degenerate interval; and there is a suitable ϕ 20 already in (0, 5 • 10 −5 ). Figure 13 shows the graphs of\n- (max (R 4 (ϕ))) corresponds to * 4 in Table 3; also compare the orange curve in Figure 13 and the dashed brown curve in Figure 12. As for the brown curve in Figure 13, min π/2≤ϕ≤π (max (R 6 (ϕ))) = 0, in accordance with the fact that V + 6 = {0} (see Figure 6). Finally, as for the black curve in Figure 13, the scaled function value around 20 • 0.445 ≈ 8.9 corresponding to ϕ ≈ π/2 + ϕ 20 is the highest point of the upper shaded rectangle in Figure 6; the highest point of the lower shaded rectangle is 20 * 20 in Table 3.\n\nMentions:\n- Figure 13 shows the graphs of\n- (max (R 4 (ϕ))) corresponds to * 4 in Table 3; also compare the orange curve in Figure 13 and the dashed brown curve in Figure 12.\n- As for the brown curve in Figure 13, min π/2≤ϕ≤π (max (R 6 (ϕ))) = 0, in accordance with the fact that V + 6 = {0} (see Figure 6).\n- Finally, as for the black curve in Figure 13, the scaled function value around 20 • 0.445 ≈ 8.9 corresponding to ϕ ≈ π/2 + ϕ 20 is the highest point of the upper shaded rectangle in Figure 6; the highest point of the lower shaded rectangle is 20 * 20 in Table 3.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 13: The extent of Sn as a function of the angle ϕ (see (14) and (15)). The orange, brown and black curves correspond to n = 4, n = 6 and n = 20, respectively. The dashed red line is placed at 1/e."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To investigate how performance is influenced by the dimensions of a M × N × K matrix multiplication, we performed a sweep of different sizes between 2 6 and 2 12 in each dimension using both gemmlowp and BitSe-rialGEMM with W 1 A 1 . Figure 3 presents a scatter plot of the performance with increasing depth (K). We observe that BitSe-rialGEMM performance is sensitive to the depth (K) dimension. For small matrix sizes, there is little or no performance advantage over gemmlowp, which should be taken into consideration when choosing the execution method for each layer. BitSerialGEMM quickly becomes faster with increasing depth and becomes advantageous over gemmlowp, up to 6.6× faster than gemmlowp for a 64 × 1024 × 4096 multiplication. With K ≥ 2048, we observe decreased performance for larger M and N values in BitSerialGEMM  due to increased cache misses, which can be addressed by adding more levels of blocking to the implementation.\n\nMentions:\n- Figure 3 presents a scatter plot of the performance with increasing depth (K).\n\nOCR Text: \"2H8 Mzuz DepUhKY\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Log-linear plot of performance versus depth (K)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In order to solve the TTK problem, we created an automated, cyclic pipeline to keep models of IOCs up-to-date with the data generated by an adversary. Fig. 1 shows a high-level view of the implementation of our system. The cyclic nature of the pipeline can be described by the following stages:\n\nMentions:\n- Fig. 1 shows a high-level view of the implementation of our system.\n\nOCR Text: \"Data Stream Dala St?er NPU Proccssor Event Labe er Inges: Engina Penaiman-e paljlaje User $ Teuncde Algarithm\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1. High-level View of the Implementation of the TTK Framework"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- As discussed later, only S c lm is significant. To illustrate how well these profiles match, Fig. 3 shows δI c lm for various modes together with fits to I ′ 0 . For the lowest degrees the fit of I ′ 0 is very good but it starts to disagree more as the degree is increased. Still, the approximation used for Eq. (4) is perhaps adequate to justify using the cross-correlation method for this case. The decrease of the visibility at higher l is also evident.\n- If the star is rotating and not observed from the pole (i = 0 • ) the results change substantially, as illustrated in Fig. 4. While δI c lm is still moderately well fitted by I ′ 0 , δI s lm is now non-zero and shows a very different profile, corresponding to a narrowing and widening of the line, which is not well fitted by the derivative. I will return to this issue below. lm (black) and δI s lm (blue) for the G2 star with i = 90 • and solid body rotation with an equatorial rotation velocity of 4 km/s. The values of (l, m) are (0, 0) (solid), (1, 1) (dotted), (2, 0) (dashed) and (2, 2) (dash-dotted). Also shown (in red) is the derivative I ′ 0 scaled to best fit each of the δI c lm . Note that the unperturbed line, and thus the derivative, is different from the one in Fig. 3 due to the rotational broadening.\n\nMentions:\n- To illustrate how well these profiles match, Fig. 3 shows δI c lm for various modes together with fits to I ′ 0 .\n- Note that the unperturbed line, and thus the derivative, is different from the one in Fig. 3 due to the rotational broadening.\n\nOCR Text: \"108F ~0.2 ~0.1 Delta 'avelerntn (Ap\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3. The perturbations δIc lm (black) for the 6173 Å line for the G2 star with 0 G, i = 0◦ and no rotation. The values of (l,m) are (0, 0) (solid), (1, 0) (dotted), (2, 0) (dashed) and (3, 0) (dash-dotted). Also shown (in red) is the derivative I′0 scaled to best fit each perturbation. The shape (but not magnitude) of the perturbations do not depend on i andm in the absence of rotation."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The graphs are generated by using the following method. i) In a square area we select n points independently, uniformly at random, where n is the size of the graph. ii) Using the first n − 1 points we compute a Delaunay triangulation. Then we add n-th point as Door vertex as an auxiliary vertex to the closest random vertex. For this simulation, we generated 50 random Delaunay graphs using the described method for each vertex set size, n = 3, . . . , 200. Then, for each input graph, we measured the number of LCM-cycles performed by both the PACK and the BLOCK algorithms. Then we computed the average runtimes of the 50 runs of both algorithms for each input size, n = 3, . . . , 200. Figure 3 shows the simulation results. The simulations are backing up the linear runtime for the BLOCK algorithm. For the PACK algorithm, the simulations suggest quadratic runtime.\n\nMentions:\n- Figure 3 shows the simulation results.\n\nOCR Text: \"Venices\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3. Simulation results for Delaunay triangulations. The x-axis represents the number of vertices, the y-axis represents the number of required LCM-cycles to finish the filling. The red curve shows the runtime of the BLOCK algorithm; the blue curve shows the runtime of the PACK algorithm."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Conv blocks form the contracting path of the network, while Upconv blocks form the expansive path. Here Conv2D (SxS, n) operation stands for the 2D convolution with n filters of size S × S , while UpConv2D (n) operation stands for the double upsampling of the feature map followed by 2D convolution with n filters of size 2 × 2. In contrast to the U-Net we use two  different expansion paths in order to detect glands and separating contours. Each expansion paths ends with a 1 × 1 2D convolution, which reduces the number of features to 2 (background/foreground) and a sigmoid activation function to get the output probability map. Thus, the first expansion path outputs the probability of gland and the second -the probability of gland contour. Uniting these two probabilities by threshold we get the final result of segmentation (Fig. 3). The network is learned using RMSprop optimizer [1], dividing the gradient by a running average of its recent magnitude. We use a sum of a binary cross entropy value and a dice coefficient as the loss for both of the network outputs.\n\nMentions:\n- Uniting these two probabilities by threshold we get the final result of segmentation (Fig. 3).\n\nOCR Text: \"Gland Crobahlc Conigur Dicnnoui: aE Cone al Con 2Dac Ilke :1) (anv K*r: UprT: EIltet F17 Nk Urcrry nljo (1201 Can\" Eck(lzn; Urtrr y @inck /[26 Ke AJJ con\" Bok6Z\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Proposed network architecture for mucous glands segmentation."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The candidate adversarial examples were generated with = 0.3, which is consistent with [MMS + 17]. Due to the high computational cost of adversarial training (and in particular, PGD adversarial training), we performed one run, and we trained the networks for 10 epochs, which was sufficient for their accuracy to plateau. In Figure 3 we compare the previously reported accuracy upper and lower bounds for MWD, with the accuracy upper bounds for adversarially-trained ReLU networks. The accuracy upper bounds for adversarially-trained ReLU networks are obtained via I-FGSM attacks, which are more effective than PGD or FGSM attacks against such networks. We see that the upper bounds for MWD networks are above the upper bounds for adversarially-trained ReLU networks for all but the smallest attacks (for ≥ 0.05). Furthermore, we see that the lower bounds for MWD networks are above the upper bounds for adversarially-trained ReLU networks for a good range of attack sizes (0.05 ≤ ≤ 0.25), in spite of both upper and lower bounds being conservative. Together, this indicates that at least in the training regimes we explored, MWD trained without the benefit of adversarial examples offer more resistance to adversarial attacks than ReLU networks trained with the benefit of adversarial examples.\n\nMentions:\n- In Figure 3 we compare the previously reported accuracy upper and lower bounds for MWD, with the accuracy upper bounds for adversarially-trained ReLU networks.\n\nOCR Text: \"Mityd MID) ub ReLC (FGEM; He_CflCSH HeilPGL Auack iulezs t\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Upper and lower accuracy bounds for MWD vs. upper accuracy bounds for ReLU networks trained with adversarial examples."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Impact of Data Size on I/O Behaviors We evaluate the impact of data size on I/O behaviors using the fully distributed Hadoop and Spark dwarf implementations, as illustrated in Fig. 13. Here we do not report the performance data of the AI dwarfs because the disk I/O behavior is little in neural network modeling, which we have illustrated in Subsection 4.3. We use the small data input as the baseline, and report the ratio of the number of the medium or large input divided by that of the small input. The bold black horizontal line in Fig. 13 shows the equal line with the small input. That is to say, the value higher than the line means larger I/O bandwidth than the value of the small input. We find that almost for all data dwarfs, their I/O behaviors are sensitive to the data size. When the data size large enough, the whole data can not be stored in memory, then the data have to be swapped in and swapped out frequently, and hence put great pressure on disk I/O access. Modern big data and AI systems adopt an distributed manner, with the data storing on an distributed file system, such as HDFS [33], the data shuffling or data unbalance will generate a large amount of network I/O.\n\nMentions:\n- Impact of Data Size on I/O Behaviors We evaluate the impact of data size on I/O behaviors using the fully distributed Hadoop and Spark dwarf implementations, as illustrated in Fig. 13.\n- The bold black horizontal line in Fig. 13 shows the equal line with the small input.\n\nOCR Text: \"CyutnMzc uM Ji:|^Hurn\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 13: Impact of Data Size on I/O Behaviors."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The law of surfing, P view (L), decreases rapidly with position L, as shown in Fig. 3. Thus u typically will only see the advocate's post if they visit Twitter relatively soon after the advocate posts. That is, most of the contribution to P visible (u) is from the first few terms of the sum. Evaluating visibility P visible (u) (Eq. 6) involves the law of surfing parameters µ and λ, and the ratio of views to posts, ν. Fig. 4 shows the distribution of P visible , showing the values span the range from 0 to 1 (and are consistent with a uniform distribution, Kolmogorov-Smirnov test p-value is 0.10). This wide variation in visibility means models of how users respond to posts should account for visibility, not just the user's activity or interest in the topic.\n\nMentions:\n- The law of surfing, P view (L), decreases rapidly with position L, as shown in Fig. 3.\n\nOCR Text: \"I nImax Tor: [cent pJ:C rec @ved (L | 1\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3. Probability, Pview(L), a user views a post at position L+ 1 on the list using values µ and λ given in Table III."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 2: Normalized kernel for a hidden layer with ReLU activations. Samples from a network with 1000 inputs and 1000 hidden units are obtained by generating R from a QR decomposition of a random matrix containing IID samples from U[0, 1], then setting x = R(1, 0) T and y = R(cos θ, sin θ) T .\n- Figure 2 shows the normalized kernels for weight distributions with the PDF\n\nMentions:\n- Figure 2: Normalized kernel for a hidden layer with ReLU activations.\n- Figure 2 shows the normalized kernels for weight distributions with the PDF\n\nOCR Text: \"3 22561\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: Normalized kernel for a hidden layer with ReLU activations. Samples from a network with 1000 inputs and 1000 hidden units are obtained by generating R from a QR decomposition of a random matrix containing IID samples from U [0, 1], then setting x = R(1, 0)T and y = R(cos θ, sin θ)T ."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In this section, we present the analytical results to study the effect of network parameters, link losses, topological changes, random links, and asymmetric links on the network robustness. The effectiveness of of analytical expressions have been verified with the extensive simulations using MATLAB. We have plotted τ versus n, for r=1 in Fig. 4. We observe that τ increases exponentially from n=25. So it has to be noted that large scale sensor networks exhibit less robustness to topology changes. In Fig. 5, τ versus r, has been plotted and we can observe a rapid decrease of τ values at r=5. Because the node or link betweenness decreases with the nearest neighbors when n is constant. From r=5, τ values are too low and almost constant despite the increase in r values. This result is justified as the r values increase the network connectivity and a fully connected network always ensures high robustness to topology changes. Similarly, to understand the network criticality of a torus network, we have plotted τ versus k 1 and k 2 as shown in Fig. 6. We observe that, τ increases exponentially with the number of nodes in two dimensions. For k 1 =k 2 =300, τ versus r plot can be seen in Fig. 7, and we find that for r-nearest neighbor torus network, a significant decay of τ values for r=5. We have also noticed that from r = 5, τ values are almost constant. To investigate the effect of network dimension m on τ , we have plotted the Fig. 8, for k 1 =16, k 2 =18, k 3 =20, and k 4 =22. We have noticed that WSN applications work in multiple dimensions with high r values exhibit high robustness to topology changes. In Fig. 9, we have compared the static ring network with the topologies discussed in Section IV for p = q = 0.2. We have observed that network robustness is drastically increases with the topology switching between nodes and random WSNs exhibits more robustness than static networks. In Fig. 10, we have plotted the τ against n for p = q = 0.7 to see the effect of communication link probabilities p and q on τ . We have observed that increase in probability q of link existence results in increase of network robustness, whereas probability of link failures p reduces the network robustness exponentially. As shown in Fig. 11, we have plotted τ versus n, varying values from 0 to 1. We observe that τ values increases with the , which reveals that network robustness increases with asymmetric link weights.\n\nMentions:\n- As shown in Fig. 11, we have plotted τ versus n, varying values from 0 to 1.\n\nOCR Text: \"Symmete Meteek Ihezelcal Asymretn: | etHCn, FiecketJ 215 Symmer: Mern: smublim nsxwrel-Leloksiwla o Ietetite KtLata Nunber € Roces\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 11. τ versus n for Asymmetric Ring Network."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Comparison to the experimental results [7] at large finite time shows that numerical results for the Hamming distance give a slightly larger value compared to the experimental data. As it has been mentioned above, the normalized Hamming distance grows with time, due to the thermalization process to reach 1/2 for the fully delocalized regime at infinite time. Since the experimental data provided are not at infinite time, the measured Hamming distance is expected to be a bit less than the numerically calculated, which can be observed on the Fig. 1.\n\nMentions:\n- Since the experimental data provided are not at infinite time, the measured Hamming distance is expected to be a bit less than the numerically calculated, which can be observed on the Fig. 1.\n\nOCR Text: \"0\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 1. The normalized Hamming distance according to Eq. (22) (red line) and extracted from the experiment [7] for finite time t ≥ 5/J0 (blue line) for α = 1.15 and N = 10."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Successful measurements with ultrahigh subpixel resolution (in the order of 1 µm) require optimization of the bias voltage for the photon energy, and a calibration of the relation between centroid positions and corresponding photon positions (using either measurement statistics, see Fig. 6, or theoretical considerations, see next subsection).\n\nMentions:\n- Successful measurements with ultrahigh subpixel resolution (in the order of 1 µm) require optimization of the bias voltage for the photon energy, and a calibration of the relation between centroid positions and corresponding photon positions (using either measurement statistics, see Fig. 6, or theoretical considerations, see next subsection).\n\nOCR Text: \"A Meem\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIGURE 6. Relation between x centroid and subpixel position; for 200 V bias (red line with squares), subpixel resolution is limited to 5 µm within ±10 % of pixel center and 1 µm elsewhere; a 100 V bias (black line with dots) allows 1 µm resolution everywhere."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- which, strictly speaking, are valid only within the BCS-based theories. A justification for such approximation is that, in the cases where both the exact CE gaps and the FTBCS1 as well as the FTLN1 are possible (including the corrections owing to coupling to the selfconsistent quasiparticle random-phase approximation), these pairing gaps are rather close to each other, especially at T ≤ 2 MeV (See Fig. 1 of Ref. [13].).\n- , which means it is impossible to carry out the exact diagonalization of the pairing Hamiltonian ( 8) with the entire single-particle spectra [13]. Therefore, knowing that pairing has a significant effect around the Fermi surface, we calculate the exact CE thermal gap ( 12) only for 14 doubly folded neutron (proton) levels situated around the Fermi one, λ(T = 0), with 7 levels below and 7 levels above it. [25]. This value agrees well with the three-point and five-point gaps calculated in Ref. [26] and shown in Fig. 1 therein. The remaining 4 neutron levels (two on 1i 13/2 and two on 2g 9/2 orbitals) and 4 proton levels (two on 1h 11/2 and two on 2f 7/2 orbitals) of this group are assumed to have the same thermal neutron and proton pairing gaps, respectively. For the levels beyond this group, pairing is assumed to be negligible so that 1 − n h = f h , n p = f p , u p(h) = 1 (0), and v p(h) = 0 (1).\n\nMentions:\n- A justification for such approximation is that, in the cases where both the exact CE gaps and the FTBCS1 as well as the FTLN1 are possible (including the corrections owing to coupling to the selfconsistent quasiparticle random-phase approximation), these pairing gaps are rather close to each other, especially at T ≤ 2 MeV (See Fig. 1 of Ref. [13].).\n- This value agrees well with the three-point and five-point gaps calculated in Ref. [26] and shown in Fig. 1 therein.\n\nOCR Text: \"0.8 0 {MeV}\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 1. (Color online) Exact CE gaps for neutrons [(red) solid line] and protons [(blue) dashed line] for 201Tl."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The optimal selling region (see Figure 2 for an illustration) is\n\nMentions:\n- The optimal selling region (see Figure 2 for an illustration) is\n\nOCR Text: \"#E M\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig 2: Illustration of the optimal stopping region under mild anxiety (u = −∞). Model parameters: c = 0.3568, r = 0.18, q = 0.003 and Laplace exponent used: ψ(β) = 0.18β+0.02β2−0.25( ββ+4 ). The investor solves a risk-neutral sale with transaction cost K = 10. Here, we have H? = 0.0603, b = 4.1903 and zc = 4.2616. The axes origin is at (3, 3) in the figure."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- An adaptive filter is implemented by means of a secondorder generalized integrator (SOGI), where the FLL estimates the frequency of the input signal. Such a scheme is termed SOGI-FLL, and has been proposed for grid synchronization of power converters [15]. Figure 1 illustrates the block diagram of the SOGI-FLL, consisting of the SOGI-QSG (SOGI quadrature signal generator) and the FLL structure. The method was first adopted, within the wave energy control context, to estimate the frequency components of the WEC velocity [12]. In this study, the method is adopted for estimating the wave excitation force frequency.\n- From Fig. 1, the state-space equations of the SOGI-FLL are\n\nMentions:\n- Figure 1 illustrates the block diagram of the SOGI-FLL, consisting of the SOGI-QSG (SOGI quadrature signal generator) and the FLL structure.\n- From Fig. 1, the state-space equations of the SOGI-FLL are\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1. Frequency estimation by the SOGI-FLL method."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- A complementary, electrical characterization was carried out to determine whether this trend in conversion and energy efficiency can be correlated with electrical measurements. As observed in figure 4, the DBD voltage (VDBD) decreases as a function of the frequency (by almost 500 V in this frequency range).\n- As the total electrical charge is conserved through the entire DBD source (Qpl = Qdiel), the relation Cpl•Vpl,eff = Cdiel•Vdiel leads to Vpl,eff/Vdiel = Cdiel/Cpl = 11.2. This ratio is rather high and this means that Vpl,eff represents almost 82% of VDBD, as represented by the red curve in figure 4. Similar values have also been reported in previous studies; for instance, in [44] Vpl,eff was stated to represent even 98% of VDBD.\n- Hence, it is clear that the microdischarge characteristics are not correlated to CO2 conversion. An alternative and convenient representation to evaluate how the frequency could change the microdischarge characteristics is to plot the distribution of their lifetimes versus their number (NS) for several frequencies, as presented in figure 7. Most of the microdischarges (NS ≈ 1200) exhibit a lifetime comprised between 7 and 25 ns. A small fraction of microdischarges exhibits a lifetime under 7 ns and above 25 ns. The drop in CO2 conversion upon rising frequency may not be induced by the microdischarges. Only the drop of the discharge voltage seems to be responsible for the drop in CO2 conversion. (from 523 to 560 K) and a slight increase in the average temperature of the ground electrode (from 375 K to 405 K). The difference Tgas-Telectrode can be considered as a representation of Joule losses. A rise in the frequency induces an increase in the gas temperature that can be linked to a decrease of the effective plasma voltage (see figure 4). Indeed, a low frequency plasma gives place to a higher effective plasma voltage, i.e. to a higher electric field where electrons can be more readily accelerated. The increase in gas temperature upon rising frequency may be correlated with the lower CO2 conversion at high frequencies since more energy may be spent for heating and less to decompose CO2. The gas temperature values which are found with the OES spectra are in good agreement, and in the same order of magnitude, with temperature values determined by other methods for a CO2 DBD in the same kind of conditions [56].\n\nMentions:\n- As observed in figure 4, the DBD voltage (VDBD) decreases as a function of the frequency (by almost 500 V in this frequency range).\n- This ratio is rather high and this means that Vpl,eff represents almost 82% of VDBD, as represented by the red curve in figure 4.\n- A rise in the frequency induces an increase in the gas temperature that can be linked to a decrease of the effective plasma voltage (see figure 4).\n\nOCR Text: \"2 DBD Nerl Frequency (kHz) 2\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4. RMS voltages as a function of frequency at Pabs = 55 W, Φ(CO2) = 200 mLn·min−1."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We then measured the runtime of grouping functionality among all the approaches. To compare the complexity of the approaches, we set the random selection as the baseline. Fig. 8 plots the relative complexity with respect to random selection in dB.\n\nMentions:\n- Fig. 8 plots the relative complexity with respect to random selection in dB.\n\nOCR Text: \"Fvi 5eych Mmnerolneer\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 8. Relative complexity with respect to random selection vs. network size."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Task accuracy with increasing data complexity:  This improvement of performance is also summarized in the ROC curves in Fig. 8, along with the value of the area under the macro-average ROC curve.\n\nMentions:\n- Task accuracy with increasing data complexity:  This improvement of performance is also summarized in the ROC curves in Fig. 8, along with the value of the area under the macro-average ROC curve.\n\nOCR Text: \"1 cIBP-VAE (area 0,9321 c-VAE (area 0 901) CNN (arca 0,8941 8o 0.2 0.4 0.6 0,8 1.0 Specificity\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 8: ROC curves of cIBP-VAE in comparison to alternative models on the clinical ECG data set."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The mean lifetime due to non-multiplication kinetics, τ other , is determined by measuring a 252 Cf source and repeating Feynman analysis; samples are typically small powders in which multiplication is negligible. The 252 Cf measurement in this work for the 10.16-cm copper reflector case yielded τ other = 20.44 ± 0.04 ns, as annotated in Fig. 6. Similar measured data for the other configurations are not available, so the measured τ other is verified with simulated results. The measurement was simulated using MCNPX-PoliMi and included all detection systems, tables, stands, shielding, and the floor. The simulation models detector response for 20-minute measurements of the 2.54, 5.08, 7.62, and 10.16 cm cases, and respective values are shown in Tab. 1. The mean non-multiplicative time constant from simulation is τ other sim = 19.83 ± 0.45 ns, consistent with the measured value. A measured value for each configuration is generally preferred; in this work, the measured τ other will be used for all configurations since there is no apparent trend shown in the simulated values. The raw Feynman-alpha, corrected Feynman-alpha (raw values minus τ other ), and simulated reference values are shown in Fig. 7. The values are also summarized and compared to the Rossi-alpha values in Tab. 2. There is excellent absolute agreement between the corrected Feynman-alpha and reference values; additionally, the Feynman-alpha values are more precise than the Rossi-alpha values (by one to two orders of magnitude) with one-standard-deviation measurement uncertainties less than 1%. The Feynman-alpha values are more accurate than the Rossi-alpha values below k eff ≈ 0.92, while the opposite is generally true above; the error for the more-accurate prompt neutron period estimate for each assembly is presented in blue in Tab. 2.\n\nMentions:\n- The 252 Cf measurement in this work for the 10.16-cm copper reflector case yielded τ other = 20.44 ± 0.04 ns, as annotated in Fig. 6.\n\nOCR Text: \"5 21,44 0.04 ns che\" Gz.e \"idlh [nsl\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6: Feynman histogram and fit value for the measurement of 252Cf in 10.16 cm of copper."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The simulated data center consists of N p homogeneous PMs with threedimensional resource capacities: CPU, memory, and network I/O, and for each PM, the total resource capacities of these resources are set as 5.0 GHz, 10 GB, and 1 Gbps, respectively. Absolute values of PM resource capacities are simulated so that the migration overhead factors can be measured using the proposed migration overhead estimation model. The power consumption for an active PM is calculated according to the power consumption model represented by Eq. 9-10 in our previous work [11] and the values for E idle and E f ull in the model are set to 162 watts and 215 watts, respectively, as used by Gao et al. [42]. Given the fact that three-tier tree network topology with core-aggregation-access switch levels are predominantly used in production data centers [60], PMs in the simulated data center are interconnected with each other using three-tier tree network topology, as shown in Figure 3, where each of the network switches have 8 ports. The maximum bandwidth capacity of inter-PM communication links used for the VM migrations is set to 1 Gbps, and the available bandwidths of such links at run-time are synthetically generated using random numbers from the normal distribution with a mean (M eanBW ) of 0.05 and a standard deviation (SDBW ) of 0.2. The network distance between any two PMs is measured as DS = h × DF , where h is the number of physical hops (specifically, network switches) between two PMs in the simulated data center architecture as defined above, and DF is the Distance Factor that implies the physical inter-hop distance. The value of h is computed using the analytical expression for tree topology as presented by Meng et al. [53], and DF is fed as a parameter to the simulation which is set to 2 for the experiments conducted. The network distance of a PM with itself is set to 0 which implies that any data communication between two VMs hosted in the same PM is performed by memory copy without going through the physical network. A higher value of DF indicates greater relative communication distance between any two data center nodes. Before the dynamic consolidation operation, N v number of VMs are considered to be running and are distributed randomly among the N p PMs in a load balanced mode. Such an initial data center state is simulated in order to provide an incentive for the offline consolidation operation so that there is scope for the algorithms to improve resource utilization and reduce energy consumption. The VM resource demands for CPU, memory, and network I/O are synthetically generated using random numbers from the normal distribution with mean M eanRsc and standard deviation SDRsc. The corresponding VM page dirty rates (v dr ) are generated using uniform random numbers from the range [0, P R * v mem ], where P R is the ratio of maximum possible page dirty rate to VM memory size, and it is set to 0.25 for the whole simulation. Thus, the VM page dirty rate (v dr ) is effectively parameterized in the simulation by the VM memory demand (v mem ) and this eliminates the need for another parameter.\n\nMentions:\n- Given the fact that three-tier tree network topology with core-aggregation-access switch levels are predominantly used in production data centers [60], PMs in the simulated data center are interconnected with each other using three-tier tree network topology, as shown in Figure 3, where each of the network switches have 8 ports.\n\nOCR Text: \"Csre Liper Mqrexje Ze\" Kele Mria Rnb; (hsn . Jusler ; 4s} Cuzte\" #\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3. Clustering data center servers based on network proximity."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Increasing machine size can extract more features of samples, and thus can increase the generalization ability. In more detail, each weight vector extracts information of samples from a different angle, and thus the more the weight vectors, the more features of samples can be extracted. Figure 11 shows the dependence of the average recognition accuracy on the machine size, which indicates that increasing the size can improve the recognition rate monotonously, though the benefit may become saturated when the size is adequate.\n\nMentions:\n- Figure 11 shows the dependence of the average recognition accuracy on the machine size, which indicates that increasing the size can improve the recognition rate monotonously, though the benefit may become saturated when the size is adequate.\n\nOCR Text: \"4 100 150 200 250\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 11. The dependence of the average recognition rate on the machine size. The hollow circles, squares, up-triangles and down-triangles are for GVMs with N=1000、2000、 3000、6000, correspondingly."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where i, j indexes a pair of neurons, and (x i , y i ), (x j , y j ) give their position on a two-dimensional lattice. This assumes that the neurons in the excitatory layer are arranged as such. It requires a parameter c inhib , which is multiplied by the distance to compute inhibition levels, as well as a parameter c max that gives the maximum allowed inhibition; see Figure 2. With proper choice of c inhib , when a neuron exceeds its firing threshold v threshold , instead of inhibiting all other neurons from firing, a neighborhood of nearby neurons will be weakly inhibited and may have the chance to fire. This encourages neighborhoods of neurons to fire for the same inputs and learn similar filters. The radius of the neighborhood is determined by the c inhib parameter. This inhibition profile is inspired by the self-organizing map (SOM) [26] and it is included in part to reduce the degree of competition imposed by the connections from the inhibitory layer. The original inhibitory scheme introduced in [14] produces a network activation which is typically too sparse during training to learn filters quickly. Namely, after an initial transient following the introduction of an input example, one neuron fires first and causes all others to become strongly inhibited and therefore quiescent. Additionally, the introduced profile causes excitatory neuron filters to self-organize into distinct clusters by similarity, which often correspond to categories of the input data.\n\nMentions:\n- It requires a parameter c inhib , which is multiplied by the distance to compute inhibition levels, as well as a parameter c max that gives the maximum allowed inhibition; see Figure 2.\n\nOCR Text: \"InreaeIno Imnideid Unkikumol emnin Mby rcitidon EJuee sialae Wud =ed; (Ue\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: Inhibition as a function of Euclidean distance"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To conclude this proof, we will therefore handle the first case, i.e. the case where P n+1 p,p+1,... is more right-priority (respectively, left-priority) than P k f,f +1,... . We assumed that P n+1 does not conflict with α n , hence in particular P n+1 p,p+1,... does not conflict with F k f,f +1,... , and does not conflict with P k f,f +1,... . Let q ≥ p and g ≥ f be the smallest integers such that P n+1 q = F k g . An example of this situation is shown in Figure 6.3. The argument follows along the same lines as Case 2 (building a closed connected component in which P n+1 starts to grow), but requires a new technique to identify the inside and outside of that connected component.\n\nMentions:\n- An example of this situation is shown in Figure 6.3.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6.3: An example of Case 3, where Pn+1 and F k (and by implication P k) both place a V + glue on `, at the same height."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 6]. This holds true even in the exchange frustrated parameter regime. The purely imaginary self-energy [57] displayed right before the breakdown of the = 6 result [Fig. 7], shows a similar behavior. Away from the one loop results the height of the quasi-particle peak at low frequencies is reduced up to ≥ 3 where it saturates, while for large frequencies the values for one and multiple loops start to coincide. The convergence of the multiloop scheme even for the strongly coupled problem we deal with here, can be rationalized by reconsidering the high frequency structure of the vertex channels. Eventually one repeatedly evaluates one loop diagrams, which, with every loop iteration, decay with higher powers of the external frequencies, such that contributions at higher become increasingly suppressed. This line of reasoning underlines the importance of incorporating a parametrization of the high frequency structure of the two-particle vertex for any pf-FRG solver.\n\nMentions:\n- The purely imaginary self-energy [57] displayed right before the breakdown of the = 6 result [Fig. 7], shows a similar behavior.\n\nOCR Text: \"G L] Q.G (- 1 2002 Q_ Matsuluzra (cocIK%\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 7. Loop convergence of the self-energy on the simple cubic lattice for J3/J1 = 0.0 (L = 11) depicted at Λ/J1 ≈ 1.0. The height of the quasi-particle peak at low frequencies is reduced for ` > 1, but converges for ` ≈ 3. At high frequencies, one and multiloop results start to coincide."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 12 -Sensitivity for operational receivers at medium-size class radio telescopes.\n\nMentions:\n- Figure 12 -Sensitivity for operational receivers at medium-size class radio telescopes.\n\nOCR Text: \"Tu d Haa Eth: Eae\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 12 – Sensitivity for operational receivers at medium-size class radio telescopes."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where A g (t) = (n t , b t , V t , W t ) and A g (u) = (n u , b u , V u , W u ); Figure 1: Computing g.\n- Note that they all have arity 1 and that showing redex and nature to be efficiently computable in compact form relative to a context is precisely what is required to prove the efficiency of useful reduction. The four functions above can all be proved to be efficiently computable (in the three meanings). It is convenient to do so by giving an algorithm computing the product function nature × redex × apvars × freevars : 1 → T × B × VARS × VARS (which we call g) compositionally, on the structure of the input term, because the four function are interrelated (for example, tu has a redex, i.e. redex (tu) = true, if t is an abstraction, i.e. if nature(t) = lam). The algorithm computing g on terms is A g and is defined in Figure 1.\n\nMentions:\n- where A g (t) = (n t , b t , V t , W t ) and A g (u) = (n u , b u , V u , W u ); Figure 1: Computing g.\n- The algorithm computing g on terms is A g and is defined in Figure 1.\n\nOCR Text: \"Ui-mLzeUl Unu: gk-KS6-H una \"-wGGu; Sh-wn4 {sa-W,\"un F-w HUM tnA#:_UK alan-g,L6KS\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Computing g."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- On the other hand, the similar growing trend is shared among the observed workloads. Hence, we can infer that the workload's memory requirements can be evaluated and quantified as the monotonic function of the input data size. 2) Diverse workloads have diverse memory capacity requirement and their requirements can be classified. In order to measure the memory requirement of different workloads, we choose 9 workloads with the same input data size, 600MB, the result is shown in Fig. 3. We can see that the memory requirements among different workloads are different, for example, for the same input data size, SVD++ and PageRank require the largest memory capacity of around 2.5GB, while the KMeans and LogisticRegression only require about 512MB. Therefore, the memory requirement across different workloads are vary widly. On the other hand, when the input size is same, these workloads can fall into multiple categories according to the amount range of the memory requirement.\n- We decide the specific characteristic with the consideration of the above five factors that impact on the workload's memory requirement. As described above, the input data size mainly determine the memory requirement of the first execution stage, while the shuffle data size determine that of the following stages. As Fig. 3 shows, with the same configuration of data block size and total task number for shuffle operations, typical Spark workloads have diverse memory requirements even with the same input data size(600MB). Hence, the variety of the shuffle data size may determine the workload classification. To verify it, we examine the shuffle data size of workloads demonstrated in Fig. 3. As shown in Fig. 6, the workloads can also be classified according to the size range of the shuffle data. For the PageRank and SVD++, the shuffle data size is larger than the input data size. The shuffle data size of SVD++ and PageRank are over than 600MB, especially the shuffle data size of SVD++ is over than 1200MB. However, for the LogisticRegression and KMeans, their shuffle data size is less than half of their input data size. For all the other workloads, the shuffle data size is as much as 0.5 to 1 time of the input data size. More interesting, the resulting workload categories are quite same as those with the memory requirement classification. We therefore determine that the ratio of the shuffle data size to the input data size can be the specific memory consumption characteristic to classify Spark workloads and define it as \"Data Expansion Ratio\". The Data Expansion Ratio of the Spark workload can be calculated as follows:\n\nMentions:\n- In order to measure the memory requirement of different workloads, we choose 9 workloads with the same input data size, 600MB, the result is shown in Fig. 3.\n- As Fig. 3 shows, with the same configuration of data block size and total task number for shuffle operations, typical Spark workloads have diverse memory requirements even with the same input data size(600MB).\n- To verify it, we examine the shuffle data size of workloads demonstrated in Fig. 3.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3: Memory capacity requirements among different workloads"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The Feynman histograms are constructed by calculating Y 2 from Eqn. (12) as a function of τ. The analysis of the BeRP Ball data used 200 values of τ logarithmically distributed between 10 2 and 10 4 ns, and the analysis of the 252 Cf data used 200 values of τ distributed between 10 1 and 10 4 . The resultant histograms are shown in Figs. 5 and 6, respectively. The histograms are fit by Eqn. (10) with a nonlinear least squares algorithm and weighting, with weights determined by the analytic method. The histograms were also fit with the one-exponential model in Eqn. (6) for comparison.\n\nMentions:\n- The resultant histograms are shown in Figs. 5 and 6, respectively.\n\nOCR Text: \"Mkit 'Rc ~{Mect ab1ct u 6: \"ittM]\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: Feynman histograms for the BeRP Ball reflected by various amounts of copper. Error bars (one standard deviation) are smaller than the markers."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- If the property cannot be verified with the current abstract precision, the verifier returns a counterexample. We need to check if this counterexample corresponds to a real bug or not. Fig. 5 shows a counterexample path, obtained by regression verification of the updated main procedure in Fig. 2. Each node on the path represents an abstract state. Recall that an abstract state consists of a program location and an abstract data state. In the figure, only the program locations are labeled, whereas the abstract data states, for simplicity reason, are skipped.\n- A path may contain procedure call operations. Replacing the procedure call operation with the corresponding intra-procedural path is called an expansion. In the following, we assume all procedure call operations, except of holes, have already been expanded. For example, the expansion of the counterexample in Fig. 5 is shown in Fig. 6. We call a path holeless if it contains no hole.\n- Consider the counterexample in Fig. 5, assume the procedure call operation at l 2 is a hole. It splits the path into two segments, i.e.\n\nMentions:\n- We need to check if this counterexample corresponds to a real bug or not. Fig. 5 shows a counterexample path, obtained by regression verification of the updated main procedure in Fig. 2.\n- For example, the expansion of the counterexample in Fig. 5 is shown in Fig. 6.\n- Consider the counterexample in Fig. 5, assume the procedure call operation at l 2 is a hole.\n\nOCR Text: \"main newo) [flag =0] inclx; flag) [xy]\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: A counterexample"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- the value being predicted. A question that now arises is how their coverage probabilities and their mean lengths compare with each other? To gain some insights into these comparisons, we performed simulation studies under the four different models described above, with each simulation run having 10000 replications. The sample sizes considered were n ∈ {30, 50, 100, 200}. In the Appendix, Tables 11, 12, 13 and 14 summarize the results of these simulations where we report the coverage probabilities (CP), mean lengths (ML), and standard deviation of lengths (SL). Examining these tables, it appears thatΓ 0 has coverage probabilities that are below the nominal level (between 3% and 4% below in Table 14 when n = 30), with the discrepancy being more pronounced when the sample size is small. As the sample size is increased, these observed coverage probabilities get closer to the nominal level. This deficiency is due to the estimation of the θ parameter and, as previously noted, theΓ 0 does not take into consideration the variability in the resulting estimator of λ(x 0 ; θ). On the other hand,Γ 1 andΓ 2 both achieve coverage probabilities that are quite close to the nominal level, especiallyΓ 1 , when n is large.Γ 0 , on the other hand, tends to have a lower mean length compared to the mean lengths ofΓ 1 andΓ 2 , with the differences in mean lengths becoming alarmingly large for the model in Table 13. Recall that for this model, the rate curve increases to ∞ as w decreases to −∞, and since the w i 's are generated from a normal distribution, on some occasions, w n+1 falls outside the range of {w i , i = 1, 2, . . . , n}. Depending on how different w n+1 is from the mean of w 1 , w 2 , . . . , w n , this could lead to a large estimate of the standard error ofλ n+1 , thus leading to very wide prediction regions forΓ 1 andΓ 2 . SinceΓ 0 simply utilized the estimate ofλ n+1 , but was totally oblivious to its variability, it was not much affected in such a situation. However, because of its rigidness with respect to this added variability, it could dramatically suffer. We demonstrate this situation by plotting an extreme realization in Figure 8  This particular demonstration warns us of the danger and pitfalls of making a prediction for a response variable that is associated with a covariate vector outside the convex hull of the covariate vectors used in the construction of the prediction regions and when the Poisson rate hyper-surface generated by the map x → ρ(xθ) is complex. As a word of caution, when performing extrapolation to do predictions, be forewarned of sinkholes littering the forecasting road -and, if it could be avoided, make no forecasts on long, especially very long, horizons. But, alas, this is the type of forecasting problem that is actually realistic and of most interest, such as that of predicting the number of cases or deaths due to COVID-19 in a future date, given the observed data up to a\n\nMentions:\n- We demonstrate this situation by plotting an extreme realization in Figure 8  This particular demonstration warns us of the danger and pitfalls of making a prediction for a response variable that is associated with a covariate vector outside the convex hull of the covariate vectors used in the construction of the prediction regions and when the Poisson rate hyper-surface generated by the map x → ρ(xθ) is complex.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 8: An extreme realization from the third model with p = 3, n = 30, θ = (3, .2,−.1,−.05), and with W ∼ N(µ = 1, σ = 2). The resulting prediction regions were Γ0 = [257, 323],Γ1 = [0, 1100],Γ2 = [0, 1665]. The realized value of Y31 was y31 = 47 associated with w31 = −4.0493."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To exploit the maximum capacity of DNNs for visual tracking, the methods shown in Table 4  This categorization is based on the objective functions of DNNs that have been used in visual tracking methods (see Fig. 3); hence, this sub-section does not include the methods that exploit deep off-the-shelf features because these methods do not design and train the networks and usually employ DNNs for feature extraction.\n- To take advantages of both foregroundbackground/category classification and ridge regression (i.e., regularized least-squares objective function), some methods employ both classification-and regression-based objective functions for visual tracking (see Fig. 3), which their goal is to bridge the gap between the recent trackingby-detection and continuous localization process of visual tracking. Commonly, these methods utilize classificationbased methods to find the most similar object proposal to target and then the estimated region will be refined by a BB regression method [55], [68], [75], [87], [101], [110]- [112], [123], [137], [153], [168], [173]. To enhance efficiency and accuracy, the target regions are estimated by classification scores and optimized regression/matching functions [56], [57], [134], [145], [146], [156], [157], [160], [163], [164], [167], [179]. The classification outputs are mainly inferred for confidence scores of candidate proposals, foreground detection, response of candidate window, actions, and so forth.\n- Based on their network outputs, the DL-based methods are classified into six main categories (see Fig. 3 and Table 2 to Table 4), namely confidence map (also includes score map, response map, and voting map), BB (also includes rotated BB), object score (also includes probability of object proposal, verification score, similarity score, and layerwise score), action, feature maps, and segmentation mask. According to the network objective, the DL-based methods generate different network outputs to estimate or refine the estimated target location.\n\nMentions:\n- To exploit the maximum capacity of DNNs for visual tracking, the methods shown in Table 4  This categorization is based on the objective functions of DNNs that have been used in visual tracking methods (see Fig. 3); hence, this sub-section does not include the methods that exploit deep off-the-shelf features because these methods do not design and train the networks and usually employ DNNs for feature extraction.\n- To take advantages of both foregroundbackground/category classification and ridge regression (i.e., regularized least-squares objective function), some methods employ both classification-and regression-based objective functions for visual tracking (see Fig. 3), which their goal is to bridge the gap between the recent trackingby-detection and continuous localization process of visual tracking.\n- Based on their network outputs, the DL-based methods are classified into six main categories (see Fig. 3 and Table 2 to Table 4), namely confidence map (also includes score map, response map, and voting map), BB (also includes rotated BB), object score (also includes probability of object proposal, verification score, similarity score, and layerwise score), action, feature maps, and segmentation mask.\n- Considerable visual tracking methods are based on the integration of deep features in the DCF framework (see Fig. 3).\n\nOCR Text: \"22394 Daeeal l Tt' LEe \"r\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3: Taxonomy of DL-based visual tracking methods."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Without loss of generality, we assume that the closest system node to point (0, 0) is source S, while the closest system node to point (100, 100) is the destination D. A snapshot of the network with an area of 100m × 100m is illustrated in Fig. 2, where λ 1 = 0.43, the corresponding number N = 47, λ 2 = 0.15, the corresponding number of jammers is 17, the equation of road is 3x + 10y − 700 = 0, P u = 0.1W, α = 2, P max = 15W , T = 0.1, and L/ρ = 1s [16].\n- In Fig. 2, the blue line indicates the selected optimal path involving moving vehicles, while the pink line is the selected optimal routing path without the moving vehicles when α = 2. Besides, it is found that the minimum energy consumption of the blue line is about 60% of the pink one, showing that the routing path involving the moving vehicles can save much energy compared with the scenario without the vehicle. What's more, the number of hops needed in the routing path with moving vehicles is more than that without moving vehicles, e.g., 8 hops versus 4 hops in Fig. 2, indicating that the average energy consumption per node is lower and thus beneficial in terms of prolonging the service time of the networks.\n- The optimal routing paths with and without the moving vehicles when α = 3 are illustrated in Fig. 3. Compared with Fig. 2, the optimal routing path is totally different. Besides, by utilizing moving vehicles, the total energy consumption can be saved up to 75%, which indicates that the path loss exponent has a great impact on routing path selection and energy consumption. To further reveal the reason behind, Fig. 4 plots the energy consumption as a function of the end-to-end outage probability threshold with different path loss exponents.\n\nMentions:\n- A snapshot of the network with an area of 100m × 100m is illustrated in Fig. 2, where λ 1 = 0.43, the corresponding number N = 47, λ 2 = 0.15, the corresponding number of jammers is 17, the equation of road is 3x + 10y − 700 = 0, P u = 0.1W, α = 2, P max = 15W , T = 0.1, and L/ρ = 1s [16].\n- In Fig. 2, the blue line indicates the selected optimal path involving moving vehicles, while the pink line is the selected optimal routing path without the moving vehicles when α = 2.\n- What's more, the number of hops needed in the routing path with moving vehicles is more than that without moving vehicles, e.g., 8 hops versus 4 hops in Fig. 2, indicating that the average energy consumption per node is lower and thus beneficial in terms of prolonging the service time of the networks.\n- Compared with Fig. 2, the optimal routing path is totally different.\n- The same can be concluded from Figs. 2 and 3.\n\nOCR Text: \"Irmen unttmn rik 6mel- ntnem\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2. Optimal routing path with and without moving vehicles when α = 2."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The Squashing function given in Definition 4 is a continuously differentiable approximation of the generalized cutting function by means of sigmoid functions (see Figure 1). By increasing the value of β, the Squashing function approaches the generalized cutting function. In other words, β drives the accuracy of the approximation, while the parameters a and λ determine the center and width. The error of the approximation can be upper bounded by constant/β, which means that by increasing the parameter β, the error decreases by the same order of magnitude. The derivatives of the Squashing function are easy to calculate and can be expressed by sigmoid functions and itself:\n- In the second experiment, the generated dataset is similar to the first. A trapezoidal area lies in the middle of the dataset and the data points are labeled with 1 (blue) and 0 (orange). This area can now be separated by four straight lines. With about 4000 epochs, the training lasts disproportionately longer than the training with two neurons, although the number of parameters only slightly more than doubled. The activation function is the Squashing function with learnable β parameter. We allow β in the AND gate (hidden layer) to be different from that in the first layer. The learning rate is set to 0.02. After about 1700 epochs, the network is able to align the four straight lines to the record. Between 2000 and 4000 epochs, the network improves accuracy significantly, adjusting parameters of the straight lines to obtain a more accurate classification (see Figure 23). During the development of the β parameters of the Squashing functions, the values for the first and for the second layer develop in different directions. Note that allowing β to be negative leads to a decreasing activation function (see Figure 1). For the interpretation of the hidden layer as a logical gate, a negative β value means that in Equation ( 14), the cutting function is replaced by its decreasing counterpart (a step function with value 1 for negative inputs and value 0 for non-negative ones), which corresponds to finding the complement of the intersection. Clearly, for a binary classifier, finding the intersection is equivalent to finding its complement. The development of the β parameters is illustrated in Figure 23. With the corresponding development of the β parameter, the error in the network decreases. The development of network loss is displayed in Figure 23.\n\nMentions:\n- The Squashing function given in Definition 4 is a continuously differentiable approximation of the generalized cutting function by means of sigmoid functions (see Figure 1).\n- Note that allowing β to be negative leads to a decreasing activation function (see Figure 1).\n\nOCR Text: \"{Umbi\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Squashing functions for a = 0.5, λ = 1, for different β values (β1 = 0.5, β2 = 1,β2 = 2,β3 = 5, and β4 = 50)"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Against this background, this paper attempts a close look at its perceived limitation in the vicinity of the axis at a time close to the current derivative singularity, using prescriptions provided by Gratton and Vargas [1]. In this endeavor, the discussion manages construction of solutions to the GV equation close to the axis for times beyond the singularity. This provides an algorithm for calculation of dynamic inductance of the so called \"pinch\" phase of DPF. The characteristic signatures of DPF in current and current derivative signals are qualitatively reproduced, without referring to any plasma phenomena such as instabilities or anomalous resistance or even a hydrodynamic shock wave reflected from the axis. This is a major new result of this work. It does not imply that these plasma phenomena do not play any role; it merely suggests the possibility that the occurrence of current derivative singularity is insensitive to the details of these phenomena; it is quite possible that finer details of the shape of the current derivative contains information specific to some plasma phenomena [6]. Another new result of this exercise is a closed form expression for the dynamic plasma inductance at the moment of singularity. A third new result is the demonstration that the model predicts a power-law dependence of the axial shock-front position on time. Although such power-law dependence is expected from theory of point explosions in ideal gases [13] and has indeed been experimentally observed [14] in DPF, the interesting point about this result is that it comes out of a mathematical model having no connection with hydrodynamics of gases or any other physical phenomena; the only physics contained in the model is the snowplow hypothesis! Also the power-law from the GV model is somewhat different from that observed from the theory, with exponent 0.478 rather than 0.4 from the theory, although this could be related to the fact that what is plotted in Fig. 4 is not real time but the dimensionless time τ . This development follows a very useful suggestion from the Referee, for which the author is extremely grateful. The scope of this paper is deliberately limited to a discussion of the Gratton-Vargas model; the issue of quantitative comparison of its results with experiments is discussed elsewhere for shortage of space [15]. An important conclusion of such quantitative comparison is that the inductance variation predicted by this paper does not agree with experimental data beyond R 1 τ = τ + , although the GV model continues to give very good fit to scaled current profiles of 4 large facilities before this instant [15]. This discrepancy is a valuable pointer to understanding DPF phenomenology around the pinch phase in terms of simple models [6].\n\nMentions:\n- Also the power-law from the GV model is somewhat different from that observed from the theory, with exponent 0.478 rather than 0.4 from the theory, although this could be related to the fact that what is plotted in Fig. 4 is not real time but the dimensionless time τ .\n\nOCR Text: \"8615 *'!-(Tet1pp\"478\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4: Comparison of the axial front position above the anode with a power-law fit."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The sparse scattering architecture is described in Figure 3. A 3 × 3 convolutional operator L is applied on a standardized scattering transform to reduce the number of scattering channels from 1539 to 256. It includes 3.5 10 6 learned parameters. The ISTC network illustrated in Figure 2 has N = 12 layers with ReLU and no batch normalization. A smaller network with N = 8 has nearly the same classification accuracy but the ISTC sparse coding does not converge as well, as explained in Section 4.2. Increasing N to 14 or 16 has little impact on accuracy and on the code precision. The sparse code is first calculated with a 1 × 1 convolutional dictionary D having 2048 vectors. Dictionary columns D m have a spatial support of size 1 and thus do not overlap when translated. It preserves a small dictionary coherence so that the iterative thresholding algorithm converges exponentially. This ISTC network takes as input an array LSx of size 14 × 14 × 256 which has been normalized and outputs a code α 1 of size 14 × 14 × 2048 or a reconstruction Dα 1 of size 14 × 14 × 256. The total number of learned parameters in D is about 5 10 5 . The output α 1 or Dα 1 of the ISTC network is transformed by a batch normalization, and a 5 × 5 average pooling and then provided as input to the MLP classifier. The representation is computed with 4 10 6 parameters in L and D, which is above the 2.5 10 6 parameters of AlexNet. Our goal here is not to reduce the number of parameters but to structure the network into well defined mathematical operators. If we set W = D in the ISTC network, the supervised learning jointly optimizes L, the dictionary D with the Lagrange multiplier λ * and the MLP classifier parameters. It is done with a stochastic gradient descent during 160 epochs using an initial learning rate of 0.01 with a decay of 0.1 at epochs 60 and 120. With a sparse code in input of the MLP, it has a Top 5 accuracy of 81.0%, which outperforms AlexNet.\n\nMentions:\n- The sparse scattering architecture is described in Figure 3.\n\nOCR Text: \"Voti \"bh pni_\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Two variants of the image classification architecture: one where the input for the classifier is the sparse code α, and the other where the reconstruction Dα is the input for the classifier."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The predicted path is derived from using the LSTM prediction at the next time step. Formally, the algorithm is the following. The results from the LSTM using all five variables as outputs seem to indicate that this approach is unable to distinguish the different boat paths. Fig. 7 The AIS data set 1: There are 20 vessels that are clustered into 19 groups via threshold N = 50 using CBTR with a 1, 000 second threshold in Step 1. The actual VIDs are presented by horizontal blue segments whereas the red segments indicate CBTR-clusters. The y-axis value represents the predicted label of the clusters. The correctness rate is 99.71%, which means that most of the red points are correctly connected into segments. The \"Missed Clusters\" are the clusters that are either separated from the true clusters or connected with other clusters. For instance, vessel no.3 is classified into the same cluster to vessel no.11. This is called a merge but not counted as a jump because vessel no.3 does not separated into multiple groups. On the other hand, some portion of vessel no.7 is connected incorrectly to vessel no.5. This contributes 1 jump and 1 merge. One can see that there are 5 groups, thus 4 jumps, in vessel no.15. In fact, the record of vessel no.15 is problematic and thus the classification is bad. Missed Clusters = 26 = 14 jumps + 12 merges Fig. 8 The AIS data set 2: There are 26 vessels that are clustered into 28 groups via threshold N = 50 using CBTR with a 1, 000 second threshold. The correctness rate is 99.85%. Missed Clusters = 21 = 10 jumps + 11 merges Fig. 9 The AIS data set 3: There are 23 vessels that are clustered into 22 groups via threshold N = 50 using CBTR with a 1, 000 second threshold. The correctness rate is 99.81%.\n\nMentions:\n- Missed Clusters = 26 = 14 jumps + 12 merges Fig. 8 The AIS data set 2: There are 26 vessels that are clustered into 28 groups via threshold N = 50 using CBTR with a 1, 000 second threshold.\n\nOCR Text: \"Missed Chses = 25 = 14,Lmjs + N3p ; Datzpoins orerec bv)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 8 The AIS data set 2: There are 26 vessels that are clustered into 28 groups via threshold N = 50 using CBTR with a 1, 000 second threshold. The correctness rate is 99.85%."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- As specified in the 3GPP LTE standards [12], the length of each CP is set as V = 144 when the active data OFDM sub-carrier number is N = 600. The sub-carrier spacing is 15 kHz, so the occupied active data bandwidth is configured as 9.0 MHz [12], leading to a CP duration of 4.68 µs. In this operation mode of LTE, the total number of subcarriers considering inactive and other ones is 1024, and the total channel bandwidth is 10.0 MHz [12], [13]. The equivalent baseband multipath six-tap channel called the ITU-R Vehicular-A channel model [56], which is widely used to emulate the wireless mobile channel, is applied, where the UE receiver velocity of 20 km/h is used to present the typical low-speed mobile channel. The maximum delay spread of the Vehicular-A channel is 2.51 µs, which is equivalent to the channel length L = 76, so the size of the IBI-free region is G = 68. In the simulations, the size of the IBI-free region can be pre-determined according to the system configuration of frame length and the maximum channel delay spread of the adopted channel. According to the CS theory in literature, the CS-based methods require the measurement vector length G to be in order of O (K log(N )) [22], which means the size of the IBI-free region is sufficient for effective recovery with overwhelming high probability. Based on the simulation results that will be reported in Fig. 4, setting the measurement vector length G to be 68 is more than sufficient for accurate recovery using the proposed sparse machine learning based algorithms. In realistic implementation, the maximum channel delay spread can also be obtained from the a priori knowledge of the channel environment and channel statistics, or from the coarse channel estimation of the path delays using the correlation of training sequences at the receiver [57]. The turbo code with code rate of 1/3 as well as the 64QAM modulation as specified in the LTE standards [12] are adopted.\n- As described in the previous numerical analysis, the parameters  SAMP [33], as well as the block SBL-based algorithm called block SBL (BSBL) [36], is also evaluated and reported for comparison. The simulation is carried out by Matlab R2017b on the platform of Intel Core i7 with frequency of 2.80GHz and RAM of 8.00GB. In the evaluation, the learning agent is the agent operating the proposed sparse machine learning algorithms on a wireless cellular transmission terminal specified by 3GPP LTE standards [12], in the presence of the interference from NB-IoT system. The mean square error (MSE) performance of NBI recovery using the proposed methods are shown in Fig. 3, with the y-axis being logarithmic. The performance of the proposed sparse machine learning based methods (SCEM, RSCEM, and S-SCEM with 4 × 4 MIMO configuration), the conventional SBL-based algorithm BSBL [36], and the conventional CSbased methods (PA-SAMP [33] and SAMP [32]) are depicted. The theoretical Cramer-Rao lower bound (CRLB) calculated by 2σ 2 w (K/G) [33], [58] is also included as a benchmark. It is noted from Fig. 3  Moreover, it is shown in Fig. 4 that the MSE of the proposed algorithms decreases fast with the increase of the measurement vector length G, i.e., the length of the received IBI-free region p in (3) utilized for NBI measurement as shown in Fig. 1, whereas the decrease of the MSE of the coventional sparse approximation methods is much slower. At the MSE of 10 −3 , the proposed algorithms of S-SCEM, RSCEM, and SCEM cost only G = 20, 28, and 35 time-domain samples of the IBIfree region, respectively, whereas the SBL-based and CS-based algorithms cost more than 55 and 65 samples. Hence, it can be concluded that the introduction of sparse machine learning will greatly reduce the amount of measurement data required for accurate recovery, achieving higher spectral efficiency than conventional counterparts. The recovery probability of the proposed method for NBI recovery versus the sparsity level K under the Vehicular-A channel is depicted in Fig. 5. The recovery probability is defined as the probability of the effective NBI estimation (i.e. correct support estimation and MSE &lt; 10 −3 ), which is calculated by the ratio of the number of effective NBI estimations to the total 10 3 simulations in Fig. 5. It is noted that the proposed algorithms reach a successful recovery probability of 0.90 at the sparsity level of K = 45, which significantly outperforms the conventional SBL-based and CS-based algorithms with K = 26 and K = 22, respectively. It is thus validated that the proposed methods can accurately recover the NBI with much larger sparsity levels using limited measurement data compared with the conventional sparse approximation algorithms. Since each NB-IoT signal occupies 13 sub-carriers in the LTE spectrum, it can be inferred that the proposed method is capable of effectively recovering and eliminating at least 3 \"in-band\" NB-IoT interfering signals in the LTE system.\n\nMentions:\n- Based on the simulation results that will be reported in Fig. 4, setting the measurement vector length G to be 68 is more than sufficient for accurate recovery using the proposed sparse machine learning based algorithms.\n- It is noted from Fig. 3  Moreover, it is shown in Fig. 4 that the MSE of the proposed algorithms decreases fast with the increase of the measurement vector length G, i.e., the length of the received IBI-free region p in (3) utilized for NBI measurement as shown in Fig. 1, whereas the decrease of the MSE of the coventional sparse approximation methods is much slower.\n\nOCR Text: \"Cuny RMe Aunyesa Cuny HeKL reeiituLCEM prehustu K-SCE Uosetu ESc Auk { Mcu: IEICeDI Veclcr Length G Cuast -\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4. MSE performance of the NBI recovery using the proposed and conventional algorithms with respect to the measurement vector length G."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- During the acute stage (I ), the HCV could have different behaviors for each patient based on his/her immune system response. For 15 to 25% of cases in this stage, the RNA of HCV becomes indistinguishable in their blood serum and the ALT level returns to the normal range. This observation is defined by the term (1 − q)γI in the proposed HCV dynamics [4,6]. Approximately, the immune system in 75-85% of the patients could not remove the hepatitis C virus in the acute stage and their disease becomes advanced to the chronic stage. Note that if the HCV RNA reamins in the patient's blood for at least six months after onset of acute infection, the chronic level of the disease will appear which is defined by the term qγI in Eq. (1) [5,6]. Finally, the defeat in the treatment process is defined by the term p. The treated population decreases by the rate of ξT and join the chronic class by the rate of pξT in the case of treatment defeat and the rest of this population (1 − p)ξT will join aware susceptible class if the treatment be successful. The schematic diagaram of the proposed nonlinear dynamics of the HCV epidemic is depicted in the Fig. 1 and descriptions of the parameters are presented in Table 1 [4]. Using the above-mentioned control inputs, the populations of unaware susceptible (S u ) and chronically infected (C ) classes will decrease by tracking some desired values. Moreover, due to decrease of the mentioned components, the number of aware susceptible (S a ) and treated (T ) individuals will increase and decrease, respectively. The Lyapunov theorem is employed to prove stability of the closed-loop system. In addition, some adaptation laws are defined in order to update estimated parameters of the system to guarantee the stability and robustness of the system against uncertainties of the dynamic model. A conceptual diagram of the proposed nonlinear feedback controller with adaptive scheme is illustrated in Fig. 2.\n\nMentions:\n- A conceptual diagram of the proposed nonlinear feedback controller with adaptive scheme is illustrated in Fig. 2.\n\nOCR Text: \"ain Ftlarkia | LTTujA wblin( !inl @ fr nnnt Vo\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: Conceptual diagram of nonlinear adaptive method developed to control the HCV epidemic in the existence of uncertainties on parameters of the model"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- A fair comparison between the GLR algorithm, which is a quickest detection algorithm and the MME block detection scheme is difficult to obtain due to the assumptions present in the two different detection problems. In the following evaluation we design the MME block detector with a block length of 10 5 samples and probability of false alarm P fa = 0.0147. Using the results from Section IV, we find the probability of detection P d = 0.9269 for the SNR α = −20 dB. When using the GLR algorithm, a run-time has to be decided because the algorithm will eventually generate a false alarm. Evidently, this run-time is to be chosen such that the mean time to false alarm τ fa is significantly higher than the algorithm run-time. We performed a monte-carlo simulation with the GLR algorithm for 1000 random seeds, with N = 10000 samples in each block and a variety of threshold values. Furthermore, two versions of the algorithm per monte-carlo instance are computed. The first one receives samples under hypothesis H 0 , i.e. it only receives noise. The second one receives samples under hypothesis H 1 , so it receives signal with additive noise. For the detection performance this is a worst-case analysis, since the GLR algorithm is initialized with the value zero. If the hypothesis would switch from H 0 to H 1 during the simulation, it might detect a longer consecutive positive drift due to the effects of the noise, resulting in potentially quicker detection. To calculate probabilities of false alarm and detection for the GLR algorithm, respectively, we consider different run-times and calculate how many of the 1000 monte-carlo instances have raised a false-alarm / have correctly detected the signal for a given run-time, threshold h and SNR α. Thus, we can compare the detection performance of the GLR algorithm to the fixed performance of the MME block detector. For the numerical evaluation of the supremum in (25), an SNR range of [−25, −5] dB was evaluated in 0.1 dB steps. In Figure 8, the GLR performance is depicted for several SNRs. The threshold h = 4.5 was chosen, such that at a runtime of 10 5 samples, i.e., the block size of the MME detector, the probabilities of false alarm are approximately equal (GLR: P fa = 0.0140 vs. MME: P fa = 0.0147). As expected, the GLR quickest detection algorithm is able to detect higher SNRs much earlier than the MME detector at a lower or comparable false-alarm rate. However, at low SNRs the MME detector is faster. When considering realistic detection scenarios, the PUs SNR cannot be known beforehand. So the MME detector is designed to have a specific performance for the lowest SNR that has to be reliably detected. Evidently, in practice it would be more precise to consider an SNR interval, say e.g., [−5, −20] dB. If no further knowledge is available, we may model the SNR with a uniform distribution over said interval. Here, we see the advantage of the quickest detection approach. For a wide range of SNRs, it offers faster detection at comparable false-alarm performance, thus wasting less time on the detection process. On the one hand, this leads to higher efficiency in using transmission opportunities, thereby increasing transmission rates. On the other hand, interference for the PU may be decreased by diminishing the reaction time to free the channel when the PU initiates a communication. As can be seen in Figure 8, there is a gap of approximately 2 dB in the low SNR performance of the GLR quickest detection algorithm compared to the MME block detector. This is due to the fact that the GLR algorithm does not use the entirety of samples, but rather blocks of N = 10000 samples to calculate the sample covariance matrices. Thus, in choosing the number of samples N , one has to trade-off the detection delay for higher SNRs and the performance gap compared to the MME block detector for low SNRs. However, it must be stressed that this comparison is not completely fair, since the GLR algorithm introduced in (25) essentially throws data away. This stems from the CUSUM analysis, where the bounds rely on the assumption that the samples of the test statistic used in the CUSUM algorithm are i.i.d.. Further improvements in this direction are discussed in Section VII.\n\nMentions:\n- In Figure 8, the GLR performance is depicted for several SNRs.\n- As can be seen in Figure 8, there is a gap of approximately 2 dB in the low SNR performance of the GLR quickest detection algorithm compared to the MME block detector.\n\nOCR Text: \"Paa = -20B Pa, & = 19dB Pa,a = 18 dR FPa,u=-17dB Pa,a=-16dB Pa &=-15dB GLR run-Litte\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 8. Performance of the GLR algorithm, evaluated as Pd and Pfa over the algorithm run-time for the threshold h = 4.5. The gray circular markers and the thin solid lines indicate the performance of the MME block detector designed for a block length of 105 samples and Pfa = 0.0147, which results in Pd = 0.9269 at SNR α = −20 dB."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The fringe contrast with the interferometer slits orientated in horizontal direction was first measured using a 530nm±5nm bandpass filter and σ-polarization of the SR beam. The raw data points and fitted Gaussian curve shown in Fig. 2 are plotted as a function of spatial frequency with an rms width of 1333 m -1 (d=11.8 mm). From Eq. 6 we infer an rms beam size of σx=120 um. Taking into account the incoherent depth of field effect, the actual beam size is about 118 um [7].\n- Fringe contrast measured along the vertical axis is also plotted in Fig. 2. In this case the curve is wider due to the low aspect ratio of the electron beam. The rms width of the curve is 8135 m -1 (d=72 mm) yielding an rms beam size of σy=20um. This value is well below the diffraction limit σd~80 um for direct visible SR imaging [8], and there is no incoherent depth of field effect. Both the horizontal and the vertical beam size measurements agree well with calculated values at the SR source.\n- Figure 3 shows the variation in fringe contrast when the double slit system and CCD camera were rotated synchronously around the SR beam axis and the polarizer angle held fixed. In this case the slits were separated by 16.94 mm and placed in the lower half of the SR beam below the cold finger. The double slit separation was a compromise between rms values of !,!~1 333 m -1 and !,!~8 135 m -1 taken from measurements seen in Fig. 2. The four data sets plotted in Fig. 3 were measured by increasing betatron coupling leading to beam rotation angles of ! =0\n\nMentions:\n- The raw data points and fitted Gaussian curve shown in Fig. 2 are plotted as a function of spatial frequency with an rms width of 1333 m -1 (d=11.8 mm).\n- Fringe contrast measured along the vertical axis is also plotted in Fig. 2.\n- The double slit separation was a compromise between rms values of !,!~1 333 m -1 and !,!~8 135 m -1 taken from measurements seen in Fig. 2.\n\nOCR Text: \"2510 s0oo 7500 10030 12500 Spalie| freqJenc} cyclesim\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: x- and y-direction fringe contrast."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where 6.1.9 and 6.1.10 are defined as the conditional moment generating function of the chain as follows 11) and λ = 1 − λ, P = 1 − P . The expressions presented here are the same ones as presented in [3], and also the mathematical development to attain them is the same. In order to find the steady state of the chains, as explained, the state equations must be solved simultaneously. The way to do so is by an iterative process, each time given the auxiliary quantities P i (1 | 1), P i (0 | 2) and P I (i), P A (i), P B (i), as shown in Figure 6.2. The iterative method used is Wegstein's iteration scheme, and after achieving satisfying convergence, one may calculate the performance metrics using the steady state of the chains. More details can be found in [3,65].\n\nMentions:\n- The way to do so is by an iterative process, each time given the auxiliary quantities P i (1 | 1), P i (0 | 2) and P I (i), P A (i), P B (i), as shown in Figure 6.2.\n\nOCR Text: \"Lync6sxi Wstul Haucr |:e {x\"[ tlls tun: EM,PPA E_ S Hice Irsi: sLot IlaJI Ajliqit\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6.2: Iterative algorithm for the solution of the coupled sets of equations for the state probabilities of the system chain and the queue length chains."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The optimum value of ν eff is the value that results in this predicted solid angle being equal to the measured value, ΩMeas. For all three bands, frequency ν eff turns out to be around 2% different from the nominal band frequencies, with ν eff = (1.0133, 1.0104, 1.0177)ν0 for the (250, 350, 500) µm bands. The corresponding monochromatic beam profile for the 250 µm band is shown in Fig. 11 for frequency ν eff , and for the two edges of the band. The predicted and measured broadband beam profiles are in good agreement, as illustrated for the 250 µm band in Fig. 12.\n\nMentions:\n- The predicted and measured broadband beam profiles are in good agreement, as illustrated for the 250 µm band in Fig. 12.\n\nOCR Text: \"Mcaurcd nedkud 2S0LM {504n 5u0Lm Fliig - Miat;\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 12. Measured (solid line) and predicted (dotted line) broadband beam profiles for the three SPIRE bands. The beam solid angles are equal by definition."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Fig. [5] we show the results of numerical simulations of the BdG equation ( 8) by plotting the spectrum of |R| 2 as a function of the initial ω for a range of values of the quantum pressure parameter α. Note that α → 0 with αβ =const being the acoustic limit. We see the presence of the critical cut-off frequency above which superradiance ceases, independently of the quantum pressure parameter α and also that superradiance appears to FIG. 4. Modification σ 2 of the overreflection coefficient vs. frequency, scaled to the overreflection cut-off frequency Ωc = mΩ, for different values of Ωc/βρ. FIG. 5. Superradiant scattering of Gaussian wavepackets from an optical Zel'dovich cylinder. The reflected amplitude is measured using our integrated current method developed in Sec. IV.\n\nMentions:\n- Modification σ 2 of the overreflection coefficient vs. frequency, scaled to the overreflection cut-off frequency Ωc = mΩ, for different values of Ωc/βρ. FIG. 5.\n\nOCR Text: \"#lc\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 5. Superradiant scattering of Gaussian wavepackets from an optical Zel’dovich cylinder. The reflected amplitude is measured using our integrated current method developed in Sec. IV."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Pj={} for (p ∈ Pj−1): // ∩ below is performed starting from smallest Ext i j (p) extp = ∩Ext i j (p) Pj = Pj ∪ extp Figure 1: Pseudocode of GJ. algorithm in reference [31] in Appendix F.\n- We base our work on the GJ algorithm (Figure 1). Given a query Q, GJ consists of the following three high-level steps:\n\nMentions:\n- Pj={} for (p ∈ Pj−1): // ∩ below is performed starting from smallest Ext i j (p) extp = ∩Ext i j (p) Pj = Pj ∪ extp Figure 1: Pseudocode of GJ. algorithm in reference [31] in Appendix F.\n- We base our work on the GJ algorithm (Figure 1).\n\nOCR Text: \"K;Ul @ (:L m}: PH (eP# Oklxipuronudsbrs umsnel KXw cx,= Exw P;Puel\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Pseudocode of GJ."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- (II) Four-Device System (less accurate): This system, depicted in Figure 14b, has the minimum number of devices so that reloading the tasks is not necessary. However, to fit  (V) 10-Device System: In the 10-device system, two more devices process temporal and spatial CNNs exploiting data parallelism. Therefore, two devices process each CNN stream, illustrated in Figure 18c. New frames and optical flows are assigned in a round-robin fashion to two devices (of each stream) and are ordered using tags in subsequent devices. (VI) 12-Device System: With 12 devices, Musical Chair increases data parallelism for temporal and spatial CNNs by assigning three devices to each stream, shown in Figure 18d. Performance and Energy Analyses: Figure 19 presents IPS values for all system architectures, the HPC machine, and the Jetson TX2. As expected, the one-device system performs poorly since one PI needs to compute all the tasks. As we expected, when the number of devices increases, Musical Chair achieves better performance by harnessing the computational power of additional devices. In fact, systems larger than five devices always perform better than the TX2 (speed up of 1.9x over TX2-GPU and 5x over TX2-CPU). Figures 20, 21a, and 21b display the end-to-end latency of one frame in the system (split in communication, computation, and reloading time), dynamic and static energy consumption per one inference, and total energy consumption per one inference, respectively. Similarly, we observe significant energy consumption and high latency for the onedevice system. In other words, in comparison with the onedevice system, Musical Chair achieves a 90x energy savings and a speedup 500x for IPS. As Figure 20 illustrates and discussed before, the reason for such speedups is that the one-device system spends most of its latency in reloading   As Figures 19 and 20 depict, although increasing the number of devices in a system also increases the communication latency notably, we observe a performance gain in the inference per second with a higher number of devices. This is because in both data and model parallelism, the systems hide latency by distributing or parallelizing tasks. In other words, in Figure 18, devices in the same vertical column reduce the effective latency for the devices in the next column by parallelizing. For the large number of devices, Musical Chair achieves not only similar energy consumption with TX2 but also saves energy as compared to the HPC machine. Figure 21b depicts that, except for the TX2 with GPU, the energy consumptions per inference (i.e., Watt /performance) of systems with more than five devices is always better than other cases (up to 4.3x and an average of 1.5x). Note that in our evaluations, the power consumption of the Raspberry PI systems is inclined to higher energy consumption because: (i) In comparison with TX2, when we increase the number of devices for Raspberry PI systems, since each device is on a development board and has several unnecessary peripherals, the energy consumption increases significantly, which is shown in static energy; (ii) The TX2 is a low-power design with power gating capabilities that power gates three cores if not needed, but the Raspberry PIs do not have such capabilities; and (iii) the energy consumption of the Raspberry PI systems also includes the energy for communication between the devices, and wasted energy of powering an idle core during data transmission. 3\n\nMentions:\n- As Figure 20 illustrates and discussed before, the reason for such speedups is that the one-device system spends most of its latency in reloading   As Figures 19 and 20 depict, although increasing the number of devices in a system also increases the communication latency notably, we observe a performance gain in the inference per second with a higher number of devices.\n\nOCR Text: \"EKrop LUrn ~KGmmuuan @Lehaetg Selem Arhlexld ?\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 20: Measured end-to-end latency of one frame."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 5: The algorithm of LSDRF. LSDDR2({x i } n i=1 ) denotes the LSDDR estimator for the secondorder density-derivative-ratios, and { θ j } D(D+1)/2 j=1 are the corresponding coefficient vectors. {y k } n k=1 are initial points to approximate the density ridge. ComputeGrad({ β j } D j=1 , {c i } b i=1 , z τ k ) computes the estimated log-density gradient g(z τ k ) and m(z τ k ) in ( 14), while ComputeProjector( g(z\n\nMentions:\n- Figure 5: The algorithm of LSDRF.\n\nOCR Text: \"Imji;1d6. SK4 LaLL #r#0 Jnti . DMo b\" Na In: Waa JL{0:;; K\"\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: The algorithm of LSDRF. LSDDR2({xi}ni=1) denotes the LSDDR estimator for the secondorder density-derivative-ratios, and {θ̂j}D(D+1)/2j=1 are the corresponding coefficient vectors. {yk}n ′ k=1 are initial points to approximate the density ridge. ComputeGrad({β̃j}Dj=1, {ci}bi=1, zτk ) computes the estimated log-density gradient ĝ(zτk ) and m̂(z τ k ) in (14), while ComputeProjector(ĝ(z τ k ), {θ̂j} D(D+1)/2"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Here the x-component (38), (39) (see figure 4, red curve) is the most familiar and describes the \"ordinary\" spin oriented parallel to the propagation direction and owing to the ellipticity of the wave polarization in the transverse yz plane. Comparison with (28) and (29) shows that it is connected to the field helicity which, in turn, is associated with the ellipticity of the incident, reflected or refracted wave. In particular, the contribution x s  (39) appears due to ellipticity of the EW polarization expressed by non-zero value of\n- Vanishing z s  in the EW is quite expected from the symmetry but, in fact, it is formed by opposite non-zero electric and magnetic contributions which exactly cancel each other [18] (separate existence of these contributions can be revealed through the mechanical torque exerted on a dual asymmetric material object). In contrast, non-zero z s  in the medium 1 (black line in figure 4) results from the geometric summation of spins carried by the incident and reflected waves [7,9] and owes to the difference in arguments of the unimodular complex quantities R  and R  . Had the reflection been perfect (that is, R R    and the only change in the polarization state had been the ellipticity sign reversal,  r = - 0 ), the normal spin component would have vanished, 0 z s   . The most interesting result is expressed by formulae (40) and ( 41) which describe the specific transverse spin (green line in figure 4) that is of a particular interest in structured optical fields  p [7,9,10,14,15,17,18,31,46,47]; its general and comprehensive description has been recently developed [23]. In addition to the \"anomalous\" direction (orthogonal to all the wavevectors of the incident, reflected and evanescent (9) waves), its remarkable feature is that it arises independently of the incident wave state of polarization and does not require any initial ellipticity.\n- x y s s     (see figure 4). This feature is related with the choice of Minkowski spin as the true intrinsic angular momentum [31] of the field in non-dispersive dielectric media and sharply contradicts to the earlier analyses dealing with the Abraham definitions [17,34].\n\nMentions:\n- Here the x-component (38), (39) (see figure 4, red curve) is the most familiar and describes the \"ordinary\" spin oriented parallel to the propagation direction and owing to the ellipticity of the wave polarization in the transverse yz plane.\n- In contrast, non-zero z s  in the medium 1 (black line in figure 4) results from the geometric summation of spins carried by the incident and reflected waves [7,9] and owes to the difference in arguments of the unimodular complex quantities R  and R  .\n- The most interesting result is expressed by formulae (40) and ( 41) which describe the specific transverse spin (green line in figure 4) that is of a particular interest in structured optical fields  p [7,9,10,14,15,17,18,31,46,47]; its general and comprehensive description has been recently developed [23].\n- x y s s     (see figure 4).\n- First to note, since the spin components s x and s y (cf. equations ( 38) -( 41) and the red and green curves in figure 4) are continuous, the differentiation in (44) produces no singularities in the spin momentum distributions that were predicted for the surface-polariton and the TR fields earlier [17,34].\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4. Spatial distribution of (red) longitudinal component of spin sx (38), (39), (green) transverse “extraordinary” spin sy (40), (41), (black, right scale) spin sz normal to the interface (42); the field configuration is the same as in figure 2."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The difference between the free energies of the homogeneous and non-homogeneous configurations (see equation (6.5)) for fixed value of the charge density ρ is displayed in figure 6. As required, we see that whenρ &gt;ρ crit , the free energy of the non-homogeneous configuration is lower than the one of the homogeneous configuration, indicating that the system will settle in the new ground state. We also see that for large enough charge density,ρ &gt; 5.4, a new branch of solutions appears. While the first non-homogeneous branch is given by an even function h(z), thus describing a vectorial current, the next branch is an odd function, describing an axial current. As the chemical potential is increased further, yet more branches occur, alternating between vectorial and axial. However, as is manifest from figure 6, all higher branches have higher values of the free energy, and thus correspond to excited states, not ground states.\n\nMentions:\n- The difference between the free energies of the homogeneous and non-homogeneous configurations (see equation (6.5)) for fixed value of the charge density ρ is displayed in figure 6.\n- However, as is manifest from figure 6, all higher branches have higher values of the free energy, and thus correspond to excited states, not ground states.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6: The difference between the free energies of the homogeneous and nonhomogeneous configurations as a function of the charge density. The dashed line depicts the axial ground state, which clearly has higher energy and is therefore unstable."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In this Letter, we consider for the first time, in the context of PBH searches, the X-ray and radio emission from the Galactic Ridge region produced by the accretion of interstellar gas onto a population of O(10) M PBHs in the Milky Way. Given current estimates of the bulge mass [14], if PBHs constitute all of the DM, there should be O (10 9 ) such objects within 2 kpc from the Galactic center (GC). Since the inner part of the bulge contains high gas densities [15], a significant fraction would inevitably form an accretion disk and emit a broad-band spectrum of radiation. We show (fig. 1) that radio and X-ray data in the Galactic Ridge region rule out, at 5 and 40σ respectively, the possibility that PBHs constitute all of the DM in the Galaxy, even under conservative assumptions on the physics of accretion.\n- Results: The main result of the Letter is presented in fig. 1. We display the 2σ, 3σ, and 5σ constraints on the DM fraction as a function of the PBH mass.\n\nMentions:\n- We show (fig. 1) that radio and X-ray data in the Galactic Ridge region rule out, at 5 and 40σ respectively, the possibility that PBHs constitute all of the DM in the Galaxy, even under conservative assumptions on the physics of accretion.\n- Results: The main result of the Letter is presented in fig. 1.\n\nOCR Text: \"Jij\": Jen X-s CLxIiia: |16 Ili-: \"M\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 1. Upper limits on the fraction of DM in PBHs of a given mass M , arising from the non observation of bright X-ray (blue shaded regions) and radio (red) BHs candidates at the GC. We assume a conservative value of λ, regulating the departure from Bondi accretion rate: λ = 0.02. The dotted grey line corresponds to 30M PBH, the hatched grey region is unphysical (fDM > 1)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- ⊲ Update uncertainty about intended program return p best = argmax p∈P S pd(p) Fig. 1. An active program learner that selects the next significant input (to query the oracle/user) at each iteration greedily based on information gain.\n- Active program learner. We now turn the passive learner into an active learner. Procedure ActiveProgramLearner in Figure 1 uses greedy information gain (Proposition 1) to implement an active program learning method. The procedure maintains its current belief of the intended program as a probability distribution pd on the program space P S. The entropy, En(pd) of this distribution is a measure of our uncertainty, and while our uncertainty measure is greater-than a threshold ǫ, we continue to add an input-output i, f (i) to the set ϕ of input-output examples. The input i, which is picked in each iteration as a significant input, is the one that maximizes entropy En(P r i ). Note that En(P r i ) is the uncertainty in the output for input i given our current belief pd of the desired program. Once a new input-output pair is added to ϕ, we use an enhanced passive learner, ppl * , to update pd in that iteration. When the loop terminates, we return the program p whose probability pd(p) is maximum as the learnt program.\n\nMentions:\n- ⊲ Update uncertainty about intended program return p best = argmax p∈P S pd(p) Fig. 1.\n- Procedure ActiveProgramLearner in Figure 1 uses greedy information gain (Proposition 1) to implement an active program learning method.\n\nOCR Text: \"Kcquirc: ,4 ' , 4 Mzalh % [4iz\"' H6iem Hintt Rcqzire: 1 JZT X FFIlts Kccuire; Jli Ulsttzinby (Lclul Mutctct _ ActPoskaMLcAHKEKK;o\" Ipu-~ukut etacks ?+ HLFst dckzs-icondem; jrix m P5 Eatw; (coctain;| abuut kr:| poem ;i + Eivpi' while = pail: tzme $ NCC \"zurkuilt\" abox iut:: [ouxtJI Eczzch i € %; la E;\"s,-,quig Hwali: pobuhibs Io E IWM %0 JUSIES,€u ColPo| EfWl Gfh Euact:1 Uesut Qby 'J6j Uil _ \"u @u jiiiedad \" S iws [zel puo c14t ahou icoks] pTM cnlee| L~da \"ubaicty Ahx iica:| j0xt M THhuI U ULIL; ~/w\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1. An active program learner that selects the next significant input (to query the oracle/user) at each iteration greedily based on information gain."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Townsend etal. [51] proposed a categorization decision-making experiment to study the interactions between categorization and decision making. In the experiment, pictures of faces varying along face width and lip thickness are shown to participants. Generally, the faces can be distributed into two different kinds: one is a narrow (N) face with a narrow width and thick lips; the other one is a wide (W) face with a wide width and thin lips (see Figure 6 for example). The participants are informed that N face had a 0.60 probability to come from the \"bad\" guy population while W face had a 0.60 probability to come from the \"good\" guy population. The experiment can The experiment results are shown in Table 1. The column labeled P (G) rep- resents the probability of categorizing the face as a \"good buy\". The column labeled P (A|G) represents the probability of attacking given categorizing the face as a \"good guy\". The column labeled P (B) represents the probability of categorizing the face as a \"bad buy\". The column labeled P (A|B) represents the probability of attacking given categorizing the face as a \"bad guy\".\n\nMentions:\n- Generally, the faces can be distributed into two different kinds: one is a narrow (N) face with a narrow width and thick lips; the other one is a wide (W) face with a wide width and thin lips (see Figure 6 for example).\n\nOCR Text: \"B\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6: The decision-making process of Markov BA model"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The designed circuit is developed and simulated, thereafter electronics circuit is tested with several capacitors in the range of 0-600pf and operation frequency of 740 Hz and circuit is calibrated using highly accurate IM 3570 Impedance Analyzer-Hioki L 2000 4 -terminal probe , calibration plot is shown in fig. 4.\n\nMentions:\n- The designed circuit is developed and simulated, thereafter electronics circuit is tested with several capacitors in the range of 0-600pf and operation frequency of 740 Hz and circuit is calibrated using highly accurate IM 3570 Impedance Analyzer-Hioki L 2000 4 -terminal probe , calibration plot is shown in fig. 4.\n\nOCR Text: \"Electroni: cizcuit [esukr = jipdauce Anahzcr Capxcitance declareel (in ply\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4: Comparison of results of our developed electronic circuit and Impedance analyzer"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Previously, Ares and Castro 13 noticed that periodic oscillations occur within the histogram, leading to spikes at differences that are a multiple of 6. Motivated by this observation, let us now examine if similar structural properties exist among the differences between primes that need not be consecutive, see (Figure 2). FIG. 2. Distribution of the differences between every combination of primes, beginning from the second prime (p2 = 3) to the 50000th prime. We find that some of the prominent spikes are at distance values that \"foreshadow\" the occurrence of jumping champions.\n- Figure 2 exhibits a significant number of spikes and a closer inspection shows that among them are all the primorials up until the primorial 2310. This very early occurrence of such large primorials suggest closer examination with more powerful tools. To this end, we will now employ Shannon sampling methods from information-theory. We will use both regular and adaptive Shannon methods to map the discrete distribution of the primes into continuous functions whose periodicities, which are related to spacings of primes, can then be analyzed using the Fourier transform.\n\nMentions:\n- Motivated by this observation, let us now examine if similar structural properties exist among the differences between primes that need not be consecutive, see (Figure 2).\n- Figure 2 exhibits a significant number of spikes and a closer inspection shows that among them are all the primorials up until the primorial 2310.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 2. Distribution of the differences between every combination of primes, beginning from the second prime (p2 = 3) to the 50000th prime. We find that some of the prominent spikes are at distance values that “foreshadow” the occurrence of jumping champions."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In such case ϕ(•) is fixed to be the supremum function, over [0, T ], of the function Z(t). Accurate prediction of daily peak load demand is very important for decision in the energy sector. In fact, short-term load forecasts enable effective load shifting between transmission substations, scheduling of startup times of peak stations, load flow analysis and power system security studies. Figure 1 provides a sample of seven daily load curves (from 07/01/2002 to 13/01/2002). Vertical dotted lines separate days and the star points correspond to the peak demand for each day.\n\nMentions:\n- Figure 1 provides a sample of seven daily load curves (from 07/01/2002 to 13/01/2002).\n\nOCR Text: \"Bxxl*riuiz M xEh \"rur Teruty\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Half hour electricity consumption in France from 07/01/2002 to 13/01/2002 (7 days). The vertical dotted lines separate days and the star points correspond to the peak demand for each day (in Giga-Watt)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Consider the following 2-step (i.e., n = 2) search tree shown in Figure 6. The deterministic transitions are shown with the arrows, each corresponding to an action in A = {L, R}. The leaves are denoted with filled circles whose posterior values are given by Q * 0 (•, •)|ω 1:t . The root state is shown as s ρ with its immediate successors as s 0 and s 1 . Assume γ = 1 and all shown actions yield 0 immediate rewards. Finally, assume the posterior distribution of the leaf values are as in Figure 6, and they are pairwise independent. In this case, we can see that no single sample from the leafs can result in a policy change at s ρ since we would need to sample both of the leaves of the left subtree at least once for the policy at the root to change from L to R. Therefore, VOC φ2 is zero for all possible computations here, and thus stops early, not achieving neither one-step nor asymptotic optimality. In contrast, VOC φ2 is greater than zero for computations concerning the left subtree.\n\nMentions:\n- Consider the following 2-step (i.e., n = 2) search tree shown in Figure 6.\n- Finally, assume the posterior distribution of the leaf values are as in Figure 6, and they are pairwise independent.\n\nOCR Text: \"O8w L  O*o Rlc 0Ks Rly Nlu ~My\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6: A search graph, where VOC′(φn)-greedy stops early."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- A longstanding problem in electromagnetism is finding the correct approach to partition the right-hand side of (1) into the rate of change of stored field energies and the dissipation rate of the field energy in a general medium. An attempt will be made in this section to solve this problem. In order to get some physical insights into the nature of the problem, let us first examine a typical RLC circuit shown in Figure 1. According to Kirchhoff\"s voltage law, we may write and the right-hand side of (2) denotes the rate of increase of energy in the RLC circuit, which can be decomposed into the sum of the rate of energy absorbed by the resistor, the rate of magnetic energy stored in the inductor and the rate of electric energy stored in the capacitor. In general, it is impossible to find an energy function R w such that the dissipated power can be written as the time derivative of R w ,\n\nMentions:\n- In order to get some physical insights into the nature of the problem, let us first examine a typical RLC circuit shown in Figure 1.\n\nOCR Text: \"1\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1 RLC circuit."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The process of querying and exporting files from GeoRocket is depicted in Figure 3. The main component in this diagram is the Merger verticle. It sends a query (see Section 2.3) to the Indexers, which in turn search the index for chunk IDs matching the query's criteria. The chunk IDs are then sent back to the Merger, which in turn loads matching chunks from the storage back end. These chunks are joined to a valid output file that is finally rendered to the client.\n\nMentions:\n- The process of querying and exporting files from GeoRocket is depicted in Figure 3.\n\nOCR Text: \"Fille Query Indexer(s} Index Chunk Chunk Storage back end Merget\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Exporting a file from GeoRocket: the merger retrieves chunk IDs from the indexer and merges matching chunks from the storage back end."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Low Energy Surface Flashover Igniter: V-I waveforms and a corresponding series of fast photographs of an independent LESF are shown in Figure 3 (VPPA = 0 V in this experiment). It was observed that breakdown occurred when VLESF reached about 8 kV. The inception of surface flashover is signified by a drop in the voltage at t = 0 as illustrated in Figure 3(a). A subsequent generation of seed plasma shorts the LESF assembly electrodes and triggers a resonant ringing in the LC-circuit, formed by the plasma column shorted flashover assembly. The current oscillations peaked at 12 A and decayed on the time scale of 100-200 ns (this decay time provides an estimate of flashover duration time). The flashover decay time of 100-200 ns is further confirmed by fast ICCD photography taken at moments of time τ1-τ4 (35, 65, 100 and 155 ns, respectively) with 3 ns exposure, depicted in Figure 3(b). Note, the ignition events photographed with ICCD were well repeatable, so images shown in Figure 3(b) were acquired in different ignition events. One can see that the flashover plasma vanished at about t = 100-155 ns after the flashover initiation. Initial energy stored in the flashover assembly's capacitance of C = 26.6 pF prior the breakdown was approximately 0 = 1 2 2 = 0.85 mJ. The oscillation period of T=25 ns observed experimentally was consistent with the shorted LESF assembly inductance of L=0.6 μH. Additional details on estimating LESF parameters can be found in our previous works [12,13]. Accelerating Channel Dynamics: It was observed in the experiment that the LESF flashover event triggered PPA discharge when DC voltage was applied to the PPA electrodes. This is illustrated in Figure 4, where voltage and current waveforms of the PPA are presented along with visual ICCD camera observation. Considerable noise was observed around t = 0 due to the LESF ignition event. Following the generation of seed plasma by LESF, capacitor voltage (VPPA) dropped from an initial value of 1.8 kV and arc current (IPPA) rose to a peak of 7.42 kA. The decaying oscillations of current and voltage shifted approximately 90 degrees, indicating operation in the underdamped LCR regime [2,18]. To visualize the dynamics of plasma acceleration in PPA, ICCD images (100 ns exposure) taken at timestamps t1-t5 of the discharge cycle are illustrated in Figure 4(b). Note, the PPA discharge events photographed with ICCD were well repeatable, so images shown in Figure 4(b) were acquired in different discharge events. A 1 second long exposure photograph of the PPA firing event is additionally depicted for reference. One can clearly see plasma front propagation inside the PPA assembly from the location of seed plasma creation (near the LESF at the bottom of the channel) towards the exit of the PPA channel. Plasma front propagation speed varied in the range 10-30 km/s. Exhaust Plume Propagation: The set of three double probes exposed to the LF-PPT exhaust plume is photographed in Figure 5(a). The IPPA and current waveforms measured by the three double probes (Ip1, Ip2, and Ip3), located at z = 5.2, 6.5, and 7.8 cm, are depicted in Figure 5(b). The moment of plasma arrival to each double probe is indicated by a corresponding rise of the double probe current. One can see that plasma reached the probes at z = 5.2, 6.5, and 7.8 cm at 3.9, 4.3, and 4.7μs, respectively. Plasma arrival times were used in conjunction with probe spacing to estimate exhaust velocity at  32 km/s. Total Current Measurement: Total ion current produced by the PPA (Iion) and measured by the large-area Langmuir probe is presented in Figure 6, along with the IPPA waveform. The negative current precursor seen at t~7 μs after PPA discharge ignition can be potentially attributed to the arrival of the front of fast ions or electrons [19]. The bulk of plasma arrival is indicated by the positive pulse of ion current which peaked at t~20 μs. Instantaneous thrust produced by the plasma ions can be roughly estimated from the measured total ion current through =̇= | |\n\nMentions:\n- Total Current Measurement: Total ion current produced by the PPA (Iion) and measured by the large-area Langmuir probe is presented in Figure 6, along with the IPPA waveform.\n\nOCR Text: \"Tia;\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6: LF-PPT discharge current and total exhaust plume current measured by the single Langmuir probe."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In the following, we shall study the light field distribution of the one-dimensional GFPCs for the positive and negative incident of light. The positive incident figure is shown in FIG. 10  10). In FIG. 12, the frequency of incident light ω is the odd times of the center frequency ω 0 . In FIG. 12 (a), (b) and (c), the incident light ω are corresponding to ω 0 , 3ω 0 and 5ω 0 . When ω = ω 0 and 3ω 0 , the light intensity of transmission are enhanced or magnified (magnification M = 10 7 ), while ω = 5ω 0 , the intensity of transmission light is weaken. From FIG. 7, we can find when ω = ω 0 and 3ω 0 the light is in conductance band, and the transmission light has been enhanced by the GFPCs. In the conventional PCs, the intensity of   transmission light can not be magnified. When ω = 5ω 0 the light is in forbidden band, and the transmission light has been weaken by the GFPCs. In FIG. 13, the frequency of incident light ω is the even times of the center frequency ω 0 . In FIG. 13 (a), (b) and (c), the incident light ω are corresponding to 2ω 0 , 4ω 0 and 6ω 0 , the intensities of transmission light have been magnified. Figures 14 and 15 are the electric field distributions of negative incident (FIG. 11). In Figures. 14 and 15, the frequency of incident light ω is the odd and even times of the center frequency ω 0 , and the intensities of transmission light have been all weaken. By calculation, we can obtain the following results for the GFPCs: (1) For the positive incident, i.e., the refractive indexes increasing along the direction of incident light, the intensity of transmission light have been magnified when the incident light frequency is in conductance band, which can be made light  magnifier (magnifying multiple 10 7 ).(2) For the negative incident, i.e., the refractive indexes decreasing along the direction of incident light, the intensity of transmission light have been weaken, which can be made light attenuator (magnifying multiple less than 1). (3) The GFPCs structure (positive incident FIG. 10 and negative incident FIG. 11) is the optical diode, since the light intensity is magnified (the light get across) when the light is positive incident, and the light intensity has been decreased (the light cutoff) when the light is negative incident, which transmits light from an input to an output, but not in reverse direction.\n\nMentions:\n- In FIG. 13, the frequency of incident light ω is the even times of the center frequency ω 0 .\n- In FIG. 13 (a), (b) and (c), the incident light ω are corresponding to 2ω 0 , 4ω 0 and 6ω 0 , the intensities of transmission light have been magnified.\n\nOCR Text: \"Moh\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 13: The light distribution of positive incident (FIG. 10) in the GFPCs. Figure (a) (b) and (c) are corresponding to the incident light frequency ω = 2ω0, 4ω0 and 6ω0."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We consider learning the representations of items and sessiongraphs with GNNs where the session-graph is represented by G s as introduced in Section 3. Consider two normalized adjacency matrices A in s ∈ R l,l and A out s ∈ R l,l corresponding to the incoming and outgoing edges in graph G s as illustrated in Fig. 2. GNN takes adjacency matrices A in s and A out s , and the normalized item embed-dingsĨ s as input, and returns an updated set of embeddings after τ iterations of message propagation across vertices in the graph using gated recurrent units [7]: [ĩ τ s,1 ,ĩ τ s,2 , . . . ,ĩ τ s,l ] = G(A in s , A out s ,Ĩ s ; θ д ), where θ д represents the parameters of the GNN function G. For any node in the graph, the current representation of the node and the representations of its neighboring nodes are used to iteratively update the representation of the node τ times. More specifically, the representation of node i s, j in the t-th message propagation step is updated as follows:\n\nMentions:\n- Consider two normalized adjacency matrices A in s ∈ R l,l and A out s ∈ R l,l corresponding to the incoming and outgoing edges in graph G s as illustrated in Fig. 2.\n\nOCR Text: \":bX\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: Illustrative flow diagram for NISER."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figures 2 to 6 clearly show that theD(r,r) has decreased to random performance and also an orderly behaviour of M SE(x, y) correctly approaching minimum distortion of the data. The actual impact on Data Analytical outcomes will be discussed in another paper that is currently in preparation.\n\nMentions:\n- Figures 2 to 6 clearly show that theD(r,r) has decreased to random performance and also an orderly behaviour of M SE(x, y) correctly approaching minimum distortion of the data.\n\nOCR Text: \"CGE' Lr ^ EEechix [0\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: The Absenteeism at Work data includes personal information and total absenteeism time over 3 years for employees at a courier company in Brazil. We have selected age as the undesirable protected characteristic and successfully removed it. Source: UCI Machine Learning Database. Creators: Andrea Martiniano, Ricardo Pinto Ferreira, and Renato Jose Sassi. D̄ values denote proportion of correct predictions of race, while all other values are arbitrarily scaled."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Since no ground truth exists, we construct an independent user study to provide realistic feedback and quantify the subjective evaluation. We collect 50 real-world rainy images from the Internet as a new dataset 1 . We use the compared five methods to generate de-rained results and randomly order the outputs, as well as the original rainy image, and display them on a screen. We then separately asked 20 participants to rank each image from 1 to 5 subjectively according to quality, with the instructions being that visible rain streaks should decrease the quality and clarity should increase quality (1 represents the worst quality and 5 represents the best quality). We show the average scores in Table II from these 1,000 trials and our LPNet has the best performance. In Figure 10, we show the scatter plot of the rainy inputs vs de-rained user scores. This small-scale experiment gives additional support that our LPnet improves the de-raining on real-world images.\n\nMentions:\n- In Figure 10, we show the scatter plot of the rainy inputs vs de-rained user scores.\n\nOCR Text: \"Soy SRCNN JORDEP Muptkl Rainy nmite\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 10: Scatter plot of user study."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In this study, we consider a three-layer fully recurrent neural network with smooth activation functions. Fig. 1\n\nMentions:\n- Fig. 1\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1. Artificial neural network structure"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Here D j (0) are the data simulated for the reference medium with no reflectivity and the right hand side is calculated using We begin the comparison between the conventional LS-RTM and the proposed ROM-GN with a study of Jacobians of the corresponding mappings. As shown in Fig. 4.4, the Jacobian of q → {D Born j } 0≤j≤2n−1 has worse conditioning compared to the Jacobian of q → L ROM (q). Consequently, while LS-RTM required regularization via SVD truncation, we did not use regularization in ROM-GN .\n\nMentions:\n- As shown in Fig. 4.4, the Jacobian of q → {D Born j } 0≤j≤2n−1 has worse conditioning compared to the Jacobian of q → L ROM (q).\n\nOCR Text: \"Singular values ROM-GN itor,| -Singular values ROM-GN iler. Singular values Doru Tnncation LS-KTM Born data Tiuncatiom LS-RTM vaw\" data 200 4OO GOO 800 index\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 4.4. Squared singular values of the Jacobian of the mapping qS 7→ {DBornj }0≤j≤2n−1 (blue curve) and of the mapping qS 7→ LROM(qS) for the first and fifth Gauss-Newton iterates (red and black curves, respectively). The LS-RTM formulations with Born and raw data are regularized using truncated SVD at the values indicated with the circle and triangle, respectively."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In the allowed region, although Method A provides more accurate results, both of them coincide with the numerical results. However, within the forbidden region and around θ c , there is a big difference between our results and the numerical ones. When the VED is far away from the interface, as shown in Fig. 3(a), the asymptotic result decays rapidly around θ c due to the effect of exp(−ik 1z d 1 ), while the red line shows a slow attenuation. At first glance, this seems strange since the numerical results also contain exp(−ik 1z d 1 ). This can be answered by Eqs. ( 3) and (6b). The numerical method treats exp(−ik 1z d 1 )/k 1z in Eq. (3) as a whole factor, which means that when both of the numerator and denominator tend to 0, k 1z slows down the decay of exp(−ik 1z d 1 ). While the asymptotic methods treat 1/k 1z as a constant, implying that only the decay of exp(−ik 1z d 1 ) is considered. In SIs, k 1z = 0 corresponds to the k 1 branch point, and at this point the integration gives a lateral wave [2][3][4][5][6]. Lateral waves are the surface waves that mainly exist in the higher RI semispace and exponentially decay in the lower RI one. This explanation is also in line with the description of the steepest descent method. Near θ c , the steepest descent path approaches the k 1 branch point and its contribution to the integral becomes inegligible. In the forbidden region, the branch point is enclosed within the deformed integration path, and causes the lateral waves to appear in the forbidden region. Since the lateral waves represent a higher-order attenuation, it is not covered by the LOA. This indicates that the differences between the asymptotic results and numerical results come from the branch point contributions. The branch point contributions can also be treated by asymptotic methods, e. g., branch cut integrals based on the steepest descent method [29][30][31], or uniform asymptotic expansions [6]. However, these methods regard exp(−ik 1z d 1 ) as a slowly varying term [6,29,30], and numerical computation is resorted [13,15,31]. It is difficult to generalize such asymptotic expressions to the cases of multilayer structures [30]. The exploration of asymptotic expressions for branch cut integrals is beyond the scope of this paper, so here we numerically calculate the decay behaviors of the lateral waves at angles 120 • and 135 • in Fig. 3, and show them in Fig. 4. It is seen from Fig. 4 that when |d 1 | &gt; 2λ 0 , the lateral waves decay as r −2 . This coincides with the case shown in Figs. 3(a) and 3(b) where r decreases by one order of magnitude while the field decreases by two orders. When λ 0 &lt; |d 1 | &lt; 2λ 0 , the way of the decay of the lateral waves turns to r −1 , so that in Figs. 3(c) and 3(d) the numerical results and the asymptotic results have similar distributions in the forbidden region.\n- Having the knowledge of the decay behaviors of the lateral waves, we discuss the NFFF boundary in the bottom layer. As can be seen from Fig. 3, the boundary may be chosen as L ∼ 1000|d 1 |, which satisfies the evaluation of the order of magnitude indicated by Fig. 4. However, this distance should be modified because of two reasons. One is that for a very small |d 1 |, e. g., |d 1 | = 0.001λ 0 , L ∼ λ 0 may not eliminate the influence of the lateral waves (numerically verified), which means that a lower boundary should be better. Considering Fig. 3(d), the boundary may be written as L ∼ 1000×Max{|d 1 |, 0.1λ 0 }, where Max{, } means \"choose the larger one\". The other is that compared to the one of a homogeneous background, the NFFF boundary here has a looser relationship with λ 0 . Although the two |d 1 | values in Fig. 3 differ by 100 times, the NFFF boundary only depends on |d 1 |. Moreover, as implied by Eqs. (3), (6a) and (6b), the field distribution has a scale invariance over λ 0 in a non-dispersive media, indicating that the field behaviors shown in Fig. 3 stand also for other light wavelengths. So, what influences does λ 0 have on the NFFF boundary? Except for the lower boundary, it mainly decides the location of the forbidden region and introduces dispersion. The dispersion considered, the NFFF boundary is suggested to be\n- The NFFF boundary in the multilayered structures is mainly affected by the lateral waves as well. The attenuation behaviors of these waves are similar to the ones shown in Fig. 4, so that Eq. ( 9) is correct in the order of magnitude. The value of the dimensional parameter D depends on whether the VED is in the top layer or in the intermediated region. When the VED is in the top layer, D equals the distance between the VED and the lowest interface at z = d Q−1 ; when the VED is in one of the intermediate layer, D is the distance between the highest interface at z = d 1 and lowest interface at z = d Q−1 , which is the total thickness of the intermediate region. The multi-reflection processes do not affect the D value for the following two reasons. Firstly, the multi-reflections have a quick convergence. All the results of Method A shown in the figures converge when m is up to 2 ∼ 4, which does not change the order of D. Secondly, as can be seen from Fig. 4, the lateral waves have a nearly r −1 decay when D is small, and have a more rapid decay when D is relatively larger. Therefore the influence from the multi-reflection may decay to a negligible one within the distance given by Eq. (9). Moreover, it is this decay characteristic of the lateral waves that makes the NFFF boundary almost independent of the wavelength.\n\nMentions:\n- The exploration of asymptotic expressions for branch cut integrals is beyond the scope of this paper, so here we numerically calculate the decay behaviors of the lateral waves at angles 120 • and 135 • in Fig. 3, and show them in Fig. 4.\n- It is seen from Fig. 4 that when |d 1 | &gt; 2λ 0 , the lateral waves decay as r −2 .\n- As can be seen from Fig. 3, the boundary may be chosen as L ∼ 1000|d 1 |, which satisfies the evaluation of the order of magnitude indicated by Fig. 4.\n- The attenuation behaviors of these waves are similar to the ones shown in Fig. 4, so that Eq.\n- All the results of Method A shown in the figures converge when m is up to 2 ∼ 4, which does not change the order of D. Secondly, as can be seen from Fig. 4, the lateral waves have a nearly r −1 decay when D is small, and have a more rapid decay when D is relatively larger.\n\nOCR Text: \"Fj dhy Fiekedy ~-I8\" #I2\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4: Results of the power series fitting the amplitude of the lateral waves at angles 120◦ and 135◦ in Fig. 3. Fitting functioin is |Ez|=ar −n, where a (not show here) and n are fitting coefficients. The fitting range is r=4000 ∼ 10000λ0 [38]."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Fig. 3, we show upperbound on µ tan β that satisfies the metastability condition B ≥ 400. In numerical calculation we set δ H = 1 and tan β = 70. It is found from Fig. 2 (top) that dependence on tan β changes the result at most 5%. We also evaluate that the dependence on δ H is less than 1% in the range that 0.65 ≤ δ H ≤ 1.3. We also show upperbound on µ tan β that satisfies a rigorous stability condition (the electroweakbreaking vacuum is global minimal in the potential) in Fig. 4. The metastability condition gives about 50% looser constraint on µ tan β than that the rigorous stability condition.\n\nMentions:\n- We also show upperbound on µ tan β that satisfies a rigorous stability condition (the electroweakbreaking vacuum is global minimal in the potential) in Fig. 4.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4: Contour plots for upperbound on µ tanβ that satisfies condition that the electroweak-breaking vacuum is global minimum."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To quantify the structural patterns of the invitation cascade, we measure the structural virality of cascades across different size groups. Following the methods of previous works (Goel et al. 2015;Anderson et al. 2015), we use the Wiener index, which gauges the average distance of paths between nodes in a tree, as the demonstration of structural virality and only those cascades with more than 100 nodes are considered. We illustrate the results in Fig. 10. The results show that the structural virality of a cascade positively correlates with its size, indicating that cascade gets deeper and relatively narrower as the size of cascade grows, which differs from Twitter cascades (Goel et al. 2015), while in line with LinkedIn cascades (Anderson et al. 2015). However, we also note that Beidian's invitation cascades demonstrate property deviations from their counterpart on LinkedIn. To be more specific, the average Wiener index of cascades with size more than 100,000 surpasses 15 on LinkedIn, but on Beidian it never exceeds 15. One possible explanation is that Beidian's users are financially motivated, so they are more likely to broadcast the invitation to as many people around them as possible (as illustrated in Fig. 6), which can lower the Wiener index to some extent but does not change its viral nature.\n\nMentions:\n- We illustrate the results in Fig. 10.\n\nOCR Text: \"0 U Cascade Size (Log Base 10)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 10: Structural virality of cascades with specific size."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- From the measurement result, increment of Enum components during call setup will increase PDD value. For traffic up to 30 call/s, the PDD value still comply with IETF standard, that can be seen in Table .3 This is caused by faster process in Enum server; PDD value with Enum only increase 0-10 ms, if compared without Enum. It's can be seen in Figure 9 below. The third scenario can show results of PDD which is still fulfill IETF standard. In this scenario the PDD value uses is higher than other scenarios. This condition is caused by time process in Asterisk server for an invite signal consume more time is 254.5 ms. The comparison of all scenarios for PDD value can be seen in Figure 10 bellow\n\nMentions:\n- It's can be seen in Figure 9 below.\n\nOCR Text: \"|'0zTnI; [6j 0xn6; 130 c05; DSnwnn; LmDO.co6: Dx-4nol LMO.cOD: Tcfk Iallsl\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 9. Comparison of PDD first and second scenario"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The complexes were also characterized by UV-vis spectroscopy (Fig. 3 and Table 1).\n- The optical properties have also been modelled by TD-DFT allowing also to characterize the nature of the electronic excited states. Figure S13 displays the calculated UV-Vis spectrum of C2, which shows an intense band centred at l ~443 nm, in reasonably good agreement with the experimental recordings (see Fig. 3). Note that the small red shift of C2 with respect to C1 observed in the experimental band maxima (see Fig. 3) is also captured by our theoretical protocol, since the convoluted spectrum of the 1:14 fac/mer-C1 mixture has an absorption band peaking at ~415 nm. 23 Interestingly, C2 has additional electronic transitions from the nitrogen atom of the 'bridge' amino group to the p* orbitals of the pyridyl-carbene ligands (n N L). The excitation energies lie in the 350 -450 nm range, although the associated oscillator strengths are small as compared to the MLCT transitions (see Fig. S15). However, certain MLCT/n N L mixing, also enhanced by the symmetry breaking induced by the tertiary amine 'bridge', might be responsible for the absorption intensity loss displayed by C2 in that region, with respect to the more efficient absorption of C1.\n\nMentions:\n- The complexes were also characterized by UV-vis spectroscopy (Fig. 3 and Table 1).\n- Figure S13 displays the calculated UV-Vis spectrum of C2, which shows an intense band centred at l ~443 nm, in reasonably good agreement with the experimental recordings (see Fig. 3).\n- Note that the small red shift of C2 with respect to C1 observed in the experimental band maxima (see Fig. 3) is also captured by our theoretical protocol, since the convoluted spectrum of the 1:14 fac/mer-C1 mixture has an absorption band peaking at ~415 nm. 23 Interestingly, C2 has additional electronic transitions from the nitrogen atom of the 'bridge' amino group to the p* orbitals of the pyridyl-carbene ligands (n N L).\n\nOCR Text: \"Wtoeenmih '0ME\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3. UV-Vis spectra of C0, C1 and C2 in in air-equilibrated CH3CN solution"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The main, conceptual formulas are given in Eqs. 29, 34 and 35, which are the terms beyond the widely discussed lowest order term in Eq. 26. At fourth order in the driving term, we find two contributions, one cut-off independent and one cut-off dependent. The cut-off independent term contributes to the effective Kramers or Hamilton operator, whereas the increasing magnitude of the cut-off dependent term indicates the breakdown of the hierarchy of time scales that was originally assumed. We apply this formalism to a parametrically driven oscillator, coupled to a thermal bath, and to a parametric oscillator chain. We obtain the magnitude of stabilization that can be achieved for these systems, and the onset of the instability. We emphasize that our formalism can be applied to a wide range of driven systems, including non-linear systems and many-body systems. It will be of particular interest to the emerging field of controlling many-body systems via external driving. Figure 6: Distribution in phase space of the driven system in the steady state, for the same parameters as in Fig. 3, but without the low-frequency filtering. For this distribution, an increase of the width in the p-direction is observed, which is due to high-frequency contributions. The additional increase of p 2 (t) is due to high-frequency contributions. For a quantitative comparison to effective, low-frequency descriptions, the low-frequency filtered observable has to be used.\n- As discussed in Sect. II, we have depicted the phase space distribution that is derived from the low-frequency filtered trajectories (x c (t), p c (t)) in Fig. 3. For comparison, we depict the phase space distribution that is derived from original trajectories (x(t), p(t)) that include all frequencies, in Fig. 6. As is clearly visible, for this distribution a broadening of the distribution in the p-direction occurs, in contrast to Fig. 3.\n\nMentions:\n- Figure 6: Distribution in phase space of the driven system in the steady state, for the same parameters as in Fig. 3, but without the low-frequency filtering.\n- For comparison, we depict the phase space distribution that is derived from original trajectories (x(t), p(t)) that include all frequencies, in Fig. 6.\n\nOCR Text: \"{*10- Te\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6: Distribution in phase space of the driven system in the steady state, for the same parameters as in Fig. 3, but without the low-frequency filtering. For this distribution, an increase of the width in the p-direction is observed, which is due to high-frequency contributions."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- With respect to finding (a)mismatch between independent and systemic impactwe repeat the analysis described in Methods, Evaluating Systemic Impact. For every additional similarity measure tested, we generate the respective adjacency matrix and rerun the 'Susceptible-Infected' model for 1,000 independent runs. In general, the number of risks where Systemic Impact is greater, or equal to, their Independent Impact is consistent across all similarity measures, highlighting the robustness of finding (a)see Table 4. Therefore, results related to finding (a) are robust. With respect to finding (b)the existence of a particular modular structurewe repeat the analysis described in Methods, Module Identification. For every additional similarity measure tested, we first generate an ensemble of 1,000 networks. For each ensemble, we identify the most frequent module in which each risk is assigned to, and compare that with its respective module assignment obtained using Cosine Distance. Figure 5 maps the overall match between the cluster assignment obtained using Cosine Distance, and the additional similarity measures. In general, module assignment under Dice, Jaccard, and Lance &amp; Williams similarity measures is almost identical to the one obtained using Cosine Distance. This is not the case for Sorgenfrei, where the match is poor. To identify the cause for this poor match, we performed a simple experiment to assess the sensitivity of each measure with respect to vector similarity. Consider vector A and B, the first being composed of 0s and the latter of 1sat this point the similarity between A and B is 0. At each time step, vector A becomes incrementally similar to vector B by randomly choosing a 0 entry and switching its value to 1. Therefore, vector A becomes increasingly similar to vector B at every time step, until they become identicalat this point the similarity between A and B is 1. By monitoring the increase in similarity using different similarity measures, we can assess the sensitivity of each measure. In this case, superlinear behavior corresponds to heightened sensitivity, whilst sublinear behavior corresponds to reduced sensitivitysee Figure 6. Evidently, measures that have a good match in terms of reporting the same modules -Cosine, Dice, Jaccard; Lance &amp; Williams; Figure 5, bar 1 to 3are ones that grow at least linearly with increased similarity, whilst measures that grow sublinearly -Sorgenfrei; Figure 5, bar 4perform poorly.\n- )see Figure 7a. As expected, the performance of this measure is exceedingly poor when considering the resulting cluster assignment, in relation to the ones obtained using Cosine Distance (Figure 7b). In conclusion, this section tests the dependency of the reported results with respect to the adopted similarity measure. The focus is on: (a) the evident mismatch between independent and systemic risk impact, and (b) the particular modular structure that characterizes the risk network. Both (a) and (b) are robust against the use of similar similarity measures, as shown in Table 4 and Figure 5 respectively.\n\nMentions:\n- Figure 5 maps the overall match between the cluster assignment obtained using Cosine Distance, and the additional similarity measures.\n- Evidently, measures that have a good match in terms of reporting the same modules -Cosine, Dice, Jaccard; Lance &amp; Williams; Figure 5, bar 1 to 3are ones that grow at least linearly with increased similarity, whilst measures that grow sublinearly -Sorgenfrei; Figure 5, bar 4perform poorly.\n- Both (a) and (b) are robust against the use of similar similarity measures, as shown in Table 4 and Figure 5 respectively.\n\nOCR Text: \"Ltt tu ( minc Lakar snltir [ol-\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: Match between module assignment obtained using Cosine Distance and other similarity measures (bar). A value of 1 indicates a complete match."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 15 shows τ and m's impact on the probability of altering strategies ω = (e ) m = 0.01 marks the same efficiency of the two algorithms, where SPBLLA alters one UAV in one iteration on average. When post-disaster scenarios are in the same degree of dynamic, lower m permits higher changing strategies probability. If we want to increase altering probability, we can limit the utility change in each iteration to reduce m. Nevertheless, such a method reduces the updated amount which also causes a negative influence on the learning rate. Then how to balance the altering probability and update amount is a new topic that needs further investigation. Besides, how to reduce m in Theorem 5 is another task that requires further research.\n\nMentions:\n- Fig. 15 shows τ and m's impact on the probability of altering strategies ω = (e ) m = 0.01 marks the same efficiency of the two algorithms, where SPBLLA alters one UAV in one iteration on average.\n\nOCR Text: \"0015 In Fm ImF1M In-oM J-Mrlagz 005 8nFjdi nnFJmS IWhQI\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 15: τ and m’s impact on the probability of altering strategies ω = (e 1 τ )m. These lines are equal probability lines with various τ and m."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Even though, in this case, the conditions of Eq. (26) are not satisfied, this does not lead to superdecoherence per se. It is clear that Γ (vac) L scales superlinearly with L only when one of the modes in D coincides exactly with a mode to which the matrix element is superlinearly susceptible. This is also illustrated by Fig. 3 and Eq. ( 7): there is superdecoherence in the array model when the state is GHZ , and π/a ∈ D. If, in the array model, π/a / ∈ D, but instead π/a + δ ∈ D, with 0 &lt; |δ| 1, there is no superdecoherence. Note this in an asymptotic statement, and that, in the latter situation (π/a / ∈ D, π/a + δ ∈ D), and for finite L, linear scaling of Γ L with L only occurs after 1/L is approximately smaller than |δ|. Further discussion on finite-size effects can be found in Sec. 5.\n- The latter situation, where there is a single peak in the occupation density that overlaps exactly with the peak in the dephasing susceptibility, occurred in the ion-trap experiment by Monz et al., where the model of single-reservoir dephasing is applicable [20]. The state |GHZ was prepared in a semistatic, semi-uniform magnetic field, which was produced with a Helmholtz coil. Fluctuations of the field, caused by current fluctuations in the coil, excited long-wavelength modes (with k ≈ 0). The dephasing susceptibility of the off-diagonal matrix element of |GHZ scales as L 2 at k = 0 (see Fig. 3), and exactly the modes k ≈ 0 where heavily excited in the experiment. If modes were excited away from In units where a = 1, the occupation densityNω is taken to be a Gaussian, with mean ω0 = π, standard deviation σ and integrated occu-pationNtot = 10. We see linear scaling with L is obtained for 1/L σ. In the limit σ → 0, the occupation density becomes unbounded, and consequently, it is only in the limit that the decoherence function scales as L 2 for all L. The plot uses an analytic solution of Eq. (15), with\n- the origin, there would not have been superdecoherence. This follows from the explicit form of the dephasing susceptibility of the off-diagonal matrix element of |GHZ (also see Fig. 3). Furthermore, even if modes near the origin were excited, almost any matrix element other than the off-diagonal element of |GHZ would not have suffered superdecoherence. This statement is shown in more detail in Appendix B.\n\nMentions:\n- This is also illustrated by Fig. 3 and Eq. ( 7): there is superdecoherence in the array model when the state is GHZ , and π/a ∈ D.\n- The dephasing susceptibility of the off-diagonal matrix element of |GHZ scales as L 2 at k = 0 (see Fig. 3), and exactly the modes k ≈ 0 where heavily excited in the experiment.\n- This follows from the explicit form of the dephasing susceptibility of the off-diagonal matrix element of |GHZ (also see Fig. 3).\n\nOCR Text: \"Dophising :usc*p ibil:,\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: The dephasing susceptibility of the off-diagonal matrix element of the state |GHZ′〉, for system sizes L = 2, 4, 6, 8. The peaks have hight L2 and width ∼ 1/L. For the off-diagonal matrix element of the state |GHZ〉, the entire graph is translated in such a way that the peaks lie above k = 0."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In this study, we work on the task of automatically designing a treatment plan from the findings included in the medical certificate written by the dentist. We used 990 certificates written by dentists for our experiments. We develop an artificial intelligence system that is input a document that describes the findings of the patient (Figure 2 (a)) and outputs a list of problems that the patient has in the order of treatment priority (Figure 2 (b)). As findings are free-form descriptions in natural language, the method of feature extraction for machine learning is not obvious. Therefore, we employ a natural language processing approach for feature extraction from text.\n- Step 1 summarizes the findings and lists the orthodontic problems each patient has. This task can be regarded as a text generation task that generates a summary of findings. However, compared to the millions to tens of millions of datasets commonly used in text generation tasks such as machine translation and automatic summarization in natural language processing, our dataset of 990 documents is significantly smaller. Therefore, we add an annotation as shown in Figure 2 (c) and tackle Step 1 as a multi-label classification problem. In this additional annotation, one dentist of the author classified orthodontic problems into 423 classes. Each problem that the patient has corresponds to one class.\n- Step 2 ranks each problem listed in Step 1 in terms of treatment priority. Treatment priorities are indicated in brackets in Figures 2 (b) and (c). Solving the above two subtasks, our system automatically creates a treatment plan from the findings contained in the medical certificate. We develop classification models of orthodontic problems on the datasets shown in (a) and (c) of Figure 2. These models take the text of the findings as input and output a list of class labels in Figure 2 (c). By working as a classification problem rather than a generation problem, it can be expected that even small datasets will be less susceptible to linguistic variation.\n\nMentions:\n- We develop an artificial intelligence system that is input a document that describes the findings of the patient (Figure 2 (a)) and outputs a list of problems that the patient has in the order of treatment priority (Figure 2 (b)).\n- Therefore, we add an annotation as shown in Figure 2 (c) and tackle Step 1 as a multi-label classification problem.\n- Treatment priorities are indicated in brackets in Figures 2 (b) and (c).\n- We develop classification models of orthodontic problems on the datasets shown in (a) and (c) of Figure 2.\n- These models take the text of the findings as input and output a list of class labels in Figure 2 (c).\n\nOCR Text: \"~axa |Fdng: Fzma %ex Fisroxe 3cexitdptenuad;ardhahxas rnh:rmlnxl , Lalarlhzn S1azhc)p: nda Treurert shn hhe\"%sxk ilardeinza Fazixgach:|rdny Fzma %e Sakuat Thenixie | hena d i3d;iadw beng | :, 8irn , [ya' T:_pce ansLzav\"dzna Kilie $ Q,ae.k. heih:, Zm (al Hedicalfindngs VGorgunl: nssi\"g f;euprdut Ted Fr tautezo |s,.3313 4 Udaz Ibi Drthodcrlic proHems kc) Orthoocric probem class\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: Annotation of orthodontic problem classes to the medical certificate."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- One of the major aims of the resonant absorption theory is to find the connection formulae, that tells us how to connect the outer solutions across the resonant point. Here, similar formulae must be found from the inner analysis to find the jump that completes the nonlinear system at large R with the outer problem. However, the singularity now occurs on a curve in y-z plane rather than a point so it is convenient to use 'bodyfitted' coordinates attached to the resonant curve (Slattery 1999). Let us consider a curve y = f (z) and write the length measured along straight lines that are normal to this curve as n, and the arc length measured along the curve as l; see figure 2. Any location in y-z plane can be specified by the position vector\n- In this section, we assume the distance between the two resonant curves is O(1); this case is called type 1 interaction and the schematic of it is shown in figure 2. Using the body-fitted coordinate near the resonant curve y = f ± (z), we can Taylor expand the streak as\n\nMentions:\n- Let us consider a curve y = f (z) and write the length measured along straight lines that are normal to this curve as n, and the arc length measured along the curve as l; see figure 2.\n- In this section, we assume the distance between the two resonant curves is O(1); this case is called type 1 interaction and the schematic of it is shown in figure 2.\n\nOCR Text: \"9 = ffle) 0K_ y = f-(2)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2. Schematic of type 1 interaction studied in section 3.2. The body-fitted coordinate (n, l) is shown for the upper resonant curve y = f+(z). The dissipative layer thickness δ is defined in (3.24). Type 2 interaction occurs when the distance between the two resonant curves becomes O(δ); see section 3.3."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- According to the achieved performance in terms of mistagging efficiency, the present method might allow much higher b-jet efficiencies and thus higher statistics in the b-tagged sample. It is important to keep in mind that the mistagging efficiencies, in particular for the light-flavor jets, must be much smaller than the b-jet efficiencies due to the very small fraction of b-jets of 3-5% in p-Pb collisions [9]. In Fig. 4, the performance for several higher b-jet efficiencies are compared.\n\nMentions:\n- In Fig. 4, the performance for several higher b-jet efficiencies are compared.\n\nOCR Text: \"#LICE Sm.Ayon lcaa 1 FYTHi - HLINS 0 * 502 Tel' E-elen-erc; Fzs.ei &7-F CLaHf 137903 renoc ICev:\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4: c- and udsg-jets (mis-)tagging efficiencies vs. generated jet pT for several efficiencies."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- It can be seen in Fig. 5 that the DBF scheme is the most consuming in terms of energy, despite the fact that fewer directions need to be investigated during each refresh or refinement. Such commonly accepted conclusion that DBF suffers from high energy consumption is the result of implicitly assuming the use, at the user's side, of high resolution and wide band ADCs (usually considered the most power-hungry blocks), that therefore dominate the overall energy budget [17] 6 . On the other hand, DBF allows for faster and potentially more frequent tracking operations. Anyway, E C reduces as T PR increases, since fewer refresh events are triggered within the same time window. As expected, Scheme A always consumes less energy than its counterpart due to the absence of any refinement. Moreover, the gap between the two configurations increases as T PR increases, since more energy-consuming refinements are performed within two consecutive refreshes (keeping the same value of t ref ). Interestingly, such gap is more remarkable when considering a DBF architecture, mak- ing it clear that, when employing a Scheme B tracking process, an analog approach should be preferred for refinements. Finally, we observe how sometimes a Scheme B procedure might still be an attractive option to perform tracking operations even in terms of energy consumption. For instance, from Figs. 3b and 5, we observe that a rate of around 0.8 Gbps can be achieved either through a Scheme A solution (with T PR = 0.15 s), which would consume E C A 1.5 J if an ABF architecture were applied, or through a Scheme B solution (with T PR = 0.3 s), which would consume E C B 2.359 J, making the two options more coherent, in terms of energy consumption, than if a Scheme B approach (with T PR = 0.15 s, as employed by Scheme A) were preferred. We recall that the advantages of the PRaR scheme are more remarkable when more unstable channels are considered.\n\nMentions:\n- It can be seen in Fig. 5 that the DBF scheme is the most consuming in terms of energy, despite the fact that fewer directions need to be investigated during each refresh or refinement.\n\nOCR Text: \"Schete Scheme Scher? ABFIDRF DBF ARF 3351 EcA 058 0,5 Tre Il 47\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 5: Energy consumed for tracking, during a simulation of duration Tsim = 10 s, vs. refresh periodicity TRH, for different beamforming architecture (ABF or DBF). tref = 0.01 s, kref=2. Narrow bars refer to Scheme A (PR)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Our goal is to develop a systematics treatment methodology that maximally removes the systematic effects while minimally removes the true cosmological signal. The two aspects may not be simultaneously accomplished, in a way that depends on the signal, noise, and the correlation between the imaging maps and the true galaxy density. In our paper we choose to prioritize losing minimal cosmological information over maximally removing systematics. One way of ensuring this is to check if the mitigation method returns the true clustering in the presence of contaminations, which will be tested using the contaminated mocks. Another way is to check if the mitigation method correctly makes a null operation on the clustering in the absence of contaminations, returningF 1. To this end, we utilize the 2D projected mocks without introducing any modulation due to imaging attributes in the galaxy density fields. Henceforth, we call this set of simulations, null mocks. Fig. 10 shows the pixel distribution histograms of the number of galaxies per pixel of the mocks in comparison to that of DR7 on the common footprint. The average ngal = 7.0/pixel of the null mocks is smaller than ngal = 13.3/pixel of the DR7 data (even after accounting for the 5% loss due to tiling completeness and 27% loss due to the redshift range, as stated in Raichoor et al. (2017)). We believe that the difference in ngal is due to the different clean photometry criteria Figure 10. Histogram of the number of galaxies per pixel for DR7 (hatched grey), null (solid blue), and contaminated (dashed red) mocks. All distributions are corrected for the pixel completeness f pix . The DR7 distribution is scaled down to account for 5% tiling completeness, and 27% to 0.55 &lt; reliable redshift &lt;1.5.\n- In detail, we pick the 10 imaging attributes, i.e., E BV, nstar, lnHI, seeing − g, sk ymag − g, sk ymag − z, exptime − r, exptime − z, m jd − g, m jd − z of DR7, which were selected from the feature selection procedure on one of the partitions, and modulate the mock density field n with F that is derived from random deviates of the imaging attribute coefficients while accounting for their covariances. Since the measured covariance of such quantities includes both the cosmological fluctuation and the fluctuations due to the imaging attributes, we rescale the measured covariance matrix of the systematics such that the random fluctuation in ngal per pixel due to contamination is at a similar level to the cosmological fluctuation from the null mocks. As a result of the random fluctuation in the contamination model we introduced, some of the pixels will be assigned a negative galaxy number. We drop these pixels from our sample. This removes 3.1% of the mock footprint, reducing our mock footprint size from 89,672 to 86,875 pixels. We then introduce the Poisson process, i.e., another random variation step, to ensure the modulated galaxy number per pixel is an integer. These two random variation processes increase the noise in the mock datasets such that the standard deviation of ngal of the contaminated mocks (= 4.4) is almost the same as that of DR7, despite the different average ngal (see Fig. 10).\n\nMentions:\n- Fig. 10 shows the pixel distribution histograms of the number of galaxies per pixel of the mocks in comparison to that of DR7 on the common footprint.\n- We believe that the difference in ngal is due to the different clean photometry criteria Figure 10.\n- These two random variation processes increase the noise in the mock datasets such that the standard deviation of ngal of the contaminated mocks (= 4.4) is almost the same as that of DR7, despite the different average ngal (see Fig. 10).\n\nOCR Text: \"DRZ Null Comt , \"gal [4/pixel] }\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 10. Histogram of the number of galaxies per pixel for DR7 (hatched grey), null (solid blue), and contaminated (dashed red) mocks. All distributions are corrected for the pixel completeness fpix. The DR7 distribution is scaled down to account for 5% tiling completeness, and 27% to 0.55 < reliable redshift <1.5. The residual difference between the mocks and DR7 shown here might be due to differences between the clean photometry criteria applied to eBOSS target selection and those we apply to DR7."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 12 shows ind /T   calculated for the detailed model [20] and for the one-step model [17]. It is seen that the difference between  \n- For a higher initial pressure the induction time decreases. Since   ind /T   decreases rapidly with increasing pressure (approximating 10 times with increasing pressure from 1atm to 10atm), the spontaneous wave speed increases rapidly (Fig. 12). Therefore, at high pressures the minimal steepness of the gradient necessary for the detonation initiation increases ( cr L decreases). Evolution of the temperature and pressure profiles during the detonation development, calculated for the one-step model, at 0 P 5bar  and 0 P 10bar\n\nMentions:\n- Figure 12 shows ind /T   calculated for the detailed model [20] and for the one-step model [17].\n- Since   ind /T   decreases rapidly with increasing pressure (approximating 10 times with increasing pressure from 1atm to 10atm), the spontaneous wave speed increases rapidly (Fig. 12).\n\nOCR Text: \"Te3ep F-Ibe 1 1JCO 1401 TIK]\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 12:  ind / T  for H2/air at initial pressure 1, 5 and 10 atm calculated for detailed (solid lines) and one-step (dashed lines) models."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Visualization of influence pattern. We visualize the infectivity matrix of ATRPP as in Fig. 6. Each node denotes one dimension which here represents one type of events. The directed edge means the influence strength from source node to destination node. The size of nodes and depth of color is proportional to the weighted degree of nodes, which indicate the total influence of the node has on others. The width of edges is is proportional to the strength of influence. Self-loop edges are located at the right of nodes without an arrow. Note this setting applies to the subsequent two experiments. As is shown, it's obvious that TIKT (maintenance) have a strong influence over all types of errors (breakdown) as maintenance can greatly decrease the probability of breakdown of machines. Also, self-loop edge of TIKT node is too small to see which indicates that maintenance has low correlation itself. All types of errors have self-loop, indicating a recurrent pattern of errors. The breakdown of communication module (COMM) often leads to disfunction of cash dispenser module (CNG), printer (PRT) and internet data center (IDC). Besides, internet data center (IDC) problems influence cash dispenser module (CNG) and printer (PRT) weakly.\n- In line with the previous works for information diffusion tracking [14], [42], [53], the public dataset MemeTracker 4 is 4. http://memetracker.org Fig. 6: Visualization of infectivity matrix for ATM events. Each node denotes different event type. The edge direction indicates the influence direction and its width denotes the effect, the wider the stronger. used in this paper, which contains more than 172 million news articles or blog posts from various online media. The information, such as ideas, products and user behaviors, propagates over sites in a subtle way. For example, when Mark Zuckerberg posted \"I just killed a pig and a goat\", the meme appeared on the theguardian, Fortune, Business Insider one after the other and it became viral. This cascade can be regarded as a realization of an information diffusion over the network. By looking at the observed diffusion history, we want to know through which site and when a particular meme is likely to spread. Besides, we want to uncover the hidden diffusion structure from those meme cascades, which is useful in other social network analysis applications, e.g., marketing by information maximization [54]. From online articles, we collect more than 1 million meme cascades over different websites. For each meme cascade, we have the timestamp when sites mention a specific meme. For the experiments here, we use the top 500 media sites with the largest number of documents and select the meme cascades diffuse over them as done in previous works [14], [42]. As a result, we obtain around 31 million meme cascades, which are randomly split into training (50%), validating (%20) and testing (%30) parts. The event sequences are the meme cascades, which contain the website (dimension) and the timestamp. We count the times that a meme is mentioned during an hour over all the websites and use it as the timeseries to reflect the hotness of the meme and the activity of websites.. The time interval of meme is shown in Fig. 9(a).\n\nMentions:\n- We visualize the infectivity matrix of ATRPP as in Fig. 6.\n- In line with the previous works for information diffusion tracking [14], [42], [53], the public dataset MemeTracker 4 is 4. http://memetracker.org Fig. 6: Visualization of infectivity matrix for ATM events.\n\nOCR Text: \"CNG\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 6: Visualization of infectivity matrix for ATM events. Each node denotes different event type. The edge direction indicates the influence direction and its width denotes the effect, the wider the stronger."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Dr. Robert Braun kindly made available data cubes and opacity-corrected H i column density maps from the Braun et al. (2009) 21 cm survey of M 31, performed using the Westerbork Synthesis Radio Telescope and the Green Bank Telescope. These data are at a spatial resolution of 18 ′′ × 15 ′′ , which corresponds to a linear resolution of 70 × 56 pc at the distance of M 31. The H i spectra have a velocity resolution of 2.3 km s −1 . From these data we used bi-linear inter- polation to extract opacity-corrected column densities and H i spectra for our targets (shown in Figure 14).\n- Fig. 14 shows that the velocities of the interstellar clouds as probed using Na i lines correspond imperfectly with the peak H i velocity in many cases. This may be due to the presence of background H i clouds that do not contribute to the Na i absorption spectra. However, for three of the Field 3 stars, the Na i velocities do not match the H i gas at all, with Na i velocities at least 20 km s −1 redwards of the H i. For 4412.17 and 4417.80, the Na i velocities are similarly displaced from the DIB velocities (see Section 3.5), but the DIBs match very closely with the positions of the H i peak velocities. Given that DIB carriers, Na i and H i are correlated (e.g. Hobbs 1974;Ferlet et al. 1985;Herbig 1993;McCall et al. 2010), and their velocities are generally closely matched in the diffuse ISM, there is likely to be a problem with the measured M 31 Na i cloud velocities in Field 3 (probably for the reasons related to sky-subtraction discussed in Section 3.3). The measured Na i velocities would be offset to the red (as observed) if the MW component is not as strong as was derived in the vapid model fits or if the sky-line subtraction was incomplete. To resolve this situation would require high resolution Na i spectra in order to separate the M 31 and MW components. The H i gas in Field 3 spans a relatively narrow range of radial velocities (from about −120 to −60 km s −1 ), and should be considered as a more reliable indicator of the velocity of the interstellar matter in these sightlines. The resulting velocities from Gaussian fits to the H i emission profiles are shown for comparison with the M 31 DIBs in Field 3 in Figures 5 to 11.\n\nMentions:\n- From these data we used bi-linear inter- polation to extract opacity-corrected column densities and H i spectra for our targets (shown in Figure 14).\n- Fig. 14 shows that the velocities of the interstellar clouds as probed using Na i lines correspond imperfectly with the peak H i velocity in many cases.\n\nOCR Text: \"#x vew y n= 6h M #50 M 41 W+87 *+85 AAHU % ISPhujus |\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 14.— Hi emission spectra derived from the survey by Braun et al. (2009). M 31 Nai velocities are plotted with dotted vertical lines.λλ5780 and 6283 DIB velocities (and 1σ error bars) are plotted with black circles and red triangles, respectively."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Figure 7 we compare the performance of RA and partial CSIT ZF with different receive filters, i.e. the MMSE receive filter that maximizes the SINR of each user by considering the interference from all other users, the global ZF receive filter that tries to minimize interference from all base stations and the local ZF receive filter that considers only interference from the own base station. We observe that both transmit schemes achieve the highest network sum rate with the SINR optimal receive filter. The performance degradation with the local and global ZF receive filter are similar for both transmit schemes.\n\nMentions:\n- In Figure 7 we compare the performance of RA and partial CSIT ZF with different receive filters, i.e. the MMSE receive filter that maximizes the SINR of each user by considering the interference from all other users, the global ZF receive filter that tries to minimize interference from all base stations and the local ZF receive filter that considers only interference from the own base station.\n\nOCR Text: \"Iis*Jit\" Heet Jubtr Wuabrk Ditt TKT Curditnn Koc iem -R6E\"1'73r17tsd 30 6\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 7. System level simulation: CDF of spectral efficiency for nr = 2; Comparing ZF and RA with partial CSIT and different receivefilters."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Moreover, using the synthetic data, the theoretical worst-case resilience bound (Equation ( 12)) of the majority 0-1 loss linear classi cation is experimentally shown in Figure 4. e blue line represents the theoretical worst-case resilience bound. Red points are the resilience V (•) over the corresponding α. Speci cally, 100 di erentD are randomly generated, whereD + andD − are drawn from two Gaussian distributions of positive and negative labels, respectively. For eachD and for each α + , which ranges from 0 to the total number of positive feature vectors, an a acker moves α + number of positive feature vectors beyond the negative features in 100 di erent ways to obtainD α so that positive and negative feature vectors cannot be linearly separable. By taking the maximum of V (•) for 100 di erentD and 100 di erentD α , the resilience V (•) is obtained for each α + , which is represented in a red cross. In Figure 4, the red crosses do not excess the theoretical bound and the increasing trend follows the bound.\n\nMentions:\n- Moreover, using the synthetic data, the theoretical worst-case resilience bound (Equation ( 12)) of the majority 0-1 loss linear classi cation is experimentally shown in Figure 4. e blue line represents the theoretical worst-case resilience bound.\n- In Figure 4, the red crosses do not excess the theoretical bound and the increasing trend follows the bound.\n\nOCR Text: \"Empirical Mpurec 00 0.1 0.2 03 0.4 0.5 06 0.7 0.8 0.9 _0\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4: e degree of the resilience of PM, `01 in the resilience metricV (·) with respect to the ability of an attacker. e blue solid and dashed line represents the theoretical robustness bound (Equation (12)) and the red cross means the empirically evaluated feasible resilience. Assume |D̂+ | = |D̂− | and α+ = α−."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Fig. 3, the TM-mode bounds scale quadratically with feature size d (black dashed line), whereas tip-based designs only exhibit linear scaling (pink dashed line). We can extend this analysis to 3D structures to compare state-of-the-art designs [6,8] to our bounds. Solving Eq. (4) can require significant computation times (7 hours on a 20-core machine even for a single 2D TM bound), which for 3D bounds will require new software implementations that are out of the scope of this paper (CVX is not optimized for large-scale problems). Yet we can apply a \"2.5D\" [66] analysis to make predictions. In prototypical on-chip implementations, waveguide modes occupy approximately λ/2n mode thickness in the third dimension, with all subwavelength confinement arising from the two in-plane directions. Extrapolating the linear scaling of Fig. 3 to the feature sizes of Refs. [6,8], with λ/2n confinement in the third dimension, we predict mode volumes within a factor of 2 of their simulated values, cf. Table I. According to the same table, our bounds suggest the possibility for two orders of magnitude improvement. A unique feature of this computational bound approach is its capability to identify bounds for multiresonant structures that can be particularly important for enhancing nonlinear-optic effects [25][26][27][28][29][30]. The condition of requiring resonances at a set of multiple frequencies, {ω 1 , ω 2 , . . .}, simply represents additional biconvex constraints in Eq. (2), resulting in a larger version of the dual problem of Eq. (4). As an example, we consider nonlinear-optical processes involving three frequencies ω 1 , ω 2 , and ω 3 that satisfy the condition 2ω 1 = ω 2 + ω 3 , and in Fig. 4 we vary the value of ω 2 relative to ω 1 . The constant dashed line in Fig. 4 is the single-frequency bound for a resonator supporting a resonance at ω 1 . The black and blue lines represent the computed bounds on average mode volume for dual-frequency cavities (cf. SM for details), at frequencies ω 1 and ω 2 (blue) or ω 1 and ω 3 (black), which represent cavities that could be used for enhancement of second-order χ (2) nonlinear processes. The bounds for these dual-resonant cavities are larger than those of the single-frequency cavity, representing   3) nonlinear response. For ω 2 = 0.5ω 1 , the averagemode-volume bound is about 20% larger than the singlefrequency bound, illustrating the penalty associated with multiresonant structures, and the unique ability of this approach to identify such penalties.\n\nMentions:\n- As an example, we consider nonlinear-optical processes involving three frequencies ω 1 , ω 2 , and ω 3 that satisfy the condition 2ω 1 = ω 2 + ω 3 , and in Fig. 4 we vary the value of ω 2 relative to ω 1 .\n- The constant dashed line in Fig. 4 is the single-frequency bound for a resonator supporting a resonance at ω 1 .\n\nOCR Text: \"TE mode i 1 01\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 4: Lower bounds for structures that support multiple resonances, e.g., for high-efficiency nonlinear optics, at frequencies ω1, ω2, and/or ω3, with 2ω1 = ω2 + ω3. Relative to the single-resonance bounds (black dashed line), the dual-resonance (blue, black) and triple-resonance (red) structures have larger bounds, exhibiting the penalty associated with requiring multiple resonances. Inset: Structures extracted from the dual-program optimization, labeled by the value of ω2 for which they are computed."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Clustering is a fundamental network property that has been shown to be relevant to a variety of topics. For example, consider the propagation of information through a social network, such as the spread of a rumor. A growing body of work has identified the importance of clustering to such propagation; the more clustered a network is, the easier it is for information to propagate (Barclay et al., 2013;Centola, 2010Centola, , 2011Lü et al., 2011;Malik and Mucha, 2013). In addition, in Fig. 1, we show experimentally a strong relationship between the final spread of information and the level of clustering in the network, with higher clustering corresponding to higher levels of expected spread. The importance of clustering is not limited to social networks; in the context of air transportation networks, Ponton et al. (2013) argued that higher clustering of such a network is beneficial, as passengers for a cancelled flight can be rerouted more easily. In this work, we use average clustering coefficient (ALCC) as our definition and measure of clustering in a network. ALCC was proposed for this purpose by Watts and Strogatz (1998).\n- To test the relationship between influence propagation and clustering empirically, we used a variety of Watts-Strogatz graphs (Watts and Strogatz, 1998); a graph generated by this model starts as a ring lattice, defined as follows. First, n circular rings are constructed: for each j ∈ {1, . . . , n}, vertices u j 1 , . . . , u j n and edges (u j i , u j i+1 ), i = 1, . . . , n − 1, and (u j n , u j 1 ). Next, add edges (u j i , u j+1 i ), for j = 1, . . . , n − 1, and (u n i , u 1 i ), for each i. Finally, all vertices within k hops of each other are connected by an edge. For these experiments, we used n = 100 and k = 3. With probability p, each edge in the graph is rewired; that is, replaced with an edge between two uniformly randomly chosen vertices. By varying p, one can control the level of clustering in the network, as shown in Fig. 1. Each graph generated in this manner has the same number of edges.\n- The expected activation was computed using a single seed node and an IT or LT realization; this computation was averaged over 1000 trials. When we normalize by the initial value, Fig. 1 shows a remarkable similarity between the normalized ALCC value and the normalized activations, for both the IC and LT models. Therefore, these results provide evidence supporting a positive correlation between ALCC and the expected activation of both the IC and LT models of information propagation.\n\nMentions:\n- In addition, in Fig. 1, we show experimentally a strong relationship between the final spread of information and the level of clustering in the network, with higher clustering corresponding to higher levels of expected spread.\n- By varying p, one can control the level of clustering in the network, as shown in Fig. 1.\n- When we normalize by the initial value, Fig. 1 shows a remarkable similarity between the normalized ALCC value and the normalized activations, for both the IC and LT models.\n\nOCR Text: \"\" 8 06 1 01 02 05 04 05 0.6 0.7 0.8 0.9\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1. Relationship between the value of ALCC and the expected number of activations under the LT and IC models, normalized by initial value. For more details and discussion ofp, see Section VII."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- This has the effect of fitting each spectrum S t (L) to the linear combination c t (L) × S global + (1 − c t (L)) × S indept-LD of MNMs and linked independent mutations. The sum over i starts at 2 to exclude singletons because they cannot be phased. Assuming that S global is a good estimate of the SFS of MNMs, we take c t (L) to be an estimate of the fraction of MNMs among linked SNPs of type t at distance L. In this way, we obtain estimates similar to the m t (L) estimates that we obtained earlier by measuring the excess of same-lineage of derived alleles (Figure 7). We find that c t (L) is larger than m t (L) for L &lt; 3 and L &gt; 50, but smaller than m t (L) at intermediate distances. The discrepancy might stem from simple noise in the data, but might also mean that processes like context-dependent mutation and strand bias create excess same-lineage pairs that do not lie in perfect LD.\n- A topic worth further investigation is the possibility of local variation in the rate of MNMs. If most MNMs are caused by error-prone polymerase activity, it is likely that high error-prone polymerase traffic should elevate rates of MNMs and simple point mutations in the same genomic regions. Both MNMs and point mutations in these regions might be subject to elevated transversion rates, and it will be important to separate the two classes of mutations to accurately study local variation of the transition/transversion ratio as in Seplyarskiy et al. (2012). Seplyarskiy, et al. reported that the transition/transversion ratio κ appears depressed in the neighborhood of all human SNPs, even transitions, but we found that the apparent depression of κ in the neighborhood of transitions disappears when SNPs in perfect LD are excluded from the analysis (Supplementary Figure S7).\n- Figure 6. Site frequency spectra of perfect LD mutations. Each of the first five panels contains site frequency spectra of transitions, mixed pairs, and transversions found in perfect LD in the 1000 Genomes data. Singletons are excluded because they cannot be phased and therefore perfect LD status cannot be determined. For comparison, each panel contains the population-wide SFS of unlinked SNPs as well as the inferred SFS of linked independent mutations. SNP pairs are binned according to the distance between them, showing that close-together SNPs and transversions have spectra closer to the population SFS, while far-apart SNPs and transitions appear more weighted toward linked independent mutations. Dotted lines show the frequency spectra predicted by Equation (1) for each length and pair type category, assuming that the green dotted line depicts the correct SFS of linked independent mutations and that Figure 7 shows the correct MNM percentages in each category. For comparison, panel F shows a population SFS and perfect LD frequency spectra obtained from data simulated under a human demographic model. In the simulated data, there is no difference between the frequency spectra of linked independent mutations that lie 1 bp apart versus 100 bp apart. Figure 10. Average r 2 LD correlations between 1000 Genomes SNP pairs. The correlation coefficient r 2 between allele frequencies at neighboring sites is often used to measure the decay rate of genealogical correlation with genomic distance. However, we have seen that multinucleotide mutation creates excess LD compared to the expectation under independent mutation. We computed the average r 2 across all SNP pairs L bp apart on chromosome 22, then corrected this value for the presence of MNM. r 2\n\nMentions:\n- In this way, we obtain estimates similar to the m t (L) estimates that we obtained earlier by measuring the excess of same-lineage of derived alleles (Figure 7).\n- Seplyarskiy, et al. reported that the transition/transversion ratio κ appears depressed in the neighborhood of all human SNPs, even transitions, but we found that the apparent depression of κ in the neighborhood of transitions disappears when SNPs in perfect LD are excluded from the analysis (Supplementary Figure S7).\n- Dotted lines show the frequency spectra predicted by Equation (1) for each length and pair type category, assuming that the green dotted line depicts the correct SFS of linked independent mutations and that Figure 7 shows the correct MNM percentages in each category.\n- Supplementary Figure S7.\n\nOCR Text: \"\"Jtj \"_' D stance Lrteeen SNPS\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 7. Two estimates of MNM prevalence. Here, the solid lines plot ct(L), our SFS-based estimate of the fraction of perfect LD mutations caused by MNM. For comparison, dotted lines plot the estimate mt(L) that is based on the excess of same-lineage derived alleles over different-lineage derived alleles in subsampled haplotype pairs."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In eq. (4) with m V is marked the minimum electron velocity at the moment m t t = . The time dependences of the coordinates of the charged particles (see eq. ( 6)) for some values of m V are shown in Fig. 2, while the graphic dependence of velocity on the charged particles coordinates for the same values of m V is given in Fig. 3. As it is evident from the curves the time dependences are monotonous, and there are no interruptions on the curves for the different values of m V , whereas in Fig. 3 a tangle of interrupted curves is observed. These curves illustrate the presence of non-monotonous and non-synonymous dependences.\n\nMentions:\n- The time dependences of the coordinates of the charged particles (see eq. ( 6)) for some values of m V are shown in Fig. 2, while the graphic dependence of velocity on the charged particles coordinates for the same values of m V is given in Fig. 3.\n- As it is evident from the curves the time dependences are monotonous, and there are no interruptions on the curves for the different values of m V , whereas in Fig. 3 a tangle of interrupted curves is observed.\n\nOCR Text: \"ALMCE tmtcM LJcX An ImWFM L\"NLI\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3. Dependence of particles velocity on the coordinate х at 0b = 8, and different mV (in non-dimensional units)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The key to better understand the complex evolutionary scheme for peculiar stars is to evaluate the observed properties of stars in conjunction with the current theoretical expectations. Populating the HR diagram is essential to refine the regions and limits of instability and update the values of mass-loss rates at critical phases e.g. a cool supergiant phase, which define the posterior evolution of the stars. The incidence of outliers could potentially highlight the role of rotation and binarity. We proceeded to investigate the nature of our five stars over the HR diagram of Fig. 6. Overplotted are shown the Geneva evolutionary tracks (Ekström et al. 2012) for initial masses 20 − 60 M ⊙ at solar metallicity and assuming rotation at the 40 % of the critical velocity. Each grid/age point of a track is color coded with respect to the expected 12 C/ 13 C ratio.\n- We indicate the location of the reported LBVs, B[e]SGs, and YHGs/WHGs in the Local Group (de Jager 1998; Smith et al. 2004;Kraus 2009;Humphreys et al. 2017b, and references therein). For several LBVs and YHGs, a dotted horizontal line connects the hot quiescent with the cool eruptive state of the star. In principle, luminosities are thought to preserve during the instability phases of the star, although Campagnolo et al. (2018) recently reported an increase in the bolometric luminosity of the LBV R40 during the eruption of 2016. The majority of LBVs in quiescence are well confined within the S-Dor instability strip. In their outbursts, counterparts with luminosities above log L/L ⊙ ∼ 5.8 are shifted over or even beyond the Humphreys-Davidson limit. The less luminous LBVs are post-main sequence stars that during their eruption may be found within the so called \"Yellow Void\". The latter locus is theoretically pre-dicted to host stars with negative density gradient in part of their extended atmospheres (Nieuwenhuijzen &amp; de Jager 1995;de Jager et al. 2001). YHGs are believed to bounce against the Yellow Void, exhibiting outbursts that shift them redwards due to the formation of optically thick winds or pseudo-photospheres (de Jager &amp; Nieuwenhuijzen 1997;Oudmaijer et al. 2009). The luminosity threshold of log L/L ⊙ ∼ 5.4 for a YHG appears to well agree with the observed low-luminosity LBVs, which renders the latter as potential descendants of YHGs. Few WHGs in M31 are found below the luminosity limit (Fig. 6), although, and as already stressed in Sec. 2.2, none of these have shown eruptive behavior. Similarly, no outbursts have been reported for the four YHGs below log L/L ⊙ ∼ 5.4, three of which are in the LMC and 6 Cas in the Galaxy. On the blue region of the HR diagram, B[e]SGs occupy a wide locus with luminosities up to log L/L ⊙ ∼ 5.7, with three stars exceeding further and up to 6.3. Similar to the low-luminosity LBVs, their location on the evolutionary diagram supports their evolved nature, with the exception of a couple of hot stars at log L/L ⊙ ∼ 5.3 (Fig. 6). Nevertheless, the latter could trace the post-RSG path of stars with M ini = 25 M ⊙ .\n- • Of our less luminous objects, J013333.22+303343.4, stands out due to its significant infrared excess requiring two different dust components; a hot dust envelope of 1 300 K located close to the star, and a cool dust volume of 500 K extending further. The star shows an evident R−band excess due to a broad Hα emission. Free-free emission from winds provides fit to the I−band measurement, which deviates from the photospheric component. Integrating the SED model yields log L/L ⊙ = 5.5, placing the star at the end of its core-hydrogen burning phase or at the hot phase of a blueward evolutionary loop (Fig. 6). The spectroscopic study of Humphreys et al. (2017a)   Humphreys et al. (2014).\n\nMentions:\n- We proceeded to investigate the nature of our five stars over the HR diagram of Fig. 6.\n- Few WHGs in M31 are found below the luminosity limit (Fig. 6), although, and as already stressed in Sec. 2.2, none of these have shown eruptive behavior.\n- Similar to the low-luminosity LBVs, their location on the evolutionary diagram supports their evolved nature, with the exception of a couple of hot stars at log L/L ⊙ ∼ 5.3 (Fig. 6).\n- Integrating the SED model yields log L/L ⊙ = 5.5, placing the star at the end of its core-hydrogen burning phase or at the hot phase of a blueward evolutionary loop (Fig. 6).\n- Comparing to evolutionary models in Fig. 6, we infer that the hidden secondary reaches a maximum M ini = 32 M ⊙ .\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6. Hertzsprung-Russell evolutionary diagram for the LBVs (circles), B[e]SGs (squares), and YHGs/WHGs (’x’ symbols) in the Local Group galaxies from the studies of de Jager (1998), Smith et al. (2004), Kraus (2009), and Humphreys et al. (2017b). A dotted horizontal line indicates the transition phases for several LBVs and YHGs. We mark with grey line the regions corresponding to the S Dor instability strip and the cooler Yellow Void. The Humphreys-Davidson limit (Humphreys & Davidson 1994) is shown with black thick line. Evolutionary models at solar metallicity from Ekström et al. (2012) are overplotted, assuming rotation at the 40% of the critical velocity. The grid points of the stellar tracks are color-coded with respect to the 12C/13C ratio. The location and 1-σ uncertainties of our discussed stars are displayed with red."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The indexed partitions over Θ r which are at most 2 N subsets 2 are refereed to as the decision region. In other words, the very same partitioning π(Θ r ) over Θ r can be indexed differently with subsets of N and so it can lead to different sets of decision regions. We denote by Π (Θ r , N ) the set of all possible indexed partitions on Θ r . Hence, if θ r ∈ D V we have a cooperative unicast network where the relays in V use CF scheme while the others relays use DF scheme, as shown in Fig. 7. Each relay has two codebooks based on x DF , so relay k decodes the source message and transmits it to the destination with the index 2 It is due to the fact that each partition subset is indexed by V.\n- Fig. 7. Selective coding strategy (SCS) over cooperative unicast networks.\n\nMentions:\n- Hence, if θ r ∈ D V we have a cooperative unicast network where the relays in V use CF scheme while the others relays use DF scheme, as shown in Fig. 7.\n- Fig. 7.\n\nOCR Text: \"4/? o eo' DF relays CF relays XO= ~O Y (0,eDDF ' (0,Edar) J(0,CD,\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 7. Selective coding strategy (SCS) over cooperative unicast networks."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- 2) Objective evaluation: Fig. 5 shows the mean-squared error (MSE) calculated during the training, comparing the CRBM with CAdam (\"CRBM+CAdam\") to the counterparts. As shown in Fig. 5, the CRBMs converged more quickly than the counterparts of RBMs, and the CAdam algorithm was considerably effective for the CSA. Table II illustrates the  showing that the proposed method outperformed the rest regardless of the number of hidden units. While the performance of the CRBMs with H = 1, 000 was comparable to that of the RBMs with H = 1, 000, the CRBMs with more hidden units highly improved the performance compared to the RBMs. This is because each hidden unit in the CRBMs represents complexvalued patterns more independently than that in RBMs; i.e., CRBMs have a higher ability of representing complex-valued data than RBMs. Table III summarizes the performance of each method under their best conditions. All methods based on CRBM and RBM were trained using the CAdam and Adam algorithms. Interestingly, the CRBM with frame-wise modeling (\"CRBM\") outperformed the RBM with trajectory modeling (\"RBM+T\") because the CRBM implicitly represents the phase information of complex-valued data, and the framewise features from the CRBM recovered speech sufficiently. Furthermore, the proposed trajectory modeling (\"CRBM+T\") improved accuracy by extracting the correlations of complexvalued features between adjacent frames and performed the best out of all the training-based methods. The performance of the proposed method is even comparable to that of the WORLD, which is one of the most high-quality synthesis methods. Unlike traditional speech coding (mel-cepstrum, cepstum, and WORLD), the CRBMs directly encode arbitrary complex-valued features into binary-valued features, which indicates that the CRBMs have a high compression ability and high versatility to complex-valued data and can be applied to speech and to other signals.\n\nMentions:\n- 2) Objective evaluation: Fig. 5 shows the mean-squared error (MSE) calculated during the training, comparing the CRBM with CAdam (\"CRBM+CAdam\") to the counterparts.\n- As shown in Fig. 5, the CRBMs converged more quickly than the counterparts of RBMs, and the CAdam algorithm was considerably effective for the CSA.\n\nOCR Text: \"CRDM Cilni CRBM CSi RBH Adaj ; iQo 120 |-0 IM |80 Z0 Epuch\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 5: MSE curve during the training of the CRBMs and the RBMs."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To illustrate the imprint of the systematic uncertainty of ρd in the TOF spectrum, Fig. 7 shows the difference between a TOF spectrum with shifted column density, Φ(ρd) = dN/dτ(ρd + ∆ ρd) and a TOF spectrum with mean column density, Φ 0 = dN/dτ( ρd ), weighted by √ Φ 0 which is proportional to the expected Poissonian uncertainty of the data. By doing so, the signature becomes visible proportionally to its impact in the χ 2 function. It can be seen that the imprint of a shifted column density is present foremost at lower flight times, which is since the energy loss causes the count-rate near the endpoint to drop. There are fluctuations at higher flight times near the retarding potential arising from the energy loss spectrum (9). However, these are weighted minimally since the differential rate in the TOF spectrum drops with higher flight times (see Fig. 5).\n- Fig. 8 shows the sensitivity for an ideal TOF mode. The results are based on three years measurement time which was distributed uniformly on the retarding potential within an interval of [4; 18.5] keV with steps of 0.5 keV. The setting was chosen in that way that a 7 keV neutrino signal [25] would roughly lie in the center of the potential distribution. For the Fig. 7 Difference between TOF spectra with shifted ρd, Φ(ρd) and default value ρd = 5 × 10 14 cm −2 , Φ 0 , weighted proportionally with the expected Poissonian uncertainty of the data ∝ √ Φ 0 . The imprint of a shifted column density is present foremost at lower flight times, due to missing events near the endpoint because of the energy loss. Fluctuations at higher flight times near the retarding potential are suppressed by a lower differential count rate. The spectra consist only of the active neutrino component, sin 2 θ = 0, and the retarding potential is qU = 18 kV. exemplary inelastic scattering systematics an initial uncertainty of ∆ ρd/ρd = 0.002 has been assumed in accordance with Ref. [30]. The statistical sensitivity of the integral mode in this simulation is in good agreement with Ref. [31]. The statistical sensitivity of the ideal TOF mode is close to that one of an ideal differential detector in the aforementioned publication. However, if the uncertainties of the column density are incorporated, the benefit by the TOF mode grows even further, since a shifted column density has a unique imprint on the TOF spectrum (see Fig. 7), which is not the case in the integral mode.\n\nMentions:\n- To illustrate the imprint of the systematic uncertainty of ρd in the TOF spectrum, Fig. 7 shows the difference between a TOF spectrum with shifted column density, Φ(ρd) = dN/dτ(ρd + ∆ ρd) and a TOF spectrum with mean column density, Φ 0 = dN/dτ( ρd ), weighted by √ Φ 0 which is proportional to the expected Poissonian uncertainty of the data.\n- For the Fig. 7 Difference between TOF spectra with shifted ρd, Φ(ρd) and default value ρd = 5 × 10 14 cm −2 , Φ 0 , weighted proportionally with the expected Poissonian uncertainty of the data ∝ √ Φ 0 .\n- However, if the uncertainties of the column density are incorporated, the benefit by the TOF mode grows even further, since a shifted column density has a unique imprint on the TOF spectrum (see Fig. 7), which is not the case in the integral mode.\n\nOCR Text: \"~WM = U M2 - ~WX w -;0 Julcn @D | Lol cib IUU \"u MUMM JU J\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 7 Difference between TOF spectra with shifted ρd, Φ(ρd) and default value 〈ρd〉= 5×1014 cm−2, Φ0, weighted proportionally with the expected Poissonian uncertainty of the data ∝ √ Φ0. The imprint of a shifted column density is present foremost at lower flight times, due to missing events near the endpoint because of the energy loss. Fluctuations at higher flight times near the retarding potential are suppressed by a lower differential count rate. The spectra consist only of the active neutrino component, sin2 θ = 0, and the retarding potential is qU = 18kV."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The NNLO loop contributions can be separated into two groups: those that survive in the infinite heavy-meson mass (m H → ∞) limit (Fig. 2) and those that vanish in the m H → ∞ limit (Fig. 3). For the first group, our results recover those of Ref. [22] in the m H → ∞ limit. For the second group, our calculation shows that they indeed vanish in the m H → ∞ limit but are not negligible in a covariant calculation, as shown below. It should be noted that all the loop contributions can be calculated analytically except the box-diagrams [Fig. (3l) and its crossed counterpart]. However, the analytical results are quite involved and therefore we refrain from showing them explicitly. As explained earlier, in the present work we are going to neglect all the NNLO counter terms. Accordingly we have removed from our loop results all the NNLO analytical terms. Therefore our loop results contain only NNLO non-analytical and higher-order terms. This way the differences between our covariant loop results and those of HMχPT are strictly recoil corrections.\n\nMentions:\n- The NNLO loop contributions can be separated into two groups: those that survive in the infinite heavy-meson mass (m H → ∞) limit (Fig. 2) and those that vanish in the m H → ∞ limit (Fig. 3).\n\nOCR Text: \"hxuhhhhma\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 2. NNLO loop contributions that survive in the infinite havy-meson mass (mH → ∞) limit."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In general, cations are solvated in helium droplets because they exhibit very attractive interaction with helium. Based on this observation, experimentalists have studied the sinking of positive ions in 4 He droplets resulting from the ionisation of heliophobic alkali and alkaline earth metal atoms. 194,202,205 This activity has motivated the corresponding TDDFT simulations of the ion sinking process. 32,77,86 Figure 23 shows the DFT time-resolved absorption spectrum of a Ba + @ 4 He 1000 6p← 6s transition after photoionisation of the surface-bound atom. The experimental data 194\n\nMentions:\n- Based on this observation, experimentalists have studied the sinking of positive ions in 4 He droplets resulting from the ionisation of heliophobic alkali and alkaline earth metal atoms. 194,202,205 This activity has motivated the corresponding TDDFT simulations of the ion sinking process. 32,77,86 Figure 23 shows the DFT time-resolved absorption spectrum of a Ba + @ 4 He 1000 6p← 6s transition after photoionisation of the surface-bound atom.\n\nOCR Text: \"IS 4S 4()t 4op mal Z2u Z YM cmn 1\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 23. TDDFT time-resolved absorption spectrum of Ba+ in a 4He1000 droplet. 77 The experimental spectrum corresponding to helium droplets with an average size of 2700 atoms is shown in red.194 The vertical lines indicate the D1 and D2 transitions of free Ba+."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Since the LLs are positive numbers, as SC decoding moves towards stage 0, their dynamic range increases. When an LL pair saturates, it is useless for making a decision. Thus, when using LLs, it is crucial to avoid saturation. In (5) and (6), two numbers with the same dynamic range are added. The simplest way to avoid all saturations is to increase the number of bits used to store the LLs by one bit per stage. This way, the only performance degradation with respect to the floating point implementation comes from the quantization of the channel LLs. Let Q ch denote the number of bits used for the quantization of the channel LLs. The performance of the list SC decoder under various quantization bit-widths using a uniform quantizer with quantization step ∆ = 1 is presented in Fig. 6. For the remainder of this paper, we choose Q ch = 3, since the degradation with respect to the floating point and to the Q ch = 4 implementations is very small.\n\nMentions:\n- The performance of the list SC decoder under various quantization bit-widths using a uniform quantizer with quantization step ∆ = 1 is presented in Fig. 6.\n\nOCR Text: \"LACemlmeeenal A LL C LISC IQ Ph\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 6: FER performance of a (1024, 512) polar code under floating-point and fixed-point list SC decoding for L = 2."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- e projection of the pair background density along x (Figure 3) shows that the number of pairs in set (A) is increased by a factor of 2-3 in comparison to the TDR scheme, and by a factor of 6-7 in sets (B) and (C). Nevertheless, the envelopes for all parameter schemes are well contained within the beam pipe. At a beam pipe radius of 12 mm, one can expect less than 10 interactions of pairs with the material of the beam pipe per bunch crossing.\n- 4 SiD vertex detector occupancies e occupancy of the SiD vertex detector is calculated by counting the hits from the pair background per pixel for a full bunch train (1312 bunch crossings). e sensor bu ers are read out a er every train, and therefore have to be suitable for the expected detector occupancy. For the study of the occupancy, the pair Figure 3: Projection of the pair background density for a full bunch train (1312 bunch crossings), which is shown in Figure 2, along the x direction. e projection is done for z = 62 mm, the z position of the rst beam pipe kink. At that position, the background envelopes are the largest compared to the beam pipe radius. e beam pipe is indicated by the pink solid lines. background events were taken from GuineaPig, as described in Section 3, and used as input for a full Geant4 [10,11] detector simulation with SLIC [12]. e crossing angle was applied in this simulation step. Regarding the SiD detector geometry, a pixel size of 20 µm × 20 µm was assumed for the vertex detector. e SiD vertex detector is a high-resolution pixel detector with the following requirements:\n\nMentions:\n- e projection of the pair background density along x (Figure 3) shows that the number of pairs in set (A) is increased by a factor of 2-3 in comparison to the TDR scheme, and by a factor of 6-7 in sets (B) and (C).\n- For the study of the occupancy, the pair Figure 3: Projection of the pair background density for a full bunch train (1312 bunch crossings), which is shown in Figure 2, along the x direction. e projection is done for z = 62 mm, the z position of the rst beam pipe kink.\n\nOCR Text: \"Tb DE AEMGJEo HA : n (EAMFa , 1 Srm L\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Projection of the pair background density for a full bunch train (1312 bunch crossings), which is shown in Figure 2, along the x direction. e projection is done for z = 62mm, the z position of the rst beam pipe kink. At that position, the background envelopes are the largest compared to the beam pipe radius. e beam pipe is indicated by the pink solid lines."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where X 0 is the radiation length which is 630.4 m 2 kg for hydrogen. Besides those such as the integration tolerance used in the calculation of f (ϑ), there are two unphysical parameters in the scattering algorithm: the cross sectional radius r σ and the number of steps. First the modified code was run with 1000 particles at 10 GeV tracked in 10000 steps through a 100 m plasma of density 10 20 cm −3 with the cross sectional radius scanned from 10 −10 m to 10 −7 m. The simulation results are plotted against theory in Fig. 10. From these results we can see that the scattering algorithm is more or less in agreement with the theoretical estimate in Eq. (C1) for r σ ≈ 10nm. Next the same simulation was run except this time the cross sectional radius was fixed at 10nm and the number of steps was scanned from 10 to 1,000,000. The simulation results are plotted against theory in Fig. 11. From this plot we can see that the growth in σ θ was only slightly affected by changing the number of steps by four orders of magnitude. The number of steps in the simulations performed for the FACET-II and LC cases are approximately 3,000,000. From the results shown in Fig. 11, we can say the scattering algorithm is a mild underestimate of the scattering predicted by Eq. (C1). However not too much faith should be vested in Eq. (C1) as it is a zeroth order approximation.\n\nMentions:\n- The simulation results are plotted against theory in Fig. 11.\n- From the results shown in Fig. 11, we can say the scattering algorithm is a mild underestimate of the scattering predicted by Eq.\n\nOCR Text: \"1 Zmi\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 11: Angular spread of electrons tracked through hydrogen gas for differing number of steps: 103 (blue), 104 (orange), 105 (green), and 106 (red). These simulation results are compared to the theoretical value of σφ from Eq. (C1) shown in black."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Shadowing can be incorporated into the model by simply drawing an appropriate set of independent shadowing factors {ξ i } for each network realization and using them to compute the {Ω i } according to (7). In Fig. 5, shadowing was applied to the same set of N = 10, 000 networks used to generate Fig. 4. Two standard deviations were considered for the lognormal shadowing: σ s = 2 dB and σ s = 8 dB, and again α = 3.5. For each shadowed network realization, the outage probability was computed for the mixed-fading model without spreading (G = 1). All other parameter values are the same ones used to produce Fig. 4. The figure shows P [ǫ ≤ ǫ T ], and the no-shadowing case is again shown for reference. As can be seen from the figure, the presence of shadowing and increases in σ s increase the variability of the conditional outage probability, as indicated by the reduced slope of the cdf curves. Interestingly, shadowing does not significantly alter the average outage probability in this example. For low thresholds, such as ǫ T &lt; 0.1, performance is actually better with shadowing than without. This behavior is presumably due to the fact that, in shadowing, the reference signal power will sometimes be much higher than without shadowing. In a finite network, the average outage probability depends on the location of the reference receiver. Rather than leaving the reference receiver at the origin, Table II explores the change in performance when the reference receiver moves from the center of the radius-r net circular network to the perimeter of the network. When the reference receiver is at the perimeter of the network, the circular network region is centered at coordinate −r net . The SNR was set to Γ = 10 dB, a mixed-fading channel model was assumed, and other parameter values are the same ones used to produce Fig. 5; i.e., r ex = 0.05, r net = 1, β = 0 dB, and p i = 0.5. The interferers were placed according to the uniform clustering model and the reference transmitter was placed at distance ||X 0 || = 0.1 from the reference receiver. For each set of values of the parameters G, α, σ s , and M , the outage probability at the network centerǭ c and at the network perimeterǭ p were computed by averaging over N = 10, 000 realizations of mobile placement and shadowing. Two values of each parameter were considered: G = {1, 32}, α = {3, 4}, σ s = {0, 8}, M = {30, 60}. The table indicates thatǭ p is considerably less thanǭ c in the finite network, which cannot be predicted by the traditional infinite-network model. This reduction in outage probability is more significant for the unspread network, and is less pronounced with increasing G. Bothǭ p andǭ c increase as M and σ s increase and G decreases.\n\nMentions:\n- In Fig. 5, shadowing was applied to the same set of N = 10, 000 networks used to generate Fig. 4.\n- The SNR was set to Γ = 10 dB, a mixed-fading channel model was assumed, and other parameter values are the same ones used to produce Fig. 5; i.e., r ex = 0.05, r net = 1, β = 0 dB, and p i = 0.5.\n\nOCR Text: \"i Siizjualn\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 5. Probability that the conditional outage probability ǫ is below the threshold outage probabilityǫ T in a network withrex = 0.05, rnet = 1.0, M = 28, Γ = 5 dB, mixed fading, and no spreading (G = 1). N = 10, 000 network realizations were drawn to produce the figure. Curves shown for no shadowing, and shadowing with two values ofσs."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 5 shows the performance of the trained generator (left, violet) and sample privacy guarantee (right, yellow) with respect to ρ. The generator's performance is measured using the F1 score that is the harmonic mean of precision and recall, where precision increases with the similarity between the generated and ground-truth images, and recall increases with the number of generable images. As ρ increases, while the sample privacy guarantee increases, the cGAN is trained with more noisy samples, degrading the trained generator's F1 score. With a more stringent sample collection latency deadline τ , the server collects fewer seed samples, leading to the generator's lower F1 score while increasing the sample privacy guarantee.\n\nMentions:\n- Figure 5 shows the performance of the trained generator (left, violet) and sample privacy guarantee (right, yellow) with respect to ρ.\n\nOCR Text: \"00r Jh Minutmimtco Suuuyr nai0n-Z8 FInLC Iuneir-i0 ETofeinecuiamgi tz75\" 0.145 0,125 0845 Comnmo%einn Mld A 1980- 0et0049t80080d Feattttt\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: Trained generator’s F1 score (left, violet) and sample privacy guarantee (right, yellow) with respect to sample compression rate ρ (N = 10,M = 2)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 7 is a plot of the exponent a versus parameter c for networks with γ = 2.0, 2.5, 3.0, k min = 2 and size N = 10 4 (squares,circles,triangles). Points are estimates of the parameters from fitting of Eq.1 (exponent α) and Eq. 2 (parameter c) to Monte Carlo Simulation data. Lines are the predictions of Eq.15. We observe a rather good agreement between the two showing that, despite it's rather complicated functional form, Eq. 15 is a useful tool in connecting the two ways of data analysis.\n- Fig. 8 shows the exponent α as a function of ∆W/W for networks of γ = 3.0, k min = 2 (Fig. 8A), γ = 2.5, k min = 2 (Fig. 8B) and γ = 2.0, k min = 1 A B C D FIG. 6: Scale-free network with 1000 nodes γ = 2, k min = 1 for ∆W = 0, 50, 100, 500. Edges are depict as red lines connecting nodes. The color of the nodes depends on the value r i = log σ log f of each node. Nodes with r i &lt; 0.6 are blue, 0.6 &lt; r i &lt; 0.75 are green,0.75 &lt; r &lt; 0.85 are yellow and r i &gt; 0.85 are red. FIG. 7: Exponent a versus parameter c for networks with γ = 2.0, 2.5, 3.0, k min = 2 and size N = 10 4 (squares,circles,triangles). Lines are predictions of Eq.15 (Fig. 8C) for different network sizes, namely N = 1000, 5000, 10000, 50000, 100000. We observe that for γ = 3.0 it is not possible to observe an increased exponent with network size in the intermediate regime.  data. Eq.15 is a valuable tool in this case showing that the expected increases of the exponent are of the order of 0.05 which is well within the statistical error for α.\n\nMentions:\n- Figure 7 is a plot of the exponent a versus parameter c for networks with γ = 2.0, 2.5, 3.0, k min = 2 and size N = 10 4 (squares,circles,triangles).\n- Nodes with r i &lt; 0.6 are blue, 0.6 &lt; r i &lt; 0.75 are green,0.75 &lt; r &lt; 0.85 are yellow and r i &gt; 0.85 are red. FIG. 7: Exponent a versus parameter c for networks with γ = 2.0, 2.5, 3.0, k min = 2 and size N = 10 4 (squares,circles,triangles).\n\nOCR Text: \"ndn |\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 7: Exponent a versus parameter c for networks with γ = 2.0, 2.5, 3.0, kmin = 2 and size N = 10 4"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The initial conditions along the aforementioned line give an adequately representative set of non-geodesic orbits to study the time it takes for the inspiraling orbits to cross through the resonant islands. Actually the non-geodesic orbits corresponding to the initial conditions that lie on the convex side of the islands (Fig. 3a) do not cross the particular resonant island; these orbits continuously recede from the islands. Thus from now on we constrain our study to non-geodesic orbits that start near the concave side of the resonant islands. On the concave side of the leftmost island there is a window of initial conditions of ρ(0) along the (ρ = 0, z = 0) line for which the corresponding non-geodesic orbits eventually enter first this particular island of resonance. Upon its entrance a non-geodesic orbit visits all the other islands of that resonance as well. An analogous window of ρ(0)s, right next to the former one, contains orbits that enter first the next island of the same resonance and so on until we reach the window that enters first the last island of the chain. After that window, it follows another window of initial conditions for which the non-geodesic orbits enter the leftmost island first. The non-geodesic orbits that start from the latter window perform an extra full rotation on the surface of section with respect to the non-geodesic orbits of the first window, before they enter the particular resonant island. During this rotation the islands move towards the center of the main island, so that they eventually encounter the evolving orbits from this new window. The succession of such windows continues, until point that the non-geodesic orbits reach the resonance is so close to the center of the main island (due the energy and z-angular momentum loss), that the islands have shrunk to negligible thickness and thus no resonance can be observed. However the linear approximation we use to produce the non-geodesic orbits ceases to be valid well before this happens. Therefore we have not extended our numerical computation that far. In order to study the observability of the resonances we have chosen a few distinct initial conditions along the above sequence and measured the time interval that the corresponding non-geodesic orbit needs to cross the 2/3-resonant islands. The time ∆t r spent by each non-geodesic orbits in the 2/3-resonance is shown in Fig. 11 as a function of their initial conditions along the line (ρ = 0, z = 0). Moreover each window in the sequence of windows in Fig. 11 is demarcated by two distinct abrupt peaks of ∆t r lying on either side of the window. Of course there are a number of parameters that affect this ∆t r time interval, like the initial energy E(0), the initial angular momentum L z (0) and the exact initial conditions of an orbit (in our case ρ(0)), as well as the physical characteristics of the EMRI, namely the parameters of the metric χ and q, and the masses of the two bodies. Especially the ratio of masses µ/M plays an important role in ∆t r ; the lower the value of µ/M is, the longer time it takes to reach a particular island, and consequently the width of the windows is larger. Our aim is not to explore the dependence of crossing time ∆t r on all these parameters, instead we explore the qualitative characteristics that relate the time of crossing with the physical characteristics of an EMRI. We observed that the exact point of entrance in the island is crucial for the time the orbit spends in that island before it exits the particular chain of islands.\n\nMentions:\n- The time ∆t r spent by each non-geodesic orbits in the 2/3-resonance is shown in Fig. 11 as a function of their initial conditions along the line (ρ = 0, z = 0).\n- Moreover each window in the sequence of windows in Fig. 11 is demarcated by two distinct abrupt peaks of ∆t r lying on either side of the window.\n\nOCR Text: \"2oqq00 Is0o00 1QQQQQ 50000 U V Mn 4.14 4,18 OIJMM\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 11: The time ∆tr needed by non-geodesic orbits to cross the chain of islands belonging to the 2/3-resonance as a function of their initial conditions ρ(0) (the initial value of the ρ-coordinate) along the line ρ̇ = 0, z = 0. The parameters used are µ/M = 8× 10−5, q = 0.95, χ = 0.9, E(0) = 0.95, Lz(0) = 3M ."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Here how we do it. We squeeze initial sets into long narrow segments. Then we intersect these segments at middle lines, so that half of the points of each segment lie on one side of all halving lines that pass through the points of the other segment (See Figure 3).\n\nMentions:\n- Then we intersect these segments at middle lines, so that half of the points of each segment lie on one side of all halving lines that pass through the points of the other segment (See Figure 3).\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: The Cross construction."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The wave vector q m is shown in Fig. 2 as a function of g/(1 + g). Open circles are exact for 24 spins. The peak remains at π up to g = 1/2 and then decreases to q m = π/2. Classical spins have pitch angle q m , with cosq m = -1/4g for g &gt; 1/4, doubly degenerate GS with long-range Néel order up to g = 1/4, and a spiral GS with LRO(q m ) for g &gt; 1/4. We find strong quantum effects at large g that compress the BOW phase and lock in q m = π/2.\n\nMentions:\n- The wave vector q m is shown in Fig. 2 as a function of g/(1 + g).\n\nOCR Text: \"Illodel Chsiaul: g(1-g)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2. Wave vector qm of the structure factor peak S(qm) of the J1J2 model with N = 24 spins as function of frustration g/(1 + g). The chain of classical spins has pitch angel χ = π for g ≤ 1/4 and χ = cos –1 (–1/4g) for g > 1/4."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Registration nodes (RA) prevent the single point of failure by developing Dynamic Demilitarized Zone (DDZ) around CA. These RAs are elected in each sector of a cluster depending upon Optimum Criteria Function (OCF) consisting trust, stability, residual energy and connectivity degree. The major responsibility of distributed RA architecture is to detect the malicious node precisely in a particular sector and broadcast to the neighbors for localization and tracking. The RA election results that the worst value of OCF is 0.47 in first sector of sixth cluster and the best value as 1 in each cluster while using weighs as w 1 =0.46, w 2 =0.22, w 3 =0.22 and w 4 =0.1 as shown in Fig. 10. Reference nodes election is run to select the three reference nodes in each cluster so that each node inside that cluster can be localized by Triangulation method. Reference nodes are selected on the basis of Best Criteria Function (BCF) which consists of the distance from cluster head, stability, residual energy and connectivity degree. From simulation, BCF is found between 0.96 to1 and candidates for reference node are increasing in different clusters using weighs as v 1 =0.44, v 2 =0.23, v 3 =0.23 and v 4 =0.1 as shown in Fig. 11. The attacks senario in six sectors of a cluster is illustrated for 60 seconds and node behaviour is found that there is always possibility of attack in each sectoral RA as shown in Fig. 12. As the node behaviour is found greater than 0.8 then it is misbehaviour or attack, otherwise it is normal behaviour. From simulation, the misbehaviour or attack is found in sector-1 between 15 to 35 secs, sector-2 between 5 to 15 secs, sector-3 between 35 to 45 secs, sector-4 between 55 to 60, sector-5 between 45 to 50 and sector-6 between 45 and 55. The malicious node detection is executed in all sectors of seven clusters using RAs which provide the public key and trust value of target node to requesting node and later requested node verifies the trust value and the public key. Simulation shows that different numbers of malicious nodes are detected in different clusters such as five malicious nodes in cluster-1, four malicious nodes in cluster-2, eight malicious nodes in cluster-3, eight malicious nodes in cluster-4, eight malicious nodes in cluster-5, seven malicious nodes in cluster-6 and five malicious nodes in cluster-6 as shown in Fig. 13. In multi-lateralation, the average tracking error is found to be ~ 1.3 m and significantly higher error at sudden sharp turnings in trajectory as shown in Fig. 22. The corresponding tracking error is shown in Fig. 23. The major cause of lower error in multilateralation is that known reference nodes form triangulation which are in neighborhood of malicious node and they do mapping location with respect to a single node for double cross check.\n\nMentions:\n- From simulation, BCF is found between 0.96 to1 and candidates for reference node are increasing in different clusters using weighs as v 1 =0.44, v 2 =0.23, v 3 =0.23 and v 4 =0.1 as shown in Fig. 11.\n\nOCR Text: \"Relttrucn \" Fn\"iui [urld ( Islt: Jl-Weacle\" utceoni Italaae v 1eue eice Jokn\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 11 Reference node Election"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Figure 6 the distribution of publication years of all other 294 highly cited sources is visu-  renewal a new initialisation made after some generations without improvement selection final genetic operator in each generation that selects for the new generation of a population the best individuals from the union of old population, its offspring, and its mutants tunneling a metaphor for going through barriers in the cost landscape (which is done in a local search if the tunnel is not too long, i.e., if the link set at its end invalidates the link set at its entry) valid cluster a cluster L that differs in at least r|L| links from any cluster with lower cost Ψ (0 &lt; r &lt; 1 measures resolution)\n\nMentions:\n- In Figure 6 the distribution of publication years of all other 294 highly cited sources is visu-  renewal a new initialisation made after some generations without improvement selection final genetic operator in each generation that selects for the new generation of a population the best individuals from the union of old population, its offspring, and its mutants tunneling a metaphor for going through barriers in the cost landscape (which is done in a local search if the tunnel is not too long, i.e., if the link set at its end invalidates the link set at its entry) valid cluster a cluster L that differs in at least r|L| links from any cluster with lower cost Ψ (0 &lt; r &lt; 1 measures resolution)\n\nOCR Text: \"1956   1961  1036   1971  1076   1981   1966 1591   1996  2c01   2006 1280 18C5 I9n5 Kods I ume\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6: Distribution of first publication year of highly cited sources (after 1950; 294 of 300 sources; 5-years periods). Red parts of bars represent IR-sources, blue parts sources outside IR."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In summary, Fig. 14 shows the contour plot of λ g and λ bb , given at 68% CL, for the hit statistical uncertainties and systematic uncertainties from luminosity fluctuations not described by the fit function. They are added in quadrature, given in Table IX. Figure 15 shows the corresponding plots for χ 2 /ndf = 1. The variations of the fit values are well described by the contours. Owing to the strong anti-correlation between λ bb and λ g a transformation of the fit results is performed. The (λ bb − λ g ) versus (λ bb + λ g ) plane is used in order to illustrate the relative uncertainty between the fit values. The comparative sensitivities of the MPX devices at 68% CL are summarised in Fig. 16.\n\nMentions:\n- The comparative sensitivities of the MPX devices at 68% CL are summarised in Fig. 16.\n\nOCR Text: \"Y5 = 8 Tev LHC FII 3236 MPXCI MApXOZ MPxC3 6606 CL MPXI hapxO5 MPX13 WE 0,86 0,63\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 16. Contour curves of(λbb −λg) and(λbb + λg), given at 68% CL, for the fit results of the average number of interactions per bunch crossing as a function of time seen by MPX01-05 and MPX13. The dots in the center of the contours indicate the fit values. The hit statistical uncertainties and systematic uncertainties from luminosity fluctuations notdescribed by the fit function are added in quadrature, given in Table IX, and thenscaled to yield χ2/ndf = 1. LHC fill 3236."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Not much is known on the structure of the invariant measure constructed in this way, or on the speed of convergence towards it. This means in particular that further arguments are needed to implement a form of subadditive theorem and to prove the law of large numbers. As if we wanted to use the classic proof using the subadditive ergodic theorem, we cut the trajectories into smaller bits (see Figure 8). Then, on each bit, we go back a distance L, look for the first zero on the right to play the role of the front and erase the zeros on its left. Thanks to the orientation of the East model, the original front is always on the left of the new ones. Those changes induce a small correction if L is chosen correctly. And now, thanks to the local equilibrium result derived in Theorem 4.7, all the new fronts have almost the distribution of a front with initial configuration chosen with the equilibrium (product) measure on the right. Moreover, if we treat separately the terms corresponding to the different dash styles in Figure 8 (with a well chosen, but fixed, number of dash styles), they are almost independent in a certain sense. With these almost iid variables, we can use the classic proof of the law of large numbers for variables with a fourth moment.\n- Figure 8: The original trajectory of the front is in blue, cut into sections of length s. The trajectories of the modified fronts are in different dash styles : \"--\",\"− • −\" or \"− • •−\". They necessarily stay on the right of the original one. The squares represent the zero boundary condition we use to define X L,3vs,• ω(ks) (u). Here we have taken j 0 = 3: we consider separately the terms in the sum corresponding to the \"--\",\"− • −\" and \"− • •−\" trajectories. Notice that after following the modified front for time s, we may either start again closer to the real front (e.g. in the picture X L,3vs,• ω(4s) (0) ≤ X L,3vs,• ω(3s) (s)), or further from the real front (e.g. X L,3vs,• ω(5s) (0) ≥ X L,3vs,• ω(4s) (s)).\n- Let us forget the j 0 − 1 first terms and decompose 1 n n−1 j=j 0 ∆ j into (see Figure 8; the different terms in the sum below correspond to different dash styles in the picture):\n\nMentions:\n- As if we wanted to use the classic proof using the subadditive ergodic theorem, we cut the trajectories into smaller bits (see Figure 8).\n- Moreover, if we treat separately the terms corresponding to the different dash styles in Figure 8 (with a well chosen, but fixed, number of dash styles), they are almost independent in a certain sense.\n- Figure 8: The original trajectory of the front is in blue, cut into sections of length s.\n- Let us forget the j 0 − 1 first terms and decompose 1 n n−1 j=j 0 ∆ j into (see Figure 8; the different terms in the sum below correspond to different dash styles in the picture):\n- Let us define the process in Z, for ω ∈ LO (see Figure 8):\n\nOCR Text: \"a JJe Uas 1X7; S-i _\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 8: The original trajectory of the front is in blue, cut into sections of length s. The trajectories of the modified fronts are in different dash styles : “- -”,“− · −” or “− · ·−”. They necessarily stay on the right of the original one. The squares represent the zero boundary condition we use to define XL,3vs,◦ω(ks) (u). Here we have taken j0 = 3: we consider separately the terms in the sum corresponding to the “- -”,“− · −” and “− · ·−” trajectories. Notice that after following the modified front for time s, we may either start again closer to the real front (e.g. in the picture XL,3vs,◦ω(4s) (0) ≤ X L,3vs,◦"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Fig. 9, we plot ρ 0 calculated with GWF and QWF. As U/t increases, the condensate fraction diminishes from the value of free bosons ρ 0 = 1, and vanishes at U = U c as the order of n/N s in the non-superfluid states for all the wave functions. Thus, superfluidity vanishes at U = U c . As shown in Fig. 9(b), the discontinuities appear for QWF with L ≥ 20 in accordance with other quantities. Recently, the condensate fraction has been actually observed by experiments of cold atoms. 48,49) Here, we discuss k = 0 elements of n(k), because it is a directly-observed quantity by cold-atom experiments. Figure 10 shows n(k) obtained by the VMC calculations under some conditions. Because GWF gives the result identical with that of the Gutzwiller approximation in fermionic cases, 23) n(k) is constant for k = 0 in both superfluid and insulating phases. On the other hand, wave functions with D-H factors yields dispersive n(k), owing to the effect of density fluctuation. In Fig. 10, we draw n(k) of the best D-H binding wave function in this study Ψ opt DH . It is marked that the difference is small between the superfluid (U/t = 20) and insulating states (U/t = 24). As k leaves the Γ(= 0) point, n(k) decreases in any direction, but its decrement is larger in the Γ → (π, π) direction than in the Γ → (π, 0) direction. This direction-dependent dispersion near the Mott critical point is actually observed as a cross-like intensity in absorption images of cold atoms, for example in Figs. 2f and 2g of ref. 2. This topic was previously argued by the perturbative correction to the Gutzwiller solution. 50) Finally, to corroborate a superfluid-insulator transition at unit filling, we compare particle-density (n) dependence of various quantities in the vicinity of n = 1 51) between U &lt; U c and U &gt; U c . Let us start with the condensate fraction ρ 0 , which is plotted in Fig. 11(a). For U = 16t (&lt; U c ), ρ 0 becomes minimum at n = 1, but preserves a finite magnitude, whereas for U/t = 24 (&gt; U c /t), ρ 0 decreases as n approaches 1, and almost vanishes at n = 1. The sign of second derivative ∂ 2 ρ 0 /∂n 2 becomes different between the two cases near unit filling. Thus, a MI state appears only at n = 1 and U &gt; U c . This is supported by the similar behavior of the onsite density fluctuation σ 2 o , eq. ( 32), as shown in the inset of Fig. 11(b). The order parameter of Mott transitions,d, depicted in the main panel of Fig. 11(b) monotonically increases for U &lt; U c , whereas it sharply decreases at n = 1 for U &gt; U c , suggesting a Mott transition. Figure 11(c) represents the chemical potential,    the Mott transition.\n\nMentions:\n- Figure 10 shows n(k) obtained by the VMC calculations under some conditions.\n- In Fig. 10, we draw n(k) of the best D-H binding wave function in this study Ψ opt DH .\n\nOCR Text: \"L-20 26 UL phasc ~ 4 Yp. C= Squan\" lattice\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 10. (Color online) The momentum distribution function, eq. (35), are compared between GWF (ΨG) and optimizing wave function (Ψopt DH ), and between superfluid (SF) and Mott insulating (MI) phases. The value of the coherent wave number k=0 is omitted. Two system sizes are used. The behavior of n(k) in the other D-H binding wave functions is similar to that of Ψopt DH ."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Nonetheless, the proposed scheme with two relays provides a significantly pronounced ESR performance for practical values of η. For example, the ESR of the proposed scheme has threefold improvement compared to the two-hop relaying for path-loss of 3. This again boosts the effectiveness of our proposed secure scheme in real-world applications such as WSN and low altitude UAV-based relaying network [13] wherein fading or blockage are undeniable. Now, in order to obtain a better understanding of the proposed system, Figs. 7 and 8 are provided to demonstrate how optimizing the network resources, i.e., allotted power to each network node, could bring improvement in terms of ESR performance. Specifically, in Fig. 7 DRAFT R n s (P s , P r 1 , P r 2 , P d ) subject to C1 : 0 ≤ P k ≤ P max , C2 :\n- where the objective function is averaging over N realization of the channels which is approximately equal to the the expected value of R s given by (9). We mention that as N goes to infinity, the approximation turns into equality based on the law of large numbers [36]. In (24), the constraint C1 represents the transmission power of each node subject to the maximum power P max , and C2 is the constraint for network power budget P tot defined as P tot = 2P s +P r 1 +2P and N = 10 5 channel realizations. The equal power allocation (EPA) curves are obtained by setting equal transmission power per phase per active node, i.e., P s = P r 1 = P r 2 = P d = Ptot 6 . This is due to this fact that one frame of transmission is conducted in three phases and two nodes are transmitting in each phase of communications. As it can be clearly found from Figs. 7   and 8, the least proportion of power budget should be allocated to S for low-to-moderate range of network power budget, whilst R 1 should enjoy a higher transmission power compared to the other nodes, in order to get the ESR improved.\n\nMentions:\n- Now, in order to obtain a better understanding of the proposed system, Figs. 7 and 8 are provided to demonstrate how optimizing the network resources, i.e., allotted power to each network node, could bring improvement in terms of ESR performance.\n- Specifically, in Fig. 7 DRAFT R n s (P s , P r 1 , P r 2 , P d ) subject to C1 : 0 ≤ P k ≤ P max , C2 :\n- As it can be clearly found from Figs. 7   and 8, the least proportion of power budget should be allocated to S for low-to-moderate range of network power budget, whilst R 1 should enjoy a higher transmission power compared to the other nodes, in order to get the ESR improved.\n\nOCR Text: \"Jt ~LLA>Ih = S  p LM 1 8 Netierk trarsrit pzier (dBr]\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 7. ESR versus network power budget."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- See Figure 1.7 for a pictorial representation of the aforementioned loss functions for classification.\n\nMentions:\n- See Figure 1.7 for a pictorial representation of the aforementioned loss functions for classification.\n\nOCR Text: \"Uya) 4 Nuul: Elhec Ibi. Ivksc Wx\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1.7: Margin-based loss functions for classification."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Our approach to proper learning of linear dynamical system could also be used in a \"prediction-error\" method for improper learning of a linear dynamical system, i.e., forecasting its next observation (output, measurement). As such, it can be applied to any time series. To exhibit this, we consider a real-world stock-market data first used in [8] and attached in the Supplementary Material. (The data are believed to be related to the evolution of daily adjusted closing prices of stocks within the Dow Jones Industrial Average, starting on February 16, 1885. Unfortunately, the data are no longer available from the website of the authors of [8].) We sample every four periods from the first 200 periods of the stock-market data and estimate the evolution for the 50 selected periods. Our method is compared to a combination of least-squares algorithms implemented in Matlab TM System Identification Toolbox TM routine ssest with order set to 1 (\"least-squares auto\"). The simulation results are shown in Figure 4. The percentages in the legend correspond to nrmse values (15), and both from the nrmse and the shape of the two curves, it seems clear that our method is superior.\n- For better clarity of presentation, the experiment in Figure 3 has been performed again with the same experimental set-up and the results are displayed in Figure 6 as four box-plots. Each box-plot shows the nrmse fitness values of one method Figure 6: Details of the performance of three other methods and our method used for comparison of Figure 4, in the same experimental set-up. at varying standard deviation of both process noise and observation noise (\"noise std\") from 0.1 to 1.0 and each box inside indicates the first quartile, mean and the third quartile of 30 nrmse values with the outliers marked by \"o\" symbols.\n\nMentions:\n- The simulation results are shown in Figure 4.\n- Each box-plot shows the nrmse fitness values of one method Figure 6: Details of the performance of three other methods and our method used for comparison of Figure 4, in the same experimental set-up. at varying standard deviation of both process noise and observation noise (\"noise std\") from 0.1 to 1.0 and each box inside indicates the first quartile, mean and the third quartile of 30 nrmse values with the outliers marked by \"o\" symbols.\n\nOCR Text: \"DLroue cnetcaer iJe Eu E0lTo\"d\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4: The results of our method compared against the \"least-squares auto\" system identification method implemented in MatlabTM System Identification ToolboxTM, on stock market data used in [8]. The percentages in legend are corresponding nrmse values."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Assuming for simplicity that H(W 1 |W 2:n ) = 1, Fig. 6 compares (30a) with the weakened upper bound in (31a). We can see that the gap can be large when n is large and T is much larger than n. Nevertheless, the weakened upper bounds in (31a) and (31b) allow us to derive lower bounds on computation time that are non-asymptotic in n, and explicit in ε, δ, and channel properties.\n\nMentions:\n- Assuming for simplicity that H(W 1 |W 2:n ) = 1, Fig. 6 compares (30a) with the weakened upper bound in (31a).\n\nOCR Text: \"calima Mild Lnyl\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 6: Upper bound in (30a) (solid line) vs. the weakened one in (31a) (dashed line) for chains."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Applying the error-and-erasure decoding algorithm of the [n, k] RS code again on Y ′ k×(ℓ−v) to reconstruct A 1 if ℓ−v ≥ k; otherwise the progressive decoding is applied. The modified decoding algorithm is summarized in Algorithm 2 and its corresponding flow chart is shown in Fig. 2.\n\nMentions:\n- The modified decoding algorithm is summarized in Algorithm 2 and its corresponding flow chart is shown in Fig. 2.\n\nOCR Text: \"Nobnt Rarraiy dori#Rt Jut ee 47.4 robreru: AeeeT An-i A= Uc Lalt -e. [\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2. Flow chart of Algorithm 2"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Our Chern number computations are done in the offresonant frequency regime. Hence, the ground state of the driven system undergoes the topological phase transitions presented in Fig. 10, 11, and 12. For a vanishing light parameter γ and high frequencies, the ground state Chern numbers are the same as in the undriven case, see Fig. 9. In Fig. 10 most Chern numbers coincide with  the case of a vanishing intensity. When the intensity is further increased the ground state Chern number exhibits a rather different behavior. Even small intensity changes can have a vast influence on the Chern number 14 , see Fig. 11. Since the Floquet-Hofstadter spectrum gets twisted in presence of circularly polarized light and keeps particle-hole symmetry for linearly polarized light it is obvious that the band structure of graphene is differently affected for the two polarization states. The deformation of the band structure and the associated gap closing and opening is related to the change of Chern numbers. Hence, we investigate as well the influence of linearly polarized light on the distribution of Chern numbers. Similar to the case of circularly polarized light, for rather small intensities only few Chern numbers deviate from the static Chern number distribution. An increase of the intensity leads to a significantly different behavior, as shown in Fig. 12. For circularly polarized light the ground state is uniquely defined. Whereas, for linearly polarized light this is not the case for all flux values. At flux values of, e.g., 6/11, 6/13 or 3/17 a band crossing of the ground state occurs. This effect can be seen at eight different flux values for q &lt; 21. The occurrence of the band crossing of the ground state seems not to follow a simple rule.\n\nMentions:\n- An increase of the intensity leads to a significantly different behavior, as shown in Fig. 12.\n\nOCR Text: \"61 02 0,3 04 05 ?6 0,7 luzb] 7 1 c1 W 03 04 65 26 0,7 Lux ze: Wii_cel\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 12. In the case of linear polarization with Ax = 0.0, Ay = γ the distribution of the Chern number is for linearly polarized light similar affected as for circularly polarized light. The frequency ω was again fixed at 6.0 g/~ and the intensity is governed by Eq. (35)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- , and both terms increase much faster than the numerator, especially when α is small. Combining these two extreme cases, we expect that smaller pathloss exponent would help the channels to become asymptotically orthogonal when M is sufficiently large. To confirm our prediction, in Fig. 10 we present p γ obtained with pathloss exponents α ∈ {2, 3, 4}, N = 1, and an interuser distance of l k, j = 70 m. The figure shows that with smaller α the channels become more orthogonal, which is line with our prediction from above. If we would instead use the three-slope pathloss model in (26), APs close to the user (distance smaller than d 1 ) will have pathloss exponent α ≤ 2, and users at larger distances have pathloss exponent 3.5. Therefore, compared to the single-slope non-singular pathloss model l(r) = min(r −3.76 , 1), the channels between two users will in the average be more orthogonal.\n\nMentions:\n- To confirm our prediction, in Fig. 10 we present p γ obtained with pathloss exponents α ∈ {2, 3, 4}, N = 1, and an interuser distance of l k, j = 70 m.\n\nOCR Text: \"Threshol?\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 10: The CDF of Xfp with different pathloss exponent α ∈ {2, 3, 4}, N = 1, and the inter-user distance lk, j = 70 m."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Our examples start with the simplest knot, trefoil, Fig. 3. It is a typical picture, the figures for other representations and other values of n look much similar, with a light variation of parameters (see the next subsection).\n\nMentions:\n- Our examples start with the simplest knot, trefoil, Fig. 3.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: The LMOV numbers as a function of genus for the trefoil and the representation Q = [4] and n = 16: N31 [4],g,16 . The blue curve is the Gaussian curve (3) with parameters µ = 6.14, σ = 1.85, I = 3.58 · 107, while the parameters of the binomial distribution are p = 0.44, n = 14.0."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We consider a system where the distance r AI between the AP and IRS is fixed to 50 m. To evaluate the impact of the user position, we assume that the user moves such that the sum of the AP-user distance r Au and the IRS-user distance r Iu is r Au +r Iu = 70 m. In Fig. 3, we compare the spectral efficiency achieved for different AP-user distances. We first observe that the proposed manifold optimization-based algorithm and fixed point iteration method achieve almost the same spectral efficiency, and both significantly outperform the SDR method. This is mainly because our proposed algorithms are guaranteed to find locally optimal solutions of P 1 and P 2 , respectively, 3 The computational complexities of Algorithm 2 and the SDR method apply to the entire algorithm not just to one iteration.  while the SDR method yields only an approximate solution as mentioned in Remark 3.\n- When the user moves close to the AP, it benefits from the strong direct channel h between the AP and the user, and therefore achieves a high spectral efficiency. A similar phenomenon can also be observed when the user moves close to the IRS, such that the user benefits from a strong reflecting channel h r . Under these two conditions, the optimal joint design of the beamformer at the AP and the IRS phase shifts is less important. Hence, the performance gap between the proposed algorithms and the SDR method is relatively small in these two regimes in Fig. 3. In contrast, when the user is located relatively far from both the AP and the IRS, e.g., r Au = 40 m, the optimal joint design of the beamformer at the AP and the IRS phase shifts is needed to achieve a satisfactory communication performance. In this case, our proposed algorithms are good candidates for improving the spectral efficiency.\n\nMentions:\n- In Fig. 3, we compare the spectral efficiency achieved for different AP-user distances.\n- Hence, the performance gap between the proposed algorithms and the SDR method is relatively small in these two regimes in Fig. 3.\n\nOCR Text: \"Vutudcu-ijlu En# uenauam- Sdemarm Fanhmm and IPS 4p_ispr Misiante\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3. Spectral efficiency achieved by different algorithms when rAI = 50 m, rAu + rIu = 70 m, Nt = 8, and M = 10."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 3 shows how the polydispersity affects the steady-state distribution of strands in the transition zone. We calculated P (f ) (j) and P (b) (j) for a random network with j = 80 in the bulk. We chose a range of the j values comparable with the simulation results of Svaneborg et al. [37] for unfilled random networks. The sharp decrease in distribution of bound strands indicates a large population of short adsorbed strands. Figure 4, shows the response of this system to a uniaxial deformation and compares it with filled networks at different j values. The effect of polydispersity on the ultimate strength is apparent. Comparatively, the networks with smaller j exhibit significantly lower strength. Due to the chosen value of ξ (b) , the network degradation is essentially due to the rupture of labile bonds and adhesive failure of the adsorbed strands. To observe the effect of polydispersity more clearly, Figure 4 also includes the stress-stretch dependency in a monodisperse network reinforced with the same volume fraction of fillers, where all strands contain 80 statistical segments. The drastic difference between the behavior of monodisperse and polydisperse networks confirms that the network alteration is indeed induced by the structural polydispersity. The deformation-induces desorption is driven by the finite extensibility and non-Gaussian behavior of short strands in the transition zone. The entropic tension developed in shorter strands increases concurrent with macroscopic deformation and leads to desorption of a large population of short strands. In the presented model, the progressive desorption of strands is taken into account by evolution of j (a) min with λ (Figure 5).\n\nMentions:\n- Figure 4, shows the response of this system to a uniaxial deformation and compares it with filled networks at different j values.\n- To observe the effect of polydispersity more clearly, Figure 4 also includes the stress-stretch dependency in a monodisperse network reinforced with the same volume fraction of fillers, where all strands contain 80 statistical segments.\n\nOCR Text: \"ClMt!\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4: The effect of average strand length, j, on the stress-stretch dependency in random networks subjected to uniaxial deformation. The monodisperse network is formed by strands with 80 statistical segments (σ1 = σ1/µ, νf = 0.1, ξ (b) = ξ(f) = 0.99, ξ(a) = 0.2, κ0 = 2.5, δ = 0.2)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Thus, we can conclude that B  As an illustration of the above discussion, Figure 4 depicts the evolution of the coefficients B\n\nMentions:\n- Thus, we can conclude that B  As an illustration of the above discussion, Figure 4 depicts the evolution of the coefficients B\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 4: Shown are the coefficients B(g) (eq.(16)) versus the fractal iteration i for various inter-electrode (m) distances. We observe a saturation after the sixth fractal generation for a range of different distances."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Reinforcement learning (RL) is an unsupervised learning algorithm. The framework in RL involves five main parameters: environment, agent, state, action, and reward. The environment is the world in which the agent moves. Due to the unsupervised nature of RL, the agent does not start out knowing the notion of good or bad actions. At each time step, it performs an action in the environment that leads to the next state and the corresponding reward (Figure 5). As training progresses, the agent learns which are good and bad actions by how much reward it can obtain from each action. Therefore, the reward function plays a vital role for the agent's ability to learn a policy-a set of actions-that leads to good performance. Actor-critic (Figure 6) is a reinforcement learning algorithm that incorporates two models. The actor model outputs the desired action at each state. The critic model takes in that action to estimate its value-the goodness of being in that state. Usually, each model implements a neural network. At each time step, the weights of the actor model are updated to perform actions to maximize the value, which the critic model predicts. Therefore, as training progresses, the actor becomes better at making good actions and the critic becomes better at estimating the value in each state. Please refer to pseudo code above for the step by step process.\n\nMentions:\n- At each time step, it performs an action in the environment that leads to the next state and the corresponding reward (Figure 5).\n\nOCR Text: \"AGEHT state reward acbion EHVROHMEHT\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: Diagram of reinforcement learning [16]. At time step t, the agent takes action at. The environment responds by outputting the new state rt+1 and reward St+1. This cycle continues until the end of training the model."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The syntax of assertions is summarized in Fig. 4. We assume an infinite set of (implicitly sorted) logical variables Var. The meta variable v stands for a variable of an arbitrary sort and the meta variable x for a variable of a sort denoting addresses. The sorts of other variables are as expected. Assertions are built from sorted terms -one sort per semantic domain of interest. The terms T d denote flow domain values, T a node label values, etc. For inflow terms T in , the constant ϵ denotes the empty inflow in e , T in + T in denotes the lifting of + to partial functions 2 , and the constant 0 denotes the total inflow that maps all addresses to 0. Terms for flow maps are constructed similarly.\n\nMentions:\n- The syntax of assertions is summarized in Fig. 4.\n\nOCR Text: \"j;hN HHK ! Ha #n8l ulahi lek V;l)yH_LL #;ud H F:HHH #:IHhk;  XN:UGHM VAJWEHRKuF hHeg l H#Hww#4!MHNKHM\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4. Syntax of RGSep[FI] terms and assertions."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Let us consider Lombardy and Italy, since these are where full social distancing was first applied and where the peak was reached earliest, on 27 March. Just after this date, i.e. the 3-day average from 28-30 March, the ratio R 12 , where 1 denotes Lombardy and 2 the rest of Italy, was R 12 ∼ 1.3, whereas for the previous few days' 3-day rolling averages (up to 28-30 April) it was R 12 ∼ 0.5 and falling (data taken from [3]), see Fig. 1. If the decay in both Lombardy and the rest of Italy is due entirely to social distancing reducing the effective R, this change seems difficult to understand, as this social distancing effect should apply equally in both cases. Taking the 32 three-day rolling averages of R 12 up to April 30th (data taken from [3]) and fitting a form a exp(−λτ ) one finds that λ = 0.028. Fixing R = 1 and 1/σ = 7 days, one obtains z 1 − z 2 = 0.196.\n\nMentions:\n- Just after this date, i.e. the 3-day average from 28-30 March, the ratio R 12 , where 1 denotes Lombardy and 2 the rest of Italy, was R 12 ∼ 1.3, whereas for the previous few days' 3-day rolling averages (up to 28-30 April) it was R 12 ∼ 0.5 and falling (data taken from [3]), see Fig. 1.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: The 3-day rolling average of R12 for Lombardy/(remainder of Italy) (which is easily calculated from the data at [3]) against the number of days since the peak in the rate of deaths."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The majority carriers were electrons as shown by the positive slope of the plot of the voltage difference between electrodes X2 and Y2 of the Hall bar (indicated by V xy ) vs. the magnetic field B, in Figure 5, that corresponds to Hall bar (4,1) whose electrical properties are described in Table I. A perfectly linear plot corroborates the validity of the assumptions employed in the standard Hall mobility measurement for one to two layer graphene grown on the Si-face of SiC, which were described above.   values, as described above. Figure 6 shows Hall mobility vs. gate voltage V G for two devices. A maximum Hall mobility of 1575 cm 2 V -1 s -1 was measured at V G = -4.5 V. Field-effect mobility, μ eff , was measured from the top-gated Hall bar device using the relation:\n\nMentions:\n- The majority carriers were electrons as shown by the positive slope of the plot of the voltage difference between electrodes X2 and Y2 of the Hall bar (indicated by V xy ) vs. the magnetic field B, in Figure 5, that corresponds to Hall bar (4,1) whose electrical properties are described in Table I.\n\nOCR Text: \"0.0010 0.0005 0.Oooo ~0.0005 -0.0010 -0.0015 112 -0.8 -0.4 04 0.8 B (T)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: Plot of the voltage difference Vxy between electrodes X2 and Y2 of the Hall bar vs. the"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Two-dimensional partial covariance mass spectrometry (2D-pC-MS) is based on calculating total ion count (TIC)-based partial covariance map of the MS/MS spectrum across multiple repeated scans. 3 By identifying signals in the MS/MS spectrum which synchronously rise and fall in intensity across repeated measurements, 2D-pC-MS is able determine pairs of fragment ions which were produced along the same or consecuitive fragmentation pathways of the parent molecule. These signals can be identified by positive islands of partial covariance on the 2D partial covariance map. The partial covariance represents the correlation between two signals once spurious 'common-mode correlations' -which cause all spectral signals to correlate to all others as a results of global scan-to-scan fluctuations in external experimental parameters -have been suppressed. 25 Within 2D-pC-MS, this is done by using the TIC as a single partial covariance parameter, representing a good approximation to the compound effect of the many experimental parameters which fluctuate from scan to scan. 3 The fidelity of correlation islands appearing on a 2D-pC-MS map is assessed using the 2D-pC-MS correlation score, which is calculated by normalising the volume of a 2D-pC-MS island to the standard deviation of that volume upon jackknife resampling. This metric enables the identification of extremely low volume true correlation islands from higher volume statistical noise. 3 Correlation signals from a 2D-pC-MS map can be ranked according to the value of their 2D-pC-MS correlation score, and it is often instructive to plot the high-ranking true correlation features as a scatter plot (e.g. Figs. 1 and 2).\n- where V (x, y) is the volume of an island on the partial covariance map between m/z values x and y and σ[V (x, y)] is the standard deviation of this volume across a series of resamples of the original scan set (this is performed by jackknife resampling). 3 Higher 2D-pC-MS correlation scores identify higher fidelity 2D-pC-MS signals. In Figs. 1 and 2, the top-ranking 2D-pC-MS signals according to S(x,y) have been plotted on a scatter plot.\n\nMentions:\n- This metric enables the identification of extremely low volume true correlation islands from higher volume statistical noise. 3 Correlation signals from a 2D-pC-MS map can be ranked according to the value of their 2D-pC-MS correlation score, and it is often instructive to plot the high-ranking true correlation features as a scatter plot (e.g. Figs. 1 and 2).\n- In Figs. 1 and 2, the top-ranking 2D-pC-MS signals according to S(x,y) have been plotted on a scatter plot.\n\nOCR Text: \"UDGAIpLJUDuBW 84440\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: 2D-pC-MS map of 13+ ion of intact protein myoglobin from equine skeletal muscle (∼17 kDa, sequence shown above map). The top 100 2D-pC-MS correlation score-ordered (see Materials and Methods) signals are plotted on the scatter plot. The blue dashed lines are the mass conservation lines identified by the restricted Hough transform, showing different charge partitions across the complementary ion correlations which fall along them (blue dots). Identities of the annotated complementary ion correlation signals, which are each labelled on only one side of the autocorrelation line for visual clarity, are given in the table to the right. The alphabetical order of the annotated correlations reflects their ranking according to the 2D-pC-MS correlation score. The 2D-pC-MS search engine has correctly identified the intact protein ion from its top-down 2D measurement by assigning it a dominant similarity score (see inset histogram), using the charge state and m/z value of the the intact protein ion and its complementary fragments measured directly from the map. Note the logarithmic scale on the y-axis of the histogram."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- were θ 0 is in radians and g equals the earth gravitational acceleration. The effect of drag is represented by the sum, which arises from the series expansion of an exponential function. It is easy to see that only one or two terms are of practical importance. The value for the velocity decay rate κ is taken from Eq. (21). Figure 14 shows three calculations of the launch angle as a function of distance: the exact Runge-Kutta calculation, the approximate result according to Eq. (23) and the same without drag (κ = 0 in Eq. ( 23) ).\n- Fig. 14 Launch angle versus target distance. The solid line depicts the numerical solution of the ballistic equations with drag, using a 4 th order Runge-Kutta algorithm. The approximate treatment of drag is shown by short dashes. If there were no aerodynamic drag, the long-dashed line would result. The data points are the result of a measurement described in the text Usually, aiming with a bow is accomplished by establishing a line of sight containing the eye of the shooter, the aiming point of the bow sight and the center of the target. Often, the eye is aided by a peep sight, a small plastic ring, wedged between the strands of the bow string, directly in front of the archer's eye.\n- We are now in a position to measure the launch angle ( ). To this effect, we shoot at targets at various distances. For each distance we vary the sight position until we hit the center of the gold. Since no help from an Olympic archer is available, we use a shooting machine. These empirically determined sight positions, converted to a launch angle θ 0,exp are shown as points in Fig. 14. The agreement between the measurement and the Runge-Kutta calculation is surprisingly good. It supports the initial assumption that no aerodynamic lift is needed to explain our empirical findings. Ballistic theory does not replace careful shooting-in of the bow but is does provide insight. In particular, the explicit expression for the launch angle ( ), Eq.(23), makes it possible to study how to change the bow sight in response to a change in launch velocity, Δv 0 , aerodynamic friction, Δκ, or the shooting distance, ΔZ. To this effect, we calculate the vertical shift Δ ( ) of the target spot caused by such changes.\n\nMentions:\n- Figure 14 shows three calculations of the launch angle as a function of distance: the exact Runge-Kutta calculation, the approximate result according to Eq.\n- Fig. 14 Launch angle versus target distance.\n- These empirically determined sight positions, converted to a launch angle θ 0,exp are shown as points in Fig. 14.\n\nOCR Text: \"100 7 1 1 target distance (m)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 14 Launch angle versus target distance. The solid line depicts the numerical solution of the ballistic equations with drag, using a 4th order Runge-Kutta algorithm. The approximate treatment of drag is shown by short dashes. If there were no aerodynamic drag, the long-dashed line would result. The data points are the result of a measurement described in the text"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In figure 14 we represent the pion mass dependence of −R(G π 0 ) which is as well exponential. In general, the corrections are negative and for θ ∈ { 0, π/8, π/4, π/3 } their absolute value increases with the angle. In this case, the corrections are smaller than those at NLO. This can be explained if we look at the contributions O(ξ 2 π ), see eq. (5.45). For θ ∈ { 0, π/8, π/4, π/3 } the contribution originating from integral I (4) (G π 0 , π 0 ) is negative but that from I (4) (G π 0 , π ± ) is positive. As the negative contribution is smaller than the positive one, the corrections estimated with −R(G π 0 ) are smaller than those evaluated with −δG π 0 at NLO.\n\nMentions:\n- In figure 14 we represent the pion mass dependence of −R(G π 0 ) which is as well exponential.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 14. Pion mass dependence of −R(Gπ0)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- then following the last observations regarding the prelogs of I 1 and I 2 , it can be seen that the information rate I converges in the high SNR regime to a finite value that is independent of σ 2 . This is not surprising due to the obvious fact that I ≤ H 2 (p). Fig. 6 shows the achievable rate for p = 0.2. It is evident that due to the fading induced by U , there is a significant decrease in the achievable rate.\n- Fig. 9 shows the achievable rate as a function of q 1 and the SNR, for p = 0.2 and q 2 = 0.3. It can be seen that the result is similar to Fig. 6, that is\n\nMentions:\n- Fig. 6 shows the achievable rate for p = 0.2.\n- It can be seen that the result is similar to Fig. 6, that is\n\nOCR Text: \"36? - ICB 86? - IECB Rauru;\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 6: Achievable rate when the sparsity pattern is carrying the information, as a function ofq and the SNR, for p = 0.2."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Thus, the external resistance introduces a coupling between all points at the electrode surface. This coupling is both global, as only the spatial average of the current is relevant, and linear. The behavior of the electrodissolution of n-doped silicon for sufficiently high illumination is identical to that of p-doped silicon [20,21]. This can be explained by the fact that the amount of holes in the valence band of the silicon is in this case always sufficient to maintain the current determined by the electrochemical parameters. In both cases the oscillations arise from a Hopf bifurcation occuring at a minimal, electrolyte specific threshold value of the external resistance [22] as shown in visible that with an increase of the external resistance first stable, sinusoidal oscillations with amplitudes increasing with the external resistance are found. This is the expected behavior close to a Hopf bifurcation. Upon further increase of R ext the shape of the oscillations then gradually changes towards a more relaxational type. Above another electrolyte specific threshold value of R ext the oscillations vanish abruptly and instead the system relaxes to a stable node. The upper boundary of the oscillatory regime with respect to the external resistance can be well understood by comparing the voltage drop across the interface in this case as shown in Fig. 4 to the CV scan shown in Fig. 1. The latter shows that the value of ∆φ int for R ext above the oscillation boundary is in the voltage region where no stable oxide can form. The extent of the oscillatory regime is thus determined by a Hopf bifurcation at the low coupling limit and a cut-off caused by leaving the experimental parameters for a stable oxide in the high coupling limit.\n\nMentions:\n- The upper boundary of the oscillatory regime with respect to the external resistance can be well understood by comparing the voltage drop across the interface in this case as shown in Fig. 4 to the CV scan shown in Fig. 1.\n\nOCR Text: \"Lricir 03ft 6ulcm: 3404CM\" Gckm <ci\" (vs. 5-EE;\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4: Phase space plots of spatially averaged time series measured during the photoelectrodissolution of highly illuminated n-doped silicon (pH 1, cF = 75 mM, U = 8.65 V vs. SHE) with varying Rext."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The range of validity is x, α ∈ R, x &gt; 0. By ratio test the generalized series Figure 1: The generalized exponential series as a function of x with α as a parameter, excluding the origin, where it is singular converges absolutely when α &gt; 0. Figure 1 indicates how the family of curves saturates when α &gt; 2.5. The limit comes out as\n\nMentions:\n- By ratio test the generalized series Figure 1: The generalized exponential series as a function of x with α as a parameter, excluding the origin, where it is singular converges absolutely when α &gt; 0.\n- Figure 1 indicates how the family of curves saturates when α &gt; 2.5.\n\nOCR Text: \"LAJ t1 tAr #AJ &al\"6 4A2 #A` #Ab ta4} 487 #+? XtI ta4# 4FlLXt\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: The generalized exponential series as a function of x with α as a parameter, excluding the origin, where it is singular"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 6 plots the outage probability as a function of the nodes distance from the intersection, considering ACN, CCN, and cooperative NOMA. We assume that the distance between , 1 , and 2 does not change through the simulation. Hence, the nodes of triplet { , 1 , 2 } move together towards the intersection. We set ‖ − 1 ‖ = ‖ − 2 ‖ = 100 m. We can see from Fig. 6, that as the nodes come closer to the intersection (200 m for 1 and 500 m for 2 ), the outage probability increases. This is because, when the nodes are at the intersection, the interfering vehicles form the road and the road both contribute to the aggregate interference, which decreases the SIR at the receiving nodes. We can also see that, ACN outperforms both CCN and cooperative NOMA at the intersection. However, we can see that there is a big gap in performance between ACN and CCN regarding 2 . This is because, the spectral efficiency of CCN decrease drastically for high data rates. This is why, ACN protocol offers a better performance for high data rates compared to CCN. Finally, we can see in CCN and cooperative NOMA, that the outage probability increases more in the last 10 m. However, there is no increases in the outage probability when using ACN. Fig. 7 plots the outage probability as a function pf 1 , considering ACN, CCN, cooperative NOMA, and cooperative OMA. We can see from Fig. 7, that ACN outperforms CCN and cooperative NOMA and cooperative OMA regardless of 1 value. We can also see that when 1 increases, the outage probability of 1 decreases, whereas the outage probability of 2 decreases.\n\nMentions:\n- However, there is no increases in the outage probability when using ACN. Fig. 7 plots the outage probability as a function pf 1 , considering ACN, CCN, cooperative NOMA, and cooperative OMA.\n- We can see from Fig. 7, that ACN outperforms CCN and cooperative NOMA and cooperative OMA regardless of 1 value.\n\nOCR Text: \"Aeu, R,-0,5 bits; $, Rz-2 bits $ Cecf cicu cutmnnae UAFiE P Cerramteon? L 4n441ane 1 Power allocalion coefficicnt peI0 734,\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 7: Outage probability as a function of the power allocation coefficient a1, considering ACN, CCN, cooperative NOMA, and cooperative OMA."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We have also carried out quantum chemical analysis with dispersion correction (DFT-D) to understand the interactions of the CNT with the DNA and RNA. The valencies of the carbon atoms at the ends of the (6, 6) CNT were satisfied by adding necessary hydrogen atoms. The structural features that distinguish RNA from DNA are the presence of uracil base and 2'-OH groups of the ribose sugars. The four systems modeled by us are as follows: (i) (6,6) CNT with one uracil nucleobase, (ii) (6, 6) CNT with one thymine nucleobase, (iii) (6, 6) CNT with one uridine nucleoside (uracil attached with C3'-endo ribose sugar) and (iv) (6,6) CNT with thymidine nucleoside (thymine nucleobase attached with C2'-endo deoxyribose sugar) as shown in Fig. S3(a). These initial structures were built using MOLDEN 53 software. Initially the nucleobases were placed in parallel to the CNT surface. Free geometry optimization of all the four systems discussed above were carried out using Gaussian09 54 without any constraints using density functional theory with WB97XD/6-31g(d,p) basis set 55 , which includes dispersion correction, giving rise to energy CNT, giving rise to energies E X0 , the two nucleobases and the two nucleosides in un-complexed isolated form having energy E Y0 . Optimizations of such supramolecular complex systems are associated with a Basis Set Superposition Error (BSSE), which mainly arise due to overlapping of the optimized orbitals. This error has been corrected by Boys-Bernardi function counterpoise method 56 . The BSSE corrected interaction energies (E int ) of each system were calculated using E int. = E XY -E X0 -E Y0 + BSSE. We have also carried out the Mulliken charge analysis of all the optimized systems to analyze the charge transfer taking place, if any, between the CNT and the associated nucleobase/nucleoside.\n- To understand the thermodynamics of the binding events, we have calculated van der Waals interaction between the siRNA and the (6, 6) CNT surface at various time intervals, shown only for 5 ns and 15 ns in Fig. 3. Different contributions to the total energy of the siRNA such as electrostatic interaction, van der Waals interaction and bonded interaction energies were analyzed.\n- We find that the van der Waals interaction is responsible for the observed unzipping and wrapping of the siRNA. Other contributions to the total energy do not contribute much to the binding as a function of time. Entropy calculations support this inference as will be discussed in section 3.2. In Fig. 3, r = 0 corresponds to the centre of mass of siRNA as shown in the schematic diagram. The interaction coordinate r was projected along the nanotube axis n . The van der Waals interaction energy between aromatic rings of siRNA and CNT surface (   r  \n\nMentions:\n- The four systems modeled by us are as follows: (i) (6,6) CNT with one uracil nucleobase, (ii) (6, 6) CNT with one thymine nucleobase, (iii) (6, 6) CNT with one uridine nucleoside (uracil attached with C3'-endo ribose sugar) and (iv) (6,6) CNT with thymidine nucleoside (thymine nucleobase attached with C2'-endo deoxyribose sugar) as shown in Fig. S3(a).\n- To understand the thermodynamics of the binding events, we have calculated van der Waals interaction between the siRNA and the (6, 6) CNT surface at various time intervals, shown only for 5 ns and 15 ns in Fig. 3.\n- In Fig. 3, r = 0 corresponds to the centre of mass of siRNA as shown in the schematic diagram.\n- This asymmetry in the interaction can result in the motion of siRNA on the nanotube (seen in Fig. 3) until it finds its optimal binding position.\n- As discussed previously, Fig. 3 signifies such van der Waals interaction strength and its impact on siRNA binding.\n\nOCR Text: \"ISns\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: van der Waals interaction energy between siRNA and (6, 6) CNT at 300 K. See the text for details of the calculation. The siRNA strongly binds to CNT at 15 ns compared to 5 ns with an increment in van der Waals energy of about 133 kcal/mol or 225.4 kBT."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Fig. 5, we show the resulting binding energies behaviour for nucleon in quenched and 2-flavor QCD. We can see there is little difference between the two lines. When they vanish, the nucleon no longer exists. So ∆E(T ) = 0 determines the dissociation temperature. From Fig. 5, we get the dissociation temperature in quenched and 2-flavor QCD are about 3.0T c and 3.3T c , respectively; in Fig. 6, we show the corresponding nucleonic radii. The dissociation temperature determined from Fig. 6 is consistent with that determined from Fig. 5. It is seen that the divergence of the radii defines quite well the different dissociation points in the two cases. The resulting dissociation temperatures have a little difference between the two cases.\n\nMentions:\n- In Fig. 5, we show the resulting binding energies behaviour for nucleon in quenched and 2-flavor QCD.\n- From Fig. 5, we get the dissociation temperature in quenched and 2-flavor QCD are about 3.0T c and 3.3T c , respectively; in Fig. 6, we show the corresponding nucleonic radii.\n- The dissociation temperature determined from Fig. 6 is consistent with that determined from Fig. 5.\n\nOCR Text: \"2-flavar QCD querchec QCD\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 5. T-dependent of binding energy in quenched and 2- flavor QCD, respectively."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- For nanopore diameters greater than about 4 nm, the surface charge density is roughly constant with σ ≈ . − 0 02 C m 2 (as shown in Fig. 7, circles). This value is low with respect to the one estimated from experimental data for transmembrane boron nitride nanotubes 18 (t-BNNT). This is not surprising, since the TMS function is supposedly not charged, contrary to the t-BNNT surface, which appears to exhibit an extremely high negative surface charge density at basic pH (~1 C m −2 for pH &gt; 10). The surface charge density of 0.02 C m −2 obtained above is very close to the one reported by Shimizu 21 et al. (0.015 C m −2 ) for native PDMS nanopores (sub-10 nm diameters). In view of the similar chemical structures for PDMS and TMS surfaces, similar values of surface charge density are coherent. This can be due to the adsorption of some OH − ions at the interface, as already observed at the water/air interface in Refs. 43-45. For nanopore diameters smaller than 4 nm, the surface charge density extracted from the model increases by about a factor of about 3 for D= 0.76 nm. One explanation for this behavior may come from the experimental nanopore design. Indeed, HDMS used for nanopore coating is performed under gas conditions; for small nanopore diameters, however, this coating might not be as homogeneous as it would be for larger nanopore diameters, due to a weaker permeability of the gas.\n- 120 nm. Although MD simulation results indicate that electro-osmotic flows through sufficiently highly charged nanopores do not exhibit slip 37,39 , the surface charge densities used in these studies were nearly an order of magnitude higher than those obtained above using our mesoscopic model without slip. The importance of slip for electro-osmotic flow in weakly charged nanopores is therefore a still open question. In order to examine the role of flow slip, we have thus analyzed the experimental conductance by adding the slip contribution to the advective one in Eq. (13), Figure 6. Experimental conductance data of Fig. 3 fitted using the hybrid mesoscopic approach: (a) conductance and (b) conductivity with slip length b = 0, using Eq. ( 13), (c) Conductance and (d) conductivity with slip length b = 30 nm, using Eqs. ( 13) and (14). The straight black line in figures (b) and (d) is the bulk conductivity (without the bulk high salt concentration correction factor, see Eq. 1 and below). with = b 30 nm to extract a modified nanopore surface charge density σ as a function of R (see Table 1 and Fig. 6c and d). We observe in Fig. 7 that the fitted surface charge densities are 3 to 6 times weaker for b = 30 nm than for the case with no slip and that the ratio between the highest and lowest values is reduced from 4 to 2 when slip is taken into account (for D &gt; 4 nm, σ ≈ . − 0 005 C m 2 ). Because these trends make sense physically, we are tempted to conclude that for weakly charged nanopores introducing slip may be important in understanding the fitted surface charge densities from a microscopic point of view (e.g., adsorption of OH − ions at the pore surface). Note that the homogeneous approximation we used is reasonable since, without slip, the fitted values of σ correspond to 0.4 &lt; σ* &lt; 1.2 for the nanopores NP 1 to NP 10 , and 0.05 &lt; σ* &lt; 0.4 when slip is taken into account. These values are sufficiently low for the homogeneous approximation to be a reasonable one. The importance of the error incurred in the advective contribution without slip for the larger diameter nanopores (for which σ* = 0.09 and 1.20) is minimized because of the weaker role of advection compared with electrical migration (&lt; 30%) over the whole concentration range. In the case of slip (with b = 30 nm), the slip contribution to the advective term dominates at low salt concentration, where it is 2 to 6 times greater than the electrical migration one.\n\nMentions:\n- For nanopore diameters greater than about 4 nm, the surface charge density is roughly constant with σ ≈ . − 0 02 C m 2 (as shown in Fig. 7, circles).\n- We observe in Fig. 7 that the fitted surface charge densities are 3 to 6 times weaker for b = 30 nm than for the case with no slip and that the ratio between the highest and lowest values is reduced from 4 to 2 when slip is taken into account (for D &gt; 4 nm, σ ≈ . − 0 005 C m 2 ).\n\nOCR Text: \"Bhu]\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 7. Surface charge of the nanopore as a function of nanopore diameter for b = 0 nm (circles) and b = 30 nm (squares): extracted from the mesoscopic model fits using Eqs. 13 and 14."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The strongest parabolic characteristics was found in the H2I scenario with a p 2 value of around 4. A rather exceptional behavior is found for scenario TCT in Fig. 7, where the waveguiding effect of tunnels lead to a distance-independent Poisson parameter. The exceptionally high MSE values of the UCT scenario are due to high variances at distances below 200 m. We conducted the goodness-of-fit χ 2 -test and an MSE estimation for both distribution functions. Due to the distance dependency, this was done per 10 m distance bin. The χ 2tests result in a null hypothesis rejection rate of 13.74% for the Poisson distribution and a rejection rate of 20.60% for the Normal distribution (5% significance level) . The average MSE N of the Poisson distribution fitting is 9.8 • 10 −3 , ranging from 2.9•10 −3 for TCT to 13.9•10 −3 for HOT (see Table IV). The average MSE of the Normal distribution fit per distance bin is 1.41 • 10 −2 , ranging from 0.31 • 10 −2 for TCT to 2.86•10 −2 for HOT. Hence, the Poisson distribution is a better choice both in terms of goodness-of-fit and accuracy.\n\nMentions:\n- A rather exceptional behavior is found for scenario TCT in Fig. 7, where the waveguiding effect of tunnels lead to a distance-independent Poisson parameter.\n\nOCR Text: \"1 Mcd LCm;IJoin Mersx dta PaNMerer maccl 2 1 Distance Im} 1\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 7. Distance dependence of the Poisson parameter λ for the number of MPCs in the TCT scenario."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- However, one can see that I's MSE-PT strategy does not completely coincide with its MSE even for high values of γ U . This is due to the fact that even when U 's probability weighting is closer to full rationality, the way U values the possible game outcomes (i.e. the possible delivery times) is based on its reference point R U and value function. Hence, even with a closely rational probability weighting, U 's MSE-PT may not equal its MSE strategy. This can, indeed, be seen from Fig. 8, which shows that even for γ U = 0.9, U 's MSE-PT strategy is different from its MSE strategy. Fig. 8 shows how U 's MSE-PT strategy changes with an increase in γ U . At lower values of γ U , U 's MSE-PT strategy consists of path 9, i.e. (3,5,9), while at higher values of γ U , U 's MSE-PT strategy shifts to choosing path 11, i.e (3,6,8). As shown in Fig. 7, at lower values of γ U , I's optimal strategy is focused on nodes 5 and 8 making path 9, chosen by U at the MSE-PT, highly risky. However, U still chooses this path, at the MSE-PT, since at such low values of γ U , U 's valuation of probabilities is highly distorted. In fact, the weighting functions ω + U (.) and ω − U (.) flatten for lower values of γ U . Hence, U would assess different paths as almost equally risky leading U to choose path 9. However, when γ U increases, U 's perception of probabilities becomes more rational. Hence, for these values of γ U , U can observe that path 9 is highly risky and chooses instead the safer path 11, composed of nodes (3,6,8) which are not attacked with a high probability by I at the MSE-PT.\n- Fig. 9a shows the resulting expected delivery times at the MSE and at the MSE-PT for various values of γ U . From Fig. 9a, we can see that the MSE-PT strategies result in expected delivery times that are longer than the expected delivery time achieved at the MSE. Indeed, for γ U = 0.25, the percentage difference between the expected delivery time at the MSE-PT and that at the MSE goes up to +21.5%. The reason is that, as shown in Fig. 8  As can be seen in Fig. 9a, a deviation from the MSE-PT path to the shortest path would have been advantageous to U as it would lead to a lower expected delivery time for the entire investigated range of γ U . However, as U subjectively assesses expected delivery times under PT, the choice of the MSE-PT path is valued to be better than choosing the shortest path, as shown in Fig. 9b, as the MSE-PT path leads to a lower PT valuation. Hence, this further highlights the negative effect that the subjective PT perception of U can have on its achieved expected delivery time. The rational response as well as the MSE strategies both lead to a better expected delivery time than the shortest path and the MSE-PT strategies, as shown in Fig. 9a. Fig. 10a shows the resulting expected delivery times at the MSE and at the MSE-PT, for the various values of λ U ∈ {1, 2.5, 5}. Fig. 10a also shows the expected delivery time achieved when U plays the rational response strategy, or the shortest path, as a reaction to I choosing its MSE-PT strategy. Fig. 10a shows that the MSE-PT strategies chosen at different values of λ U result in an expected delivery time that is only slightly higher than the one achieved at the MSE. At higher values of λ U , this difference in expected delivery times decreases. Indeed, at λ U = 1, the percentage difference between the MSE-PT and the MSE expected delivery times is +4.14% while this difference drops to only 1.3% at λ U = 5. However, when U plays a rational response strategy, in response to I's MSE-PT strategy (which consists of choosing path 12 for all the three values of λ U , i.e. 1, 2.5 and 5), U can achieve an expected delivery time that is up to 11% lower than the expected delivery time achieved at the MSE. Choosing the shortest path by U would lead to a better expected delivery time only for λ U = 1. Fig. 10b shows U 's valuation of the MSE-PT strategies as compared to choosing the shortest path, which highlights the reason for which a deviation from the MSE-PT path to the shortest path is not valued to be advantageous by U as it leads to an increase in the valuation. In all cases, choosing the rational response is the most advantageous to U , as shown in Fig. 10a.\n\nMentions:\n- This can, indeed, be seen from Fig. 8, which shows that even for γ U = 0.9, U 's MSE-PT strategy is different from its MSE strategy.\n- Fig. 8 shows how U 's MSE-PT strategy changes with an increase in γ U .\n- The reason is that, as shown in Fig. 8  As can be seen in Fig. 9a, a deviation from the MSE-PT path to the shortest path would have been advantageous to U as it would lead to a lower expected delivery time for the entire investigated range of γ U .\n\nOCR Text: \"Ralional (MSE: (MSE-P [\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 8. U ’s equilibrium path selection strategy for different γU=γ−U=γ + U ."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Section 4 describes the on-host portion of the new solution, which solves the first problem mentioned in Section 2, that of recording insufficient data at host-level. The second problem was the large latency between an error being detected on a host and a ticket being generated. In LinkedIn, there is separate monitoring infrastructure in place that is used by application SRE teams to monitor app-level performance metrics. This infrastructure already includes the components required to send metrics to a single source for plotting, thresholding, alerting and ticket generation. This pipeline has an end-to-end latency of up to 5 minutes, from on-host data collection, to an alert being generated. This is a significant improvement over the extant system, and is sufficiently low for our use case. Thus, we can use the existing pipeline, without having to develop a new solution. Since this infrastructure was not developed as part of the effort described in this paper, we provide a short summary of this. Figure 1 provides a high level view of the monitoring pipeline employed as part of our solution.\n\nMentions:\n- Figure 1 provides a high level view of the monitoring pipeline employed as part of our solution.\n\nOCR Text: \"Tuce\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: System architecture"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Returning to Figure 1, it is clear that higher resolution runs are associated with higher relaxed current densities. Furthermore, an even higher transitory current peak is visible in both cases. We do not investigate this phenomenon further in this study, since we are interested mainly in the near-singular, relaxed state. It seems worth remarking, however, that resistive relaxation in a full MHD configuration often involves inertial overshoots that lead to oscillatory null-point reconnection (see, e.g., Craig &amp; McClymont 1991;Craig &amp; Watson 1992). It is likely therefore that the large transient currents in the pseudo time evolution may reflect the strong initial implosion of the disturbance field towards the null.\n- In the runs of Figure 1 the null point lies inside the computational domain D at the point (0, 0, −3/4), following from our parameter choice b = 0.3, µ = −0.4. We expect therefore to see current density distributions concentrated about the null. Figure 2 confirms this expectation, as illustrated by the red isosurface. The black field lines give the fan structure around the null, aligned roughly to the y-z-plane. The blue isosurface legs of weaker current, aligned to the x-and y-axis, respectively, indicate that spine and fan currents form simultaneously (cf. the exposition in §2.2). Further away from the null, in the upper part of the domain, the field lines illustrate the additional QSL structure of the field, associated with steep field line gradients aligned to the z-axis. The blue isosurface confirms the simultaneous formation of currents along the QSL with a magnitude similar to the fan and spine currents.\n\nMentions:\n- Returning to Figure 1, it is clear that higher resolution runs are associated with higher relaxed current densities.\n- In the runs of Figure 1 the null point lies inside the computational domain D at the point (0, 0, −3/4), following from our parameter choice b = 0.3, µ = −0.4.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1. Pseudo-time evolution of the maximum current density during the relaxation to a force-free equilibrium for two different resolutions (N = 61, black and N = 81, red). Time T has been independently normalized in both cases to give comparable values of order one."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We estimate the tunneling current with the non-equilibrium Green's function approach (NEGF) combined with DFT calculations 32 , along with the WKB approximation. The NEGF+DFT approach may give the transmission coefficient, , and further the tunneling current is obtained by Landauer formula 33 . Figure 3 shows the tunneling current of holes in the zigzag direction, and   sm ME  , from the intrinsic phosphorene to the graphene (inset). Under an increasing forward bias but lower than bi V , the current is found to increase exponentially. Higher doping concentration yields a shrunk Schottky diodes are stacked up with a spacing of 2 nm, at 0.1 V the current density could be 10 4 times higher than that of a traditional 3D W-Si junction (2.9 mA/cm 2 ) 34 .\n\nMentions:\n- Figure 3 shows the tunneling current of holes in the zigzag direction, and   sm ME  , from the intrinsic phosphorene to the graphene (inset).\n\nOCR Text: \"5*10' 0o9y 1 4 54*m\" Ereay @ 10 1 Va (V) \"4n; Joooooooooo 100000000d0000o\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3 The quantum transport of holes in the zigzag direction of phosphorene under forward bias. Inset shows the intrinsic transmission coefficient of the graphene-phosphorene junction in the absense of carriers at zero-bias regime."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Ts has a fractional part with a relatively small integer denominator, the variations in the RDF are significant, and the variations depend on the sampling time offset. However, when Tps Ts approaches an irrational number, the period of the sampled variance function becomes very long, and consequently, the RDF is approximately constant and independent of the sampling time offset. As an example, consider Tps Ts = 2.5 and t dc = 75%: For sampling time offset φ = 0 the RDF takes a value of 1.469 bits per sample, as shown in Figure 4 while for the offset of φ = 1 16 the RDF peaks to 1.934 bits per sample as we see in Figure 5. On the other hand, when approaching asynchronous sampling, the RDF takes an approximately constant value 1.85 bits per sample for all the considered values of Tps Ts and this value is invariant to the offsets of φ. This follows since when the denominator of the fractional part of Tps Ts increases, then the DT period of the resulting sampled variance, p n , increases and practically captures the entire set of values of the CT variance regardless of the sampling time offset. In a similar manner as with the study on capacity in [17], we conjecture that since asynchronous sampling captures the entire set of values of the CT variance, the respective RDF represents the RDF of the analog source, which does not depend on the specific sampling rate and offset. Figures 4-5 demonstrate how slight variations in the sampling rate can result in significant changes in the RDF. For instance, at φ = 0 we notice in Figure 4 that when the sampling rate switches from T s = 2.25 • T ps to T s = 2.26 • T ps , i.e., the sampling rate switches from being synchronous to being nearly asynchronous, and the RDF changes from 1.624 bits per channel use to 1.859 bits per sample for t dc = 75%; also, we observe in Figure 5 for t dc = 45%, that when the sampling rate switches from T s = 2.5 • T ps to T s = 2.51 • T ps , i.e., the sampling rate also switches from being synchronous to being nearly asynchronous, and the RDF changes from 1.005 bits per source sample to 1.154 bits per source sample.\n\nMentions:\n- As an example, consider Tps Ts = 2.5 and t dc = 75%: For sampling time offset φ = 0 the RDF takes a value of 1.469 bits per sample, as shown in Figure 4 while for the offset of φ = 1 16 the RDF peaks to 1.934 bits per sample as we see in Figure 5.\n- For instance, at φ = 0 we notice in Figure 4 that when the sampling rate switches from T s = 2.25 • T ps to T s = 2.26 • T ps , i.e., the sampling rate switches from being synchronous to being nearly asynchronous, and the RDF changes from 1.624 bits per channel use to 1.859 bits per sample for t dc = 75%; also, we observe in Figure 5 for t dc = 45%, that when the sampling rate switches from T s = 2.5 • T ps to T s = 2.51 • T ps , i.e., the sampling rate also switches from being synchronous to being nearly asynchronous, and the RDF changes from 1.005 bits per source sample to 1.154 bits per source sample.\n\nOCR Text: \"2 1045 0=0 075,0=0 Scmpling literval sigiLL pctioc En\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4. Rn(D) versus Tps Ts ; offset φ = 0."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- During the optimization, indirectly, the D-VIB algorithm allocates the complexity for each description U k of the observation X k . Figure 9 shows the resulting accuracy achieved with D-VIB 1 shot to estimate Y with the main estimator Q Y |U1,U2 obtained in Figure 2, as well as the accuracy, achieved by using the regularizing decoders\n\nMentions:\n- Figure 9 shows the resulting accuracy achieved with D-VIB 1 shot to estimate Y with the main estimator Q Y |U1,U2 obtained in Figure 2, as well as the accuracy, achieved by using the regularizing decoders\n\nOCR Text: \"Lrrodr (4.03 1-R Ma Utl Mud Lud s 3 Bk Luu L7 [et H-E + Keuwantatm\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 9: Train and test accuracy for the two-view MNIST dataset with K = 2 encoders, from the estimators using both (U1, U2), QY |U1,U2 , and only U1 or U2, i.e., QY |U1 and QY |U2 , from the application of the D-VIB algorithm for n = 50.000 and s ∈ [10−10, 1]."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The DSDIN network consists of a public blockchain and a myriad of independent state machines (also known as wormholes). The wormholes surround the blockchain and result in a large number of state channels, as shown in Fig. 1. In the blockchain design for DSDIN, we believe that it is not necessary to maintain the state in the blockchain. We only need to store the state information on the wormhole and use the blockchain to deal with the economic consequences of any information exchange, including money transfer, mining, etc., as well as the fallback in case of dispute. So, in the DSDIN network, we propose another blockchain architecture that places Turing's complete smart contract on the wormhole, rather than the blockchain. This increases the extensibility of the system and the throughput of the transaction, because a wormhole is an independent state machine (a single process or process tree based on DSDIN VM), making all transactions become independent and enabling parallel processing. In addition, this also means that the contract is not written to the shared state, greatly simplifying the testing and verification. At the same time, this separates the economic logic from the data storage, allowing us to supplement the blockchain with a good distributed storage scheme, improving the efficiency, privacy and security of data storage in the physical world.\n- As shown in Fig. 1, each wormhole is a stand-alone virtual machine (DSDIN VM) that implements smart contracts, ISA and actual transactions through state channels, with the transaction information finally recorded on the blockchain. Each wormhole is similar to a process in a computer, protecting its own state data, including: Data generated during running, data in the database and data acquired through interfaces (such as HTTP, TCP, Email, Serial Port etc.). The wormholes exchange information with each other via Message-Passing but do not share data so as to keep their independence, similar to the standard Actor-Model program [24][25][26]. This allows transactions in the entire system and ISA to realize highthroughput running and also guarantees the security of private wormhole data.\n\nMentions:\n- The wormholes surround the blockchain and result in a large number of state channels, as shown in Fig. 1.\n- As shown in Fig. 1, each wormhole is a stand-alone virtual machine (DSDIN VM) that implements smart contracts, ISA and actual transactions through state channels, with the transaction information finally recorded on the blockchain.\n\nOCR Text: \"HMM Cc Fc Cnl51ed\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1. The blockchain and wormhole in DSDIN. Note that wormholes communicate with each other by Message-Passing and only the information regarding transactions happening in wormholes is stored onto blocks."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- At any epoch t in the GD dynamics a binarized configurationŴ t i = sign (m t i ) can be used to compute the training errorÊ t = 1 M H Ŵ t . We consider a training set D where each input component x µ i is sampled uniformly and independently in {−1, 1} (with this choice we can set y µ = 1 ∀µ without loss of generality). The evolution of the network during GD is shown in Fig. 1. The training error goes progressively to zero while the mean squared norm of the control variables q t = 1 N i (m t i ) 2 approaches one. Therefore the distribution Q m concentrates around a single configuration as the training is progressing. This natural flow is similar to the annealing of the coupling parameter manually performed in local entropy inspired algorithms [25,26]. We also show in Fig. 1 the probability over the realizations of D of finding a solution of the binary problem as function of the load α = M/N . The algorithmic capacity of GD is approximately α GD ≈ 0.63. This value has to be compared to the theoretical capacity α c ≈ 0.83, above which there are almost surely no solutions [9], and state-of-the-art algorithms based on message passing heuristics for which we have a range of capacities α M P ∈ [0.6, 0.74] [12,22,29]. Therefore GD reaches loads only slightly worse than those reached by much more fine tuned algorithms, a surprising results for such a simple procedure. Also, for α slightly above α GD the training error remains comparably low, as shown in Fig. 1. In our experiments most variants of the GD procedure of Eq. (7) performed just as well: e.g. SGD ors GD computed on the fields h t i = tanh −1 (m t i ) rather than the magnetizations [30]. Other updates rules for the control parameters can be derived as multiple pass of on-line Bayesian learning [31,32]. Variations of rule (7) towards biological plausibility are discussed in the SM.\n- In Fig. 1 we show the stability line β c (q ), defined by the condition αγ E γ S = 1, for different values of α. Due to numerical problem arising in computing integrals at high β, we explore a small q window. We note that β c (q ) stays finite in the range of parameters we explored and that the β ↑ ∞ limit of the RS solution cannot be taken carelessly. Nonetheless the β c (q ) is generally quite high, although decreasing with α. In the main text, where we presented the results for α = 0.55, we set the inverse temperature to β = 20, where the RS results are supposedly correct and quantitatively close to the β = +∞ limit.\n\nMentions:\n- The evolution of the network during GD is shown in Fig. 1.\n- We also show in Fig. 1 the probability over the realizations of D of finding a solution of the binary problem as function of the load α = M/N .\n- Also, for α slightly above α GD the training error remains comparably low, as shown in Fig. 1.\n- In Fig. 1 we show the stability line β c (q ), defined by the condition αγ E γ S = 1, for different values of α.\n\nOCR Text: \"FSi sa Il; ie3 3 082 0 3 08 09 3\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1. Critical value βc for the stability of the RS solution for different loads α = 0.5, 0.55, 0.6 (from top to bottom) as a function of q?. Above βc the RS solution is unstable and a replica symmetry breaking ansatz should be considered to obtain the correct solution."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- An overview of the results is presented in Table 2. The test data produced by automated curation contains images with varying levels of diagnostic quality. Testing on all levels (low-high quality) results in poor performance. Using mediumhigh quality immediately improves all metrics. The multitask loss provides a considerable improvement and the weighted multitask approach (λ ∝ L CHD , Eq. 4) further improves performance. Using robust inference (described in Section 3.4) helps to remove predictions that are deemed less reliable. The fact that performance increases indicates that robust inference is effective in identifying and excluding predictions that are less accurate. A closer examination of robust inference is given in Figure 2. Using regular inference (blue), the ROC-AUC becomes severely compromised when including low quality images. However, the performance can be recovered by using robust inference (green) to automatically determine when a prediction is less reliable.\n- The white hatched bars (Figure 2) indicate the percentage of images which are deemed reliable in each set. The percentage of reliable images increases as the expert-rated quality increases. This indicates that the proposed reliability Fig. 2: ROC-AUC comparison of regular and robust inference. Including low quality frames quickly leads to a sharp decline in performance (blue). Robust inference can help to identify frames that are more reliable (white hatch, right y-axis). Evaluating only reliable frames leads to more accurate overall predictions (green).\n- We find that robust inference is an important component because the automated curation pipeline includes many images of low diagnostic quality. Typically, these frames would have to be manually vetted in order to reduce the noise in the overall prediction. However, robust inference can serve as a filter to remove predictions from low quality images. For low-high quality images, robust inference improves the ROC-AUC from 0.83 to 0.92. This is on par with the ROC-AUC achieved with regular inference on only high quality images (0.92). This is highlighted in Figure 2 with a red dotted line.\n\nMentions:\n- A closer examination of robust inference is given in Figure 2.\n- The white hatched bars (Figure 2) indicate the percentage of images which are deemed reliable in each set.\n- This indicates that the proposed reliability Fig. 2: ROC-AUC comparison of regular and robust inference.\n- This is highlighted in Figure 2 with a red dotted line.\n\nOCR Text: \"Minitkid RElar: Ftletne%are] 14i Et bweim)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2: ROC-AUC comparison of regular and robust inference. Including low quality frames quickly leads to a sharp decline in performance (blue). Robust inference can help to identify frames that are more reliable (white hatch, right y-axis). Evaluating only reliable frames leads to more accurate overall predictions (green)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We further examine the robustness of the different models w.r.t latent representation when the data is corrupted by additive Gaussian noise. In Figure 4 using non-linear dimensionality reduction techniques such as t-SNE, we visualize the 2D latent structure of the different models. As shown in Figure 4, even with corrupted data, the latent structure of the Fisher AE is still preserved and the clusters associated to different classes are relatively distinguishable. This is not the case for VAE and WAE-GAN where the clusters in the latent space are somewhat mixed up when the data is perturbed by noise. This behavior is quantitatively confirmed in Figure 3 where test data is perturbed with Gaussian noise with variance σ 2 , then encoded with each model encoder and projected with t-SNE and finally clustered using k-means. The qualtiy of clustering is measured using the normalized mutual information: NMI = 2I(Ω;C) H(Ω)+H(C) , where Ω is the model clusters for a given noise level, C is the true class labels, I(.; .) denotes the mutual information and H(.) denotes the entropy. Clearly, Fisher AE exhibits a better behavior in terms of clustering robustness where the decay in performance is nearly linear whereas for both VAE and WAE-GAN, the performance decays faster.\n\nMentions:\n- This behavior is quantitatively confirmed in Figure 3 where test data is perturbed with Gaussian noise with variance σ 2 , then encoded with each model encoder and projected with t-SNE and finally clustered using k-means.\n\nOCR Text: \"LeMaa 1\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Robustness of latent space clustering in terms of the normalized mutual information on MNIST test set."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The total loss rate diverges in the case of an everywhere-empty loss cone, m ⋆ → 0, but is finite for finite m ⋆ . The right panel of Figure 7 shows integrated loss rates for the same set of models. As expected,Ṅ attains a welldefined limit at low E, i.e. large a, in each model. These values are listed in Table 1, and plotted against m ⋆ in Figure 8. In three of the models, the adopted energy grid probably did not extend to low enough values of E to yield accurate values for the total loss rate; these numbers have been placed in parentheses in Table 1. It is useful to have an approximate analytic expression for the loss rate. Since most of the stars lost to the SBH are orbiting in the \"Newton\" regime prior to capture (Figure 7), it is reasonable to adopt the classical expressions for the angular-momentum diffusion coefficients at all energies. We simplify the derivation even more by (i) adopting for the density profile a Bahcall-Wolf cusp, unmodified by resonant relaxation and by loss-cone effects; and (ii) assuming that stars at any given energy are either in the empty-loss-cone, or the full-loss-cone, regimes at the time of capture. In the full-loss-cone (FLC) regime, f is assumed to be independent of R, f FLC (E, R) = f (E). In the empty-loss-cone (ELC) regime, equation (47) for f can be written\n- Equations ( 57) and ( 61) are the desired expressions. The predicted values forṄ FLC andṄ ELC are plotted as the curves in Figure 8. The agreement is quite good considering the approximations made; one implication is that the modifications to the Bahcall-Wolf cusp resulting from resonant relaxation have little effect on the total loss rate. The predicted ratiosṄ ELC /(Ṅ ELC +Ṅ FLC ) are given in the final column of Table 1; in all cases considered here, the two regimes contribute roughly equally to the total loss rate. We emphasize again that these results apply only to the model considered here, which does not include the contribution of the distributed (stellar) mass to the gravitational potential.\n\nMentions:\n- These values are listed in Table 1, and plotted against m ⋆ in Figure 8.\n- The predicted values forṄ FLC andṄ ELC are plotted as the curves in Figure 8.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 8.— Loss rates for the models of Figure 7. Scaling assumes M• = 4×106M⊙. Plotted points are the same numbers given in Table 1; dashed and dotted curves are the approximate analytic relations derived in the text for the full- and empty-loss-cone regimes respectively, and their sum is shown as the solid curves. The open circles are from numerical integrations in which the E- grid probably did not extend to low enough values to yield the correct, total loss rates (see the right panel of Figure 7)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The MP-DCCP solution has been implemented as a kernel module in the Linux kernel 4.4, (supporting up to 4.19) as a proof-of-concept based on the current DCCP implementation in the Linux kernel [28] and the CCID2 congestion control algorithm [13]. The MP-DCCP source code consists of mandatory source files that are required for basic MP-DCCP operation, and optional loadable modules which can be loaded or unloaded during a MP-DCCP session. Many of the concepts used in MP-DCCP originate from the MP-TCP project and have been verified extensively in terms of stability, performance and usability. The operation of the SRTT scheduler can be illustrated by increasing the latency of the preferred path during the transmission. As soon as the new latency value exceeds that of another network path, this network path will become the new preferred path. Two network paths have been set up to provide symmetric bandwidths of 10 Mbps. Additionally, the first path has been configured to 10 ms, and the second path to 20 ms of latency. In this scenario, the SRTT scheduler selects the first path. After 15 seconds, the latency on the first path has been increased from 10ms to 100 ms, forcing the scheduler to switch to the second path. Fig. 5 shows the bandwidths measured on both network paths, as well as the estimated SRTT of each path as reported by the congestion control algorithm. During the first two seconds of the measurement, when the congestion windows limit the capacity of the flows, the scheduler uses both paths to forward the incoming traffic. Once the congestion windows are established, the scheduler prefers the first path because of its lower estimated SRTT. At this point, the bandwidth demand can fully be satisfied by the first path, and no traffic is sent over the second path anymore. Once the SRTT has been increased from 10 ms to 100 ms on the first path, the scheduler reacts and switches all traffic to path 2. This behavior matches the theory of operation; the SRTT scheduler seems to be able to correctly identify and select the path with the lowest SRTT. The major advantage of this technique is that it does not introduce additional overhead; however, it is only effective as long as there is traffic on the network, whereas latency changes on unused paths remain undetected. This is easily noticeable in Fig. 5 where the SRTT over path 2 (blue dotted line) remains at a high value up until around 15 seconds when new traffic is again transmitted over the path.\n\nMentions:\n- After 15 seconds, the latency on the first path has been increased from 10ms to 100 ms, forcing the scheduler to switch to the second path. Fig. 5 shows the bandwidths measured on both network paths, as well as the estimated SRTT of each path as reported by the congestion control algorithm.\n- This is easily noticeable in Fig. 5 where the SRTT over path 2 (blue dotted line) remains at a high value up until around 15 seconds when new traffic is again transmitted over the path.\n\nOCR Text: \"Mile 4 tann 6iti '\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 5. Handover between network paths using the SRTT scheduler"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- A burning question is whether KO or BO scaling are applicable to the boundary layers of RBC. The flux arguments of Sec. 3.2.3 provide some insights into the dynamics of boundary layers. Here, typically u z u ⊥ , hence the flow is quasi-2D, and we expect an inverse cascade of KE. Using Π u (k) &lt; 0, F B (k) &gt; 0, and dΠ u (k)/dk ≈ F B (k), we may argue that Π u (k) may increase with k as shown in Fig. 11. An application of scaling arguments of Sec. 3.2.1 may yield spectra and fluxes according to Eqs. (67-70), i.e., Bolgiano-Obukhov scaling for k &lt; k B . For k &gt; k B , the KE spectrum may exhibit a mixture of k −5/3 (regime of inverse cascade of energy) and k −3 (regime of forward cascade of enstrophy) depending on where the effective forcing band lies in relation to k B . Thus, in the boundary layer, RBC may exhibit BO scaling, and it needs to be investigated carefully using numerical simulations and experiments. The aforementioned scaling arguments may work for 2D RBC (xz plane in which the buoyancy is along the z direction), as well as in quasi 2D RBC (when L x L y ). Toh and Suzuki [94] simulated 2D RBC and reported E u (k) ∼ k −11/5 and Π u (k) ∼ −k −4/5 in line with the above arguments. Calzavarini et al. [18] also reported similar results in their structure function computations.\n\nMentions:\n- Using Π u (k) &lt; 0, F B (k) &gt; 0, and dΠ u (k)/dk ≈ F B (k), we may argue that Π u (k) may increase with k as shown in Fig. 11.\n\nOCR Text: \"#A>0\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 11. A possible schematic diagram of the kinetic energy flux Πu(k) for twodimensional RBC."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In fig. 9, we show 2 as a function of wavenumber for our high-resolution, strongly stratified simulations. For reference, we show 2 for the unstratified turbulence run (Ri0, Fr ⊥ − → ∞) from MFS20. The quantity 2 is relatively flat with respect to , for all of these simulations, which implies that (¯) and (M ) follow similar spectral scaling. We also show 2 , the amplitude of 2 averaged over 6 ≤ ≤ 15 as a function of Fr ⊥ in fig. 10 for the high-resolution simulations (1024 2 × 1536) of this study and MFS20 5 . For the two simulations with ≠ 2.33, we show 2 scaled = 2 × (2.33/ ). We fit the data using the expression for 2 /M 2 from eq. (13e) and M = 0.25 and = 2.33, with 1 and 2 as fit parameters. The fitted factor of (3.4 ± 0.1) is close to the expected factor of 3 from the 3D to 1D conversion; see eq. ( 16). We have also plotted 2 scaled for cluster simulations from Zhuravleva et al. (2014b) in the olive coloured shaded region in fig. 10, which are slightly higher than our values and scaling relation. The differences in values of 2 scaled may arise from the choice of averaging regions, in this case 500 kpc regions in the clusters of Zhuravleva et al. 2014b. In addition to this, since turbulence in these cosmological simulations is driven more naturally by galaxy mergers and in-falls, they may include compressive components (which are excluded in our simulations), increasing the value of the turbulence driving parameter and generating larger density fluctuations. The cosmological simulations may also suffer from a lack of resolution on small scales and have a higher numerical viscosity compared to our idealised box simulations.\n- The amplitude of 2 has the same dependence on M, Fr ⊥ , and , as the ratio of the rms fluctuations¯2/M 2 . Since¯2 buoy is the main component of¯2 for Fr ⊥ ≈ 0.1, 2 scales similar to¯2 buoy /M 2 . Hence 2 is roughly independent of M, weakly dependent on Fr ⊥ (since versus Fr ⊥ peaks at around Fr ⊥ ≈ 0.1), and linearly dependent on , in agreement with eq. (13d). Combining the results of this work with those of Mohapatra &amp; Sharma (2019) and MFS20, we have studied the variation of 2 with M, Fr ⊥ , and , scanning the parameter space relevant for the ICM. We find that the ratio 2 is almost invariant with the wavenumber 6 . The amplitude of 2 varies as¯2/M 2 . Based on eq. (13e) and the fit in fig. 10 For Fr ⊥ 1, 2 approaches 1 and is independent of M, which was the limit studied in Zhuravleva et al. (2014b) and Gaspari et al. (2014). However, as we showed in figure 1 of MFS20 (also in figure 2 of SZ19), the ICM can have Fr ⊥ between 0.1-100. Thus, we suggest that observational studies use eq. ( 17) to obtain a more accurate estimate of turbulent velocities from density (surface brightness) fluctuations. Since Fr ⊥ 1 also marks the onset of large-scale density anisotropy (see lower panels of fig. 1 and also fig. 10 in MFS20), one could use the peak value of 2 , where it varies slowly with Fr ⊥ for relating the ICM density and velocity fluctuations\n\nMentions:\n- We also show 2 , the amplitude of 2 averaged over 6 ≤ ≤ 15 as a function of Fr ⊥ in fig. 10 for the high-resolution simulations (1024 2 × 1536) of this study and MFS20 5 .\n- We have also plotted 2 scaled for cluster simulations from Zhuravleva et al. (2014b) in the olive coloured shaded region in fig. 10, which are slightly higher than our values and scaling relation.\n- Based on eq. (13e) and the fit in fig. 10 For Fr ⊥ 1, 2 approaches 1 and is independent of M, which was the limit studied in Zhuravleva et al. (2014b) and Gaspari et al. (2014).\n- Since Fr ⊥ 1 also marks the onset of large-scale density anisotropy (see lower panels of fig. 1 and also fig. 10 in MFS20), one could use the peak value of 2 , where it varies slowly with Fr ⊥ for relating the ICM density and velocity fluctuations\n\nOCR Text: \"MFSZO Zhuravleva RiJ,oJ? RJ.OL RiJ.05 R7D.2 Ri.5 Th s study LU;llesWiTlt_0.l6 MuRes! {Fi WA5 Rill MUzRrelgiFi_U.IS MiSike ! ilFT 0@ mMMI (+0? ctagm Resl J-' ID !\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 10. The scaled density to velocity fluctuation ratio ( 𝜂2scaled ) as a function of Fr⊥ for the high-resolution simulations from this study and from MFS20. The ratio (𝜂2 𝑘 ) is averaged over the inertial range of turbulence (6 ≤ 𝑘 ≤ 15). The fit (solid line) shown is plotted using M = 0.25 and 𝑅𝑃𝑆 = 2.33, corresponding to the parameters used in MFS20. The ratio (𝜂2 𝑘 ) roughly scales as 𝜎2?̄?/M2 and is independent of M in the strongly stratified turbulence limit. We also show the values of 𝜂2scaled and Fr clusters from Zhuravleva et al. (2014b) in the olive coloured shaded region. The values seem to be slightly larger than predicted by our scaling relation at low Fr⊥. We discuss about this discrepancy in the main text"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- IoT devices in that cell, where that randomly chosen IoT device belongs to in (5). Fig. 3 plots the preamble detection probability P detection and the preamble transmission success probability P versus the SINR threshold γ th for a single time slot using ( 10) and ( 7), respectively. As expected, the preamble transmission success probability of a randomly chosen IoT device is always lower than the preamble detection probability of a randomly chosen BS, due to that a randomly chosen IoT device has higher chance to associate with a BS with large number of intra-cell interfering\n\nMentions:\n- Fig. 3 plots the preamble detection probability P detection and the preamble transmission success probability P versus the SINR threshold γ th for a single time slot using ( 10) and ( 7), respectively.\n\nOCR Text: \"Fene etinaR nateean HarLennttfu InennNte ecnene neni BLAMHollln AnlKB=|\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3: The preamble detection probability P1detection and the preamble transmission success probability P1 versus the SINR threshold γth for the 1st single time slot. We set T 1 = 1−e0.1, ρ = −90 dBm, σ2 = −90 dBm, λB = 10 BS/km2, α = 4, γth = −10 dB, and the baseline scheme is considered with R1 = 1."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The phase sensitivity in co-propagating measurements is more than twice better for the same duration of Raman pulses (τ π/2 ), irrespective of the interferometer duration 2T . This sensitivity improves when increasing the duration of the Raman laser pulses. This indicates that the noise is dominated by the contribution of the laser phase noise at high Fourier frequency. These results are compatible with the expected phase noise, calculated out of the power spectral density of Raman phase fluctuations. This is not a fundamental limitation of the system, phase noise can be improved by optimizing the electronics and by using a longer pulse duration. We performed gravity measurements using the four configurations measurement protocol we routinely implement for the CAG measurement to reject most of the systematic effects which affect the gravity measurement. This protocol, based on interleaved measurements with opposite Raman wavevector orientations k ↑ and k ↓ and two different Raman laser intensities, is described in detail in [40]. The results, displayed in Fig. 10, follow the expected gravity variations due to tides. The difference with the local tide model is represented in grey. The Allan standard deviation of this difference is displayed as a black line in Fig. 11. As discussed in [40], the correction of the two-photon light shift leads to a degradation of the short-term sensitivity with respect to, for instance, an average over the four measurement configurations simply corrected from tides, which is displayed as a blue line. Here, the Raman power of the ILS is sufficiently stable, meaning that fluctuations of two-photon light shifts are not a limitation to the long-term stability of the measurement. Therefore, using the complete protocol alternating over four configurations is not necessary in this case. A basic alternation of k ↑ and k ↓ is sufficient for measurements that are both stable and precise, provided that the two-photon light shift has been previously measured. This would reduce the effective cycling time and improve the short term sensitivity.\n\nMentions:\n- The Allan standard deviation of this difference is displayed as a black line in Fig. 11.\n\nOCR Text: \"1 3 1OC0 6o10 tis)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 11. Allan deviations of gravity fluctuations. The black line corresponds to the residuals of Fig. 10, which are corrected from tides and two-photon light shift. The blue line displays the average over the four measurement configurations, which is also corrected from tides, but not from two-photon light shift."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Then, the energy measurement program is initialized to listen on a socket and receive commands from the control program using inter-process communication (IPC). At the beginning of each experiment, the control program communicates with the digital switch through GPIO pins controlled by the BCM library to turn on the minion. At the same time, the control program communicates with the energy measurement program through the socket to start power measurement. After the start of the boot up process, the control program uses the WiringPi library to detect and decode a message sent by the minion when the experiment reaches the desired completion point. The end of the operation depends on the experimentation scenario and refers to the cases such as the end of P usi or the completion of a user application. The control program logs the duration and energy consumption of this operation for each experiment. Figure 4 shows the states of the minion during one single experiment from the instance the minion is powered on until the instance the minion sends the signal to the master after it has completed the desired set of operations.\n\nMentions:\n- Figure 4 shows the states of the minion during one single experiment from the instance the minion is powered on until the instance the minion sends the signal to the master after it has completed the desired set of operations.\n\nOCR Text: \"Power on Boo' cacer cc imand Usacnce cincalionvanrIcatm execub rcaved Yat @cr Haler Urworm mnoster by sendng Ihe Bd stream Whwri 57 NJown\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4: The state machine of the minion device during an experiment. The minion executes the SendBitStream program to send a message to the master after reaching the desired level of completion."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The mean and standard deviation are estimated from the training data and then applied to the test data. The final discriminant is computed using Equation (4) where the feature vector is the normalized feature vector. The optimal feature weighs are shown in Figure 14. Since all the features are normalized, Features 1, 3 and 8 have the most influence on the final discriminant function. If the discriminant D ≤ 0, then the object is declared a beacon. Otherwise, it is declared a non-beacon (and ignored by the LiDAR beacon detection system). Once the SVM is trained, implementing the discriminant given in Equation ( 4) is trivial and uses minimal processing time.\n\nMentions:\n- The optimal feature weighs are shown in Figure 14.\n\nOCR Text: \"Opralsyneainht cauce\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 14. Optimal feature weights chosen by the SVM training optimizer."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- . Each sentence in the text corpus is processed to extract specific patterns of syntax relationship tuples in the form of (arg 1 , rel, arg 2 ) where arg1 and arg2 are noun phrases, and rel is a verb or other type of phrase. Our relation extraction combines dependency tree and Semantic Role Labeling (SRL) [64] [65]. A similar, albeit more limited, approach to actant-relationship extraction is described by Samory and Mitra in their work on conspiracy theories [19]. In that work, their goal is to cluster semantically similar agent-action-target triplets, manually label the clusters, and align those labeled clusters with a manually curated topic model of a broader target corpus [19]. As opposed to limiting our extractions to agent-action-target triplets, we design a set of patterns (such as Subject-Verb-Object (SVO) and Subject-Verb-Preposition (SVP)) to mine extractions from dependency trees by using the NLTK package and various extensions [64]  [75]; the patterns are based on extensions of Open Language Learning for Information Extraction (OLLIE) [76] and ClauseIE [77]. Second, we form extractions from SEN-NAs Semantic Role Labeling (SRL) model [70]. We combine dependency-based extraction techniques with SRL to increase the recall of our system. Then we apply cleaning and de-duplication techniques to select unique and high-precision extractions. A list of all the syntax relationship patterns, their definitions, and related examples are provided in the S1 File. The While extracting syntax relationships is an integral part of our framework, our work differs from previous syntax extraction work in one key aspect: We take a holistic approach to extracting actants and their relationships. As described previously, and in more detail in the following, the noun phrases arg1 and arg2 are aggregated across the entire corpus to group them into semantic categories or actants. This aggregation process (based on the generative model of narratives) also takes into account contextual differences, where the relationships between actants change in different situations. Such corpus-level structure cannot be inferred by simply extracting relationships seen in sentences. In our approach, syntax-based relationships, such as SVO (subject, verb, object), are tuned to capture story-specific syntactic forms of expressions. For example, to fit our generative model, we often break up three-way relationships into multiple pairwise relationships: a sentence, such as \"The spark for the attack was the cache of emails stolen from John Podesta, chair of Clinton's campaign,\" is broken up into three pairwise relationships:: (The spark, was, cache of emails); (emails, stolen from, John Podesta); and (John Podesta, is, chair of Clinton campaign), as illustrated in Fig 4.\n\nMentions:\n- For example, to fit our generative model, we often break up three-way relationships into multiple pairwise relationships: a sentence, such as \"The spark for the attack was the cache of emails stolen from John Podesta, chair of Clinton's campaign,\" is broken up into three pairwise relationships:: (The spark, was, cache of emails); (emails, stolen from, John Podesta); and (John Podesta, is, chair of Clinton campaign), as illustrated in Fig 4.\n\nOCR Text: \"1SV Jc*Hu NN i4 ctescl Stab\"-Ig aeetMF Tidaal e 1t 14 tefo-R-i U) _ Jat\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig 4. An example of syntax-based relationship extraction patterns. The sentence, “The spark for the attack was the cache of e-mails stolen from John Podesta, chair of Clinton’s campaign” is analyzed to extract three relationship triples. These relationships are then aggregated across the entire corpus to create the final narrative network."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- HG-LSTM consists of two submodels: a Fact model and High-Level model (HL model), which are both Gated LSTMs. Figure 1 presents the architecture. Every word in a fact sequence is projected using an embedding matrix and processed by the Fact model. The final hidden state of the Fact model for each fact is then passed to the HL model as an input vector. We consider the final hidden state of the HL model after reading each fact representation to be a the high-level representation for the entire sequence of facts. The hierarchy of the submodels explicitly leverages the problem structure, and allows fine grain attention control at two levels of abstraction.\n\nMentions:\n- Figure 1 presents the architecture.\n\nOCR Text: \"MM elw \"S0R M ebly ~ Xwk I M\" eblw \"Mur SU\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Architecture for Hierarchical Gated-LSTMs"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We now consider in detail the readings for Bear N024, recorded on January 6, 2016. First we show (some of) the raw data readings. Figure 7 shows a sample of the first twelve data points for the car GPS V , showing latitude, longitude, and altitude. Figure 8 shows the readings from the bear GPS B, including UTM and elevation. Finally, Figure 9 shows a combination of the text reading T and the radiocollar R, including the five variables UTM, elevation, and bearing and range to the bear. Ranges in incremenents of meters according to Table 3. Since the sensors read out on different time scales, interleaving of the data records is required, as illustrated in Figure 10 for the first 18 integrated readings, encompassing 909 seconds. Figure  11 shows a similar record, now compacted and sampled at the minute interval, showing repeated entries for sensors across all time stamps. A map is shown in Figure 12, with the red dots showing the position of the bears GPS collar, and the blue dots and lines showing the measured positions of the human, including V, T , and R. And finally, the time sequence of the consistency radius is shown in Figure 13, where data points are adorned with the addresses visited over time, on a general basis, along with a linear trendline.\n- Considering the specific case of Collar N024, the difference between the VHF-estimated and GPS-estimated bear positions determines the consistency radius. This is shown by the presence of RG being the largest value in Figure 16 for most time values. Because of this, baseline comparisons of the observational error the VHF sensor computed by the dynamic linear model in Table 5 broadly agrees with the typical consistency radius shown in Figure 13.\n\nMentions:\n- And finally, the time sequence of the consistency radius is shown in Figure 13, where data points are adorned with the addresses visited over time, on a general basis, along with a linear trendline.\n- Because of this, baseline comparisons of the observational error the VHF sensor computed by the dynamic linear model in Table 5 broadly agrees with the typical consistency radius shown in Figure 13.\n\nOCR Text: \"Otk Er[rR-* Mlah Ldtea mae hlm Wnl_ Tntolsji Hie MNit? Azaiti Ie\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 13: Consistency radius ∗ over time for bear N024."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where the symbol P in front of the integrals denotes the principal value. These relation show that vanishing of f ′ (ω) implies that of f ′′ (ω) and viceversa, and therefore neither f ′ (ω) nor f ′′ (ω) can be identically zero. By virtue of this property of the window functions, it follows from Eq. ( 7) that both the real and imaginary parts of ǫ(ω) are needed to evaluate ǫ(iξ) (unless the standard choices f (z) ≡ 1 or f (z) = i z are made). We also note (see Fig. 1 and 2) that the real and imaginary parts of f (ω) do not have a definite sign. This feature also is a general consequence of our key demand that f (z) vanishes in the origin, as it can be seen by taking ω = 0 in Eqs. ( 11) and (12). Since the l.h.s. of both equations are required to vanish, the integrand on the r.h.s. cannot have a definite sign. Finally, in Fig 3 we show plots of two of our window functions f (iξ), versus the imaginary frequency ξ, expressed in eV, for the same two choices of parameters of Fig. 1 and 2. It is important to observe that the window functions f (z) are real along the imaginary axis (as it must be, as a consequence of the symmetry property Eq. ( 6)). However, the sign of f (iξ) is not definite, and as a result of this f (iξ) admits zeros along the imaginary axis. When using Eq. (7) for estimating ǫ(iξ) it is then important to choose the window function such that none of its zeroes coincides with the value of ξ for which ǫ(iξ) is being estimated.\n- We considered two choices of parameters for our window functions in Eq. (10), i.e. p = 1, q = 2 (grey triangles) and p = 1, q = 3 (black squares). In both cases, we took for the parameter w the constant value w = (1 − 2 i) eV/h (See Figs. 1, 2 and 3). It is apparent from Fig. 5 that both window functions perform very well, for all considered Matsubara modes. The error made by using J (1,2) exp (ξ n ) is less than one percent, in absolute value, while the error made by using J exp (ξn) as a estimator, in the hypothesis that data are available from ωmin = 0.038 eV/h to ωmax = 30 eV/h. The integer on the abscissa labels the Matsubara mode ξn = 2πnkBT /h (T = 300 K). Grey triangles are for the window function having p = 1, q = 2, black squares for p = 1, q = 3. In both cases w = (1 − 2 i) eV/h. tive errors in Fig. 5 (around n = 6 for the grey dots, and n = 14 for the black ones) correspond to the approximate positions of the zeroes of the respective window functions f (iξ) (see Fig. 3). Such jumps can be easily avoided, further reducing at the same time the error, by making a different choice of the free parameter w for each value of n. We did not do this here for the sake of simplicity. It is clear that in concrete cases one is free to choose for each value of n, different values of all the parameters p, q and w, in such a way that the error is as small as possible.\n\nMentions:\n- Finally, in Fig 3 we show plots of two of our window functions f (iξ), versus the imaginary frequency ξ, expressed in eV, for the same two choices of parameters of Fig. 1 and 2.\n- In both cases w = (1 − 2 i) eV/h. tive errors in Fig. 5 (around n = 6 for the grey dots, and n = 14 for the black ones) correspond to the approximate positions of the zeroes of the respective window functions f (iξ) (see Fig. 3).\n\nOCR Text: \"0,004 0,002 002 004 3 +0.006 0,008 -0.01 imaqinary frequency 'in el\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 3: Plots (in arbitrary units) of the window functions f(iξ) in Eq. (10) versus the imaginary frequency ξ (in eV/h̄). The window parameters are p = 1, q = 2 (dashed line), p = 1, q = 3 (solid line). In both cases, the parameter w has the w = (1− 2 i) eV/h̄."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Starting from a vertex v 0 ∈ V (K n ) and along the path of k edges, we can construct, as explained in Figure 2 The number of assignments that can be made on the k edges, where the time labels assigned are distinct (different from each other) is:\n\nMentions:\n- Starting from a vertex v 0 ∈ V (K n ) and along the path of k edges, we can construct, as explained in Figure 2 The number of assignments that can be made on the k edges, where the time labels assigned are distinct (different from each other) is:\n\nOCR Text: \"EoSSF ~hiss hur tk: }Ed] 'xghedbio,\" 0#\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: Number of assignments on a path of length k, when k < a"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 11 shows the results of the same experiment if OTIAS scheduling was deployed. Comparing with the results in Fig. 9, it is possible to see that packet scheduling has greatly reduced the number of packets that have been received out-oforder. There is a scattered quantity of packets with large delay variation, but the great majority will have minimal variation. The results when SRTT scheduler is used are very similar.\n\nMentions:\n- Fig. 11 shows the results of the same experiment if OTIAS scheduling was deployed.\n\nOCR Text: \"ilekF MMCtm\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 11. Packet latency variation when OTIAS scheduling algorithm is applied"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We simulate different loads on the worker by varying the number of available cores from 1 to all 28 cores, and keeping the number of cores at the coordinator fixed at 28. We set the distinct cardinality of R.b to be 2, and the query timeouts at 60 seconds. The sampling rate is 10% and we compare both cluster sampling and random sampling. The result is shown in Figure 2. The number of available worker cores has no impact on data shipping due to our use of RDMA. The response time for function shipping decreases when the number of worker cores increases. When the number of cores in the worker is 8 and 9, data shipping has higher performance for random sampling but has lower performance for cluster sampling. This is because random sampling is more computation intensive and favors data shipping when the worker has limited computing resources. For the same sampling method, we can see that data shipping has lower response time when the number of cores is small and is up to 6.5× faster than function shipping, as the saving in network traffic is offset by the slow workers in query execution.\n- How does the performance change if we increase the size of the aggregation result? As discussed in Section II-A, the cost of function shipping COST(F S) increases when the result size increases. Data shipping will be preferred when the result size increases and the cross point between function shipping and data shipping will move to the right of the horizontal axis in Figure 2.\n\nMentions:\n- The result is shown in Figure 2.\n- Data shipping will be preferred when the result size increases and the cross point between function shipping and data shipping will move to the right of the horizontal axis in Figure 2.\n\nOCR Text: \"8 Ooooo 8 o90 Coo CIEf Dal Chtot Funclior Randan DEI 3 pancim Futelnn 8 3 Number ol cores available in worker 003 poo\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2. As the computation resources available at worker increases, function shipping gets cheaper."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- A natural question is: how will the estimates improve if higher order terms in the matrix O are taken into account. We found that the improvement is negligibly small as shown in Fig. 4 that shows the comparison for the inclusion of different orders. This is an interesting point to discuss. The random variables, i.e. the strengths of the impurities, α n , appear in the exponent of exponential function. Therefore, the average transmission coefficient is dominated by the realizations with very small impurity strengths, the contribution of other realizations are exponentially suppressed. Because at small impurity strengths the contribution of higher order terms in Eq.(36) are also small, we find that the leading order actually gives a good estimate of all orders.\n- where L = (N − 2)a is the length of disordered regime. One can see that the term in the square bracket may not be small depending on the wavenumber k. But, the term goes to zero upon the integration over k (see the Fig. 9). This is the case for all higher order corrections. By carefully performing the expansion of the generating function, the general expression for m th order correction of two indices can be deduced: ], Eq.(71), for different orders of expansion, m, as function of the wave number k. At most of wave numbers the ratio exceeds unity and this leads the stronger fluctuation of the transmission coefficient for the higher expansion order terms as shown in Fig. 4 as approaching the perturbation limit.\n- where l up = min(n, m−n). Again, the term in the square bracket is on average of wavenumber k. In Fig. 10 the ratio between the correction and the leading order term computed in the main text is plotted for different orders m. The fluctuation with wavenumber is larger for higher order terms. Nevertheless, within the perturbation limit, those large fluctuations are not noticeable because they are suppressed by (W/v F ) m . However, this fluctuation explains why the inclusion of higher order results in the poor behavior near the non-perturbative regime, observed in Fig. 4.\n\nMentions:\n- We found that the improvement is negligibly small as shown in Fig. 4 that shows the comparison for the inclusion of different orders.\n- At most of wave numbers the ratio exceeds unity and this leads the stronger fluctuation of the transmission coefficient for the higher expansion order terms as shown in Fig. 4 as approaching the perturbation limit.\n- However, this fluctuation explains why the inclusion of higher order results in the poor behavior near the non-perturbative regime, observed in Fig. 4.\n\nOCR Text: \"~ {v\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 4: log T with ka = 0.45π is numerically computed (lines) considering different upper bound Mup of summation in the exponent of Eq.(36), and compared with exact numerical results (dots) for a system of N = 30. The improvement by including the higher order correction is slight."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where −p sets the plot option with rays and depth planes being depicted as seen in Figs. 3 and 4. Starting the tkinter-based GUI application is either done by running a bundled executable file or by\n- Light-field geometry results are provided as text values or graphical plots with two types of views. An exemplary cross-sectional plot is shown in Fig. 3. In addition, the results can be displayed in 3-D, as depicted in Fig. 4, with triangulation planes Z (G,∆x) for disparities ∆x. For details on the design trends, scientific notations and deviations due to paraxial approximation, we may refer to our preceding publications [6,10] for further reading.\n\nMentions:\n- where −p sets the plot option with rays and depth planes being depicted as seen in Figs. 3 and 4.\n- An exemplary cross-sectional plot is shown in Fig. 3.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Refocusing plot for shift parameter a = 1 with refocusing distance da and depth of field limits da± as well as main lens principal planes H1U , H2U and focal plane FU ."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- and plasma concentration n e (r, z) immediately after the pulse. Also we calculated linear plasma concentration ρ lin (z) which was measured in the experiment [14]: Figure 2 shows spatial distributions of fluence F (r, z) in the pulse obtained in numerical simulation. It is clearly seen that nonlinear phase modulation of the pulse obtained in the fused silica plate leads to the increase in the filament length. Due to the greater on-axis wavefront curvature the central time slices of the pulse are focused closer to the focusing mirror thus stimulating earlier filament and plasma channel onset (Fig. 3, solid lines). The slices on the leading edge of the pulse which have smaller wavefront curvature are focused at about the same distance as without any plates.\n- Dark regions in fluence distributions (Fig. 2) and the peaks in the distribution of linear plasma concentration (Fig. 3, solid lines) correspond to the pulse refocusing during filamentation. The number of refocusings increases with plate thickness. Additional peaks of the concentration at the end of the plasma channel in the presence of the plate are the cooperative result of refocusing of the trailing edge of the pulse and self-focusing of its leading edge. Thus in the presence of the plate the plasma channel starts earlier than without a plate but ends at practically the same distance due to these additional peaks. The channel therefore undergoes elongation. In addition, the total number of free electrons in generated plasma increases from slightly 8•10 13 cm -1 without a plate to 9.5•10 13 cm -1 with 10.5 mm plate.\n- A series of experiments were performed to verify influence of pass-through optics on plasma column of femtosecond laser filament in air. In our experiments we used Ti:Sapphire laser system [15] which produced 100 fs FWHM pulses with energy 2.2 mJ at the wavelength of 744 nm. The laser pulses were focused with 3-m spherical mirror. In front of the mirror at the distance of 70 cm off it fused silica plates 4.5 mm or 10.5 mm in thickness were placed. Another thin probe glass plate moving along the propagation axis was used to determine the filamentation onset [16]. When the probe plate was near the filament onset, filamentation started in the plate with clearly observable supercontinuum generation. Taking into account the ratio of air and glass nonlinearity coefficients, we had to add the probe plate thickness increased by three orders of magnitude to the registered position of the plate. We used 0.15 mm probe plate, so the accuracy of the filamentation onset measurement was about 15 cm. Fig. 3 Linear plasma concentration calculated in numerical simulation along the optical axis immediately after the laser pulse ρ lin (z) (solid lines) and concentration averaged over the registration time ρ ave (z) (dotted lines) for plates of various thickness: ∆z = 0 mm (red line), 4.5 mm (green line), 10.5 mm (blue line).\n\nMentions:\n- Due to the greater on-axis wavefront curvature the central time slices of the pulse are focused closer to the focusing mirror thus stimulating earlier filament and plasma channel onset (Fig. 3, solid lines).\n- Dark regions in fluence distributions (Fig. 2) and the peaks in the distribution of linear plasma concentration (Fig. 3, solid lines) correspond to the pulse refocusing during filamentation.\n- We used 0.15 mm probe plate, so the accuracy of the filamentation onset measurement was about 15 cm. Fig. 3 Linear plasma concentration calculated in numerical simulation along the optical axis immediately after the laser pulse ρ lin (z) (solid lines) and concentration averaged over the registration time ρ ave (z) (dotted lines) for plates of various thickness: ∆z = 0 mm (red line), 4.5 mm (green line), 10.5 mm (blue line).\n- π ρ (7) The averaged electron concentration distributions are shown in Fig. 3 by dotted lines.\n\nOCR Text: \"4MMi Ilsm 8 2 2 (sm;\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3 Linear plasma concentration calculated in numerical simulation along the optical axis immediately after the laser pulse ρlin (z) (solid lines) and concentration averaged over the registration time ρave (z) (dotted lines) for plates of various thickness: ∆z = 0 mm (red line), 4.5 mm (green line), 10.5 mm (blue line)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Designing the TE and TM polarization-independent ultra-broadband absorber with the conditions mentioned in the previous section led to the structure for which the parameters are summarized in Table 2. The proposed structure has 1.78 THz central frequency with a bandwidth of 2.64 THz. The normalized bandwidth of the device is 148% which is to best of our knowledge the highest normalized absorption bandwidth among ever-reported graphene absorbers in the THz regime. We have normalized the input impedance of the device to the free space impedance, and its real and imaginary part is depicted in the Fig. 3. It is apparent that in the central frequency and its proximity, not only the imaginary part is almost zero, but also it is flat without any sharp change. Also at these frequencies, the real part of the normalized input impedance is near unity which guarantees input impedance matching condition. These outstanding characteristics of proposed absorber result in an ultra-wideband THz absorber. To further analyze our proposed structure, we have performed full wave simulation using Finite Element Method (FEM) to validate our TLM. As it is obvious from Fig. 4, the two methods are in good agreement with each other. The bandwidth of the device is 2.64 THz ranging from 0.46 to 3.1 THz. The inset of the Fig. 4(a) shows the absorption over 90% of the device. Since the gold layer reflects the incident wave, the absorption can be determined by\n\nMentions:\n- We have normalized the input impedance of the device to the free space impedance, and its real and imaginary part is depicted in the Fig. 3.\n\nOCR Text: \"Re{Zin} Im(Zin} Frequency ITHZ)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3. Normalized Input Impedance by TLM."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We also generated learning curves for the parsing task, varying the amount of training data given to the classifiers in increments of a thousand sentences, from one thousand sentences, up to eleven thousand, which was roughly the entire training set for two of the four languages. For Danish, however, the training set was only 5190 sentences long, so the experiments for that language cut off at six repetitions, and for Portuguese, the training set contains 9071 sentences, so the tenth and eleventh iterations were the same. In all cases, we show the labeled attachment score curves in Figure 1. The unlabeled attachment scores varied similarly, and of course were higher; some of these are given in Figure 4. In Figure 2 we also show the best available CoNLL-X labeled attach-  We report results for six different classifiers, which are listed in Figure 3. Three of the classification approaches that we tested, LIBSVM with a polynomial kernel, LIBLINEAR's linear support vector machines, and LIBLINEAR's logistic regression (also known as a \"Maximum Entropy\" classifier), are available by default with MaltParser. The remaining three made use of the malt-libweka interface; J48 decision trees and the Naive Bayes classifier are familiar algorithms that have implementations in Weka. The TiMBL memory-based learner also made use of the maltlibweka interface, through a new wrapper that was implemented for these experiments.\n\nMentions:\n- In Figure 2 we also show the best available CoNLL-X labeled attach-  We report results for six different classifiers, which are listed in Figure 3.\n\nOCR Text: \"TNT<\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: Labeled attachment score for the various languages and classifiers, at the maximum training corpus size, with the winning CoNLL-X scores, for comparison."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- (1) O for MR-PQ. For each incoming connection, Bro's BubbleDown function requires scanning multiple times (one for each timer type) the timers' binary heap, which leads to excessive cache thrashing. On the other hand, MR-PQ simply requires linking a single element into a doubly linked list. In Fig. 5, the reduction in the instructions per cycle (IPC) can be explained as a consequence of the improvements on the cache performance. Since MR-PQ reduces cache miss ratios, this leads to lower stalling cycles waiting for data fetches from main memory to reload the cache, which leads to better scheduling of the CPU instruction pipeline and hence an improved IPC metric. Fig. 6, 7 and 8 focus on system wide performance metrics. Fig. 6 provides total percentage of packets dropped by the network interface card (NIC). These packets are dropped by the NIC due to the Bro application not being able to process packets fast enough. Again thanks to removing the bottleneck produced by the BubbleDown function, MR-PQ substantially reduces packet drops across the board for all traffic rates.\n\nMentions:\n- Fig. 6 provides total percentage of packets dropped by the network interface card (NIC).\n\nOCR Text: \"Packet dion rate | % Z22) 4CO0 Booj ICOOD Vrout cale (Mcpesl\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 6. Packet drop rate."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- For the individual power constrained cases, or when weighted sum rate is considered, as shown in Figs. 3, 6, and 4, 7, the gaps between the SCP and the proposed algorithms become larger. This is due to the mismatches between the SCP and these scenarios. To show that our modification to the original SCP is meaningful, we show the performance of the original (unweighted) SCP together with that of the \"weighted SCP\" in Fig. 4 and Fig. 7. These two figures clearly show that the original SCP is not suitable when weighted sum rate is considered. The performance gap between the \"weighted SCP\" and the proposed algorithm in Fig. 7 is due to the aforementioned \"random pairing\" effect of the SCP (as in the total power constrained case in Fig. 6). However, these trends do not appear in Fig. 9 and Fig. 10. For Fig. 9, this is because the strong RD link makes the sum rate not limited by the low relay power constraint. Therefore, for the SCP, the situation is very similar to that with the total power constraint.\n- As a result, the SCP is almost optimal. For Fig. 10, the strong RD link makes mode selection dependent almost only on the channel gains of the SR and SD links. Thus, mode selection is almost independent of the pairing scheme and weighting factors. For the source subcarriers that have relatively lower SR gains and are in the direct-link mode, all schemes yield similar performances. On the other hand, for the subcarriers in the relay mode, pairing better RD subcarriers with SR subcarriers having higher weighted channel gains can improve the weighted sum rate. Both the original SCP and the weighted SCP can do that for the SR subcarriers that are strong enough. Thus they both perform well and almost optimally.\n\nMentions:\n- However, these trends do not appear in Fig. 9 and Fig. 10.\n- For Fig. 10, the strong RD link makes mode selection dependent almost only on the channel gains of the SR and SD links.\n\nOCR Text: \"Cc-F JiEd ediap\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 10. Weighted sum rates for the systems with total power constraint, andE[|hSR|2] = 1, E[|hSD|2] = 1, E[|hRD|2] = 5."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In this section, we report results on the application of the algorithm (8) to individual data streams. In Fig. 11, we have plotted the evolution of the sum statistic for data from the camera C1. The count data is shown in Fig. 4. The parameters learned from the data on Sept. 10th are λ 0 = 0.001, λ 1 = 2, λ 2 = 4, λ 3 = 6, λ 4 = 8, and λ 5 = 55. We have also used (8) with α = 0. In Fig. 12, we have plotted the sum statistic corresponding to data from camera C1 on the non-event days, Fig. 9: The evolution of the sum statistic (7) for data from the non-event days. Fig. 10: The evolution of the sum statistic (7) for total car data from the event day. We see sustained high values for the statistic around the time of the event.\n- We proposed a theoretical framework for event detection in nonstationary environments using multimodal data. Motivated by the statistical behavior of count data extracted from CCTV images and social network posts, we formulated the event detection problem as a quickest change detection problem for detecting changes in count rates from one family of rates to another. We then obtained structural results for the optimal policy for the resulting POMDP and motivated a belief sum algorithm. We then applied the algorithm to real data collected around a 5K run in NYC to detect the event. For simplicity, Fig. 11: Evolution of the sum statistic (8) with α = 0 applied to data from camera C1 on the day of the event.\n\nMentions:\n- In Fig. 11, we have plotted the evolution of the sum statistic for data from the camera C1.\n- For simplicity, Fig. 11: Evolution of the sum statistic (8) with α = 0 applied to data from camera C1 on the day of the event.\n\nOCR Text: \"Sum Stat Stct 24C1\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 11: Evolution of the sum statistic (8) with α = 0 applied to data from camera C1 on the day of the event."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- ( 3 ) Adding positive and negative Gaussians to model the P Cygni structure (figure 7) yields a significantly improved fit, with ∆χ 2 of 21 for 5 fewer d.o.f. The absorption line at 6.81±0.02 keV is unresolved (and then fixed at the pn resolution) while the emission component has a 1σ width of 115±38 eV and energy 6.71±0.05 keV. The two components 3 An alternative possibility not explored further here is that the diskline parameters have changed to give a more sawtooth profile.\n\nMentions:\n- ( 3 ) Adding positive and negative Gaussians to model the P Cygni structure (figure 7) yields a significantly improved fit, with ∆χ 2 of 21 for 5 fewer d.o.f.\n\nOCR Text: \"fisptHnj FIF Meikp\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 7. The Fe K profile for rev 1727, coinciding with a strong soft X-ray (and UV) flux, is notably different from that for the summed 2009 data, with only the lower energy absorption line at ∼6.8 keV clearly seen and visual evidence for P Cygni emission. The figure includes positive and negative Gaussians to model that profile"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We calculate the optical depths of γ-ray photons in the framework of \"flat\" geometry of the BLR, in which \"flatness\" is characterized by the aperture angle α. For illustrating the dependence of τ on α, we consider three positions as above, R o = 0.01, 0.1, 0.3 pc, while the energies of γ-ray photons are considered from 50 to 100 GeV energy bands, the results are shown in Figure 4. It shows that when the γ-ray emitting region locates at position R o = 0.01 pc, the energy density of the diffuse field is large and opaque to all of the γ-ray photons with energies beyond 50 GeV as shown in the upper panel in figure 4. In middle panel, the γ-ray emitting region is close to the inner boundary of the BLR, the γ-ray photons undergo the short path before they escape out the BLR under a certain α. We can observe the γ-ray photons with energy 50 GeV in the case of α ∼ 24 • , 60 GeV in the case of α ∼ 20 • , 70 GeV in the case of α ∼ 17 • . When γ-ray emitting region locates within the clouds of the BLR, as long as the aperture angle α &lt; ∼ 30 • , the γ-ray photons from 50 GeV to 100 GeV will be observed. Because the optical depth depends on the height of the emitting region and the aperture angle, when τ γγ = 1, we can find a critical height R c of γ-ray transparency for the aperture angle α shown in Figure 5, for γ-ray photons with the energies from 50 GeV to 100 GeV. It is shown that the critical aperture angle α c moves toward smaller angle with the increase of the γ-rays energies, because the decrease of α will reduce the probability of \"head-on\" collisions, leading the increase of threshold energy. For the γ-ray with 100 GeV, R c = 0.1 pc, the critical aperture angle α c is nearly 15 • , and the geometry of the BLR becomes disc-like.\n- The geometry of the BLR is usually assumed to be an isotropic shell, the absorption of the γ-ray photons by the diffuse field is serious when the γ-ray emitting region locates within the cavity of the BLR (Liu &amp; Bai 2006;Tavecchio &amp; Ghisellini 2012). The present work extends an isotropic BLR to a \"flat\" BLR described by a aperture angle α. In the previous work (Tavecchio &amp; Ghisellini 2012), the BLR clouds are assumed to intercept a fraction C = Ω BLR /2π = 0.1 of the illuminating continuum, where Ω BLR is the solid angle surrounded by the BLR clouds, e.g., Ω BLR = 2π sin α min . The minimum aperture angle is then given by α min = arcsin C, we get α min ≃ 5.7 • . Giving this limit, we can determine the upper limit α max that increases with the decline of γ-ray energy shown in Figure 5. Alternatively, the geometry of the BLR could be reflected by the deprojection factor f. In the commonly assumed isotropic shell,\n\nMentions:\n- Because the optical depth depends on the height of the emitting region and the aperture angle, when τ γγ = 1, we can find a critical height R c of γ-ray transparency for the aperture angle α shown in Figure 5, for γ-ray photons with the energies from 50 GeV to 100 GeV.\n- Giving this limit, we can determine the upper limit α max that increases with the decline of γ-ray energy shown in Figure 5.\n\nOCR Text: \"Ifeqi\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 5. Setting τ = 1, dependence of critical position Rc on the aperture angle α. The different colored lines correspond to different γ-ray photon energies, from top to down, Eγ=100, 90, 80, 70, 60, 50GeV, respectively. The two horizonal dashed lines represents the inner and outer radius of the BLR."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Common fairness notions from the disparate treatment framework include direct effect, indirect effect, and path-specific effect. An effect can be deemed fair or unfair by an expert of the scenario at hand. Unfair effect is called discrimination. Direct discrimination is assessed using causal effect along direct edge from to while indirect discrimination is measured using the causal effect along causal paths that pass through redlining/proxy attributes 6 . Figure 3 presents another causal graph of the firing example (Section 1) involving a redlining variable (e.g. the number of disruptive incidents reported in the classrooms, which is clearly dependent on the initial level of students in the class, and hence a proxy for that sensitive attribute) and an explaining variable (e.g. the difference between the final level (at the end of the academic year) and the initial level, which is a legitimate feature to consider in the decision making.\n\nMentions:\n- Figure 3 presents another causal graph of the firing example (Section 1) involving a redlining variable (e.g. the number of disruptive incidents reported in the classrooms, which is clearly dependent on the initial level of students in the class, and hence a proxy for that sensitive attribute) and an explaining variable (e.g. the difference between the final level (at the end of the academic year) and the initial level, which is a legitimate feature to consider in the decision making.\n\nOCR Text: \"(ndirect dcrmhnouin 4-8- \"dlirect dscrimnagon 4- cxplalnablo oltocts 4-1- 1-CX\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Teacher firing example where : initial level of students,. : firing decision, ': number of reported disruptive incidents in class, : difference between final level and initial levels of students. → . : direct discrimination. → ' → . :indirect discrimination through the redlining variable '. → → . : explainable effect of on . due to ."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In addition, we apply the Borel-Padé method to improve the series in 1/g (see Ref. [12] for details). The comparison with the numerical values for y = 10, ξ = 0 and ℓ = 1 is shown in Figure 2.\n\nMentions:\n- The comparison with the numerical values for y = 10, ξ = 0 and ℓ = 1 is shown in Figure 2.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2. The dependence of the octagon Oℓ=1(χ̂ (s)) on the coupling constant for y = 10 and ξ = 0. Black dots denote numerical values, red and blue curves describe weak and strong coupling expansions, respectively."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The chemistry in the outer envelope of IK Tau has been modeled by Willacy &amp; Millar (1997). This chemical kinetic model aims at deriving the abundance stratification in the outer envelope (between 2 × 10 15 and 2 × 10 18 cm). The chemistry is driven by a combination of cosmic-ray ionization and ultraviolet radiation and starts from nine parent species injected into the envelope (see Table 2). The CSE of IK Tau was assumed to be spherically symmetric with a constant mass loss rate and a constant expan- (2) with r 0 = 2 × 10 15 cm and T (r 0 ) taken to be 100 K forṀ &gt; 5 × 10 −6 M ⊙ /yr and 300 K otherwise. For IK Tau T (r 0 ) was assumed to be 100 K. The derived fractional abundances for the molecules studied in this paper are represented in Fig. 2. The model of Willacy &amp; Millar (1997) succeeded in reproducing the observed values of certain species but failed for some other molecular abundances: the calculated abundance of HCN was too low and the injected abundance of the parent species SiS was about 10 times higher than observed. Duari et al. (1999) noted that the input molecular abundances of some parent species are sometimes questionably high since there exists no observational or theoretical evidence for the formation of these species in the inner and intermediate envelopes of O-rich Miras (see Sect. 3.3.2). More importantly, Duari et al. (1999) showed that HCN should form in the inner envelope or extended stellar atmosphere due to non-equilibrium shock chemistry, and may be a parent species injected to the outer envelope. Recent observational studies also indicate that HCN must be formed in the inner envelope (Bieging et al. 2000;Marvel 2005). These results are in contrast to the modeling efforts of Willacy &amp; Millar (1997) where HCN was not yet considered as a parent species.\n- Carbon-bearing molecules have been identified in the envelopes of many oxygen-rich AGB stars (e.g., Bujarrabal et al. 1994) and it was first thought that the observed carbon species were produced in the outer wind of O-rich stars via photochemical processes. However, Duari et al. (1999) showed that shock-induced Figure 2. Theoretically predicted molecular abundance stratification in the envelope of IK Tau. The full lines represent the results as derived by Willacy &amp; Millar (1997) for the outer envelope of IK Tau assuming a mass-loss rate of 1 × 10 −5 M ⊙ /yr. The diamonds represent the abundances derived by Cherchneff (2006) in the framework of non-equilibrium chemistry due to the passage of shocks in the inner envelope of TX Cam at 2 R ⋆ ; the filled circles indicate the results by Duari et al. (1999) for IK Tau at 2.2 R ⋆ . The inner wind predictions are projected onto the dustcondensation radius derived in Sect. 4.1. The dotted line represents the Gaussian distribution of circumstellar H 2 O as used by Maercker et al. (2008). non-equilibrium chemistry models predict the formation of large amounts of a few carbon species, such as HCN, CS and CO 2 , in the inner envelope of IK Tau: these molecules are hence formed in the post-shocked layers and are then ejected in the outer wind as 'parent' species. For some parent species, the non-equilibrium chemistry does not alter significantly the initial photospheric TE abundances. But other species, abundant in the TE photosphere, are quickly destroyed in the outflow by the non-equiblibrium chemistry generated by shocks (e.g., OH, SiS and HS). Again other species (such as SO) appear to be absent in the inner regions of the wind, and are thought to be produced by ionmolecule reactions in the photo-dissociation regions of the outer wind. Cherchneff (2006) continued on the study of shock-induced non-equilibrium chemistry in the inner wind of AGB stars. She demonstrated that whatever the enrichment in carbon of the star (i.e. the C/O ratio), the atomic and molecular content after the passage of the first shock in the gas layers just above the stellar photosphere is very much the same, and in many cases, totally different from what would be expected from thermodynamic equilibrium (TE) calculations. In the case of the oxygen-rich envelope around TX Cam -being almost a stellar twin of IK Tau, but with slightly lower luminosity - Cherchneff (2006) found that while, e.g., the TE abundance of HCN (CS) is predicted to be ∼1.9 × 10 −11 (∼2.5 × 10 −11 ), the non-TE fractional abundances at 2.5 R ⋆ are predicted to be ∼9 × 10 −5 (∼1.85 × 10 −5 ). The fractional abundances derived by Cherchneff (2006) differ from the abundances of the injected parent molecules in the study of Willacy &amp; Millar (1997) (see Table 2 and Fig. 2): Willacy &amp; Millar (1997) did not consider CS and HCN as being parent molecules, and the (injected) abundance of SiS (3.5 × 10 −6 ) is much higher than the abundance stratification derived by Cherchneff (2006) (see their Fig. 8).\n- From the descriptions of theoretical abundance estimates in the inner and outer envelope (Sect. 3.3.1 -3.3.2) it is clear that there is still some debate about the abundance structure in the envelope. SiS and HCN were already given as an example, but other molecules such as e.g. CS and SO also pose a problem. This is illustrated in Fig. 2, where one notices for a few molecules a large difference between the theoretically predicted fractional abundance in the inner envelope by Duari et al. (1999) and Cherchneff (2006) and the abundance of the parent molecules injected in the outer envelope by Willacy &amp; Millar (1997). One of the big questions still existing concerns the modifications of the molecular abundances in the intermediate wind region due to gas-grain reactions. Currently, no theoretical efforts have been made to model this region in terms of molecular 'leftovers' after the dust formation has occurred. In the case of O-rich envelopes, it is thought that CO, HCN and CS are quite stable and travel the entire envelope unaltered until they reach the photo-dissocation region of the outer wind as these molecules do not participate in the formation of dust grains such as silicates and corundum (Duari et al. 1999). In contrast, SiO is a candidate molecule for depletion in the intermediate wind region due to the formation of SiO 2 (via a reaction with OH) whose condensation product, silica, is tentatively identified in post-AGB stars (Molster et al. 2002) and is claimed to be the carrier of the 13 µm feature in low mass-loss rate AGB stars (Speck et al. 2000, but other studies argue that this feature is due to spinel). The theoretical modeling of Duari et al. (1999) and Cherchneff (2006) predict an SiS abundance 2 to 3 orders of magnitude lower than the observed value, indicating that SiS is produced in the outer envelope of IK Tau. However, recent observational results by Decin et al. (2008b) argue for a formation process in the inner envelope.\n\nMentions:\n- The derived fractional abundances for the molecules studied in this paper are represented in Fig. 2.\n- However, Duari et al. (1999) showed that shock-induced Figure 2.\n- The fractional abundances derived by Cherchneff (2006) differ from the abundances of the injected parent molecules in the study of Willacy &amp; Millar (1997) (see Table 2 and Fig. 2): Willacy &amp; Millar (1997) did not consider CS and HCN as being parent molecules, and the (injected) abundance of SiS (3.5 × 10 −6 ) is much higher than the abundance stratification derived by Cherchneff (2006) (see their Fig. 8).\n- This is illustrated in Fig. 2, where one notices for a few molecules a large difference between the theoretically predicted fractional abundance in the inner envelope by Duari et al. (1999) and Cherchneff (2006) and the abundance of the parent molecules injected in the outer envelope by Willacy &amp; Millar (1997).\n- Looking to silicon isotopic ratios derived from presolar SiC grains (Fig. 2 in Zinner et al. 2006), the silicon isotope ratios of IK Tau correspond to the X-grains, which are thought to originate in Type II supernovae (however major discrepancies between model predictions and observed isotopic ratios still exist; Nittler et al. 1995).\n\nOCR Text: \"Isinney AVo |eson, Anexensie gr dhas been calculated Vih parameters tary hom 2OCI to ZOQOK Tor the stelar temperatua; Fom 02 \"16x Iu\" Gmtortestelarradius , aninner (dst Iersabon} radius between 2 andKBQ dzance bemen Jand 3UUpc;andacorsa tmass-losstate beiedi-' I5 * 'Wsu As biety exlired n Sd34,alg; linood mettog |DecieiEU 20074 5 usedto finc the Frest: celand cue a 953' ccnfdence inevaltor te mode oa: Iptars The reSUswlkerresentedin \"Uct 4\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2. Theoretically predicted molecular abundance stratification in the envelope of IK Tau. The full lines represent the results as derived by Willacy & Millar (1997) for the outer envelope of IK Tau assuming a mass-loss rate of1× 10−5M⊙/yr. The diamonds represent the abundances derived by Cherchneff (2006) in the framework of non-equilibrium chemistry due tothe passage of shocks in the inner envelope of TX Cam at 2R⋆; the filled circles indicate the results by Duari et al. (1999) forIK Tau at 2.2R⋆. The inner wind predictions are projected onto the dustcondensation radius derived in Sect. 4.1. The dotted line represents the Gaussian distribution of circumstellar H2O as used by Maercker et al. (2008)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The trajectory of a 1 in the first quadrant as n increases is shown in figure 14 for adsorbing walks in L 2 + , and for n ∈ [10,500]. The estimate a + c = 1.77564 is denoted by a bullet on the real axis (see reference [24]) . With increasing n the trajectory of a 1 tends to straigthen out and it approaches the real axis at a critical angle θ + c of about 55.3 o .\n- then a least squares fit to the data in figure 14 gives The cumulative angular distribution function α n (φ) of the Fisher zeros of A + n (a) around the origin in the a-plane is given by\n- The analysis of Yang-Lee zeros proceeds similar to Fisher zeros (as in section (4.2)). The zeros were determined by deflating G N (a, t) for t = 1 µ 2 and are plotted in figure 16 for N ∈ {100, 200, 300, 400, 500}. In general, the location of zeros did not change in significant ways with changes in the value of t. The results for these zeros are very similar to those results seen for Fisher zeros. For example, the trajectory of the leading zero is very similar to figure 14, and the cumulative angular distribution of zeros has the same profile as seen in figure 15.\n\nMentions:\n- The trajectory of a 1 in the first quadrant as n increases is shown in figure 14 for adsorbing walks in L 2 + , and for n ∈ [10,500].\n- then a least squares fit to the data in figure 14 gives The cumulative angular distribution function α n (φ) of the Fisher zeros of A + n (a) around the origin in the a-plane is given by\n- For example, the trajectory of the leading zero is very similar to figure 14, and the cumulative angular distribution of zeros has the same profile as seen in figure 15.\n- The trajectory of the leading Fisher zero a 1 in figures 14 and 26 follows an approximately parabolic path with increasing length of the walk, and approaches the real axis at an angle which was estimated to be about 55 o in the square lattice.\n\nOCR Text: \"eadi19 zcl: et ~553\"\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 14: The trajectory of the leading Fisher zero a1 as a function of n for adsorbing walks in the square lattice. With increasing n the leading zero approaches the real axis, where it, and its complex conjugate, squeeze an edge singularity in the limiting free energy of the model at the critical adsorption point a+c . The dotted line is a least squares parabolic curve fitted to the trajectory of the leading zero. Assuming a critical point at a+c = 1.77564 [2] allows one to estimate the angle of incidence θ+c ≈ 55.3o . The approximating parabola intersects the real axis at the estimated critical point 1.8049 at a critical angle of size 58.2o . If the least squares parabola is constrained to pass through the estimated critical point at 1.77564, then the critical angle has approximate size 53.7o ."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where π is the mapping of the disjoint sum A. Observe that a tuple a is in Each tree in B h (i) is called a binary tree encoding of i with parameter h. An example of a binary tree encoding of i = 42 with parameter h = 1 is depicted in Figure 3. An induction on the parameter h shows that every number i ∈ [0, Tower(h+3)−1] has at least one binary tree encoding with parameter h. The following easy observation gives an upper bound on the height of binary tree encodings.\n- Proof. Let h &gt; 1 and i ∈ [0, Tower(h+3) − 1]. By definition of B h (i) (see also Figure 3) it is easy to convince oneself that each binary tree in B h (i) has height less than 2 + h k=0 Tower(k+1) &lt; 2 • Tower(h+1).\n\nMentions:\n- An example of a binary tree encoding of i = 42 with parameter h = 1 is depicted in Figure 3.\n- By definition of B h (i) (see also Figure 3) it is easy to convince oneself that each binary tree in B h (i) has height less than 2 + h k=0 Tower(k+1) &lt; 2 • Tower(h+1).\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: A binary tree from the set B1(42), i.e., a binary tree encoding of the number 42 with parameter 1. The numbers depicted within some of the nodes are not part of the tree encoding; they are just indicated here to illustrate which number is encoded by the subtree starting at the respective node."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The gender predicted by Ratio 1 and Ratio 2 is correct for all 83 samples. The height estimation error for different height ranges is shown in Figure 3(c). The overall mean estimation error for the 83 samples is 1.87 centimeters. Compared to [23], our system has obtained competitive accuracy with a much simpler algorithm. The results of our system can be qualitatively evaluated from Figure 5. To verify the prediction accuracy, we applied the Iterative Closest Point (ICP) [22] registration algorithm to obtain the mean geometric error between the retrieved 3D mesh model and the raw point cloud data for the four subjects shown in Figure 5. The results are shown in Figure 6. We simply aligned the skeleton joints for ICP initialization. After 6 ICP iterations, the mean registration error converges to below 9 centimeters, for all four subjects. This is because the feature vector gives more weights to frontal measurements. The mesh alignment can be fine tuned by ICP after several iterations. However, the initial joint locations are only accurate on the Figure 4: Runtime. Runtime of our complete system with increasing size of the synthetic dataset. For synthetic dataset with given number of samples, we evaluate the average runtime end-end, to estimate relevant measurements, naked 3D models and clothed 3D models.\n- Compared to the Kinect TM SDK which only works on Windows Platforms, OpenNI is more popular in open source and cross-platform projects. Although both SDKs provide access to basic RGB images and depth maps, the human silhouette tracking and joint location estimation are implemented in different algorithms. Accord-Figure 6: ICP Error. Mean registration error per ICP [22] iteration for different samples. We evaluated ICP registration error for 4 samples in the dataset. The raw depth maps and estimated body shapes for the 4 samples can be found in Figure 5. ing to [29], OpenNI performs better for head and neck tracking, whereas Microsoft R Kinect TM SDK is more accurate on other joints.\n\nMentions:\n- The results are shown in Figure 6.\n- Accord-Figure 6: ICP Error.\n\nOCR Text: \"3etclel Sarple? Satcle} 1 Sqgky 7 CP Herzlions L\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6: ICP Error. Mean registration error per ICP [22] iteration for different samples. We evaluated ICP registration error for 4 samples in the dataset. The raw depth maps and estimated body shapes for the 4 samples can be found in Figure 5."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In our studies, the property of symmetric ABA triblock copolymers is characterized by three tunable molecular parameters: χ (The Flory-Huggins interaction parameter), N (The chain length of copolymer) and N st /N ns (the length ratio of each hydrophobic end block to hydrophilic middle block). In this paper, when the chain length is changed, the value of N st /N ns (≃ 0.23) remains constant. The aggregation behavior of micelle morphologies is focused when the length of copolymer is changed. Figure 1 shows the phase diagram of the systems with different chain length N . When χ is increased, the unimer-micelle transition occurs. At fixed N , the χ value on micellar boundary increases with decreasingφ P . When N is increased, at fixedφ P , the χ value on micellar boundary shifts to a small value. It is noted that although the length ratio of each hydrophobic end block to hydrophilic middle block remains constant, the increase in the chain length of copolymer is also favorable to the occurrence of micelles in the system.\n\nMentions:\n- Figure 1 shows the phase diagram of the systems with different chain length N .\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1. The phase diagram for the amphiphilic symmetric ABA tribolck copolymers with different chain length N . The boundary between homogenous solutions and micelle morphology is obtained. The squares, triangles and diamonds correspond to the boundaries for N = 17,26,34, respectively."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- BMDL achieves flexible multi-domain learning by first constructing a NB factorization model of NGS counts, and then explicitly establishing the relevance of samples across different domains by introducing domain-dependent binary variables that assign latent factors to each domain. The graphical representation of BMDL is illustrated in Fig. 1.\n- Given the BMDL model in Fig. 1, we derive an efficient Gibbs sampling algorithm with closed-form updating steps for inferring the model parameters by exploiting the data augmentation technique in Zhou and Carin [2015]. The detailed Gibbs sampling procedure is provided in the supplemental materials.\n\nMentions:\n- The graphical representation of BMDL is illustrated in Fig. 1.\n- Given the BMDL model in Fig. 1, we derive an efficient Gibbs sampling algorithm with closed-form updating steps for inferring the model parameters by exploiting the data augmentation technique in Zhou and Carin [2015].\n\nOCR Text: \"j-l_ 4Lz\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: BMDL based on multi-domain negative binomial factorization model."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- periodic tumbling of polymers in shear flows, characterised by configurations with large extensions alternating with states of strong chain contraction, see, e.g., the studies reported in Refs. [67,68]. For these conditions of well separated rings, the distribution of residence times t r , that each ring is situated in one of the two well-separated states close to the cylinder ends, is shown in Fig. 5 for different crowding fractions φ. The mean residence time t r extracted from these histogram is the characteristic time scale for the internal ring swapping dynamics. As shown in the inset of Fig. 5, t r mildly increases with increasing φ. The maximum of the residence time histograms in Fig. 5 shifts at higher crowding fractions to larger values because of the associated slower polymer dynamics.\n\nMentions:\n- For these conditions of well separated rings, the distribution of residence times t r , that each ring is situated in one of the two well-separated states close to the cylinder ends, is shown in Fig. 5 for different crowding fractions φ.\n- As shown in the inset of Fig. 5, t r mildly increases with increasing φ.\n- The maximum of the residence time histograms in Fig. 5 shifts at higher crowding fractions to larger values because of the associated slower polymer dynamics.\n\nOCR Text: \"003 002 0 001 5oo00 100000 150000 J*10'\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5. The distribution of the residence times tr in the well-segregated ring states. The histograms are obtained from the zigzag traces similar to those presented in Fig. 4, but at varying crowding fractions. The inset shows the mean residence time of rings 〈tr〉 in well-separated states as a function of φ."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In order to understand the mechanisms controlling the incubation and layer growth, we first discuss the dependence of growth kinetics on the AsH3 flow at 420 o C and TMGa pressure of 13x10 -5 Pa (Fig. 2). AsH3 partial pressure was initially set to 1 Pa (see also Supplementary Video 2) and subsequently increased monotonically. After conducting growth at the highest AsH3 pressure (5.6 Pa), the AsH3 flow was reduced again to 1 Pa to verify reproducibility of the observations. The time for completing each layer (purple squares) decreases with increasing AsH3 supply, indicating that layer completion is restricted by the amount of arsenic present in the catalyst. This suggests that there are not sufficient (excess) As atoms present in the droplet at any instant to form a complete layer. This is not surprising since the solubility of As in the Au-Ga alloy is very low, and experimental reports 29,30 and theoretical predictions 31 show very low concentration of As species in the catalyst. Incubation time is plotted as a function of the AsH3 partial pressure as cyan circles in Fig. 2; we see that incubation, unlike layer completion, increases slightly as we increase the As precursor pressure .\n\nMentions:\n- In order to understand the mechanisms controlling the incubation and layer growth, we first discuss the dependence of growth kinetics on the AsH3 flow at 420 o C and TMGa pressure of 13x10 -5 Pa (Fig. 2).\n- Incubation time is plotted as a function of the AsH3 partial pressure as cyan circles in Fig. 2; we see that incubation, unlike layer completion, increases slightly as we increase the As precursor pressure .\n\nOCR Text: \"6 8 AsH; parial pressure (Paj\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2. Incubation and layer completion as a function of AsH3: The average layer completion time (purple squares, axis on left side) and incubation time (cyan circles) for growing a layer is plotted as a function of the As-precursor pressure. The layer growth of each individual layer is faster at higher AsH3 flow. Error bars denote standard deviation across the measured 10 or more events."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In this section, aperture is defined as the values of the gap function g, which reside on the nodes of the slave side. In its implementation, g differs from theory, as on a finite element mesh, normals are defined on the surfaces of the element sides and not at the nodes. The direction of g is therefore computed as the average of the normals of all sides of the mesh surrounding each node of the contact side. Still, we believe this to be a more accurate description of the distance between the fracture surfaces than a simple projection in the normal (0, 0, −1) t -direction and we thus use the value of g as aperture. For the assembly of the mortar operator, only intersections up to a reasonable distance of 0.21 mm are detected and the maximum value of the aperture is fixed at 0.21 mm. This does not impact the accuracy of the contact method, as the displacements of the rock are smaller than this maximum value (compare also with Figure 11).\n- Rock fractures subjected to normal loading show a characteristic nonlinear fracture closure curve, where fracture closure becomes increasingly smaller for the same load increment, until it approaches the behavior of elastic deformation in a solid body [4,27,51,41,43,20]. The fracture closure curve, obtained from our numerical experiment, is shown in Figure 11 for increasing axial loads from 0.25 to 20 MPa. Here, we increased the axial load incrementally in steps of 0.25 MPa from 0 to 2 MPa and then in steps of 1 MPa from 2 to 20 MPa. The fracture closure (displacement) is measured as the average displacement of nodes in zones of about 1 mm thickness, approximately 2.5 cm above and 2.5 cm below the fracture. Displacement is measured at such a small distance from the fracture to avoid large heterogeneities in displacement due to the heterogeneous distribution of surface height and contact area over the entire fracture surface. To isolate displacements of the fracture, elastic displacements caused by the deformation in the lower specimen half, are excluded by subtracting the average displacements 2.5cm below the fracture from the average displacements 2.5cm above the fracture(see also [43]). The curve in Figure 11 shows, that the displacements become smaller when the confining stresses become larger, eventually leading to a linear relationship between axial load and displacement. The shape of the curve can be explained by noting that an increase in loading results in more areas of the fracture being in contact, increasing the overall resistance to the confining stress until a quasilinear elastic response is reached. This is a well-known characteristics of loading curves and, together with the obtained boundary stresses for the Hertzian contact, underlines the soundness of our approach.\n\nMentions:\n- This does not impact the accuracy of the contact method, as the displacements of the rock are smaller than this maximum value (compare also with Figure 11).\n- The fracture closure curve, obtained from our numerical experiment, is shown in Figure 11 for increasing axial loads from 0.25 to 20 MPa.\n- The curve in Figure 11 shows, that the displacements become smaller when the confining stresses become larger, eventually leading to a linear relationship between axial load and displacement.\n\nOCR Text: \"3 1 C,OC 0,05 C.10 0.15 0 20 425 0.30 035 Displace ment , timn]\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 11: Simulated loading curve, showing displacement (i.e., fracture closure) versus axial load."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- As far as the X-ray efficiency from the globular cluster millisecond pulsars is concerned it can be seen from Figure 19 that their X-ray luminosities are well within the scatter of other data points at this spin-down energy level. Within the uncertainties of the deduced X-ray luminosities it is therefore not justified to conclude that these pulsars have an X-ray efficiency which is different from the one observed for e.g. field millisecond pulsars. Millisecond pulsars in the galactic plane and in globular clusters thus appear to show no distinct differences in their X-ray emission properties. Backer &amp; Sallmen (1997). Two phase cycles are shown for clarity. The X-ray pulse profiles are characterized by two narrow peaks with a phase separation of ∼ 0.449 ± 0.0009 between the two peaks. The radio profile at 800 MHz depicts three pulse components, with the main radio peak leading the second and third. The dominating radio pulse is leading the main X-ray pulse by 0.0243 in phase. The uncertainty of the ralative phase is ±1 bin (∼ 38µs) in the X-ray profile. Phase zero corresponds to JD(TDB@SSB)=2451468.5009909072. (Becker &amp; Trümper 1997) while the dot-dashed and dotted lines with its gray-shaded 1σ uncertainty range is from a fit to all X-ray detected millisecond pulsars (Becker 2009; cf. Figure 15). The dashed line and its gray-shaded 1σ uncertainty range is from correlating the spin-down energy and the X-ray luminosities of the globular cluster millisecond pulsars only.    (1996, update 2003). N H from the optical reddening E(B-V) of the corresponding globular clusters. c.Ė from Table 4 of Bogdanov et al. (2006). d. Long-orbit triple system with a white dwarf and a planet.\n\nMentions:\n- As far as the X-ray efficiency from the globular cluster millisecond pulsars is concerned it can be seen from Figure 19 that their X-ray luminosities are well within the scatter of other data points at this spin-down energy level.\n\nOCR Text: \"mli\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 19.— Isotropic X-ray luminosity vs. spin-down energy of X-ray detected millisecond pulsars. Galactic field millisecond pulsars are represented byfilled squares, 47 Tuc pulsars byopen circles and other globular cluster pulsars byfilled circles. The solid line representsLX,0.1−2.4keV = 10−3Ė (Becker & Trümper 1997) while the dot-dashed and dotted lines with its gray-shaded 1σ uncertainty range is from a fit to all X-ray detected millisecond pulsars (Becker 2009; cf. Figure 15). The dashed line and its gray-shaded 1σ uncertainty range is from correlating the spin-down energy and the X-ray luminosities of the globular cluster millisecond pulsars only."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To test the dependence of the stability analysis on the eccentricity of the binary we repeated the FMA for a model with e B = 0.4, the most frequent value encountered in binary systems according to Duquennoy &amp; Mayor (1991). In Fig. 3 the FMA outcome is illustrated in this case. Again, the limit of stability is wider compared to that predicted by Holman &amp; Wiegert (1999) since stable configurations are found when the semimajor axis of the planet is around 4.1 AU while a c = 3.67 AU. The most interesting feature is, however, the bimodal distribution of the stability index c s as a function of the semimajor axis. As in Fig. 1, the yellow filled circles are unstable orbits while the blue filled squares are stable ones. The two outer unstable systems have the planet ejected on a hyperbolic trajectory in less than 60 Myr. The inner case with an initial semimajor axis around 3.04 AU takes more time and its evolution is shown in Fig. 4. The orbit shows clear signs of chaotic behaviour from starting, visible only by inspecting the orbit on a short timescale. Finally, the ejection of the planet from the system occurs after about 3.3 Gyr possibly because the path to full instability is long.\n- To understand the reason of the coexistence of stable and unstable orbits for similar values of semimajor axis we have to explore the frequency space. In Fig. 5 the main frequency of the planet orbit is shown as a function of the initial semimajor axis. There are empty stripes which possibly correspond to the families of unstable orbits identified by Michtchenko &amp; Malhotra (2004) as due to non-linear secular resonances which appear in their generalized numerical secular perturbation theory. These same resonances were retrieved at moderate eccentricities by the order 12 secular theory developed for two coplanar planets by Libert &amp; Henrard (2005). There are significant differences between the configuration explored by Michtchenko &amp; Malhotra (2004) and ours in particular concerning the mass ratio between the two bodies of the system other than the primary star. Michtchenko &amp; Malhotra (2004) consider a mass ratio ranging from 1 to 1/4 between two planets perturbing each other while in our case the ratio between the companion star and the planet is about 1 × 10 3 . However, we may expect that the origin of the instability stripes may be ascribed to similar non-linear resonances which appear only when the companion star is on a highly eccentric orbit (e B = 0.4). In fact, for e B = 0, there is no sign of similar unstable regions in the frequency vs. semimajor axis space and this is in agreement with the findings of Michtchenko &amp; Malhotra (2004) predicting the presence of secular resonances at moderate to large eccentricities. Another way to estimate a stability limit for single planet orbits in binaries is to use the Hill stability criterion described in Marchal &amp; Bozis (1982). We use its formulation as described in Andrade-Ines &amp; Michtchenko (2014) and compute a radius of the stability sphere R s using their equations 3) and 4). Since the criterion is defined on the radial distance, for applying it to keplerian orbits it is necessary to adopt the following rule: an orbit satisfies the Hill stability criterion if a pl (1 + e pl ) &lt; R s , in other words the aphelion of the orbit must lie inside R s . We applied this criterion to the two models with a B = 25 AU and e B = 0 and e B = 0.4, respectively. To compare our sample of orbits with R s , we select from the whole sample those with c s &lt; −6, as a conservative choice for stable orbits. For these we compute the maximum value of the aphelion distance over the integra-Fig. 4. Time evolution of the eccentricity of the system marked by a yellow circle in Fig. 3 whose initial semimajor axis is close to 3.04 AU. The system is chaotic and after about 3.3 Gyr the planet is ejected on a hyperbolic trajectory. The origin of its instability is possibly due to a non-linear secular resonance. tion timespan and compare it with R s . For the case described in Fig. 1, i.e. a B = 25 AU and e B = 0, we find that, all over the sample of stable orbits, the maximum aphelion distance is 9.6 AU which is in good agreement with the prediction of eq. 4) in Andrade-Ines &amp; Michtchenko (2014) for µ = 1 which gives R s = 9.8 AU. In the second case illustrated by Fig. 3, i.e. a B = 25 AU and e B = 0.4 the maximum aphelion distance is 4.8 AU against a value of 5.9 AU predicted by the analytical equation. The Hill criterion based on the definition of R s is assumed to be limited to 'short-term' stability, as argued by Andrade-Ines &amp; Michtchenko (2014), however it is interesting that the discrepancy is larger for higher values of e B .\n\nMentions:\n- The inner case with an initial semimajor axis around 3.04 AU takes more time and its evolution is shown in Fig. 4.\n- For these we compute the maximum value of the aphelion distance over the integra-Fig. 4.\n\nOCR Text: \"{0-0 1m3 1533 3 253+79 3407 35-2 Tm2\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4. Time evolution of the eccentricity of the system marked by a yellow circle in Fig.3 whose initial semimajor axis is close to 3.04 AU. The system is chaotic and after about 3.3 Gyr the planet is ejected on a hyperbolic trajectory. The origin of its instability is possibly due to a non–linear secular resonance."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The beam loss limit has been set to 525W to allow workers to maintain all elements in the Booster tunnel without excessive radiation exposure. Figure 1 shows the historical beam loss in the Booster versus protons per hour. The total loss depends on the beam intensity. The present operational beam intensity at injection is about 5E12 ppp and extraction is 4.5E12 ppp. The total energy loss is 0.075 kJ in one Booster cycle and hence 1150 W when the cycle rate is 15Hz. The loss has to be reduced to half by 2016. Figure 2 shows the intensity and loss during normal operations. Beam studies and upgrades that will be done to reduce the beam losses will be discussed in this paper.\n\nMentions:\n- Figure 1 shows the historical beam loss in the Booster versus protons per hour.\n\nOCR Text: \"525 4# Administrative Limit Allows wcrk on components FuuuilaarkLY\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Beam power loss for 3 year operations (blue: 2005, red: 2011 and green: 2014)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- One such comparison is in Figure 4, where we plot the naive SWAP counts for all allocations of a benchmark circuit (hwb6 56) against their energy for a particular run 2 In Section IV A, where we present more formal results, the number of SWAPs presented is computed using a proper routing procedure. 14. As sample number increases, these histograms tend to approach a log-normal distribution. The general trend in energies is that for smaller circuits the plot will be very skewed towards lower energies, as the 'best' initial allocation (in terms of fewer added SWAPs) will be found for the majority of anneals, while for larger circuits the sample space is too large to find convergence on a particular allocation, and the energies become more normally distributed.\n\nMentions:\n- One such comparison is in Figure 4, where we plot the naive SWAP counts for all allocations of a benchmark circuit (hwb6 56) against their energy for a particular run 2 In Section IV A, where we present more formal results, the number of SWAPs presented is computed using a proper routing procedure.\n\nOCR Text: \"Swaps = Ene 3, 25, - 41y5lul LAUU Ei Aa 417SCCC ammm Enercx\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4. Comparison of SWAP count and allocation energy for a 1000 sample anneal run for benchmark circuit hwb6 56, using the coefficient form in Equation 14. SWAP count here is computed using the naive method used to generate the coefficients, without further routing. While there is significant variation, there is a general tendency for lower-energy allocations to also have lower SWAP counts, indicating that the choice of cost function is able to produce quality solutions."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The function F + c (α) is shown in figure 18. For small α, this suction force has the asymptotic form\n\nMentions:\n- The function F + c (α) is shown in figure 18.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 18. Contact force function F+c (α) = 4 sin 2 α(α− sinα cosα)−1 for corner flow."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Note that ν 1 (H α ) = α/2 n . Fig. 4 shows that, generically, the average sign monotonously depends on the non-stoquasticity. Indeed, as expected for large m, the dependence is an exponential one.\n\nMentions:\n- Fig. 4 shows that, generically, the average sign monotonously depends on the non-stoquasticity.\n\nOCR Text: \"MilI\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4. The figure shows the inverse average sign for 100 randomly chosen instances of 5-qubit Hamiltonians Hα for β = 1 and m = 100 Monte Carlo steps as a function of dν1(Hα). We find a roughly exponential dependence of the inverse average sign with ν1(Hα) as 1/〈sign〉β,m(Hα) ∝ exp(a · dν1(Hα)) for a > 0."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Note that, while in fig. 2 we resaled the wave number k with the viscoelastic length ξ ⊥ , the characteristic length separating these two regime is not simply ξ ⊥ , but is generally ωτ -dependent. In fig. 3, we show the \"diagram\" in the (k, ω) plane. Here the boundary line separating the two regimes is identified as the point at which u decreases the half amount of the full decrease in fig. 2, that is, where u = (1+u ∞ )/2. From this diagram, we see that increasing ω (for a given k) enlarges the uniform viscoelastic regime, where the Newtonian-like response, u/u N ∼ 1, is obtained within the linear analysis. This result for the first time clarifies in which condition the use of spatially uniform viscoelastic constitutive equations is justified. Note that in this regime, nonlinear elastic effects have to be consid-ered as shown in the previous studies [16][17][18][19].\n\nMentions:\n- In fig. 3, we show the \"diagram\" in the (k, ω) plane.\n\nOCR Text: \"vuu Uniformly 1001 viscoelastic Meso-structure irrelevant ~t Gel-like UN Meso-structure relevant p(1 20/\", A+% 0.01\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3: The diagram of the pumping response on the (k, ω) plane. Two qualitatively different regimes are found. The boundary line between the two regimes is determined as described in the main text, for different value of the solution viscosity: ν0 = 10 (dotted, green), ν0 = 100 (broken, blue), and ν0 = 1000 (solid, red)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- PMM also achieves higher or comparable power eciency compared to DRAM in all tested workloads, except the write-only workload. We dene the power eciency as the peak bandwidth at 24 threads per socket (one thread per core) per dynamic memory power and report in Figure 7. For the read-only workload in Figure 7, PMM-numa-local and PMM-fsdax-local congurations achieve up to 47% higher power eciency than DRAM-local. As expected, due to the high write energy to PMM, the power eciency of PMM congurations is 20% lower than DRAM-local conguration for the write-only workload. This observation restates the importance of isolating writes from PMM and also shows the potential of using PMM for meeting a low power envelope on large-scale systems. Non-temporal writes again signicantly impact Memory mode. Without NT-write, the MemoryMode-local conguration consumes similar dynamic power as the DRAM-local conguration. However, with NT-write accesses, MemoryMode-local consumes 13% additional dynamic power. This impact is even more profound in power eciency. The MemoryMode-local conguration shows 49% lower power eciency than the DRAM-local conguration for NT-write accesses while it can reach similar eciency for all other access patterns. In fact, the power eciency in the MemoryMode-local conguration with NT-write accesses is even lower than directly writing to PMM. Consequently, the MemoryMode-local conguration consumes more memory energy than the DRAM-local conguration, as reported in Figure 8. This nding is consistent with the conclusion in the bandwidth evaluation to avoid non-temporal writes when DRAM is congured as a cache to PMM.\n\nMentions:\n- We dene the power eciency as the peak bandwidth at 24 threads per socket (one thread per core) per dynamic memory power and report in Figure 7.\n- For the read-only workload in Figure 7, PMM-numa-local and PMM-fsdax-local congurations achieve up to 47% higher power eciency than DRAM-local.\n\nOCR Text: \"(pt \"ata| Uutu\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 7: Dynamic memory power eciency calculated as bandwidth per dynamic memory power."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- 5G is the fifth generation of wireless communications technologies which is 10 times faster than fourth-generation technology. Presently, smartphones are very common among people for voice calls, social networking activities, online banking, telemedicine, etc. COVID-19 is at its peak and seeking to maintain social distancing as the most prominent solution for stopping the spread of the virus, people are shifting towards fully online mode like e-learning, conferences, meetings, shoppings, telemedicine, etc. As the number of active internet users are drastically increased, existing mobile networks are not sufficient to cope up the situation. Hence, 5G technology is the primary requirement to fulfill the current and future requirements. Unlike its prede-   cessor, 5G offers greater bandwidth and higher download speed that can scale up to 10 gigabits per second (Gbps). Fig. 25 highlights the evolution of wireless technologies [120]. With increased bandwidth in 5G, it can compete with other internet service providers (ISP) like cable net that is utilized in laptop or desktop devices and can also widen the scope of applications in IoT and machine-to-machine (M2M) enabled regions. This quality of telecommunication can serve as a foundation of many aforementioned technologies. 5G is now recognized as a disruptive technology with its forthcoming deployment between 2020 and 2040 across the world. Table 14 highlights the literature surveys conducted for 5G and related telecommunication technologies. Since networking and mobile connectivity is the base for every technology, the implication of 5G will directly impact other technologies in terms of data access and transfer rates.\n\nMentions:\n- Fig. 25 highlights the evolution of wireless technologies [120].\n\nOCR Text: \"ap-mn Jac Fu Yotrge GSLS Ijon ILY Tnnolt Gxu #CJ Ceuan Jouit= Thce Maeolta 4b 0u14:49 Dn F Meu dha Meeeet \"k I4 uncac er-nt 0: K= o\"I;\"4 eIk ddiaoa . GTMfm EN(l L: T funec MKm\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 25: Evolution of wireless generation of telecommunication technologies."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 15 shows the next step in the multistage traceback process.\n\nMentions:\n- Figure 15 shows the next step in the multistage traceback process.\n\nOCR Text: \"SPIE Traceoxk Inceback: Controller (P RT Allack Gnaph DGA Roujer Router Router Nmg-l\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 15. Multi-stage Trace – Step 3"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- For the sake of comparison, the training time is calculated by the CPU time of commonly used personal computer (specifically with 2.0 GHz). Figure 3  Together with the fact that large machines may improve the fit precision, our strategy thus prefers larger machines than small ones. Over-fitting problem of large machines can be suppressed by controlling the design risk.\n\nMentions:\n- Figure 3  Together with the fact that large machines may improve the fit precision, our strategy thus prefers larger machines than small ones.\n\nOCR Text: \"@ 3\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3. Training time as functions of the machine size."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Proof of Proposition A.1. Let M = (x, p) be a single-item auction that allocates the item to the bidder with the highest reported value, and charges at most the reported value. Consider a 2-bidder setting, with signal spaces S 1 = S 2 = [0, 3π] and the following valuations (depicted in Figure 2 as a function of s 1 + s 2 ): v 1 (s) = sin(s 1 + s 2 ) + 2(s 1 + s 2 ) and v 2 (s) = sin(s 1 + s 2 + π) + 2(s 1 + s 2 ).\n\nMentions:\n- Consider a 2-bidder setting, with signal spaces S 1 = S 2 = [0, 3π] and the following valuations (depicted in Figure 2 as a function of s 1 + s 2 ): v 1 (s) = sin(s 1 + s 2 ) + 2(s 1 + s 2 ) and v 2 (s) = sin(s 1 + s 2 + π) + 2(s 1 + s 2 ).\n\nOCR Text: \"1 Mjusl-$nls  +51-2$.+521 6usi-;0is +5+0+219+4 9t9 djue\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: Valuations of agents 1 and 2 as functions of s1 + s2. For every bid of one agent, the other has a winning bid."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In [61], the authors introduce an approach based on the Markov chain model to analyze the behavior as well as performance of a selfish mining pool. Figure 13 illustrates the progress of the blockchain as a state machine. The states of the system, i.e., the numbers in the circles represent the lead of the selfish pool in terms of the difference in block number between the private branch and the public branch. In Figure 13, state 0 is the original state when the selfish pool has the same view as the public chain. State 0 ′ indicates that two branches of the same length are published in the network by the selfish pool and the honest miners, respectively. The transitions in Figure 13 correspond to the mining event, i.e., a new block is mined either by the selfish pool or the honest miners. α in Figure 13 represents the computational power of the selfish mining pool. Note that the transition from state 0 to state 0 ′ depends on not only the computational power of the selfish pool, but also the fraction, i.e., µ of honest miners that mine on the selfish pool's branch. In [61], the analysis on the steady state probability of the Markov chain leads to the following two important observations:\n- The key idea behind the stubborn mining strategy is that the selfish miner is stubborn and may only publish part of the private blocks even when it loses the lead to the honest nodes. As shown in Figure 14, the major difference between the two selfish strategies lies in how the selfish miner publishes the private blocks. For example, at state 2, the typical selfish miner will immediately publish all the private blocks once the lead to the honest miners decreases by one block (see Figure 13). Then, the system transits to state 0. In contrast, every time when the honest miners mine a new block, the stubborn miner will stubbornly reveal one block of the private chain, even by doing so it will lose the lead. Simulations in [61] show that stubborn mining achieves up to 13.94% higher gains than selfish mining strategy.\n- that, if the selfish miner mines on the next block in state 0 ′′ , it will immediately publish that block instead of holding it. Compared with the original selfish mining model in Figure 13, state 0 transits to state 1 with probability α(1 − e −β ) and to state 0 ′′ with probability αe −β , where β is the size of the mining block. The new factor β is introduced to model the impact of transaction fees on the miner's decisions. With the revised transition probability, if the selfish miner finds a block of high value in state 0, it may publish the block (i.e., transiting to state 0 ′′ ) instead of holding it (i.e., transiting to state 1). The analysis in [135] shows that this improved selfish mining strategy leads to positive profit for all miners regardless of their hashrates.\n\nMentions:\n- Figure 13 illustrates the progress of the blockchain as a state machine.\n- In Figure 13, state 0 is the original state when the selfish pool has the same view as the public chain.\n- The transitions in Figure 13 correspond to the mining event, i.e., a new block is mined either by the selfish pool or the honest miners. α in Figure 13 represents the computational power of the selfish mining pool.\n- For example, at state 2, the typical selfish miner will immediately publish all the private blocks once the lead to the honest miners decreases by one block (see Figure 13).\n- Compared with the original selfish mining model in Figure 13, state 0 transits to state 1 with probability α(1 − e −β ) and to state 0 ′′ with probability αe −β , where β is the size of the mining block.\n\nOCR Text: \"HLl-4 W- 1A1 1.0 1-d\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 13. Blockchain state transition in the presence of a selfish pool (adapted from [61])."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- This quantity is shown in Fig. 10, for different values of λ GB and ϑ. From Fig. 10 we see that, for λ GB ą 0, the peak in the spectrum of photons becomes higher, widens and gets shifted to the right. For λ GB ă 0, the peak becomes smaller, sharpens and gets shifted to the left. This should be contrasted with the results of [108] for a top-down higher derivative correction of the form α 13 R 4 , where the peak in the spectrum becomes higher, sharpens and gets shifted to the left, approaching the weak coupling result [98], which shows a very sharp peak at small w in the photon spectrum. Therefore, the inclusion of the α 13 R 4 correction (which corresponds to a finite 't Hooft coupling correction in the gauge theory) goes into the direction of the weak coupling results, while this does not seems to be possible in the case of Gauss-Bonnet. However, a partial agreement between these two types of corrections is found when λ GB ă 0, where the peak in the photon spectrum sharpens and moves to the left, but it also becomes smaller, contrary to what happens at weak coupling. We can understand this partial agreement noting that, for λ GB ă 0, the ratio η{s \" p1´4λ GB q{p4πq increases, which also happens with η{s when finite 't Hooft coupling corrections were taking into account. Since at weak coupling the shear viscosity over the entropy density ratio is proportional to the mean free path of momentum isotropization, we can associate the approaching of the weak coupling results (negative λ GB corrections or α 1 -corrections) with a larger mean free path in both cases. From top to bottom, the value of the Gauss-Bonnet coupling is identified as λ GB \" 0.1 (blue), λ GB \" 0.05 (purple), λ GB \" 0 (black), λ GB \"´0.05 (brown), λ GB \"´0.1 (red). We have fixed ϑ \" 0 and a \" 0.2.\n\nMentions:\n- This quantity is shown in Fig. 10, for different values of λ GB and ϑ.\n- From Fig. 10 we see that, for λ GB ą 0, the peak in the spectrum of photons becomes higher, widens and gets shifted to the right.\n\nOCR Text: \"0 015 - 0 0 010 0 0C5 0 OCD\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 10: Total photon production rate as a function of w “ k0{2πT0. From top to bottom, the value of the Gauss-Bonnet coupling is identified as λGB “ 0.1 (blue), λGB “ 0.05 (purple), λGB “ 0 (black), λGB “ ´0.05 (brown), λGB “ ´0.1 (red). We have fixed ϑ “ 0 and a “ 0.2. The results for different angles are very similar to the plot above due to the smallness of the anisotropy."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 7 shows the topology of the first test scenario with 3 Wi-Fi Direct hosts created. In this topology we have a radio medium, an IPv4 Configurator for IP address assignment and three hosts of type WirelessHost with their manager module set to Ieee80211MgmtSTAWifiDirect. The wirelessHost used can be found in the INET project at inet.node.inet.WirelessHost. In the Discovery phase, each host firstly scans and doesn't detect any GO available. Then, they switch to alternating between the listen and search states to try to discover another Wi-Fi Direct host. Fig. 8 presents partially the Discovery phase when host[0] and host [1] discover each other. We can see that host[0] sends a Discovery Probe Request in channel 0 at event #403. Host [1] is in the listen state over this channel at that time and responds with a Probe Response message at event #419. Host[0] sends an acknowledgement (ACK) frame at event #432 and moves on the GO negotiation phase. Fig. 9. Duration of the discovery phase Fig. 9 shows the duration of the discovery phase that we have measured by our simulations. The average discovery time is about 2-3 seconds. This result corresponds the average discovery time that we have experienced with two smartphones supporting Wi-Fi Direct. Fig. 10. The GO negotiation phase Fig. 10 presents the GO negotiation phase with the threeway handshake between host[0] and host [1]. At the end, host [1] becomes the GO and starts sending Beacons. Host[0] finishes the group formation by completing the Provisioning phases 1 &amp; 2 with host [1]. We have implemented these phases by the exchange of a number of Authentication frames which can be configured in the simulation configuration .ini file. Upon the reception of Beacon frames sent by host [1], host [2] starts the joining procedure to join the group. Fig. 11 shows the test of connectivity within the group by sending a Ping request from host[0] to host [2] (event #2362) and another Ping request from host [1] to host[0] (event #2493). For the first Ping request, we can see that host [1] plays the role of AP relaying the Ping request from host[0] to host [2] (event #2390). The Ping reply is also transferred to host[0] via host [1], the GO of the group (event #2450). For the second Ping request, host[0] responds by a Ping reply directly (event #2525).\n\nMentions:\n- Duration of the discovery phase Fig. 9 shows the duration of the discovery phase that we have measured by our simulations.\n\nOCR Text: \"2 1 1 1 ] 6 1 8 Experence nUmuer 6\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 9. Duration of the discovery phase"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The Kepler space telescope discovered the four sub-Neptune-sized planets Kepler-107b, c, d and e (ref. 9 ), which transit (that is, pass in front of) their host star and have orbital periods (P) of 3.180, 4.901, 7.958 and 14.749 d, respectively. The four planets form a compact near-resonant system with period ratios close or almost equal to ratios of small integers, specifically P c /P b =1.541 (~3:2), P e /P c =3.009 (~3:1), P d /P b =2.503 (~5:2). The near resonances imply that the Kepler-107 planets likely formed further out and then migrated inward getting trapped into resonances during the migration process 11 . The orbital eccentricities of all four planets were determined to be low, consistent with zero 12 , as expected from both dynamical stability criteria and orbit circularization times for Kepler-107b and c (see Methods). Despite the proximity to resonances, no significant transit timing variations from gravitational planet-planet interactions could be detected for any planet 13 , preventing the direct determination of planetary masses from the Kepler light curve alone. To determine the planet masses, and hence their bulk densities, we observed Kepler-107 from 2014 June 21 to 2017 April 22 with the fibre-fed high-resolution (R=115,000) HARPS-N spectrograph 14 at the Telescopio Nazionale Galileo in La Palma. Our time series with 114 radial velocities has a scatter of 6 m s -1 and shows two significant periodicities at 4.9 d and at about 14 d (see Methods and Supplementary Figs. 5 and 6). The former is straightforwardly attributed to the Doppler signal induced on the star by the second inner planet Kepler-107c; the latter is likely related to stellar magnetic activity variations though at low level, as shown by the low amplitude of Kepler flux variations (see Methods and Supplementary Fig. 2). From the 114 HARPS-N spectra we determined the stellar atmospheric parameters (see Table 1 and Methods). Even though the parent star is richer in iron than most of the stars that are known to host low-mass planets 15 , it does not present an overabundance of iron relative to Si and Mg having solarlike Fe/Si and Mg/Si photospheric ratios (see Methods and Supplementary Table 3). To determine the stellar fundamental parameters (radius, mass and age) we performed a detailed asteroseismic analysis with all the available Kepler short-cadence data. We extracted the frequencies for individual modes of stellar oscillations from the power density spectrum of the Kepler light curve (see Methods and Supplementary Fig. 4). We modelled them with the BAyesian STellar Algorithm 16 and found M ✶ =1.238±0.029 M ⊙ , R ✶ =1.447±0.014 R ⊙ , ρ ✶ =0.574±0.029 g cm -3 , logg=4.210±0.013 (cgs), in agreement with the spectroscopic value within 1σ, and an age t= 4.29 −0.56 +0.70  Gyr (see Methods). These parameters are consistent with those that were previously determined 17 but are more precise and accurate. By using the constraint on the stellar density as provided by our asteroseismic analysis, we\n- modelled the transits of the four Kepler-107 planets (Supplementary Fig. 1) and derived their radii and associated uncertainties by employing a differential evolution Markov chain Monte Carlo Bayesian technique 18 (see Table 1 and Methods). We confirm the absence of significant transit timing variations (see Methods and Supplementary Fig. 8). In the same Bayesian framework, we modelled the HARPS-N radial velocities with four Keplerians by using the updated ephemerides from the transit fitting, and an additional sinusoid to account for the activity signal at ~14 d (see Fig. 1 and Methods). More sophisticated models to take the activityinduced correlated noise into account 19 provided very consistent results (see Methods and Supplementary Table 2). The radial velocity semi-amplitudes and derived planetary masses are listed in Table 1. Only an upper limit could be established for the mass of Kepler-107d because its Doppler signal is undetected due to its expected low mass and its orbital period longer than planets b and c. The positions of the Kepler-107 planets in the radius-mass diagram of small planets with a bulk density measured better than 3σ are shown in Fig. 2 along with iso-composition 20 curves. The two innermost planets Kepler-107b and c have nearly equal radii, that is, R p,b =1.536±0.025 R ⊕ and R p,c =1.597±0.026 R ⊕ , but dissimilar masses: M p,b =3.51±1.52 M ⊕ and M p,c =9.39±1.77 M ⊕ . This means that they have different bulk densities with ρ p,b =5.3±2.3 g cm -3 and ρ p,c =12.6±2.4 g cm -3 (Table 1). According to theoretical models of planetary interiors 20 , both Kepler-107b and c are consistent with a rocky composition. However, Kepler-107c appears Mercury-like in composition and has an iron fraction that is at least a factor of two greater than that of Kepler-107b. Kepler-107c's iron core makes up ~70% of its mass, with the silicate mantle making up ~30% (see Methods). Kepler-107d could also be rocky, while Kepler-107e is a mini-Neptune containing a significant fraction of volatiles in the form of a thin H/He envelope and/or water ice (see Fig. 2 and Methods). Unlike the pair of Kepler-36 neighbouring planets 21 , the difference in planetary density between Kepler-107b and c cannot be explained by atmospheric escape (or photoevaporation) caused by the high XUV stellar irradiation. This irradiation may partially or totally strip the gaseous H/He envelopes of mini-Neptunes at close distances from the star and thus may yield a diversity in exoplanet bulk densities 7 . However, it cannot be responsible for the observed dissimilar densities of Kepler-107b and c because it would have led to a higher density for Kepler-107b than for Kepler-107c (see Methods), which is at odds with our results. Alternatively, the difference in density of the two inner planets can be explained by a giant impact on Kepler-107c that removed part of its mantle, significantly reducing its fraction of silicates with respect to an Earth-like composition 22 . The radius and mass of Kepler-107c, indeed, lie on the empirically derived collisional mantle stripping curve for differentiated rocky/iron planets 10,23 (grey dashed line in Fig. 2). Smoothed particle hydrodynamics simulations show that a head-on highspeed giant impact between two ~10 M ⊕ exoplanets in the disruption regime would result in a planet like Kepler-107c with approximately the same mass and interior composition (see Fig. 3 and Methods). Such an impact may destabilise the current resonant configuration of Kepler-107 and thus it likely occurred before the system reached resonance. Multiple less-energetic collisions may also lead to a similar outcome 24 . Other planets with a Mercury-like composition such as K2-106b (ref. 25 ) and K2-229b (ref. 3 ) have been recently discovered. However, they are all the innermost planets in their systems and thus alternative mechanisms to the giant impact scenario may have given rise to their high iron content, for instance mantle evaporation for the hottest planets 3 or photophoresis and disc aerodynamic fractionation yielding a depletion of silicates in the inner regions of the protoplanetary disc 26 . These other processes cannot explain the dissimilar densities of Kepler-107b and c (see Methods). An alternative explanation might be that planet c formed closer to the parent star than planet b and afterwards crossed its orbit; if so, the iron-rich composition of Kepler-107c might actually be related to formation in the silicate-poor inner region of the protoplanetary disc 26 with no need to invoke a giant collision. However, this orbit crossing should have occurred before the dispersal of the disc, so that the resulting eccentricities could have been damped. Given the relatively large mass of planet c, it seems unlikely that there would have been sufficient time for such a scenario to operate, or that it may have occurred without the eccentricities of the lower-mass planets becoming large enough to destabilize the system. Giant impacts are thought to have occurred in our Solar System and have been invoked to explain the composition of Mercury 27 , the origin of the Earth-Moon system 28 and the high orbital obliquity of Uranus 29 . We have shown that they likely occurred in the exoplanetary system Kepler-107 and shaped the compositional properties of its two inner planets. If catastrophic disruption impacts occur frequently, we predict a clustering of small exoplanets along the maximum-collisional stripping curve 10 in the mass-radius diagram, as an increasing number of exoplanets are characterized with precise radius and mass determinations. Correspondence and requests for materials should be addressed to A.S.B.\n- The long-cadence PDC light curve with transits removed shows a peak-to-peak amplitude of 0.1%, with a maximum of ~0.2% between 1100 and 1300 BJD TDB -2,454,900 (Supplementary Fig. 2). We performed a weighted autocorrelation function of this light curve finding the highest, though low-amplitude, peak at ~14 d (Supplementary Fig. 3). This peak might be the stellar rotation period P rot , which would be consistent with the upper limit given by the projected rotational velocity Vsini (Table 1) assuming an edge-on stellar equator, that is, P rot &lt;20.3 −2.5 +3.3 d ( P rot &lt;20.3 −6.0 +14.5 d) at 1σ (3σ). No additional information on P rot could be extracted from the stellar activity indicators (see below).\n\nMentions:\n- The former is straightforwardly attributed to the Doppler signal induced on the star by the second inner planet Kepler-107c; the latter is likely related to stellar magnetic activity variations though at low level, as shown by the low amplitude of Kepler flux variations (see Methods and Supplementary Fig. 2).\n- The positions of the Kepler-107 planets in the radius-mass diagram of small planets with a bulk density measured better than 3σ are shown in Fig. 2 along with iso-composition 20 curves.\n- Kepler-107d could also be rocky, while Kepler-107e is a mini-Neptune containing a significant fraction of volatiles in the form of a thin H/He envelope and/or water ice (see Fig. 2 and Methods).\n- The radius and mass of Kepler-107c, indeed, lie on the empirically derived collisional mantle stripping curve for differentiated rocky/iron planets 10,23 (grey dashed line in Fig. 2).\n- The long-cadence PDC light curve with transits removed shows a peak-to-peak amplitude of 0.1%, with a maximum of ~0.2% between 1100 and 1300 BJD TDB -2,454,900 (Supplementary Fig. 2).\n\nOCR Text: \"K-107 6 VIFise\" 224 K-^07 {127 d MKI 1 3 706949\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2 | Mass–radius diagram of exoplanets smaller than 3 R⊕ . Only known planets with bulk densities measured to better than 33% precision through radial velocities (RV, circles) and transit timing variations (TTV, triangles) are shown. Error bars represent 1σ confidence intervals. Colours of symbols are a function of the planet irradiation level (or equilibrium temperature) assuming an Earth-like albedo. The solid lines are theoretical mass-radius curves20 for planets with different compositions as specified in the figure (see also Methods). The grey dashed line displays the minimum radius predicted from collisional stripping models as a function of planetary mass10."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The evolution of the thermal contraction as a function of temperature is given in Fig. 4 for some materials as well as the CTE at room temperature in Table 3.\n\nMentions:\n- The evolution of the thermal contraction as a function of temperature is given in Fig. 4 for some materials as well as the CTE at room temperature in Table 3.\n\nOCR Text: \"uilmint -Sarless >teel 7 Copper Tanm 1 L Temperature IKI\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4: Thermal contraction of some materials"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Any multilingual calls that are not resolvable before run-time pose software engineering risks and should be flagged for review. Figure 4 depicts a potential security issue in which a Python program verifyAccount.py calls a JavaScript program, but the identity of the JavaScript program cannot be determined before run time.\n- The graph in Figure 5 is an example of such a circular system, in which the Python program veriftyAccount.py calls the JavaScript program welcome.js, and then the same JavaScript program welcome.js calls the Python program verifyAccount.py. The circularity of the system is flagged visually on the graph in Figure 4 by the node outlined with double dotted perimeter. The call may be fine, but because it is hidden from regular software engineering inspection behind the interoperability API, it requires another look.\n\nMentions:\n- Figure 4 depicts a potential security issue in which a Python program verifyAccount.py calls a JavaScript program, but the identity of the JavaScript program cannot be determined before run time.\n- The circularity of the system is flagged visually on the graph in Figure 4 by the node outlined with double dotted perimeter.\n\nOCR Text: \"Vetu #aHl IrM I[ pu\"am TEmd JuNts CHlculy iUESt Kocyros [vpne f2m\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4. Statically unresolved call from Python to JavaScript"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The basic assumption in simulation-based optimization is that a call to an expensive simulation software component present in the constraints or objectives is needed. The framework is divided in three exchangeable components, as shown in Fig. 3, to encapsulate the major behavioral patterns of the framework.\n- Every available MPI process will take up one of the three available roles (see Fig. 1): one process acts as Pilot, the remaining processes are divided amongst Worker and Optimizer roles. Both, the Worker and the Optimizer can consist of multiple MPI processes to exploit parallelism. As shown in Fig. 3, the Pilot is used to coordinate all\n\nMentions:\n- The framework is divided in three exchangeable components, as shown in Fig. 3, to encapsulate the major behavioral patterns of the framework.\n- As shown in Fig. 3, the Pilot is used to coordinate all\n\nOCR Text: \"OPIL CPILIZERS PiLor WcRKERS Wu\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 3. Schematic view of messages passed within the network between the three roles. The dashed cyan path describes a request (job j1) sent from Oi to the Pilot being handled by Wj . Subsequently the result rk is returned to the requesting Optimizer (Oi). The work Wj are beam dynamics simulation within OPAL."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Let us now pull together the results of the section IV and summarise the conditions under which an eye is likely to form, at least in the particular model system investigated here. This is summarised in cartoon fashion in Figure 14 as a phase diagram of E versus Re, with Pr and ε both held fixed.\n\nMentions:\n- This is summarised in cartoon fashion in Figure 14 as a phase diagram of E versus Re, with Pr and ε both held fixed.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 14. Schematic structure of the phase diagram for the appearance of eyes in the model system considered in this paper. Eyes are not expected to form outside the shaded region limited by the four curves shown. Extension of this domain to the right of this figure is so far unexplored."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Here we present a summary of the curriculum that will be offered as part of the course: \"Artificial Intelligence assisted Malware Analysis\". This course will have 6 modules as shown in Figure 1, including (1) Cyber Threat Intelligence (CTI) and malware attack stages (2) Malware knowledge representation and CTI sharing (3) Malware data collection and feature identification (4) AI assisted malware detection (5) Malware classification and attribution (6) Advanced malware research topics and case studies. Here each module consists of lectures and lab sessions. Figure 1, shows a strong bias towards experimentation/lab exercises.\n\nMentions:\n- This course will have 6 modules as shown in Figure 1, including (1) Cyber Threat Intelligence (CTI) and malware attack stages (2) Malware knowledge representation and CTI sharing (3) Malware data collection and feature identification (4) AI assisted malware detection (5) Malware classification and attribution (6) Advanced malware research topics and case studies.\n- Figure 1, shows a strong bias towards experimentation/lab exercises.\n\nOCR Text: \"Wl-Latut Akalmt \"hul Kallh Halitue En Nt nudt Ktyal uldlunc Meatc Cecanm mm Entm Mn O Eentan TAA Whe 06: Eeat Caxui Ita raLT enlaa UXM\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Overview of Course Modules and Labs."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We learn P (t|e) with a general architecture; see Figure 1. The output layer has size |T |. Unit t of this layer outputs the probability for type t. \"Entity Representation\" ( v(e)) is the vector representation of entity e -we will describe in detail in the rest of this section what forms v(e) takes. We model P (t|e) as a multi-label classification, and train a multilayer perceptron (MLP) with one hidden layer:\n- We experiment with four architectures to produce character-level representations in this paper: FORWARD (direct forwarding of character embeddings), CNNs, LSTMs and BiLSTMs. The output of each architecture then takes the place of the entity representation v(e) in Figure 1.\n- We implement the following two feature sets from the literature as a hand-crafted baseline for our character and word level models. (i) BOW: individual words of entity name (both as-is and lowercased); (ii) NSL (ngram-shape-length): shape and length of the entity name (cf. Ling and Weld (2012)), character n-grams, 1 ≤ n ≤ n max , n max = 5 (we also tried n max = 7, but results were worse on dev) and normalized character n-grams: lowercased, digits replaced by \"7\", punctuation replaced by \".\". These features are represented as a sparse binary vector v(e) that is input to the architecture in Figure 1.\n\nMentions:\n- We learn P (t|e) with a general architecture; see Figure 1.\n- The output of each architecture then takes the place of the entity representation v(e) in Figure 1.\n- These features are represented as a sparse binary vector v(e) that is input to the architecture in Figure 1.\n- We then build a simple entity representation by averaging the embeddings of the top k words (wrt tf-idf) of the description (henceforth, AVG-DES). 9 This representation is used as input in Figure 1 to train the MLP.\n\nOCR Text: \"Output Layer (type probabilities) Hidden Loyer Entity Representation\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Schematic diagram of our architecture for entity classification. “Entity Representation” (~v(e)) is the (one-level or multi-level) vector representation of entity. Size of output layer is |T |."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 2: Groups of features used by our system and how they relate to the tweet itself, the sender, the recipient etc. ese steps are based on the preprocessing used in GloVe [12] to turn words into embeddings [11]. Hence, in a future extension of our system one could easily use GloVe embeddings.\n- Group 1 (Fig. 2, Table 1) contains features that examine the tweet itself (e.g., length, if it contains a URL or not, if it mentions a Twi er account). Longer tweets, or tweets that contain URLs of longer posts (e.g., news articles) or photographs may be more informative and, thus, more interesting. Tweets that mention other user accounts may be parts of dialogues, which may be uninteresting to recipients, unless they interact frequently with the sender (see also Group 4). Hashtags may indicate trending topics. Tweets that have already been retweeted or favoured by many users are more likely to be important. Exclamation marks indicate surprise or strong feelings.\n- Group 2 (Fig. 2, Table 2) contains features that examine how similar the incoming tweet is to particular collections of tweets (e.g., all tweets previously posted by the sender). e similarity between the incoming tweet t and a collection of tweets C = {c 1 , . . . , c n } is computed as the average TF-IDF cosine similarity between t and each c i . e intuition in Group 2 is that recipients may prefer tweets that are similar or dissimilar (if they prefer surprising posts) to the posts of the particular sender, or their own posts, or the posts they usually see or retweet.\n\nMentions:\n- Figure 2: Groups of features used by our system and how they relate to the tweet itself, the sender, the recipient etc. ese steps are based on the preprocessing used in GloVe [12] to turn words into embeddings [11].\n- Group 1 (Fig. 2, Table 1) contains features that examine the tweet itself (e.g., length, if it contains a URL or not, if it mentions a Twi er account).\n- Group 2 (Fig. 2, Table 2) contains features that examine how similar the incoming tweet is to particular collections of tweets (e.g., all tweets previously posted by the sender). e similarity between the incoming tweet t and a collection of tweets C = {c 1 , . . . , c n } is computed as the average TF-IDF cosine similarity between t and each c i . e intuition in Group 2 is that recipients may prefer tweets that are similar or dissimilar (if they prefer surprising posts) to the posts of the particular sender, or their own posts, or the posts they usually see or retweet.\n- Group 3 (Fig. 2, Table 3) contains features modeling the network in uence, popularity, and authority of the sender and the recipient. ese features include Twi er account statistics (number of followers, number of posts, days active for, list subscriptions), features that may indicate authority (veri ed accounts, URLs in the description elds of their pro les), as well as scores obtained from Klout, a service that estimates a user's social in uence by taking into account their activity in various social networks. 4 Group 4 (Fig. 2, Table 4) contains features that capture the interaction between the sender and the recipient (e.g., whether or not tweets of the sender mention the recipient). e intuition is that recipients are more likely to be interested in posts of senders they interact more closely with.\n- Group 5 (Fig. 2, Table 5) contains features that a empt to estimate the timeliness of the incoming tweet.\n\nOCR Text: \"CCC1E ~Aneui\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: Groups of features used by our system and how they relate to the tweet itself, the sender, the recipient etc."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Despite porosity controlling secretory rate in Models 3 and 4, the overall porosity of the initial pore influences the average infilling rate less than the presence or absence of highly curved concavities. Figure 10 in the appendix shows simulations of random pore 8 scaled down by 70% and scaled up by 130%. While the smaller pore infills slower than the larger pore due to the porosity dependence of the secretory rate k f , the difference in average interface velocity (MAR) (Fig. 11 in the appendix) is less than that induced by differences in the irregularity of the initial pores (Figs. 8-9). The nonlinearity of the curvature-induced cell crowding makes this influence dominant for the overall speed of pore infilling.\n- Figure 10 shows simulations of the infilling of random pore 8 of Fig. 7 scaled down by 70% and scaled up by 130%, performed with Model 3 and D = 0.00075mm 2 /day. The corresponding plot of the average interface velocity versus the average pore radius are shown in Fig. 11.\n\nMentions:\n- While the smaller pore infills slower than the larger pore due to the porosity dependence of the secretory rate k f , the difference in average interface velocity (MAR) (Fig. 11 in the appendix) is less than that induced by differences in the irregularity of the initial pores (Figs. 8-9).\n- The corresponding plot of the average interface velocity versus the average pore radius are shown in Fig. 11.\n\nOCR Text: \"X10 ' @oralha Uceeraci MIcaldame Infoifozin Intuifutt [ 0* E Intalfoz?; 1204 1 9 0.02 average pore radis 'R' [rm]\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 11 – Comparison of the average interface velocity versus the average pore radius between different scales of random pore 8 (solid lines) and experimental data (black squares)"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- At energy densities ρ &gt; ρ c the vacuum state maintains the SO(4) symmetry, leading to | φ| = 0. The metric elements are therefore g 00 = 1 and g ii = 1, representing the fourdimensional Euclidean space. This region is however forbidden due to the constraint ρ ≤ ρ c present in LQC. In turn, below the critical energy ρ ≤ ρ c , the minimum of the potential is located at | φ| = 1 − ρ ρc in some spontaneously chosen direction (See Fig. 3). Without loss of generality, let us assume that the SSB takes place in direction φ 0 , for which\n\nMentions:\n- In turn, below the critical energy ρ ≤ ρ c , the minimum of the potential is located at | φ| = 1 − ρ ρc in some spontaneously chosen direction (See Fig. 3).\n\nOCR Text: \"critical point forbidden region\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 3. Modulus of the filed φµ as a function of the energy density ρ. The region ρ > ρc is forbidden within the model."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The absence of additional objects in the image obtained offers clear constraints to the presence of very low mass stars or brown dwarfs orbiting WASP-33. In Fig. 4 we show the sensitivity of the present observations for different filters, as a function of the angular separation. The solid line is the observational limit obtained with the Kc-filter, respectively the dash line -Hc-filter, dot line -Jc-filter and, dash-dot line -Fe II filter. The Kc-filter offers the greater accuracy and the presence of different objects can be ruled-out as a function of the distance to WASP-33. Fig. 5 shows an estimation of this limit (using K-filter as reference) in terms of the mass in solar units and distance from the star in AU for the age determined for WASP-33. The isochrones used to convert luminosities to masses are made from the latest NextGen models (Baraffe et al. 1997) down to 2.300 ō K, and DUSTY models (Baraffe et al. 2002) up to 2.300 ō K. The limiting region is bounded by the errors in the determination of the magnitude of WASP-33, its distance and its age. The age of the objects is assumed to be in the range [10,500] Myr, that is, from ZAMS to the largest age estimated for WASP-33. The discontinuities in the lower boundary of the limiting band are a reflection of the small discontinuity found in the transition between the two models used.\n\nMentions:\n- Fig. 5 shows an estimation of this limit (using K-filter as reference) in terms of the mass in solar units and distance from the star in AU for the age determined for WASP-33.\n\nOCR Text: \"Detzcticn limit pntktl phesical dis AIle [AU\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 5. Maximum mass in solar units of possible objects out of the detection limits of our observations as a function of theprojected physical distance to the central star (in AU). The observational errors in the determination of the magnitude, distance and age have been taken into account. See text for details."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To select the optimal layer for image representation we performed an image classification task on a subsection of images from SUN scenes (Xiao et al., 2010). We extract the features from the various layers of ResNet-50 and evaluate the classification performance (Figure 1). The results increase during the first layers but stabilize from Block-4 on. Based on these results and considering that a higher spatial resolution is better, we have selected layer 'res4f relu' (end of Block-4, after ReLU) for the experiments on multimodal MT. We also compared the features from different networks on the task of image description generation with the system of Xu et al. (Xu et al., 2015). The results for generating English descriptions (Table 3) show a clear performance improvement from VGG-19 to ResNet-50, but comparable results are obtained when going to ResNet-152. Therefore, given the increase in computational cost, we have decided to use ResNet-50 features for our submission.\n\nMentions:\n- We extract the features from the various layers of ResNet-50 and evaluate the classification performance (Figure 1).\n\nOCR Text: \"85 Hkz Hock3 bck Hock5 ResNetL (Lavers\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Classification accuracy on a subset of SUN scenes (Xiao et al., 2010) for ResNet-50: The colored groups represent the building blocks while the bars inside are the stacked blocks (He et al., 2015)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Thus, pores in the 0.5Fe alloy are more spherical. Fig. 9c shows the exponential relationship between the sphericity and equivalent diameter of pores in the 0.5Fe and 1.0Fe alloys; the equations are Y = 7.14*X -1. 29 and Y = 7.06*X -1.20 , respectively. Notably, pores with an equivalent diameter greater than 40 μm have a sphericity value between 0.1 and 0.3. This indicates that the greater the equivalent diameter of pores, the smaller the sphericity. The mean curvature of pores follows a normal distribution (Fig. 9d). The distribution peak position (μ) of the pores increases from 0.05 to 0.09 and the standard deviation (σ) decreases from 0.49 to 0.32 as the Fe content increases from 0.5 % to 1.0 %. The difference in the mean curvature of pores is correlated to the gap constrained by the Fe phases, Al2Cu, and Al matrix. Al2Cu and Fe phases in the 1.0Fe alloy possess high mean curvature; thus, pores in the 1.0Fe alloy possess high mean curvature owing to the pinching effect. To better understand the influence of intermetallic phases and pores on the mechanical properties of alloys, tensile tests were performed on the 0.5Fe and 1.0Fe alloys. The stressstrain curves of the 0.5Fe and 1.0Fe alloys are shown in Fig. 10. The ultimate tensile strength and elongation of the 0.5Fe alloy are 233.7 MPa and 5.51 %, respectively, whereas the corresponding values for the 1.0Fe alloy are 199.8 MPa and 3.64 %, respectively. The mechanical properties of the alloys decrease as the Fe content increases from 0.5 % to 1.0 %, which is in accordance with previous reports [15,41].\n- Fig. 10 Stress-strain curves for the 0.5Fe and 1.0Fe alloys\n\nMentions:\n- The stressstrain curves of the 0.5Fe and 1.0Fe alloys are shown in Fig. 10.\n- Fig. 10 Stress-strain curves for the 0.5Fe and 1.0Fe alloys\n\nOCR Text: \"0,5Fe alloy 1,0Fe alloy Alloys UTS (MPa} Elongation (%} 5 OSFe 223.7+13,9 351+0.78 LF e 199,816,0 3.6410,J8 Strain %)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 10 Stress-strain curves for the 0.5Fe and 1.0Fe alloys"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where we use α = 1 and β = 2. The strength coefficients A and B are then adjusted so that nuclear matter saturates at ρ s = 0.153 fm −3 and the associated compression modulus is K = K N + K w = 300 MeV, where K N = − 6 5 E F ≈ −43 MeV is the contribution from the Fermi motion of the nucleons (which is negative). The binding energy of nuclear matter is then also roughly reproduced. The resulting equation of state for nuclear matter is shown in Fig. 8.\n\nMentions:\n- The resulting equation of state for nuclear matter is shown in Fig. 8.\n\nOCR Text: \"1 3 1 Compressinn pYpe\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 8: The equation of state, pT (ρ), for the (ρ, T ) region relevant for ordinary nuclear matter: the dependence of the pressure on the compression for specified T (shown in MeV)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- i2b2 [40] is an open source analytic query tool built on web services. i2b2 consists of a set of server-side software modules called Cell and uses XML message for inter-cell communications which is illustrated in Figure 1. Data is stored in a relational database such as Oracle using a common star schema data model. i2b2 is used by more than 200 healthcare institutions for cohort selection. Although using the relational database gives the advantage of SQL, with web based architecture the proposed services are not as flexible as SQL itself. Furthermore, relational databases have the horizontal scalability issue and also are not optimized for large unstructured data. In the context of reproducibility, i2b2 allows users to share their queries within a group to be repeated with the same data or used for a new set of data. However, this notion of reproducibility is only at the query level and there is no mechanism to share statistical methods and analytics pipelines. Although i2b2 provides a set of pre-loaded machine learning and statistical algorithms, its web based architecture allows to develop more sophisticated and complex algorithms as new web services for advanced programmers or engineers.\n\nMentions:\n- i2b2 [40] is an open source analytic query tool built on web services. i2b2 consists of a set of server-side software modules called Cell and uses XML message for inter-cell communications which is illustrated in Figure 1.\n\nOCR Text: \"Gheni Scrvcr onte Halerlnkc Canent LuntteEg npeniury\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: i2b2 Architecture[40]: The Component and Connector view shows a Client/Server view of i2b2’s instances and the protocols they use for connection."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- As mentioned in Section 3.2.4, in a standard collusion attack, colluding SPs select (and share) the most commonly observed value for each data point. In this attack, although the utility of the data leaked by the colluding SPs (U Y ) is high, detection techniques can detect the colluding SPs with high probability. Hence, we defined a probabilistic majority attack in Section 5.1 that also includes correlation and flipping attacks. With this attack, the colluding SPs decrease the accuracy of the detection algorithm by reducing the utility of data. In this section, we first compare the utility and robustness under these two attacks. We set c = 10 and r = 5 as the Boneh-Shaw parameters. Hence, we can create 10 different Boneh-Shaw codewords with block size of 5. Note that c is the number of codewords and Alice can share her data more than c SPs in the proposed scheme by repeating codewords as we discussed before. We set the total number of SPs (that received data owner's data) to 10, τ c = 0.1, and p f = 0.1. We quantified both utility and fingerprint robustness for different values of number of colluding SPs (n). As shown in Figure 7, using probabilistic majority attack decreases the colluding SPs' probability of being detected by reducing the data utility. Since the probabilistic majority attack is more powerful than the standard one for the colluding SPs, we perform the probabilistic majority attack for the rest of the experiments.\n- As mentioned, Boneh-Shaw codes are not designed to be robust against majority attacks. They assume the colluding SPs decide the value of a data point randomly if they observe more than one value in their copies. Hence, Boneh-Shaw codes do not provide guarantees against the proposed probabilistic majority attack (including flipping). Furthermore, in order to provide the robustness guarantees of Boneh-Shaw codes, the number of fingerprinted points needs to be high. For instance, to create Boneh-Shaw codes for c = 10 with 90% robustness guarantee, the block size needs to be greater than 1,000 and we need more than 10,000 fingerprinted data points. Fingerprinting such a high number of data points also decreases the utility of the data owner (U i ) significantly. Therefore, direct use of Boneh-Shaw codes do not provide guarantees, especially for small data sizes (we only use approximately p • l = 100 data points for fingerprinting in our experiments). To show these shortcomings, we implemented Boneh-Shaw codes as a standalone fingerprinting scheme and observed its robustness against the probabilistic majority attack using the same parameters with the previous experiment (Figure 7). We observed 55% detection accuracy when the number of colluding SPs was 2 (for the same scenario, the accuracy of the proposed scheme is more than 99%, as shown in Figure 7). Therefore, we conclude that using Boneh-Shaw codes as a standalone fingerprinting scheme does not provide robustness against probabilistic majority attack (which includes correlation and flipping attacks). While our proposed scheme utilizes the Boneh-Shaw codes to increase its robustness against collusion attacks, Algorithm 5.1 generates unique fingerprints to provide robustness against correlation and flipping attacks.\n\nMentions:\n- As shown in Figure 7, using probabilistic majority attack decreases the colluding SPs' probability of being detected by reducing the data utility.\n- To show these shortcomings, we implemented Boneh-Shaw codes as a standalone fingerprinting scheme and observed its robustness against the probabilistic majority attack using the same parameters with the previous experiment (Figure 7).\n- We observed 55% detection accuracy when the number of colluding SPs was 2 (for the same scenario, the accuracy of the proposed scheme is more than 99%, as shown in Figure 7).\n\nOCR Text: \"Joaz 4 Staudud Haurt# Eia Preb Lilislic Mijicity Atick di;: Stati Ard Mejuit Auter Puklic \"_ Tle purober of colhuticz SPs\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 7. Fingerprint robustness against standard and probabilistic majority attacks for different values of n (number of colluding SPs). Right y-axis is used to show the utility of the attackers (UY ) after performing majority attack."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- These rules support the construction of larger systems from the components using concurrent and/or sequential compositions. Together with these rely/guarantee rules, we will also make extensive use of Equation ( 15) because it provides the transport of probabilistic choices to the \"upper level\" of the specification automaton. That is, it allows us to write simple rely properties and postconditions of the form (bad + good) p ⊕ good or even bad p ⊕ good where good and bad are usually standard automata. With the testing interpretation of simulation, we conclude that the maximal probability for bad to happen is bounded from above by 1 − p. where Q is given by the diagram in Figure 3. The first quintuple has been established algebraically in the previous section, that is, M ≤ H. As for the second one, it is clear that H U ′ ≤ Q ′ which can be established by direct automata calculation or using the algebra as before.\n\nMentions:\n- With the testing interpretation of simulation, we conclude that the maximal probability for bad to happen is bounded from above by 1 − p. where Q is given by the diagram in Figure 3.\n\nOCR Text: \"Wo W1 Wz kick Kicktea W3 W4 kick tca Ws\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: The postcondition for the system in the formQ= coin ·Q′."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To find out the representativeness for the corpus, it has been segmented into 15 samples. Each sample is comprised of 1,267 words on average. We plotted the cumulative frequency of the most frequent technical terms in the samples. Fig. 2. Representativeness of the corpus with domain-specific technical terms Fig. 2 depicts that the presence of the domain-specific technical terms becomes stationary after a few samples. This is one of the criteria showing the representativeness of the corpus. After a certain point, no matter how much text we add to the corpus, the frequencies of the terms are becoming stationary.\n\nMentions:\n- Representativeness of the corpus with domain-specific technical terms Fig. 2 depicts that the presence of the domain-specific technical terms becomes stationary after a few samples.\n\nOCR Text: \"Rlrla CQ\"H; Frielije Ot 12 # Sah 49\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2. Representativeness of the corpus with domain-specific technical terms"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 1 validates the achieved analytical results. It can be seen that the analytical results as well as the asymptotic curves perfectly fit with the simulation ones. We can also notice that the outage probability decreases as K increases. Furthermore, it is obvious that the diversity order linearly increases with the number of relays K although we only use one relay. Fig. 2 illustrates the impact of number of reflecting elements N at the source on the system performance. Clearly, a good matching is happening between the analytical and asymptotic results with the simulation ones in this figure. In addition, the two cases studied in the asymptotic analysis section are clear in this figure. We have the case of N = 1 and the case of N ≥ 2. The diversity order for both cases is G d = K, whereas the coding gain G c is a function of the interference parameters at the relay nodes and the outage threshold for the case where N = 1, and is a function of the interference parameters at the destination node and the outage threshold for the case where N ≥ 2 with a minor effect of N . It is also clear that as N becomes close to K, the gain achieved in the coding gain, and hence, the system performance becomes smaller till it vanishes when N becomes ≥ K − 1. Fig. 3 studies the impact of interference on the system performance. It is obvious from this figure that the interference at the relays has a negligible effect on the system performance, where clearly the case ofγ I r = 15 dB andγ I d = 30 dB gives almost the same performance as the case ofγ I r =γ I d = 30 dB with a very small enhancement at the low range of average SNR. In contrast, it clear that the interference at the destination is noticeably impacting the system performance where the case ofγ I r = 30 dB andγ I d = 15 dB is outperforming the case ofγ I r =γ I d = 30 dB. This enhancement in the performance comes through the coding gain G c . Compared to results in [20], we can see here that the presence of RIS at the source reverses the effect of interference at the relays and destination on the system performance, where the interference at the destination becomes more impactful. In addition, we can see that utilizing RIS mitigates the impact of interference at the relays. Finally, the best performance is achieved at the case ofγ I r =γ I d = 5 dB, where decreasingγ I d from 15 dB to 5 dB is again enhancing the coding gain of the system. Fig. 4 illustrates the impact of K and N on the system performance. We can see from this figure that K is affecting the diversity order of the system, where changing K from 2 to 3 increased G d from 2 to 3. On the other hand, it is clear that changing N affects the coding gain G c of the system and not the diversity order. We can also see that regardless of the value of K, increasing N from 1 to 2 always enhances G c with a clear amount. On the other hand, we can see that the gain achieved in the system performance when N keeps increasing becomes very minor as N becomes close to K. In addition, at very high values of SNR, all the cases where N ≥ K − 1 converge to the same performance. VI. CONCLUSION This letter investigated the impact of CCI on the performance of a DF relaying network with a RIS-assisted source. Closed-form approximate expression was derived for the system outage probability assuming Rayleigh fading channels. In addition, the system performance was studied at the high SNR regime, where closed-form expressions were derived for the asymptotic outage probability, diversity order, and the coding gain. The results showed that N has a small effect on G c and not on G d . Furthermore, findings illustrated that K is affecting G d and is more impactful on the performance than N . Finally, results showed that utilizing RIS at the source node mitigates the interference effect at the relay nodes.\n\nMentions:\n- Fig. 4 illustrates the impact of K and N on the system performance.\n\nOCR Text: \"6k Lb-lt-%=6dB TS] 34 Mezk 2 s 43 S S4R M AI\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4: Pout vs SNR for different values of N and K ."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We compare the observed colors to stellar population models for passively evolving galaxies with the same parameters described above for Figure 4. The faintness of the optical counterparts is consistent with being at the cluster redshift. We find that the colors of the counterparts are generally consistent with the galaxy population expected in clusters at this redshift for this evolutionary model. Only one galaxy, MOO J0300+0124, has a redder i −[3.6] color (topmost point in Fig. 4) than expected for a passive cluster member.\n- While it is premature to draw any concrete conclusions about this tight relation due to the limited sample size, we note that this relation is much tighter (σ LAS|D = 11%) than what is expected from geometric considerations alone (σ geo ∼ 22%). Considering the additional physical factors that should add even more scatter, this relation is surprisingly tight. Bird et al. (2008) demonstrate that for a given power and age, the probability distribution function of the expected size of an FR II source of a particular age, power, and external density is strongly peaked (see their Fig. 4). This peak of the PDF provides theoretical support for the observed relation between size and cluster-centric radius being driven by the density of the surrounding medium.\n\nMentions:\n- We compare the observed colors to stellar population models for passively evolving galaxies with the same parameters described above for Figure 4.\n- Only one galaxy, MOO J0300+0124, has a redder i −[3.6] color (topmost point in Fig. 4) than expected for a passive cluster member.\n- Bird et al. (2008) demonstrate that for a given power and age, the probability distribution function of the expected size of an FR II source of a particular age, power, and external density is strongly peaked (see their Fig. 4).\n\nOCR Text: \"3.G\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4.— Color-color diagram of radio source counterparts. The blue line indicates the expected evolutionary path of a galaxy as a function of redshift which was calculated using Z = 0.03 and the Conroy et al. (2009) models. The red triangles indicate counterparts that only have optical lower limits. The colors of most of these counterparts are consistent with the galaxy population expected in clusters at z & 1."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Transient Timescale (days) With the ability to estimate the transient detection rate as a function of cadence, it is then possible to choose a cadence interval to optimize the total number of detections for one or several source classes. Figures 10 and 11 show the dependence of the detection rate on the chosen survey cadence for decay rates τ eff = 20 and 1 days mag −1 .\n\nMentions:\n- Figures 10 and 11 show the dependence of the detection rate on the chosen survey cadence for decay rates τ eff = 20 and 1 days mag −1 .\n\nOCR Text: \"Eia\" 74 ExAS #3S18 Laleneeita\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 10.— Number of M = −19, τeff = 20 days mag−1 transients detected in at least two epochs by various surveys as a function of cadence interval, assuming a constant volumetric rate equal to the local SN Ia rate. At short cadences (∆t << τeff), high Ω̇ surveys like ZTF and ATLAS can discover comparable numbers of transients to deeper surveys like DECam and HSC. At long cadences (∆t >> τeff), the detection efficiency of all surveys declines because all but the brightest nearby events decay too quickly to be observed in a second epoch."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- with τ ≡ τ(p) = τ 0 p α . Figure 5 shows an exemplary solution of eq. ( 16) for the case τ ∝ p [60]. At sufficiently large times, the particle distribution above injection approaches a power law shape\n\nMentions:\n- Figure 5 shows an exemplary solution of eq. ( 16) for the case τ ∝ p [60].\n\nOCR Text: \"1\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5. Time-dependent solution f (p, t) of the Fokker-Planck diffusion equation for non-relativistic gradual shear acceleration assuming an impulsive, mono-energetic injection with p0 at t0 = 0. A linear momentum-dependence τ ∝ p (α = 1) has been used for the scattering time The distribution broadens with time due to momentum dispersion. The (double logarithmic) inlet illustrates the formation of a power law like distribution n(p) ∝ p2 f (p) ∝ p−2 above p0 for t′ ≥ 0.3. From Ref. [60]."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Code-switching evaluation dataset: We created a new smallscale dataset especially for code-switching evaluation. We used bilingual sentences scraped from Wikipedia. For each language, we picked 80 sentences with a few foreign words (20 sentences for each of the 4 other languages); Chinese was romanized. We replaced foreign names with their native forms (see Fig. 3).\n\nMentions:\n- We replaced foreign names with their native forms (see Fig. 3).\n\nOCR Text: \"M: V J Iwu U J kwhh KuklH KMMwNHwHhU_Mhl HMH uHnHyyHkw\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Examples of code-switching evaluation sentences."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The case of a lognormal N -pdf was analyzed by Lombardi et al. (2010), and the mass surface density is given by Figure 2. Σ vs A 0 , for a cloud that exhibits a N -pdf given by a lognormal function. Here we assume Amax = 0.1. The different curves follow eq. ( 17) with σ = 0.29 and 0.59 (the range of variation in σ for the lognormal functions fitted by Kainulainen et al. 2009), plus two extreme cases: σ = 1, and 1.5. We note that, even for these unrealistic cases, the inferred column density Σ is the quite the same, within a factor of 3-5.\n- We now turn to the intra-cloud mass-size relation, i.e., the M − R relation for a single cloud. In this case, Fig. 2 by Lombardi et al. (2010) show that none of the M − R diagrams exhibit a power-law of the form 3 :\n- has a slope of 2. As we can notice, the slope of the masssize relation at small radii approaches to 2, as in Fig. 2 by Lombardi et al. (2010).\n\nMentions:\n- The case of a lognormal N -pdf was analyzed by Lombardi et al. (2010), and the mass surface density is given by Figure 2.\n- In this case, Fig. 2 by Lombardi et al. (2010) show that none of the M − R diagrams exhibit a power-law of the form 3 :\n- As we can notice, the slope of the masssize relation at small radii approaches to 2, as in Fig. 2 by Lombardi et al. (2010).\n\nOCR Text: \"7 3 5.,01 0,-0 18,03 mag\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2. Σ vs A0, for a cloud that exhibits a N-pdf given by a lognormal function. Here we assume Amax = 0.1. The different curves follow eq. (17) with σ = 0.29 and 0.59 (the range of variation in σ for the lognormal functions fitted by Kainulainen et al. 2009), plus two extreme cases: σ = 1, and 1.5. We note that, even for these unrealistic cases, the inferred column density Σ is the quite the same, within a factor of 6 3–5."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Radiative cooling has a modestly significant effect on cloud lifetime, as discussed in previous works (see, e.g., Section 5.3 of Klein et al. 1994). The basic effect of cooling on gas is to soften its equation of state (lower γ), which effectively renders the cloud more compressible (Scannapieco &amp; Brüggen 2015). This makes the cloud more strongly crushed in the direction transverse to the flow, forming a thinner, denser filament with a smaller cross section. Although KH instabilities can grow more violently on this thinner cloud than for an adiabatic cloud because it moves faster with respect to the hot medium (due to its smaller drag), the net effect is for the cloud to survive modestly longer than an equivalent cloud with no cooling due to its higher density. This behavior is nicely illustrated by the comparison of the black and blue curves in Figure 9. Moreover, as shown in the left two panels of Figure 8, cooling can also enhance the formation of smaller, denser cloudlets in the wake (McCourt et al. 2018). This effect, however, can be suppressed by magnetic fields (Figure 8, see also Grønnow et al. 2018). Detailed analyses of the cloudlet properties have been carried out in several recent works (e.g., Sparre et al. 2019).\n- (20) Ldrape is the path length that a cloud must travel for the accumu- 10 We emphasize that the context in which draping was originally proposed referred to much larger structures, namely \"bubbles\" and jets emanating from AGN in the CGM of massive halos/clusters, which have physical size scales ∼ 10 − 100 kpc and travel 100 kpc, vastly different from what we model here. lated field to appreciably alter its destruction (assuming Ptherm Pram for supersonic clouds). However, Ldrape is much longer than the length scale over which clouds are destroyed, Lcc ≈ tcc vcl ≈ 9 pc Rpcv100M −1 cl . In other words, CGM magnetic fields are nowhere near sufficiently strong to dynamically suppress cloud destruction. This can be seen visually by comparing the second and third panels of Figure 8 (or the relevant lines in Figure 9), which shows how MHD and hydrodynamic simulations remain very similar without the effects of conduction. We have also confirmed this conclusion by re-running a subset of our simulations with plasma β multiplied or divided by a factor of ∼ 1000, which makes no difference to the\n\nMentions:\n- This behavior is nicely illustrated by the comparison of the black and blue curves in Figure 9.\n- This can be seen visually by comparing the second and third panels of Figure 8 (or the relevant lines in Figure 9), which shows how MHD and hydrodynamic simulations remain very similar without the effects of conduction.\n\nOCR Text: \"1*K lu_hu = 4= Cema. Oulit Ccolur cond0-l LEC Ll (lia (inu:|, Til \" S :Hilil: Uume camnttcn Onkc\" 3 HAe Nsr Iri \"6 Cj 0\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 9. Evolution of the normalized cloud mass, fcl (defined in Figure 1) versus time, for the simulations shown in Figure 8 (Th = 106 K, vcl = 100 kms−1, nh = 10−3 cm−3, Lcl = 1 pc) with different physics included (labeled as in Figure 8). The cloud mass versus time is remarkably similar across these runs, given the different physics and morphologies in Figure 8."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In the present work we have investigated the cosmological dynamics of dark energy in the context of scalar-torsion f (T, φ) gravity, where f (T, φ) is a function of the torsion scalar T , associated with the Weitzenböck connection in the context of modified teleparallel gravity, and the scalar field φ. Particularly, we have studied the class of theories with Lagrangian density f (T, φ) = −T /2κ 2 − F (φ)G(T ) − V (φ), with F (φ) ∼ e −σκφ , V (φ) ∼ e −λκφ and G(T ) ∼ T 1+s , plus the canonical kinetic term for the scalar field. The exponential scalar potential is the usual one studied in the context of dark energy which admits scaling solutions, while the exponential coupling function of the scalar field is the simplest natural choice to be assumed with this potential [41]. Furthermore, the function G(T ) generalizes to the nonlinear case the typical choice of a linear function in T for the nonminimal coupling to gravity [44,45]. It has been shown in Ref. [38], that in order to generate primordial fluctuations during inflation from f (T, φ) gravity, non-linear terms in torsion scalar need to be considered to construct the coupling function, when the nonminimal coupling to gravity is switch on. So, for the FLRW background, and in the presence of radiation and cold dark matter, we have defined the effective energy and pressure densities of dark energy. Thus, we have obtained the autonomous system associated with the set of cosmological equations and then we have performed the dynamical analysis in the phase space for the cosmological model at hand, by getting the critical points, their cosmological properties and stability conditions. Some interesting features have been found that make the model appealing as a viable dark energy candidate. For s = 0, we have found the existence of new attractors solutions describing the dark energy dominated era and new scaling solutions representing the scaling radiation and scaling dark matter eras. So, since the latter are saddle points, we have scaling regimes during the radiation and cold dark matter epochs followed by the dark energy attractor with accelerated expansion. In FIGS. 1 and 4, as well as in FIGS. 3, and 6, we have numerically confirmed that the dynamics of the model can allow the two scaling regimes previous to the dark energy dominated epoch, satisfying the cosmological constraints for the early-time dark energy density from BBN [56] and CMB bounds [3]. Therefore, the final attractor can be either a de Sitter solution (w de = −1), or a dark energy dominated solution with Ω de = 1, and equation of state with quintessence-like, phantom-like behaviour or experiencing the phantom-divide crossing, as it has been illustrated in FIGS 2 and 5. We also have found some ranges for the parameters of the scaling solutions, where, these solutions can also be attractors, but in this case, they cannot explain the current accelerated expansion. On other other hand, the phantom-divide crossing during the cosmological evolution indicates a distinctive feature of the model as compared with minimally coupled scalar field in GR, and that has been inherited from the case with s = 0 [44].\n\nMentions:\n- In FIGS. 1 and 4, as well as in FIGS. 3, and 6, we have numerically confirmed that the dynamics of the model can allow the two scaling regimes previous to the dark energy dominated epoch, satisfying the cosmological constraints for the early-time dark energy density from BBN [56] and CMB bounds [3].\n\nOCR Text: \"0,00S\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3. We plot the physical evolution curves in the phase space for the values of parameter s = −1.20, σ = 0.3 and λ = 150, and for the three different set of initial conditions xi = 1 × 10−9, yi = 4 × 10−7, zi = 9.1 × 10−30, %i = 0.999877 (red dashed), xi = 1 × 10−33, yi = 1 × 10−41, zi = 1.05 × 10−29,%i = 0.999877 (brown), and xi = 0.0108, yi = 0.00765, zi = 9.3 × 10−30, %i = 0.999788 (black)"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- While considering the probabilistic setting, we can also consider and plot the response to varying the stress scenario given by a. This is depicted in Figure 4 by plotting the terminal price q(1) as a function of the price without market impacts f t (1). The setting without market impacts is the diagonal line by definition. Market impacts cause feedback effects that drive the price below f t (1). All settings coincide for low stress scenarios (f t (1) 0.98) as few banks are driven to the  regulatory threshold. Further, the analytical stress test bound is demonstrably worse than the numerical terminal value for most stresses; however, these occur at typically unrealistic stresses. Example 5.3. Consider a single bank (n = 1) and single asset (m = 1) system with crisis that lasts until terminal time T = 1. For simplicity, assume that this bank holds no liquid assets, i.e.,\n\nMentions:\n- This is depicted in Figure 4 by plotting the terminal price q(1) as a function of the price without market impacts f t (1).\n\nOCR Text: \"Mcazt DiExdjercus Stess inancial Crizis I Menft s 3y ettira Sirass f (1;\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4: Example 5.2: The impacts of the stress scenario on market response in a 20 bank system under the exact differential equation and the analytical stress test bounds."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Taking a look at Fig. 13, it can be seen that the distributions show mostly the same trend in the case of Fe, Co and Ni, but the TARDIS model does not contain any cobalt or nickel (blue) models (Fink et al. 2014). The meaning of the two time axes are the days since the explosion assumed by the deflagration models (left) and the days with respect to B-max in the cases of the observed spectra (right). above 12,000 km s −1 . This seems to contradict the pure deflagration scenario, which predicts high mass fractions of Fe-peak elements at all velocities. Note, however, that this dominance of IGEs in the outer layers does not lead to the emergence of extremely strong Fe/Ni/Co lines in the N10def model, see Fig. 11 (while the presence of early, excessively strong Fe lines is an argument against uniform abundance TARDIS models, see text and Fig. 8 in Sec. 4.1).\n- In Fig. 13 we also show the mass of chromium which has a mass fraction of ∼0.001 in the whole ejecta based on the N10def model. This value is two orders of magnitude less than what we use in TARDIS to fit the blue part of the spectra obtained at +7 and +10 days. This discrepancy makes the high amount of chromium in the TARDIS model questionable, because only the flux depression between 3,500 and 3,800 Å makes its presence necessary. Moreover, only the +7 and +10 days spectra cover this spectral region, which could also be fitted using other elements, like cobalt or titanium, based on the literature. The best-fit TARDIS model contains a high amount of cobalt originating from the nickel-decay, but it shows only a limited contribution to the associated flux depression. Titanium, on the contrary, is not included in our TARDIS model, since it produces strong absorption features in other parts of the observed spectral range, which decrease the goodness of the fit. Nevertheless, since chromium and titanium are not synthesized independently during the nuclear processes, we cannot exclude the possible presence of Ti in the ejecta of SN 2011ay.\n- The oxygen masses from the two models are also plotted in Fig. 13. Since we use the oxygen as a \"fill up\" element (see Sec. 3.2), this comparison is less informative. Thus we avoid to take any conclusion about the amount of oxygen.\n\nMentions:\n- Taking a look at Fig. 13, it can be seen that the distributions show mostly the same trend in the case of Fe, Co and Ni, but the TARDIS model does not contain any cobalt or nickel (blue) models (Fink et al. 2014).\n- In Fig. 13 we also show the mass of chromium which has a mass fraction of ∼0.001 in the whole ejecta based on the N10def model.\n- The oxygen masses from the two models are also plotted in Fig. 13.\n\nOCR Text: \"Mtu Cj -FI f Fah 'mai Mo]lhd IIsl III] IIzID Yulcdem\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 13. The comparison of IGE and oxygen shell masses we get from the TARDIS fits (solid lines) with those originating from the N10def pure deflagration model (dashed lines) at +7 days regarding to B maximum."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The SCF convergence problems for states where degenerate orbitals are unequally occupied arise because the electron density represented by a single determinant of KS orbitals obtained at each step is not well defined and this can lead to large changes in the orbitals involved in the excitation from one step to another, and oscillations between different critical points. 40 Several options can be used to help convergence of the SCF: different mixing of the density, alternative DIIS extrapolation techniques, 89 or tuning modifications such as damping, level shifting of the iterations, or electronic smearing. 37 However, DO is able to follow the same solution more consistently without such modifications. It can obtain convergence in these difficult cases if the chosen quasi-Newton method guarantees sufficiently accurate Hessian updates for the given form of the preconditioner, as shown here. The robustness of the DO approach in calculations involving orbital degeneracies has been previously recognized for ground states of systems with vanishing HOMO-LUMO gap. 40 For some excited states, a DO method can converge on a solution with higher energy than the solution obtained by SCF or another DO method for the same excited state. For example, the solution obtained for the 1 1 E(n → 3p) state of ammonia with L-BFGS or L-SR1 DO-MOM lies ∼0.03 eV higher in energy with respect to the SCF-MOM solution. The occurrence of higher-energy solutions, which we refer to as \"local saddle points\", is indicated in Tables 1 and 2. We stress that the multiple solutions that are obtained for a particular case are all saddle points of the same order and correspond to the same excited state. Multiple solutions corresponding to the same excited state are found to differ in the orientation of the highest occupied molecular orbitals (see Figures S2 to S6 in the Supporting Information). Similar to what is observed here for saddle points, the geometric direct minimization method of reference 40 exhibits a tendency to converge on local minima of energy functionals compared to SCF minimizers. Defining the \"optimal\" approximation to an excited state among multiple variational solutions might not be trivial. Indeed, variational solutions of a nonlinear optimization are in general not orthogonal to one another, and hence higher solutions are not necessarily upper bounds to the exact excited states, but only upper bounds to the ground state. 90 Besides, for many practical applications, such as calculations of PECs or molecular dynamics, one is usually only interested in consistently converging on the same stationary point. For these cases, DO-MOM can be used without modifications. For cases in which the lowest energy saddle point of a given excited state is desired, a possible strategy could be to combine the DO approach with techniques for guiding the convergence towards a global solution, such as the one presented in reference. 91 Finally, we note that due to the small size of the molecules considered here, the computational effort of SCF-MOM and DO-MOM is comparable, as indicated by similar values of the elapsed time per iteration. For larger systems, care must be taken that the memory of the quasi-Newton algorithm used within DO-MOM, which here is chosen as m = 20, does not degrade the computational performance of the method. From test calculations, where we compare the convergence of L-BFGS and L-SR1 with different levels of memory, we find that L-SR1 tends to become less robust with lower memory faster than L-BFGS. Therefore, for large systems, L-BFGS might represent the best compromise between speed of convergence and computational effort among the various limited-memory inverse Hessian update schemes.\n\nMentions:\n- Multiple solutions corresponding to the same excited state are found to differ in the orientation of the highest occupied molecular orbitals (see Figures S2 to S6 in the Supporting Information).\n\nOCR Text: \"{ 1 38 1 6 1 0 1 1 { 1 | 1 1 1 1\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: Total number of iterations when DO-MOM with L-SR1 is used to converge the singlet excited states for which SCF-MOM fails."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The underlined factors above are combinatorial factors for each diagram in Figure 14.\n\nMentions:\n- The underlined factors above are combinatorial factors for each diagram in Figure 14.\n\nOCR Text: \"Ik XO MhIZCCME H YHG_ hhh 3 PA\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 14: The simplified diagrams when m→∞"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Using this notation defined above, the set of individual chance constraints ( 57) is represented as follows:  The non-convex problem (Problem 6) can be optimally solved by repeatedly solving the relaxed convex subproblems using the algorithms presented in Section 4. The following subsections introduce the algorithms that construct a search tree with relaxed convex subproblems, such as the one in Figure 9-(b).\n- Algorithm 4 shows the most straightforward way to obtain lower bounds. It simply solves the convex relaxed subproblems (Problem 7) using the methods presented in Section 4.2. The optimal objective value of a relaxed subproblem gives a lower bound of the optimal objective value of all the subproblems below it. For example, the optimal solution of the relaxed subproblem at Node 2 ′ in Figure 9-(b) gives a lower bound of the objective value of the subproblems at Nodes 4 and 5. This is because the constraints of the relaxed subproblems are always a subset of the constraints of the subproblems below. Note that optimization problems are formulated as minimizations.\n\nMentions:\n- The following subsections introduce the algorithms that construct a search tree with relaxed convex subproblems, such as the one in Figure 9-(b).\n- For example, the optimal solution of the relaxed subproblem at Node 2 ′ in Figure 9-(b) gives a lower bound of the objective value of the subproblems at Nodes 4 and 5.\n\nOCR Text: \"U6 X\"\"\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 9: Branch-and-bound search tree for a sample disjunctive convex programming problem (Problem 6) with constraints (60). (a) Tree of non-convex subproblems, (b) Tree of relaxed convex subproblems."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Several previous efforts on verifying quantum programs have defined the semantics of quantum programs as operators on density matrices [15,30]. We follow this convention here, giving programs their standard interpretation. The density matrix semantics of general sQIRe programs is given in Figure 5.\n- Figure 5: sQIRe density matrix semantics. We use the notation P dim d to describe the semantics of program P with a global register of size dim. The definition of ueval is given in the caption of Figure 1. We use i⟩ q ⟨ j as shorthand for I 2 q ⊗ i⟩⟨ j ⊗ I 2 dim−q−1 , which applies the projector to the relevant qubit and an identity operation to every other qubit in the system.\n\nMentions:\n- The density matrix semantics of general sQIRe programs is given in Figure 5.\n- Figure 5: sQIRe density matrix semantics.\n\nOCR Text: \"pildrp)-bo*pxdu [; Pky): (Bli :[uVp) huevHlu)xpxuewlu)  welytyped [Ua aiv) Iu Oherwasc [eudi ):VWpULM:VWVpWH Fewedk ()-VWW-WHM\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: sQIRe density matrix semantics. We use the notation JPKdimd to describe the semantics of program P with a global register of size dim. The definition of ueval is given in the caption of Figure 1. We use ∣i⟩q⟨ j∣ as shorthand for I2q ⊗∣i⟩⟨ j∣⊗ I2dim−q−1 , which applies the projector to the relevant qubit and an identity operation to every other qubit in the system."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The performance of the voltage reference after irradiation is crucial. Therefore FE-I4B bare ICs have been irradiated in Los Alamos irradiation facility using 800 MeV protons. The output voltage of the band gap based voltage reference has been measured during the irradiation and is plotted as a function of the total ionizing dose in figure 7. It has been observed that the band-gap based reference voltage increases with dose. This could endanger the IC after irradiation if the analog input voltage of the IC increases above 1.6 V. Consequently this reference voltage can not be used by itself for the analog LDO.\n\nMentions:\n- The output voltage of the band gap based voltage reference has been measured during the irradiation and is plotted as a function of the total ionizing dose in figure 7.\n\nOCR Text: \"Hmt Rc Jigital Burdc Wiel Ji) #\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 7. The output voltage of the band gap based reference voltage measured during the 2011 irradiation campaign at Los Alamos."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We have chosen the block diagram shown in Fig. 3, consisting of a preamble matched filter (PMF), a decimator, an address-spreading matched filter (AMF), and an address decoder. All MFs in our design are finite impulse response (FIR) filters with the transfer function F (z) = J−1 i=0 f i z −i where J is the number of filter taps, and the values of the filter impulse response f i s are the reversed known sequence, i.e., the preamble, the address spreading, and the node address, we are looking for. All these MFs are also followed by a comparator acting as decision device and, for simplicity, we include this component when using the term MF. Feeding the input x[n] in the form of a bit stream to the MF, the filter output y[n] becomes\n- After preamble detection and bit-synchronization, we do not need to continue at the oversampled rate and can operate at normal bit rate when de-spreading and detecting node address. This, as previously mentioned, saves energy and reduce area compared to operating directly on the oversampled sequence. The decimator in Fig. 3 is, therefore, used to perform the down-sampling. Using the position of bit-timing/clock-phase from the PMF output, the decimator down-samples the sequence by adding κ oversampled bits at a time. The result of this is thresholded to decide whether the down-sampled bit is a zero or one. Figure 5 shows the hardware implementation of a decimator, consisting of a (2κ − 1)-bit SR, indicated by SRD, a (κ − 1)-bit κ to 1 multiplexer, an adder and a comparator. With κ times oversampling, there are κ possible correct bit-timing/clock-phase for the decimator to perform the summation of the incoming oversampled bits. To have access to all x[n]...x[n − (2κ + 2)] samples needed to calculate the output for any of the clock phases, the (2κ − 1)-bit SRD is used. The input x[n] is directly connected to the oversampled bit sequence and the SRD stores all above samples by shifting the incoming bits every clock cycle. The multiplexer inputs are then fed with κ choices of incoming sample sequences, grouped based on the possible clock phase. The peak position of the PMF output is fed to the multiplexer control input. The control output in return feeds through the correct (κ − 1) samples to the adder for further processing. Since input sample x[n − κ + 1] is present in all sums, independent of the peak position, it is fed directly to the adder instead of through the multiplexer. This allows us to use a multiplexer with a smaller size, saving both on energy and area. The comparator output is set to zero if the output of the adder is smaller than κ/2 while it is set to one for the other values.\n\nMentions:\n- We have chosen the block diagram shown in Fig. 3, consisting of a preamble matched filter (PMF), a decimator, an address-spreading matched filter (AMF), and an address decoder.\n- The decimator in Fig. 3 is, therefore, used to perform the down-sampling.\n\nOCR Text: \"WHYWH Ia JZ\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3. Digital base-band (DBB) block diagram."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The first variational calculations on the J 1 -J 2 -model were done in a space that we in the following shall call R 0 . 22,23 It is spanned by the states in which there are domains that have one of the two ground state configurations of the MG-chain and which are separated by one soliton. Examples can be seen in Figs. 1 and 2. The arrows in Fig. 2 serve to fix the phase of the dimers that make up the ground state. Our convention is such that if the arrow goes from site i to site j the spins are in the state: an odd number of sites, a set of single soliton states can be generated by leaving the chain maximally dimerized and taking the remaining site to be in the S z =1/2-state. For a chain with open boundary-conditions, the soliton can only reside on every second site. The dimension of this variational subspace is then D=(N + 1)/2. We use N to denote the length of the chain. For calculations on odd length chains with periodic boundary conditions it is necessary to allow a nearest neighbor dimer across the boundary and to let the soliton cross the boundary by going from site N to site 2. In this case R 0 has dimension N and incorporates states with the soliton at every site with the remaining spins paired in nearest neighbor dimers. (For odd N and PBC it becomes difficult to distinguish the 2 dimerization patterns since they twist into each other at the boundary. Still, the soliton clearly denotes a 'domain wall' between the two patterns).\n\nMentions:\n- The arrows in Fig. 2 serve to fix the phase of the dimers that make up the ground state.\n\nOCR Text: \"ek uwber\" siles siugke solleu slale: @0 Aa684 by Jerrilg tbe qltinally diwerizc TilIG tbe : C[O;IID} %t: hc' Il te $ -1/2state_ chain with ppcn bcuzdari-condlitions. chc snlicct 'Il; numT St Sil: . Tir ditc| Llis !ariali LJ aospale Lleu D_4 Wee use CeMe lezgth tbe\" cbair; Fuz calulatiurs odld lereth chains Iith perodic honncaty conditinx ITc\" #c; TNG- Trizhkr dimcr irTtSS boudory a*4d tc; Ict thc salilon cnxs Uhc hounlarg hi' $ :le _ doylp ? Iz Ulis (Ese Ilo Les elistertsicL E4 Icu puratas Iat #itb Jua 027 Ep HI;ilg %ll; pze Mcie- Ieighhor dimer: Fo\" odd an:l PR(' 1 kcnmrs difticult dlistingmsh dimeriration poucrts E \"isl Ultler ~muuuac; 3.ill, Ue: sulilu ceat  dleucles (unam JplgupM Tbe t%u DatlerzLsl eJu JeFib ur?\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 2. Two variational states used in the calculations within R0 for a chain with an odd number of sites. The arrows between sites are used to fix the phase of the dimers (see main text)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- applied to dependency graph given in Fig. 1 iṡ\n- for the dependency graph in Fig. 1 x\n- Applied to dependency graph of Fig. 1, we geṫ\n\nMentions:\n- applied to dependency graph given in Fig. 1 iṡ\n- for the dependency graph in Fig. 1 x\n- Applied to dependency graph of Fig. 1, we geṫ\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1. Dependency graph with a directed spanning tree rooted at node 3."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We first compare our ranking to the ranking output by Edge boxes. Figure 2 plots Recall vs Number of Proposals in PASCAL VOC 2007 for IoU=0.7 1 . We observe that DeepBox outperforms Edge boxes in all regimes, especially with a low number of proposals. The same is true for IoU=0.5 (not shown). The AUCs (Areas Under Curve) for DeepBox are 0.74(0.60) vs Edge box's 0.60(0.47) for IoU=0.5(0.7), suggesting that DeepBox proposals are 24% better at IoU=0.5 and 26% better at IoU=0.7 compared to Edge boxes. Figure 3 plots the same in COCO. The AUCs (Areas Under Curve) for DeepBox are 0.40(0.28) vs Edge boxes's 0.28(0.20) for IoU=0.5(0.7), suggesting DeepBox proposals are 40%(43%) better than Edge boxes. If we re-ranked top 2048 proposals instead, the AUCs are 0.38(0.27).\n\nMentions:\n- Figure 3 plots the same in COCO.\n\nOCR Text: \"CUCO Lvalualu aleu DpRat Elzchotes Hlmilict upruileale\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3. MS COCO Evaluation IoU=0.7. The strong gain demonstrated by DeepBox on COCO suggests that our learnt objectness is particularly helpful in a complicated dataset."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- 2) Effect of path loss exponent (α): Fig. 6 shows the average achievable rates as a function of α for both the policies. Observe that as α increases, the average achievable rate reduces exponentially as h k,i ∝ D −α i . However the effect of increasing α is more severe on the farther transmitter Tx 2 as D 2 &gt; D 1 , and thus h 2,i (includes the path loss) reduces more rapidly than h 1,i . Also, as α increases, the rate achieved by Tx 2 approaches to zero.\n\nMentions:\n- 2) Effect of path loss exponent (α): Fig. 6 shows the average achievable rates as a function of α for both the policies.\n\nOCR Text: \"S-ale SRM) Suu-Lale (MRM; K (SIEM) (MRW} (SkAI) (MRM} Path Joss @XDOLcult\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 6. Comparison of the effect of the path loss exponent α on rates achieved by both transmitters, M = 4."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- V T These analytical results are compared to numerical solutions of Eq. (1) in Fig. 9, for τ = 2ε = 0.04 and V T = 1.3. Like for the plane case we observe that the absence of particle trapping closely\n- t = 2.5 T t = 0 z x Figure 10: Plot of a particle cloud released at the center of the throat, for the conditions of figure 9 and ω = 0.5.\n\nMentions:\n- (1) in Fig. 9, for τ = 2ε = 0.04 and V T = 1.3.\n- t = 2.5 T t = 0 z x Figure 10: Plot of a particle cloud released at the center of the throat, for the conditions of figure 9 and ω = 0.5.\n\nOCR Text: \"Al\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 9: Plot of the numerical computation of the averaged particle path length in the unsteady constricted pipe flow (black dots), together with the analytical expression of A(ω, VT )−k |m(VT )| (solid line, Eqs. (17) and (18)). The fluid velocity at C is Vc ≃ 1.56, and VT = 1.3, ε = 0.02, τ = 0.04. The absence of particle trapping corresponds to the absence of zeros in the Melnikov function (i.e. A(ω, VT )− k |m(VT )| < 0)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We notice that the vast majority (70%) of airlines rarely appear in a competitive combination itinerary (Fig. 2), i.e., they have a less than 10% chance of appearing in the top ten of search results. The popularity of airlines is highly skewed too. The top 25% of airlines appear in 80% of the search results whereas the remaining 75% of airlines account for the remaining 20%. We found no correlation between airlines' popularity and its ability to appear in a competitive combination itinerary. The absence of a correlation with popularity is even more vividly seen in the analysis of combination performance on different search routes (Fig. 3). The share of competitive combinations on unpopular and medium popular routes is rather stable (≈ 45%) and big variations appear only in the tail of popular routes. In fact, some of those very popular routes have almost a 100% chance to have combination itineraries in the top ten results, whereas some other ones of a comparable popularity almost never feature a competitive combination itinerary.\n\nMentions:\n- The absence of a correlation with popularity is even more vividly seen in the analysis of combination performance on different search routes (Fig. 3).\n\nOCR Text: \"rcute popularity\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3: Search results with competitive combinations across routes with different popularity. Red: the cumulative distribution function of the volume of searches across different origin and destination pairs (routes). Blue: the share of search results with competitive combinations (y-axis) on the routes of a given popularity (x-axis)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Now we investigate the Q-dependence of the radiation power and the dissipation caused by plasma oscillation. As shown in Fig. 5, the radiation power reaches its maximum when it equals the plasma dissipation. The reason for the existence of a maximal radiation power is that there exist two competing factors in the system: a good cavity associated with large Q makes a strong resonance on one hand, but it suppresses energy leak on the other hand.\n- FIG. 5: (Color online) Q-factor dependence of radiation power and plasma dissipation at resonance for the PMC-like (1,1) mode with β = 0.05 and R = 100µm. The radiation power reaches its maximum when it equals plasma dissipation.\n\nMentions:\n- As shown in Fig. 5, the radiation power reaches its maximum when it equals the plasma dissipation.\n- FIG. 5: (Color online) Q-factor dependence of radiation power and plasma dissipation at resonance for the PMC-like (1,1) mode with β = 0.05 and R = 100µm.\n\nOCR Text: \"Radigtor Purzr Diesipator FeJ\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 5: (Color online) Q-factor dependence of radiation power and plasma dissipation at resonance for the PMC-like (1,1) mode with β = 0.05 and R = 100µm. The radiation power reaches its maximum when it equals plasma dissipation."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Once the stellar masses of the galaxies of our catalogue are determined, we can estimate down to which stellar mass the catalogue is complete. The stellar mass limit for completeness is a function of the redshift (see Figure 3). To explore the degree of completeness of satellites down to M ⋆ ∼10 9 M ⊙ , we have used our most distant redshift interval 0.023&lt;z&lt;0.025. The peak on the mass distribution of the galaxies in the catalogue at z∼0.024 is log(M ⋆ /M ⊙ )∼ 8.95.\n\nMentions:\n- The stellar mass limit for completeness is a function of the redshift (see Figure 3).\n\nOCR Text: \"Ing(MhMJ) 3.95 MM;) log(Mm ;) 11.0J 0.005 0.010 0.015 0.020 00051.015 (*10j\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3. Left panel: the points in grey correspond to stellar mass distribution versus redshift for all the galaxies in the ’specphoto’ SDSS DR10 catalogue. The pink points represent our sample of massive host galaxies. The black points indicate those objects in the right panel within the redshift (0.023<z<0.025) used to estimate the stellar mass completeness. Right panel: stellar mass distribution for the spectroscopic catalogue in the redshift interval 0.023<z<0.025. The red dashed line is the completeness limit ∼8.9×108M⊙ whereas the blue line represents the minimum stellar mass for the sample of massive galaxies (1011M⊙). A conservative estimation for the stellar mass completeness of the spectroscopic sample is showed by the orange dashed line:∼2.6×109M⊙ (see text for details)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We give the data matrix M which has x i 's as rows, as an input to the PCP algorithm to recover the low rank matrix L. The rank r of L is found to be 18. Figure 2 shows the error ||M − L − S 0 || F where is the Frobenius norm of the matrix M − L − S 0 is considered, as the constraint in the optimization problem was L + S 0 = M . The stopping criteria is set to an error less than 10 −7 ||M || F . PCP required 5970 epochs to perform the RPCA.\n- In the TE process dataset {x i } 4801 i=1 , x i ∈ IR 41 , the vector x i consists readings of sensors at each timestamp. In the TE process dataset 83% data points belongs to normal and the rest are observations corresponding to anomalous behavior of the system. The data matrix M is of dimension (4801 * 41), was given to PCP to find a low rank matrix L of rank r = 24. Figure 2 shows the corresponding error at each epoch. The matrix A comprising of the orthonormal basis vectors of L is of dimension (41 * 24). 5.1.3. Govt. Sponsored Power Testbed . The power testbed is a three phase power distribution testbed in which several loads are connected like industrial automation testbed, conveyor belt and water treatment testbed. The various physical parameters like voltages, current and power are measured with the help of sensors and remote terminal unit (RTU) and are sent to the SCADA from PLC by using industrial communication protocols like MODBUS where the overall state of the system is maintained.\n- The data matrix M is of dimension (12310 * 31) is given as an input to the PCP algorithm to recover the low rank matrix L. The rank r of L is found to be 9. Figure 2 shows the corresponding error after each epoch. The matrix A comprising of the orthonormal basis vectors of L is of dimension (31 * 9). The median is a (31 * 1) vector.\n\nMentions:\n- Figure 2 shows the error ||M − L − S 0 || F where is the Frobenius norm of the matrix M − L − S 0 is considered, as the constraint in the optimization problem was L + S 0 = M .\n- Figure 2 shows the corresponding error at each epoch.\n- Figure 2 shows the corresponding error after each epoch.\n\nOCR Text: \"Error of PCP Algorithm MMaT D00 10000 Epoch\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2. Error ||M−L−S0||F calculated after each epochs for Principal Component Persuit algorithm on SWaT, Tennessee and Power dataset. The decreasing error shows that algorithm successfully recovers a low rank matrix L"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- As promised in Section 3.1, we now evaluate the performance of our approach using more practical measurement schemes. We compare the performance achieved using an i.i.d. Gaussian matrix to that achieved using an M ×N random demodulator matrix [73] and to a random sampling approach where M samples are taken uniformly at random on the length-N Nyquist grid. The results are shown in Figure 8. We observe very similar performance among the three approaches, with the Gaussian matrix performing best, then the random demodulator, followed by random sampling. Note that while there is certainly a gap between these three approaches, it is also most pronounced in a regime which is likely irrelevant in practice, since it is rare to find applications where a recovery SNR of 200 dB or more is feasible.\n\nMentions:\n- The results are shown in Figure 8.\n\nOCR Text: \"2OOF 0 6 [QOF GAussiam MalT Rancom demudmator RauOM StmMIIA Oversampling factor (64T\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 8: Comparison of recovery SNR using i.i.d. Gaussian measurements, random demodulator measurements, and random samples."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- d (t) are hardly affected. This asymptotic information is displayed for a particular case in Fig. 5. The left side shows the behavior as a function of time with each successive bounce along with the Fourier power spectrum. This spectrum is computed using the FFTW library [18] acting on φ (1) 3 (t) once it has been interpolated onto a uniform grid over t (recall the use of AMR produces nonuniform in time information for the asymptotic behavior). The power is computed as simply the square of the (complex) Fourier amplitude.\n- )), we examine the spectral content of the latter. Fig. 5 displays snapshots of the scalar field for successive bounces during an evolution. The initial pulse becomes increasingly more compact, approaching something like a Kronecker delta function. As it does so, the FFT of each pulse similarly approaches the FFT of a delta function, in particular, a flat, plateau-like spectrum. In general, if we define the effective bandwidth of the power spectrum as the frequency at which the plateau ceases, then we observe this bandwidth to increase rapidly during an evolution. The only exception appears, for some runs, during the last few bounces, during which the bandwidth decreases modestly from its maximum. However, we speculate that this behavior can be attributed to the nonlinearities of gravity near Before discussing the behavior of the bandwidth, we should comment on the falloff behavior of the power spectrum. It is customary in studies of turbulence in fluid dynamics to observe the rate of falloff (the slope on a log-log plot). In particular, a famous result of Kolmogorov expects a slope in the energy spectra of −5/3 within the inertial regime of turbulent flows (independent of spatial dimension d). Here, one might consider the scalar field, in analogy with a fluid, as a \"gas\" of oscillons in which structure transitions to increasingly higher frequencies. An analysis of the energy spectra over the oscillon basis in the bulk in [17] finds a fall-off at the rate of −1.22. As well, an analysis of the Fourier spectrum over the time series of the Ricci scalar at the origin in [19] reports a slope of −1.61, seemingly close to that of Kolmogorov. However, the Ricci scalar is an energy density and thus the expected scaling is not −5/3. Indeed, we can retrace Kolmogorov's argument for such quantity by the following dimensional considerations. The Fourier transform of Π 2 (which scales as E ∝ T −2 ) scales asÊ ∝ LT −2 for a typical wavelength L. If turbulent behavior transfers energy to different scales, for Π 2 it does so with an efficiency η which has dimensions of T −3 (transfer per unit time). Thus, one would expect these quantities to be related bŷ\n\nMentions:\n- This asymptotic information is displayed for a particular case in Fig. 5.\n- Fig. 5 displays snapshots of the scalar field for successive bounces during an evolution.\n\nOCR Text: \"Ini:i\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: The behavior of the scalar field at the boundary with successive"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- These features do not appear in Fig. 4 where the ratio R decreases monotonically without the presence of oscillations. For α = 1 and 4096 × 4096 resolution, R begins to decay strongly at T ∼ 11.5, i.e. when the central filament breaks by secondary instabilities (Fig. 9). It is interesting to compare the time evolution of R and of the quantity max ∇θ , which enters the modified Beale-Kato-Majda criteria (20). Figure 10 shows that at T ∼ 11.5 the max ∇θ starts to exhibit non-smooth oscillations associated with the breaking of the filament by secondary instabilities. These results show that the selective decay appears when the flow becomes unstable by secondary instabilities and these instabilities change its topology, allowing for the flow to evolve to a state with minimum values of R.\n- The comparison of the results at 4096 × 4096 resolution with the integrations at lower resolution is in agreement with the observation that selective decay is scale-dependent. Lowering the resolution shows an anticipation of the selective decay, in agreement with the anticipated breaking of the central filament at lower resolution (Fig. 10).\n\nMentions:\n- Figure 10 shows that at T ∼ 11.5 the max ∇θ starts to exhibit non-smooth oscillations associated with the breaking of the filament by secondary instabilities.\n- Lowering the resolution shows an anticipation of the selective decay, in agreement with the anticipated breaking of the central filament at lower resolution (Fig. 10).\n\nOCR Text: \"2748 4336 E\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 10 Behavior of max‖∇θ‖ for different resolutions. The vertical dashed lines represent the times T = 8, T = 11.5 and T = 15.9 respectively."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Here we study how our self-supervised imitation system compares to the other possible combinations of training signals. The performance of each combination is reported in Table III using the maximum amounts of data available (10 sequences for Human supervision and 30 sequences for TC supervision), while Fig. 13 varies the amount of human supervision. Models such as the \"TC + Self\" or \"Self\" do not make use of any human supervision, hence only appear as single points on the vertical axis. Models that do not include TC supervision are simply trained as end-to-end regression problems. For example the Self model is trained end-to-end to predict internal joints from third person observations of the robot, and then that model is applied directly to the human imitation task. For reference, we compute a random baseline which samples joints values within physically possible ranges. In general, we observe that more human supervision decreases the L2 robot joints error. It is interesting to note that while not given any labels, the self-supervised model (\"TC + Self\") still significantly outperforms the fully-supervised model (\"Human\"). The combination of all supervision signals performs the best. Overall, we observe that adding the TC supervision to any other signal significantly decreases the imitation error. In Fig. 15, we vary the amount of TC supervision provided and find the imitation error keeps decreasing as we increase the amount of data. Based on these results, we can make the argument that relatively large amounts of cheap weaklysupervised data and small amounts of expensive human supervised data is an effective balance for our problem. A non-extensive analysis of viewpoint and scale invariance in Sec. E seems to indicate that the model remains relatively competitive when presented with viewpoints and scales not seen during training.\n\nMentions:\n- In Fig. 15, we vary the amount of TC supervision provided and find the imitation error keeps decreasing as we increase the amount of data.\n\nOCR Text: \"TC+sf IC+ ruman+@f #Lneueen5e: hutan ^ Qu_r]pa\" seqletces\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 15: Varying the amount of unsupervised data: increasing the number of unsupervised sequences decreases the imitation error for both models."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- An important requirement for designing an effective solution that meets our requirements is to define the information flow constraints which should be enforced. If we design the solution well, coinciding with the multi-level hierarchy that is natural to traceability chains, we can ultimately reduce the number of critical components by an order of magnitude. For subsequent discussion, we refer to the information flow model depicted in Figure 3. Multi-Level Integrity Our first requirement is to maintain the integrity of calibration. By maintaining the integrity of calibration reports at the root level, we can reduce the damage inflicted to intermediate levels between the root level and the components which make up a safety-critical system. As a reminder, if compromise occurs at the root level in a traceability chain, then the validity of all subsequent levels towards the field level is put into question. By maintaining the integrity of calibration, we can limit any damage to its immediate locality, as opposed to inflicting widespread damage.\n- Upon review of the information flow constraints related to calibration traceability, we note this presents us with a novel access control challenge to which the unification of the BLP, BIBA and Chinese Wall models fits well. However, the constraints assigned to information flow defined by BIBA and BLP contradict each other. Fortunately, by taking Sandhu's observation [23] that BLP and BIBA are in fact the same model, we can see that our information flow (Figure 3) is an instance of this observation and by reversing BIBA, we can relieve this contradiction.\n\nMentions:\n- For subsequent discussion, we refer to the information flow model depicted in Figure 3.\n- Fortunately, by taking Sandhu's observation [23] that BLP and BIBA are in fact the same model, we can see that our information flow (Figure 3) is an instance of this observation and by reversing BIBA, we can relieve this contradiction.\n\nOCR Text: \"RAA / ile = Hich {Lav Urlexi Cenideniialmy\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3. Information Flow Model"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We consider topology optimization between two materials: high permeability iron and air. The high permeability iron has a non-linear B − H curve as provided in the Comsol material library. This material data is shown in Fig. 12 in the Appendix. The saturation magnetization is around 2 T. Previous studies on non-linear materials have shown that it is important to account for the full B − H curve of the materials [12]. The permanent magnet material, which is considered fixed, has a linear B − H relation with a permeability of µ r = 1.05 and a remanence of 1.4 T. However, as long as the permeability is close to 1 the resulting magnetic field in the bore is proportional to the remanence, which means that the system can be parameterized by the Halbach formula for the field in the bore B = B rem ln(R o /R i ).\n- Shown in Fig. 12 is the relative permeability data as function of magnetic field for iron that is used throughout this work.\n\nMentions:\n- This material data is shown in Fig. 12 in the Appendix.\n- Shown in Fig. 12 is the relative permeability data as function of magnetic field for iron that is used throughout this work.\n\nOCR Text: \"200 = COQ 800 600 400 200 PJIHII]\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 12. The relative permeability, µr, and the corresponding norm of the magnetic flux density, |B|, as function of the norm of the magnetic field, |H|, for the iron as used throughout this work. Outside the plotted range of |H| the values are assumed constant with the same value as the last data point. The data is taken from the Comsol material library."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- A striking design feature of the primate retina is the manner in which images are spatially sampled by retinal ganglion cells. Sample spacing and receptive fields are smallest in the fovea and then increase linearly with eccentricity, as shown in Figure 1. Thus, we have highest spatial resolution at the center of fixation and lowest resolution in the periphery, with a gradual fall-off in resolution as one proceeds from the center to periphery. The question we attempt to address here is, why is the retina designed in this manner -i.e., how is it beneficial to vision?\n\nMentions:\n- Sample spacing and receptive fields are smallest in the fovea and then increase linearly with eccentricity, as shown in Figure 1.\n\nOCR Text: \"Fiio;\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Receptive field size (dendritic field diameter) as a function of eccentricity of Retinal Ganglion Cells from a macaque monkey (taken from Perry et al. (1984))."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Occupation numbers provide an additional insight into performance or SEET. In Fig. 2, we plot the occupation numbers as a function of the interatomic distance from low-level methods as well as different variants of SEET. FCI occupation numbers are listed for comparison. Both GF2 and GW occupation numbers differ from 2 and 0 when the interatomic distance is large but remain significantly different from the FCI reference. All SEET variants correctly describe the transition from the weakly to strongly correlated regime and SEET occupation numbers follow the trend displayed by FCI very well.\n\nMentions:\n- In Fig. 2, we plot the occupation numbers as a function of the interatomic distance from low-level methods as well as different variants of SEET.\n\nOCR Text: \"SEETIFCIGFZI-IZc) SEETIFCIGWI-[2] SEETIFCW Ho] Ak_hIa\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 2. Occupation numbers of H2 in TZ(Dunning) as a function of interatomic distance."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To validate the proposed protocol, a CV-SI-QRNG experimental setup is built based on balanced homodyne receiver to measure the vacuum fluctuations as in Fig. 9. We implemented an all-in-fiber setup with off-the-shelves devices. The local oscillator (LO) is a narrow line 1550 nm laser (Thorlabs SLF1550P, linewidth 50 KHz), and the LO power is carefully adjusted to obtain the optimal performance. The 50:50 beamsplitter (BS) brings LO signal interfered with the vacuum states to the balanced receiver (Thorlabs PDB480C, bandwidth 1.6GHz). The phase of LO is randomly shifted between 0 and π/2 by a phase modulator (PM) to realized the random sampling between X and P quadratures measurements, based on an initial random seed t. Finally, the measurement results of the balanced receiver are sampled in real-time by a 12-bit ADC (TI ADC12D1800, bandwidth 3.5GHz) with a sampling rate of 1.8G SPS to acquire the raw data, which is to be analyzed by the proposed model to extract secure randomness.\n\nMentions:\n- To validate the proposed protocol, a CV-SI-QRNG experimental setup is built based on balanced homodyne receiver to measure the vacuum fluctuations as in Fig. 9.\n\nOCR Text: \"bHR LO Lasr FO4\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 9. (color online) Experimental setup of the proposed CV-SI-QRNG. LO: local oscillator, VOA: variable optical attenuator, PM: phase modular, BS: 50/50 beam splitter, VS: vacuum state, BHR: balanced homodyne receiver, ADC: analog-to-digital converter. The CVQRNG is realized with off-the-shelves components."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We follow the setup of . We compute per-pixel uncertainty estimates using dropout and measure calibration over all the individual pixels. We recalibrate on all the pixels in the training set. This yields an improvement in calibration error from 5.50E-02 to 2.41E-02, while preserving accuracy. The full calibration plot is given in Figure 5.\n\nMentions:\n- The full calibration plot is given in Figure 5.\n\nOCR Text: \"Calibraiion of a DenseNel for Deplh Eslimalion Recalbrated (rms-3 91} Uncallbraled (rms=3 92} 5 06 { [ Expected Coniidence Level\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5. Calibration curves of a DenseNet model for depth estimation on the Make3D dataset, before and after calibration."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 7 presents L 2 norms of the same degeneracy measures for the same ringdown dataset. While this avoids the choice of a specific coordinate location for observation, we should note that there is still a certain amount of coordinate ambiguity in this quantity. The L 2 norm is only over a certain subset of the spatial slice. The inner boundary is the excision boundary of the simulation, slightly inside the apparent horizon. The outer boundary is a boundary of subdomains in the simulation, with coordinate radius 5. The purpose of this outer boundary is to avoid numerical difficulties when the Weyl scalars become too small to calculate accurately the roots of the polynomial in Eq. (3). Again in Fig. 7, we see the larger value of ∆ ij hanging up while both values of Θ ij decay exponentially.\n- The other figures in Sec. V tell much the same story, though in somewhat less dramatic terms. Fig. 6 shows indications of principal null directions directly crossing the tetrad null vectors, repeatedly causing the null directions to be represented by the point at infinity in λ space, due purely to the choice of spatial tetrad. Fig. 7 shows that the hangups in the degeneracy measure ∆ ij are not limited to the partially symmetry-preferred x and z axes. In fact, ∆ ij , computed everywhere in a tetrad adapted to spherical coordinates, clearly shows this hangup in Petrov Type II even in an integral L 2 norm, while Θ ij does not.\n\nMentions:\n- Figure 7 presents L 2 norms of the same degeneracy measures for the same ringdown dataset.\n- Again in Fig. 7, we see the larger value of ∆ ij hanging up while both values of Θ ij decay exponentially.\n- Fig. 7 shows that the hangups in the degeneracy measure ∆ ij are not limited to the partially symmetry-preferred x and z axes.\n\nOCR Text: \"Ll Smzlte Secohd Sralles ) SueI44 & ScohStalles 2 8402 Cumat hctcm= Ms\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 7: L2 norms of the two smallest values of ∆ij and Θij , integrated over a thick spherical shell extending from just within the apparent horizon, r = 2.2 in code coordinates, to an outer boundary at r = 5 code coordinates. Again, both values of Θij fall quickly to zero at the same rate, while the larger ∆ij value hangs up initially, and eventually falls only to a level over two orders of magnitude above its smaller counterpart."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where the graphs E i are as given in Figure 1. In order to prove the statement of Theorem 4, we first prove a more general lemma stating that an M-condition over P can be shifted over a morphism p with domain P . It is a generalised version of Lemma 3 in [7], but the proof is almost identical as interpretation constraints are not manipulated by this transformation, and both sides of the statement are interpreted in the same graph.\n- where the graphs E i and Y i are as given in Figure 1 and M\n- where the graphs Y i are as given in Figure 1. This M-constraint is only satisfied by graphs that do not have any edges between distinct nodes, because of the assertion that every match (i.e. every node) must be in X and not in Y. Were an edge to exist -i.e. a path -then the M-constraint asserts that its target is in Y; a contradiction. ⊓ ⊔\n\nMentions:\n- where the graphs E i are as given in Figure 1.\n- where the graphs E i and Y i are as given in Figure 1 and M\n- where the graphs Y i are as given in Figure 1.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1. Applying the construction in Examples 3 and 4"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where diag represents a diagonal matrix in the spatial index r throughout the whole Josephson junction, with ∆ being the magnitude of the superconducting order parameter in the two SCs (also the energy gap) and ϕ L(R) the phase of the L(R)SC. Throughout this work, we fix the energy parameters to t i = t i = t SC = 1, m 0 = 0.1, and the lattice constant to a = 1. Moreover, we consider mainly undoped WSMs with µ W = 0, unless explicitly mentioning another doping (inset of Fig. 6). For the SCs we set µ SC = 2 and ∆ = 0.1. We use zero phase ϕ L = ϕ R = 0 when studying the pair amplitudes, but set the phase difference to ϕ R − ϕ L = π/2 to find the maximum Josephson current (appearing very close to π/2). For the WSM-SM tunneling process, we set the two orbital tunneling amplitudes equal, t a W-SC = t b W-SC = 0.5 &lt; t, except in Fig. 4, where we study asymmetric tunneling. We show results for N WSM = 41 layers of the WSM and N SC = 40 SC layers, but have also studied other system sizes. This system size choice is sufficient to induce bulk conditions in both the interior of the WSM and the SC leads as the SC coherence length, ξ = v F x /∆ ∼ 10a (using the Fermi velocity along the x-direction), is much smaller than the system size. All parameters have realistic values, except the SC gaps that are a bit larger in order to get a finite Josephson current for a still computationally manageable system size. However, our assumptions for all parameter values do not affect the qualitative behavior of our numerical results.\n- We are mainly concerned with the maximum current, achieved for ∆ϕ = π/2 or very close to it, which we then plot in Fig. 6 as a function of β, keeping the other parameters the same as in Fig. 3. As seen, the maximum current is almost always monotonically increasing with β. In the light of the chirality blockade, this increase is a completely unexpected behavior. Still, we can fully attribute this current to the bulk proximity effect as follows: Current mediated by non-bulk effects must be carried by overlapping surface Fermi arcs, each with the penetration length ξ c ∼ v F /β into the bulk of WSM [29,31]. Such surface contributions to the Josephson current thus decay exponentially as exp(−L/ξ c ) [29], and then necessarily give a decreasing current for increasing β. But this is completely opposite to the result in Fig. 6, and therefore the current in Fig. 6 cannot have a surface origin. Moreover, we choose to study long Josephson junctions with a junction length much larger than the superconducting coherence length ξ ∼ 10a , such that the current, if only carried by surface states, would be very limited for all values of β. Finally, in the calculation of J we have again limited the summation in k-space to an interval ±k c around each Weyl node, thus automatically severely limiting the surface arc contribution, especially for the larger Weyl separations given by increasing β. We can thus definitively conclude that the current in Fig. 6, and especially its growth with increasing β, cannot be attributed to Fermi arc contributions but must necessarily be a WSM bulk property.\n- Having established that the increasing Josephson current in Fig. 6 is a WSM bulk property, we must attribute the current to the finite s-wave inter-orbital p y -wave intra-and inter-orbital pair amplitudes inside the WSM, which both grow with increasing β, exactly as the total current. In particular, for s-wave pairing it is only the spin-singlet and mixed-spin triplet inter-orbital pair amplitudes that survives at larger β and can therefore also be the only components responsible for a finite Josephson current. In clean systems, such as the system we study, current contributions are also present from the p ywave pairing. As a consequence, the Josephson current in longer WSM junctions become a probe of the finite inter-orbital pairing.\n\nMentions:\n- Moreover, we consider mainly undoped WSMs with µ W = 0, unless explicitly mentioning another doping (inset of Fig. 6).\n- We are mainly concerned with the maximum current, achieved for ∆ϕ = π/2 or very close to it, which we then plot in Fig. 6 as a function of β, keeping the other parameters the same as in Fig. 3.\n- But this is completely opposite to the result in Fig. 6, and therefore the current in Fig. 6 cannot have a surface origin.\n- We can thus definitively conclude that the current in Fig. 6, and especially its growth with increasing β, cannot be attributed to Fermi arc contributions but must necessarily be a WSM bulk property.\n- Having established that the increasing Josephson current in Fig. 6 is a WSM bulk property, we must attribute the current to the finite s-wave inter-orbital p y -wave intra-and inter-orbital pair amplitudes inside the WSM, which both grow with increasing β, exactly as the total current.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 6. Maximum Josephson current achieved at ∆ϕ = π/2 as a function of β. Inset: Maximum Josephson current at ∆ϕ = π/2 as a function of the chemical potential of the WSM for β = 0.6. Other parameter values are same as in Fig. 3."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The raw samples of the aggregated current and voltage i agg and v agg was frame blocked in frames of 160 samples. For every frame a feature vector was calculated consisting of 4 statistical features (peak voltage (V ), rms current/voltage (iFRMS, vFRMS), crest factor of current (cF)) and 15 electrical features (current (I), voltage (V), active power (P), frequency (f), reactive power (Q), apparent power (S), load angle (ϕ), total harmonic distortion current/voltage (iTHD, vTHD), 3 rd /5 th /7 th harmonic current/voltage (iHD3/5/7, vHD3/5/7) resulting to feature vectors of dimensionality equal to F = 19. In order to calculate the statistical importance of the 19 features the ReliefF feature ranking algorithm [45] was used by averaging the ranking scores across the 20 measured video signals. The results are illustrated in Fig. 3. As can be seen in Fig. 3 most of the electrical features in general outperform the statistical features with the most dominant features being the frequency, apparent power, raw current as well as of set current and voltage harmonics. This is in line with previous publications reporting high importance of electrical features [46], [47]. Regarding current and voltage harmonics, they carry significant amount of the video playing energy signal's information, e.g. both iHD5 and iHD7 have high feature ranking scores. Active power P , reactive power Q and apparent power S carry similar information as they can be computed by the relation S = P 2 + Q 2 , thus Q has a relatively low feature ranking score as it can be computed by S and P . Furthermore, as it is not clear how discriminative electric measurements of monitors are when being used for TV channel classification, e.g. electrical measurements might be filtered through a large capacitor at the monitor input, the time domain signals of eight different features are illustrated for two different video sequences in Fig. 4. As illustrated in Fig. 4 all eight features show different shapes for the two different video signals respectively. Specifically, the following four instances marked with bounding boxes i) -iv) are analysed. In specific, the first case denoted as '(i)' illustrates the voltage envelop over time for both videos showing a significant stronger decrease of the red curve compared to the blue curve. The second case denoted as '(ii)' illustrates the difference between the two signals for the active power consumption. In detail, the time envelop of the signals is inverse showing a decrease in the blue signal and a increase in the red signal. The third case denoted as '(iii)' illustrates the envelop of the THD of the voltage showing a significant peak for the blue signal while the red signal is relatively constant. The last case '(iv)' shows the time envelop of the 7 th voltage harmonic, with the red signal being relatively constant while the blue signal has a significant drought. To summarize, Fig. 4 illustrates that especially features with high feature ranking showing significantly different patterns in the time domain for two different videos.\n\nMentions:\n- The results are illustrated in Fig. 3.\n- As can be seen in Fig. 3 most of the electrical features in general outperform the statistical features with the most dominant features being the frequency, apparent power, raw current as well as of set current and voltage harmonics.\n\nOCR Text: \"1 1 1\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3. Feature ranking for the set of 4 statistical (red) and 15 electrical features (blue)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 1: The normalised lattice fluid viscosity, ν/ν l as a function of the relaxation parameter ω. The dual structure around the quick-interaction value ω = 1 is apparent. For a geometrical interpretation in terms of entropy minimization see [10].\n\nMentions:\n- Figure 1: The normalised lattice fluid viscosity, ν/ν l as a function of the relaxation parameter ω.\n\nOCR Text: \"#e: +lrrcrim Srrco-lrarec io (uick:-|ltL .I (uier-Rclnxalicm Td?'ehm (Jillcjn\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: The normalised lattice fluid viscosity, ν/νl as a function of the relaxation parameter ω. The dual structure around the quick-interaction value ω = 1 is apparent. For a geometrical interpretation in terms of entropy minimization see [10]."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where the first and second terms in the right-hand side give contributions from identical nucleon pairs and proton-neutron pairs, respectively. The wave function amplitudes X as well as the coefficients C can be calculated numerically by diagonalizing the Hamiltonian matrix. The thus calculated coefficients are usually irrational numbers and depend on the interactions employed [3]. With the expression given above, calculations have been done to explore the energy expressions in several single-j shells. As examples, in the upper panel of Fig. 1  coefficients C I J are plotted as a function of spin I for yrast states of 96 Cd. Calculations are done in the 0g 9/2 shell with two-body interactions extracted from Ref. [15].\n- As an illustration, in the lowe panel of Fig. 1 we gave calculations for coefficients C I J along the yrast states of 43 Sc and 43 Ti. Calculations are done in the 0f 7/2 shell with two-body interactions from Ref. [3].\n\nMentions:\n- As examples, in the upper panel of Fig. 1  coefficients C I J are plotted as a function of spin I for yrast states of 96 Cd.\n- As an illustration, in the lowe panel of Fig. 1 we gave calculations for coefficients C I J along the yrast states of 43 Sc and 43 Ti.\n\nOCR Text: \"010F 11 1 15\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 1: (Color online) Upper: expansion coefficients CIJ as a function of spin I for yrast states of 96Cd in the 0g9/2 shell; Lower: same as the upper panel but for those of 43Sc (and 43Ti) in the 0f7/2 shell."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Very schematically, to better manage a crisis situation and therefore make the most appropriate decision, it was necessary to ensure a very good collection of information on the crisis situation that occurs, and this, in real time. The coordinating agent (decision maker) uses several operator agents. Each agent, has a specific task. Operator agents send the information in real time to the coordinator, who will process this data and choose an appropriate decision for the situation and then make a recommendation to operators who are observers. So, the role of the coordinating agent who is the decision maker is to ensure the sharing of tasks and results. He has several tasks to ask others to do a game, then he has to integrate the result. The crisis situation will be distributed among several agents, and these agents are cooperative agents who communicate with the coordinating agent. Two-way communication is provided by the FIPA-ACL protocol. We have set up an ontology and a model based on MAS to manage road accidents. We integrated domain ontology with Jena Framework and SPARQL. To validate our contribution, we are dealing with a concrete case of a road accident. Figure 3 illustrates the actors involved in this case of experimentation. The model makes use of three agents: decision maker, observer agent 1, observer agent 2 and camera agent. The sequence is as follows:\n\nMentions:\n- Figure 3 illustrates the actors involved in this case of experimentation.\n\nOCR Text: \"Accident Camcr7 Agunt- Agoni Agent 2 Decision-maker\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3 Functional architecture of the developed prototype"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The general architecture for Kafka is illustrated in Figure 1.\n\nMentions:\n- The general architecture for Kafka is illustrated in Figure 1.\n\nOCR Text: \"Topc \" Jittca| (rnsk nuAt 3]\"do , LtWtIL Fxai hdc } (jart\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Kafka Architecture"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Throughout this work, we group stability into buckets to visualize our results (e.g., Figure 1). We use buckets of 5% (e.g., 0-5% stability, 5.1-10% stability). Doing this allows us to see patterns in stability for a corpus that are not visible from a single overall average.\n- First, we consider downsampling with replacement. We sample five sets of 500,000 sentences multiple times, controlling the amount of overlap between downsamples (from 10% to 60%). 7 Stability is calculated using GloVe embeddings and the words that occur in every downsample for every overlap percentage. Figure 1 shows the results. While stability trends are similar for different overlap amounts, we see stability is consistently higher as the overlap amount increases. Using this method for multiple corpora of different sizes would mean that stability on the downsampled corpora could not be reliably compared, because the overlap amount would vary depending on the size of the original corpus. In our case, the Bible corpus is substantially smaller than the Wikipedia corpus, so if we used this downsampling method, we would not be able to accurately compare stability between the Bible and Wikipedia. For this reason, we do not use downsampling with replacement in our experi-7 Data drawn from an English Wikipedia corpus of 5,269,686 sentences (denoted \"Large English Wikipedia\").\n\nMentions:\n- Throughout this work, we group stability into buckets to visualize our results (e.g., Figure 1).\n- Figure 1 shows the results.\n\nOCR Text: \"10 % 40% 6076 100_ \" 6 10-- 10-2_ Stabilitv (bucketed)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Percentage of words that occur in each stability bucket when varying the five Large English Wikipedia downsamples (with replacement) to have different amounts of overlap. Each line shows results for a different percentage of overlap."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We also explore the impact of the encoder on the model performance. As shown in Fig. 6 and Fig. 7, and Table 2, the models with more parameters tend to perform better than those with fewer parameters. The best models are LSTM 1-128 and GRU 1-128, respectively. As shown in Fig. 6 and Fig. 7, the 1-128 models outperform all the models with only 64 units by a large margin, which indicates that getting the network wider results in a better performance than getting it deeper in our experiment.\n\nMentions:\n- As shown in Fig. 6 and Fig. 7, and Table 2, the models with more parameters tend to perform better than those with fewer parameters.\n- As shown in Fig. 6 and Fig. 7, the 1-128 models outperform all the models with only 64 units by a large margin, which indicates that getting the network wider results in a better performance than getting it deeper in our experiment.\n\nOCR Text: \"JEi ~MAT al Aer-{i C\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 7: The ROC of the seq-to-seq models with GRU layer, with the smoothing frame n = 12."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The temperature, pressure and RH measured in 24 hours has been plotted as a function of time in Figure 11. The variation of these three ambient parameters during the day and night is observed and recorded in our laboratory situated in Bhubaneswar, India. These three ambient parameters measured continuously for 15 days in an interval of 60 seconds has been plotted as a function of date-time in Figure 12.\n\nMentions:\n- These three ambient parameters measured continuously for 15 days in an interval of 60 seconds has been plotted as a function of date-time in Figure 12.\n\nOCR Text: \"Ghin 7r711 Lid JUth Atgghmehtic lrot\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 12. Temperature, pressure and RH measured continuously for 15 days, as a function of date-time."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Figure 3, the effect of the jitter is studied by adding a random delay for each triggers of the test data. This delay is drawn from a normal distribution with a zero mean and standard deviation from 0ms to 55ms.\n\nMentions:\n- In Figure 3, the effect of the jitter is studied by adding a random delay for each triggers of the test data.\n\nOCR Text: \"shldt XdaWA Snd ceap rm\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3. Effect of the jitter on the performances."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The fitting line shown in fig. 6 for r = 512 (δ = 1/2) is 0.72 × γ −0.25 .\n\nMentions:\n- The fitting line shown in fig. 6 for r = 512 (δ = 1/2) is 0.72 × γ −0.25 .\n\nOCR Text: \"1024\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6: Log-log plot versusγ for various positionr. The line is a fitting curve for = 512 given in eq.(39)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Let us start with 2 PIDs case for an easier illustration of the problem. Figure 1 shows the plots of the convergence speed (given by the ratio of r k + s k and the number of iterations) in logscale on y-axis, when starting with a static partition sets of 250 + 750, 500 + 500 and 750 + 250 (in this case, L = 9543). When the Ω k is not set correctly (too big or too small), the gain of the parallelism is reduced. We remark that when the fluid exchange cost is neglected, K = 2 (500 + 500 case) can improve by factor above 2. Figure 2 shows the plots of the convergence speed integrating the fluid exchange cost: the relative gain to the single PID case is much less important than previous results, illustrating the importance of this factor even when K is small. Figure 4 illustrates the evolution of the partition sets when starting from 750-250 (here, we took Z = 1 for a quicker adaptation).\n\nMentions:\n- Figure 2 shows the plots of the convergence speed integrating the fluid exchange cost: the relative gain to the single PID case is much less important than previous results, illustrating the importance of this factor even when K is small.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: Illustration of convergence speed: fluid exchange cost integrated."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Section 3.4, we study the ability of our model to simulate a secondary response, also called memory response. Our model has first been calibrated in order to reproduce an in vivo primary response against Listeria monocytogenes (Lm) infection from [63] (see Figure 8). Then, the same parameter values have been used to simulated both a primary and secondary responses. However, secondary response simulations are performed with initially 3 APCs and 30 memory CD8 T-cells (instead of 30 naive CD8 T-cells for the primary response) that are able to bind an APC to become pre-activated, then the differentiation scheme presented in Section 2.3.1 applies. The molecular profile of the initial memory cells is set as the asymptotic molecular profile developed by memory cells at the end of a primary response, as discussed in Section 3.2.\n- On Figure 8, we compare in silico primary and secondary responses from our model with in vivo primary and secondary responses against Lm infection from [63]. Since our model has been calibrated to fit the primary response data, we do not aspire to reproduce the quantitative dynamics of the secondary response, but rather to study its qualitative properties. Namely, the secondary response is characterised by a slower and weaker contraction phase, from the peak of the response D7 p.i. to the last time point D29 p.i.. This weaker contraction could be explained by an early production of memory cells that leads to a large population of memory cells, as it is the case in our model (Figure 7).\n- In addition to reproduce primary responses, our model easily produces secondary responses. Memory cells generated during the in silico primary response succeeded in mounting a stronger secondary response upon antigenic stimulation (Figure 7 and Figure 8). It should be noted that the differences between outcomes of the primary and secondary in silico immune responses only depend, in this work, on the difference between the molecular profile of memory and naive CD8 T-cells and do not take into account a lot of characteristics of the secondary response described in the literature such as: biggest initial CD8 T-cell population [8], shorter cell cycle [66] or sensitivity to inflammatory cytokines, such as IL12 [15].\n\nMentions:\n- Our model has first been calibrated in order to reproduce an in vivo primary response against Listeria monocytogenes (Lm) infection from [63] (see Figure 8).\n- On Figure 8, we compare in silico primary and secondary responses from our model with in vivo primary and secondary responses against Lm infection from [63].\n- Memory cells generated during the in silico primary response succeeded in mounting a stronger secondary response upon antigenic stimulation (Figure 7 and Figure 8).\n\nOCR Text: \"Days post-infection\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 8: Number of CD8 T-cells, normalised by CD8 T-cell population size D7 p.i., during in silico primary (black full line) and secondary (blue dashed line) responses (mean over 10 simulations) compared with in vivo primary (black squares) and secondary (blue triangles) responses against Listeria monocytogenes from [63]."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The terms C l χ and R a depend on the SNR value and the power allocations (ρ 1 , ..., ρ N b ). For a given SNR, the power Finally, Figure 2 shows the capacity in (4) obtained with optimal power allocations for N b = 2, N b = 3 and the capacities of the QPSK and 16-QAM modulations. In the range of SNR from 0 to 5 dB, the systems with N b = 2 and N b = 3 obtain the same capacity. Thus, in the rest of this paper, we analyze the system with N b up to 3. This prevents to use large modulation orders as needed in satellite communications, e.g., quadrature amplitude modulation with order greater than 16 are not used in [11]. On the cooperative strategy curves, we also give the power allocations (ρ 1 , ..., ρ N b ) obtained from (5) for several SNR values. These power allocations are used for the simulations in Section IV.\n- In practice, it is unlikely that signals of multiple sources arrive at the destination at the exact same time with the same 1 Digital Video Broadcasting -Satellite -Second Generation  ρ 2 gives the best performance. Note that the throughput achieved with LDPC codes is close to the capacity. For the scenario with N b = 3, despite the good capacity for high SNR presented in Figure 2, simulations show that signals transmitted by our scheme cannot be decoded by LDPC codes with rates greater than 2/5. This is due to the asynchronous conditions which penalize the throughput more than in the N b = 2 case. Thus, a throughput above 2.4 bits per symbol period cannot be achieved with the parameter N b = 3. Subsequently, the parameter N b is set to 2 to keep the good performance in terms of throughput regarding to the TDMA scheme.\n\nMentions:\n- For a given SNR, the power Finally, Figure 2 shows the capacity in (4) obtained with optimal power allocations for N b = 2, N b = 3 and the capacities of the QPSK and 16-QAM modulations.\n- For the scenario with N b = 3, despite the good capacity for high SNR presented in Figure 2, simulations show that signals transmitted by our scheme cannot be decoded by LDPC codes with rates greater than 2/5.\n\nOCR Text: \"QPSK (0.25,06 1 16-Qam 0025 |,4 Nb-2 Nb 3 76,00 15 410,251 010.5, Esko uB)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2. Capacities comparison"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To that end, we started our experiments by searching for the best instance of the model where the agents search as individuals, i.e. ρ = 0 (see below the definition of searching performance). Note that when ρ = 0 the model depends only on the distribution of turning angles, determined by σ (r 1 and r 2 are meaningless in this case). We therefore searched over different values of σ = {0 − 10, 15, 20, 30, 50} running 1, 000 simulation for each instance. Then we searched for a social model where ρ &gt; 0 improves searching performance. In order to reduce the number of model parameters we first searched for concentric radii values that showed good performance. We did this by searching over four parameters simultaneously: ρ = {0.1 : 0.1 : 0.9}, σ = {1, 3, 5, 10, 15, 20, 30, 50}, and r 1 = {0 : 0.1 * r s : r s } and r 2 = {0 : 0.1 * r s : r s } (r 1 ≤ r 2 ), total of ∼ 4, 500 instances (4-parameters sets); each instance was tested over 50 simulations. According to the results, we fixed the concentric radii, r 1 and r 2 that achieved on average the minimal searching time: r 1 was set to 3 10 r s and r2 was set to 7 10 r s . Later, we isolated some of the additional agent parameters and tested their effect on the performance of the model while focusing on changing the social weight ρ = {0 : 0.1 : 0.9} (we always tested different values of σ = {3, 10, 20, 30, 50} for each model instance Fig. S6). In these experiments (after r 1 and r 2 were fixed) each instance was tested over 1, 000 simulations. We tested the model for different numbers of agents, n = {2, 5, 10, 50, 100, 200, 500}, different ratios between the social detection radius to the target detection radius, r s r t = {1, 15, 100}, different rates of updating the movement direction, c f = {10Hz, 5Hz, 2Hz, 1Hz} and different numbers of targets, t = {1, 2, 5, 10, 30, 50, 100}.\n\nMentions:\n- Later, we isolated some of the additional agent parameters and tested their effect on the performance of the model while focusing on changing the social weight ρ = {0 : 0.1 : 0.9} (we always tested different values of σ = {3, 10, 20, 30, 50} for each model instance Fig. S6).\n\nOCR Text: \"0 = L ~0 =0.3 E ~0 =0.6 5 200 $ (Degrees ~0 =\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6: Mean searching time for different combinations of ρ and σ. In these simulation the following parameters were used: n = 50, rs/rt = 15, t = 1."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 3 shows the performance of each of the training, validation and test sets. As the network was trained, the mean squared error (mse) decreases rapidly, and best validation performance is found at 3.5531 × 10 −7 . Figure 4 shows the gradient and validation check, the gradient is about 8.3542 × 10 −7 and the validation check is about 0 at epoch 27 respectively. The weight and bias is adjusted to Optimum value which is shown as a low value of gradient and better network learning which results network more accurate and reliable, reduced false predictions. The sufficient network learning shown in validation plot, and the stop point of training and the starting of the over fitting of the data. Table 1 gives the mean square error and percentage of error during the training, validation and testing, which shows the proper classification of samples. Figure 5 shows the how error sizes are distributed at different instances as an error histogram, and the error approaches to zero. Figure 6 shows the neural network receiver operating characteristic for data and relation between the false positive and true positive rates as the output thresholding is varied from 0 to 1. The training, validation, test and collective ROC describe the neural network performance.\n\nMentions:\n- Figure 3 shows the performance of each of the training, validation and test sets.\n\nOCR Text: \"Trainta Pendrn alepc- 4 1 27 Encchs\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3 Mean square Error performance"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The now-validated two-region model is compared to the oneregion model to determine regimes of applicability. Note that the two-region model is a generalization of the one-region model and that R in Eqn. (4) reduces to zero or unity when reflector is negligible [12]. Therefore, the two-region model can always be used and is not prohibitively more computationally expensive. The relative deviation between the one-and tworegion models ((α 1 − α 2 )/α 2 ) is shown in Fig. 8. It is shown that the models deviate by more than 10% as the reflector and reflection increase, and are nearly identical when there is only a small amount of reflector.\n\nMentions:\n- The relative deviation between the one-and tworegion models ((α 1 − α 2 )/α 2 ) is shown in Fig. 8.\n\nOCR Text: \"1 L Copper Thickr e83 [cm]\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 8: Relative deviation of the one-region model estimate of the prompt neutron period from that of the two-region model."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Using these constraints, the potential scale and its shape could be estimated during inflation so that Fig. 15 clearly illustrates the scalar field potential. In contrast to the strong regime, the scalar field during the inflation is larger than the Planck mass. Also the potential is of order 10 62 which is bigger than the potential in strong regime. On the other side, the constraints on the parameters could be used to compare the energy densities of the scalar field and radiation. Fig. 12 portrays them versus the scalar field during the inflation, and it could be seen that the scalar field is strongly the dominant component of the Universe.\n\nMentions:\n- Fig. 12 portrays them versus the scalar field during the inflation, and it could be seen that the scalar field is strongly the dominant component of the Universe.\n\nOCR Text: \"@x10t9\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 12: Energy densities of the scalar field (blue line) and radiation (red line) is depicted in term of the scalar field during the inflation times, where the constant parameters are taken as: n = 2, m = 1, N = 60, and H0 = 1 × 10 −27. The parameter ρ̄i is defined by ρ̄i = ρi(φ)"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To more accurately compare the used zero-tree-based schemes and to better evaluate the proposed AVDZ algorithm, we use 24 different male and female speech signals in 3 languages of English, German and Farsi (Fig. 10). The first and second letters of each file name specify the language and gender, respectively. All these files have an equal time length of 4 seconds, and as mentioned previously are sampled at 16 kHz sampling frequency and represented with 16 bit/sample. These speeches include also those used in the previously given tables. The same kind of comparative results are obtained for music signals but are not included in this figure because of their high bit-rates that would jeopardize the presentations. Figure 11 shows the average coding, decoding and total computation times, averaged on 24 speech files, for each method. The total time is the sum of the coding and decoding computation times. As seen in this figure, the proposed algorithm is considerably superior to all others, by about 35% and 50% relative to Modified SPIHT and EZW algorithms, respectively. Both perceptual (in PESQ scores) and objective (in SEGSNR (dB)) measures of the reconstructed signals, indicated in Fig. 12, show the good performance of the codec. Moreover, the good resulting perceptual qualities could be considered as a sign of the correct performance of the proposed perceptual quantizer of section 2.2. To evaluate the correct performance of our proposed perceptual quantizer, we compare it with the case of quantizing all coefficients with 7 bits, referred to as 7-bit uniform quantizer. In both of these codecs, the AVDZ algorithm is used as re-encoder. As seen in Fig. 13, the proposed perceptual quantizer can reduce the average bit-rate by about 15-35% with almost the same or even better perceptual quality.\n\nMentions:\n- To more accurately compare the used zero-tree-based schemes and to better evaluate the proposed AVDZ algorithm, we use 24 different male and female speech signals in 3 languages of English, German and Farsi (Fig. 10).\n\nOCR Text: \"SHIIT M_ils 77 H:i6JSpHI Pfo #n Kamcfcftancaia\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 10. Comparison of the average bit-rates of the re-encoders for 24 different Male and Female speeches in 3 languages of English, German and Farsi (The first and second letters of each speech name introduce its language and gender, respectively)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where E r is assumed to be constant. The results of this test of the approach to thermal equilibrium are shown in Figure 4. The agreement with the analytic solution is good, especially in the first case. For the second case, the cooling in the numerical calculation is initially slower than that in the analytic solution for the first few steps in agreement with the results of Turner &amp; Stone (2001). The slow cooling is a result of the backward Euler method and a relatively large time step at the beginning of the decay of the gas temperature.\n\nMentions:\n- The results of this test of the approach to thermal equilibrium are shown in Figure 4.\n\nOCR Text: \"10'2 10 U Tme Isi\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4.— Evolution of internal energy density of gas for calculations of the approach to thermal equilibrium. Numerical results are shown in symbols, whereas the analytic solutions are shown in solid lines. Two cases with an initial internal energy density of 1010 erg cm−3 (upper solid line and squares) and 102 erg cm−3 (lower solid line and circles) are shown. The time step is fixed at ∆t = 10−11 s."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Nevertheless, looking at the convergence curves of the residues with respect to the iterations (Fig. 9), we observe that the unpreconditioned DDM Y0 converges much slower than DDM Y2. In particular, after 28 iterations, DDM Y0 has not reached a residue of 5.10 −2 , whereas DDM Y2 has reached a residue of 10 −4 . As a conclusion, this preconditioner remains very efficient for finer geometries.\n\nMentions:\n- Nevertheless, looking at the convergence curves of the residues with respect to the iterations (Fig. 9), we observe that the unpreconditioned DDM Y0 converges much slower than DDM Y2.\n\nOCR Text: \"DDM with araytic preconcizioner Sotic Mej at Je: Wt-can: IjjrC Lk+ Iceijf: DIA* Tuii e emicoci M uT\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 9. Convergence curves to reach a residue of order 10−4, for the spherical mesh with 5292 DoF, at a frequency of 400 MHz, respectively for DDM Y0 (unpreconditioned) and for DDM Y2 (with analytic preconditioner)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To verify the proposed approach of using automatic fundamental diagram estimation algorithm, we conducted numerical experiment for one of the segments of the Moscow Ring Road (see Fig. 8). For this, we used data from two traffic detectors, denoted # 1 and # 2, with distance 2.2 km between them, and no entrances or exits. We used data from detector # 1 (flow and speed) as boundary conditions and verified the numerical results against data from the downstream detector # 2. In simulation, we use transparent boundary conditions at the location of detector # 2 in: ∂Q/∂x = 0, ∂v/∂x = 0. We simulated 24 hours of a weekday. Results are shown in Figs. 9-13 for each lane separately, and the aggregated data over all lanes are shown in Fig. 14  We used data from detector # 1 (flow and speed) as boundary conditions, and verified the numerical results against data from the downstream detector # 2.  two lanes. We see that the calculated flows in the first lane are lower than the observed ones (Fig. 9), and the opposite is true for the second lane (Fig. 10).\n\nMentions:\n- To verify the proposed approach of using automatic fundamental diagram estimation algorithm, we conducted numerical experiment for one of the segments of the Moscow Ring Road (see Fig. 8).\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 8: The segment of the Moscow Ring Road containing traffic detectors # 1 and # 2. We used data from detector # 1 (flow and speed) as boundary conditions, and verified the numerical results against data from the downstream detector # 2."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Attribute abstraction allows the set of attributes to differ between the concrete and abstract networks. This ability may be used to convert attributes with concrete nodes into those with related abstract nodes. For example, in the BGP network in Figure 7, f maps b i nodes to the abstract node b, while h maps the concrete AS path to its abstract counterpart.\n\nMentions:\n- For example, in the BGP network in Figure 7, f maps b i nodes to the abstract node b, while h maps the concrete AS path to its abstract counterpart.\n\nOCR Text: \"(10O,0,[a,b d) (10O,0, ,6,4)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 7: Example abstraction for a BGP network."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Another advantage for this type of vehicle, is that there are eight propellers generating thrust. Thus, the thrust requirements per propeller are 1/8 of what would be needed for a single propeller. This means lower RPMs, torque requirements per propeller and rotor tip velocities, and the avoidance of cavitation. The performance characteristics of a single propeller operating in air and underwater was evaluated through the use of a static load test apparatus built. It allowed measuring the static thrust at different RPM both in air and water. The results are shown in Fig. 9 for one of the propellers used.\n\nMentions:\n- The results are shown in Fig. 9 for one of the propellers used.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 9 – Propeller static thrust performance curves for air and water"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- and Madrid catalogs, while Figure 7 shows the distribution of the distance between matched groups with the same computation scheme as in Figure 4.   register minimum areas of 1 msh, neither the RGO nor the Madrid catalog is complete below 5 msh (however, the RGO catalog is more complete that its Spanish counterpart).\n\nMentions:\n- and Madrid catalogs, while Figure 7 shows the distribution of the distance between matched groups with the same computation scheme as in Figure 4.   register minimum areas of 1 msh, neither the RGO nor the Madrid catalog is complete below 5 msh (however, the RGO catalog is more complete that its Spanish counterpart).\n\nOCR Text: \"ZUUU 1500 Hoan 1RJ 1000 500 Dlst matched groups Madnd-RGO (deg)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 7. Distance between matched groups in Madrid and RGO catalogs (bins of 0.1 degrees). The red line represents the mean value."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The sterile neutrino lifetime is shown in Fig. 4. As we can see from the decay rate for each channel in App. A, the heavier the sterile neutrino is and the larger the mixing angle is, the shorter is the lifetime.\n\nMentions:\n- The sterile neutrino lifetime is shown in Fig. 4.\n\nOCR Text: \"ilelit 3 1oC2 IjQ0Q 13.0 12.5 12.0 F15 FM 410,5 logu Ksin? 0\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4: The sterile neutrino lifetime."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Fig. 6 we show the dependence of skyrmion radius on curvature radius for the same nanostrips as in Fig. 5. Starting from the planar limit (R a), we notice that R sk remains constant as R decreases up to the point that the two radii become approximately equal. Then a sudden drop of R sk indicates the skyrmion instability and annihilation. This behavior is also observed for higher field values (h/J = 0.020, 0.024), where the skyrmion radius is slightly reduced. Seen from the point of view of competing length scales, the curvature radius is a geometrical length while the skyrmion radius a physical length. Skyrmions are stable when R/R sk 1 and instability occurs when R/R sk ∼ 1. This geometrical argument summarizes the instability condition of skyrmions on cylindrical nanostrips as a competition between length scales. FIG. 7. (Color online) Scaled field versus scaled anisotropy phase diagram showing the evolution of the skyrmion phase with sample curvature. Lines indicate the boundary of the skyrmion domain, defined for Q &gt; 0.5. Curvature angles are indicated on the boundary lines for φ0 = 0 0 (solid), φ0 = 50 0 (dash), φ0 = 100 0 (dot), φ0 = 120 0 (dash -dot) and φ0 = 150 0 (short dash -dot). The star indicates the material parameters (k = 0.4,h = 0.4) used in the present work. Parameters: sample size 17a × 17a, d/J = 0.656 and kBT /J = 10 −3 .\n- We conclude this section with the anisotropy-field phase diagram 34 under increasing sample curvature. The equilibrium magnetization distribution of a nanostip is the outcome of a balance between competing energy terms leading to parallelization of the moments (anisotropy, applied field) and orthogonal arrangement of the moments (DMI), both expressed in terms of the exchange energy. Thus, the scaled anisotropỹ k = kJ/d 2 and scaled applied fieldh = hJ/d 2 are the only two dimensionless parameters required to quantify the rela-tive strength of these two competing factors. 44 Notice that the scaled parameters (k,h) are independent of the discretization level and they depend solely on the material parameters (A, D, K u , M s ). We choose the ratio p/L 10.8 equal to the value used in our simulations for systems supporting a single skyrmion (Section III B) in order to have the same finite size effects. We simulate a field-cooling process for different values of the scaled parameters. When the topological charge of the final state is Q &gt; 0.5 we consider it a skyrmion hosting state. The resulting lines shown in Fig. 7 represent the boundary of the skyrmion domain in the phase diagram. For a planar nanostrip we reproduce the triangular skyrmion domain discussed previously by Keesman et al. 34,45 The skyrmion domain shows a gradual shrinkage with curvature and a shift of the skyrmion boundary towards lower anisotropy and higher field values. The increase of the lower critical field values means that stronger fields are required to stabilize skyrmions on curved surfaces, because only the radial component of the field assists the stability of skyrmions. A particular choice of material parameters is represented by a fixed point in the anisotropy-field diagram. The mark (star) in Fig. 7 corresponds to the material parameters used in our simulations (see Section II). The displacement of the skyrmion boundary relative to the fixed mark implies an instability of the skyrmion phase for curvature angles larger than φ 0 120 0 , as the material mark lies outside the skyrmion boundary. This result is consistent with our previously discussed results regarding the suppression of skyrmion circularity (Fig. 5) and skyrmion radius (Fig. 6) around the same angle. A final remark would be that, despite the fact that we do not proceed with a finite size scaling of our results, we anticipate that our findings for the role of curvature will be qualitatively valid for other values of the p/L ratio.\n\nMentions:\n- In Fig. 6 we show the dependence of skyrmion radius on curvature radius for the same nanostrips as in Fig. 5.\n- This result is consistent with our previously discussed results regarding the suppression of skyrmion circularity (Fig. 5) and skyrmion radius (Fig. 6) around the same angle.\n\nOCR Text: \"3-0016 x\" M:iIp 3U-064 10 20 30 40 50 60 70 80 90 RYa\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 6. (Color online) Dependence of the (effective) skyrmion radius on curvature radius for nanostrips with size 50a× 50a in a uniform applied field. The dashed line is theRsk = R plot that serves as guide to the eye. Skyrmion annihilation is observed when Rsk ' R. Parameters: d/J = 0.2, k/J = 0.016 and kBT/J = 10−3."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- On Figure 7.2 are shown results of tests that were performed on random sparse bipartite graphs of size up to N = 10 5 and mean connectivity up to d = 6. We obtain as well good convergence properties, with no convergence failures, thanks again to a damping factor of .7 for d = 3 to .9 for d = 6. Concerning computational time we observe a scaling in N α which deviates from the linear one as expected as the graph becomes denser for a fixed temperature, α ranging from 1.2 at d = 3 to 1.55 at d = 6. Heterogeneous graphs with larger mean connectivity have a tendency to contains more highly connected nodes for which C v 1. We suspect these nodes to be mainly responsible for a slowing down of convergence. On the same figure we also show the computational time needed by our approximate pre-processing cycle basis stochastic optimization. The scaling is quadratic when the heuristic detailed in Section 4 is used in its complete version, but the very small multiplicative constant allows us to go for relatively large size, before becoming a limiting factor for GCBP around N 10 4 for d = 3 and N 10 5 for d = 4. Since collecting most important small loops has a linear complexity, the way to overcome this issue at large scale is then to limit ourselves to an incomplete set of independent cycles.\n\nMentions:\n- On Figure 7.2 are shown results of tests that were performed on random sparse bipartite graphs of size up to N = 10 5 and mean connectivity up to d = 6.\n\nOCR Text: \"GCBP $ GCFF GCBF \"\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 7.2: Computational times of GCBP and the MCB search algorithm on random bipartite graphs at β = 1 for different mean connectivity d."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Contribution function profiles are plotted in Figure 12, demonstrating that pressures between 5 and 100 mbar are constrained by the observations. As with WASP-43b, all constraints on the temperature profile outside  . Our median retrieved temperature profile agrees with those from the CHIMERA retrieval suite, though our 1-σ range is much tighter. This may be because of differences in line lists and statistical framework.\n\nMentions:\n- Contribution function profiles are plotted in Figure 12, demonstrating that pressures between 5 and 100 mbar are constrained by the observations.\n\nOCR Text: \"JSciHon Mut]LolinuJni S zcnsm Cans 7c7a M <an; I /4is Spze Manb Eec\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 12. Contribution functions for HD 209458b’s dayside atmosphere at 1.25, 1.4, 3.6, and 4.5 µm, corresponding to outside the 1.4 µm H2O feature, inside the 1.4 µm H2O feature, and channel 1, 2, 3, and 4 from Spitzer, respectively."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- First of all we sorted the found works by years to check the relevance of the topic. The results of this sort divided in before and after refining are shown in figure 2. The amount of paper found prior to refining increased sharply from 2015 onwards, suggesting a hype surrounding the subject. The small amount of work in 2019 can be explained by the timing of the mapping study. The search for the work was already completed in October 2019, which is why the quantity would be significantly higher if the study were to be repeated in 2020. Looking at the number of papers after refining, the continuing hype until 2019 is even clearer. Although only 202 papers were found by 2019, 17 survived the refining process. This means that the quotient between work before and after refining is highest this year. The next step was to evaluate the sources from which the present work originated. In total 55 different sources were found. Of these, 39 sources with exactly one paper are part of this study. All sources with at least two papers contributed are shown in Figure 3.\n\nMentions:\n- The results of this sort divided in before and after refining are shown in figure 2.\n\nOCR Text: \"2049 #Betore Retining   nArar Retinirg\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: Paper by years"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Table 1. The positive values of the spectrum of the previous graphs. Remark 4.10. Note that the condition is not necessary. Also, since the bound n + 2 − 2 √ n grows with the size of n it is not immediately useful for finding families of Ramanujan graphs, but it may be useful for the construction and verification of Ramanujan graphs.  (1,3), (2,4), (3,4), (1,2,3,4), (1,3,2,4), (1,4,2,3), (1,4,3,2)} is such that |S| = 8 satisfy the bound of the corollary, so the corresponding G(G, H, S) graph is Ramanujan. Its spectrum consist of ±8 with multiplicity 1, ±4 with multiplicity 2 and 0 with multiplicity 18. Note that it contains the eigenvalues ±4 and as expected by Theorem 4.5, the corresponding 4-regular graph, generated by S ′ = {(1, 2), (3,4), (1,3,2,4), (1, 4, 2, 3)} has 3 connected components. Both of the pair-graphs are shown in figure 10. In this case, the pair-graph G(G, H, (G − H) − S) of figure 12, generated by the complement of S in G − H, is a 7-regular Ramanujan graph. This shows that the result of Corollary 4.9 is not a necessary condition, as mentioned in the above remark.\n\nMentions:\n- In this case, the pair-graph G(G, H, (G − H) − S) of figure 12, generated by the complement of S in G − H, is a 7-regular Ramanujan graph.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 12. The Ramanujan pair-graph G(G,H, (G −H)− S)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- For comparison of the proposed method with other related methods, an experiment is performed in case of Lena image. A capacity-PSNR chart is illustrated in Fig. 4. It is observed that our method, for the same capacity, has better PSNR compared to the other six comparable methods. Also, visual quality after relatively high amount of embedding is acceptable. It is important to note that in the proposed algorithm, more capacity can be obtained by more transformation levels or iterations. Finally in order to evaluate the visual quality of the watermarked images, in Fig. 5 the original and the watermarked images are shown. In Fig. 5b, a sample watermark logo is shown for embedding. Fig. 5c shows watermarked image with capacity of 1.5 BPP. Fig. 5 shows that the watermarked images are not visually different from the original ones. IV. CONCLUSION Novel reversible image watermarking method was presented in medical images based on integer wavelet transform. Improving distortion and embedding capacity was considered in the proposed method. Embedded process was performed in two iterations with a compensation method. It was possible for modified coefficients in the first iteration, to recover its original value in the second iteration. One bit was embedded on each iteration, hence maximum capacity of 1.5 BPP was obtained. Simulation results demonstrated that the proposed reversible image watermarking provided suitable capacity-distortion in comparison with the other methods.\n\nMentions:\n- A capacity-PSNR chart is illustrated in Fig. 4.\n\nOCR Text: \"cacacity-distorticn eMfuEu 1 Gpquepp\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4. Comparison of capacity-distortion of proposed method with six other works for reversible embedding in Lena image."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The last numerical result in the paper compares the SyncB and Split-Merge models right at the theoretical stability boundary of the SyncB model. Figure 5 plots the running average of the sojourn times of the first t jobs, for up to 30,000 jobs. The plot is consistent with both models being unstable, and the important insight we obtain is that the stability regions for the two models seem to be very close, if not the same. This observation provides further support for the use of SyncB as a qualitatively good approximation for Split-Merge.\n\nMentions:\n- Figure 5 plots the running average of the sojourn times of the first t jobs, for up to 30,000 jobs.\n\nOCR Text: \"Mean sojour time at stability boundary Sunch_ Spli: mrju Yuke:\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: Mean sojourn time. Simulated values of the average sojourn time of the first t jobs for the SyncB and Split-Merge models. In both cases, the number of servers is n = 1000, the arrival rate of jobs is λn, with λ = 0.2095, the service requirements of the pieces of a job are i.i.d. U(0, 1), and the number of pieces is computed as N̂ = N ∧n, with N − 1 a mixed Poisson random variable with Pareto(α, β) rate, α = 3, and β = 2/3 (E[N ] = 2). The parameters are such that the SyncB model is at its theoretical boundary of stability. Both simulations were run using the same arrivals and jobs for a total of 30,000 jobs."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 3 shows the loudness level of bandpass filtered pink noise centered at 1 kHz, plotted as a function of bandwidth, as predicted by the Cambridge loudness model and the DNN. The predictions of the DNN are a little below of the Cambridge loudness model, especially for small bandwidths. These deviations are  probably due to the fact that the DNN does not sum the loudness density across frequency at any stage, but rather performs a regression from the 61 input levels to the output loudness levels. It is of interest, however, that the results predicted by the DNN are more consistent with recent psychophysical results [13]. Note that the sounds used for figures 2 and 3 were presented to the DNN during training. The predictions for these sounds are shown because the effects of frequency and spectral summation are fundamental aspects of loudness.\n\nMentions:\n- Figure 3 shows the loudness level of bandpass filtered pink noise centered at 1 kHz, plotted as a function of bandwidth, as predicted by the Cambridge loudness model and the DNN.\n\nOCR Text: \"DMM Orginal mojel Uaon orheo taode\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Loudness level of a bandpass-filtered pink noise with an overall level of 60 dB SPL and centered at 1 kHz as a function of its bandwidth."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 1 illustrates the model we use for the devicedirected detection. This architecture has been proposed for end-pointing task in [7]. The model consists of 4 main components (i) acoustic embedding using LSTM, (ii) ASR decoder features, (iii) character embedding using LSTM, and (iv) classification layer.\n- Once the acoustic and char LSTMs [16] are trained, the 2 embeddings are extracted and concatenated with decoder features to form a 22-dimensional utterance vector (f = [a, c, d]). This vector is used as input to train a fully connected network (classification layer in figure 1). The classification performance is evaluated in terms of equal-error-rate (EER %). EER correspond to the point on detection-error-tradeoff (DET) curve where false positive rate is equal to false negative rate. We also report DET curve to asses whether the improvement is consistent across several operating points. Char LSTM: A deeper analysis of device-directed and nondevice-directed utterances indicated that, devicedirected speech is structured, containing short phrases and similar words. In comparison, nondevice-directed speech is more spontaneous and less grammatical. This is reflected even in 1-best hypothesis of ASR. Table 2 illustrates a few1-best hypotheses. We can infer from the table that 1-best hypotheses of device-directed speech is markedly different from nondevice-directed speech. Inspired by this, we use embedding of 1-best hypothesis. Table 3 shows the EERs of char LSTM models as a function of input embedding size. It can be inferred from the table that a 200 dimensional embedding is optimal for this task.\n\nMentions:\n- Figure 1 illustrates the model we use for the devicedirected detection.\n- This vector is used as input to train a fully connected network (classification layer in figure 1).\n\nOCR Text: \"pdevice-Jirectedky * Classiication teatu te; From AS? de c Jer KCUSDC Embeddirz 1ttrpfiom ASR de_cder\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Proposed device-directed model, based on the combination of three features types: acoustic embedding, 1-best hypothesis embedding, and decoder features."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 2 presents the cost of executing distributed tasks on the cloud with respect to the number of time blocks used by each plan. It shows that our framework predicted the number of used time block for all plans accurately. The decentralised approach represented by plan 3 is cheaper than the centralised one that requires 4 hours. In other words, the benefit of reducing the network distance can outweigh the deploy time suffered by each additional VM. In order to evaluate the performance of different plans, we measured the overall running times and plotted them in Figure 3. As predicted, using more instances results lower running time. The centralised plan has the highest running time, since it does not execute in parallel and most tasks are not assigned to their nearest instances.\n- Figure 2 and Figure 3 also highlight the trade-off between different decentralised plans; higher the cost with respect to the number of time blocks better performance with respect to execution time is obtained. Moreover, it also reflects the decision that user has to make: whether she favours performance over monetary cost or vice versa.\n\nMentions:\n- In order to evaluate the performance of different plans, we measured the overall running times and plotted them in Figure 3.\n- Figure 2 and Figure 3 also highlight the trade-off between different decentralised plans; higher the cost with respect to the number of time blocks better performance with respect to execution time is obtained.\n\nOCR Text: \"aun 1 305 ' 2 7796 * Jeu Iin Va I1L2*0 102875\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Running time of different plans"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where T rel and T cm denote the kinetic-energy operators of the rel. and c.m. motion, respectively, and V rel and V cm are the separable parts of the potential energy. Thus, W contains only the non-separable terms of the potential energy. U int (r) is the interparticle interaction. In Fig. 1 the eigenenergy spectrum of the Hamiltonian in Eq. (1) for two ultracold atoms interacting via the δ pseudopotential\n- a being the s-wave scattering length and m the atomic mass, is shown. The atoms are confined in an isotropic, i. e. spherically symmetric, harmonic trapping potential. The energies are plotted for a varying inverse s-wave scattering length d ho /a, d ho = 2 mω being the harmonic oscillator length. The spectrum of H rel (Eq. (2)) contains a bound state |ψ b bending down to negative infinity for a → 0 + and trap states, the energetically lowest one denoted as |ψ 1 . In case of a harmonic trapping potential, the coupling W between the c.m. and the rel. motion vanishes. Hence, in order to obtain the spectrum of the full Hamiltonian (Eq. ( 1)) the c.m. energies are added to each state of the rel. spectrum, resulting in the middle plot of Fig. 1. Each state of the rel. spectrum appears now with an infinite series of c.m. excitations. Crossings appear between c.m. excited bound states, e. g., |ψ b Φ (2,0,0) , and trap states, e. g., the lowest trap state |ψ 1 Φ (0,0,0) . Introducing a coupling W (r, R) = 0 between c.m. and rel. motion turns the crossings to avoided ones (besides modifying the energies of the states also globally) and enables an adiabatic transition of a c.m.-excited molecular state and a state of an unbound atom pair in the c.m. ground state a indicated in the lower plot of Fig. 1.\n- The full two-body spectrum Fig. 1 shows a plethora of crossings. A full six-dimensional treatment of the twobody problem involving the c.m.-rel. coupling is possible [4,24], but numerically quite demanding. Since c.m.-rel. coupling resonances can have a substantial influence on the stability of an ultracold atomic gas, the knowledge of the position and coupling strength of the resonances is of great interest. The understanding and assignment of these resonances is in particularly important for an unbiased identification of other few-body resonances of, e. g., Efimov type. Therefore, simplified models that allow for the estimate of resonance positions and widths are desirable.\n\nMentions:\n- In Fig. 1 the eigenenergy spectrum of the Hamiltonian in Eq.\n- Hence, in order to obtain the spectrum of the full Hamiltonian (Eq. ( 1)) the c.m. energies are added to each state of the rel. spectrum, resulting in the middle plot of Fig. 1.\n- Introducing a coupling W (r, R) = 0 between c.m. and rel. motion turns the crossings to avoided ones (besides modifying the energies of the states also globally) and enables an adiabatic transition of a c.m.-excited molecular state and a state of an unbound atom pair in the c.m. ground state a indicated in the lower plot of Fig. 1.\n- The full two-body spectrum Fig. 1 shows a plethora of crossings.\n- Hence, the harmonic approximation is used and the position of a c.m.-rel. coupling resonance is determined by the position of the crossing of the c.m. excited bound state |ψ b Φ n , where n = (n x , n y , n z ) is the quantum number of the c.m. excitation, that separates spatially for an optical-lattice potential, and a trap state |ψ t Φ m as illustrated in Fig. 1.\n\nOCR Text: \"Hk: HwH_R4Wh I K: T GWHUM W Har: ERKR\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1. (Color online) Eigenenergy spectrum of two ultracold atoms in an isotropic harmonic trap interacting via a δ pseudopotential [1, 23] for a varying s-wave scattering length a (or varying confinement length dho which would, however, also change ω continuously). The upper panel shows the energy of the rel. motion Hamiltonian Hrel, the lower one the combined energy of the rel. and the c.m. motion Hamiltonians, i. e. Hrel +Hcm. Introducing a coupling between the rel. and c.m. motion makes the crossings avoided as illustrated in the lower part as solid lines, while the black dashed lines indicate the diabatic curves. Passing through the crossing adiabatically (on the solid line) allows for a transformation of the bound state with c.m. excitation into a trap state in the c.m. ground state."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We further investigate the effect of stochastic immediate rewards on Composite Q-learning on the chain MDP shown in Figure 5a. All actions lead further into the chain, however they yield different expected rewards. Action a has an expected reward of −0.8, corresponding to an immediate reward of −1 with 80% chance and 0 with 20%. Action b, on the other hand, has an expected reward of −1.01, however it is +1 with 99% chance and −200 with 1%. Composite Q-learning with learning rates of 10 −2 for the full Q-function, 10 −1 for the Shifted Q-functions and 10 −3 for the Truncated Q-functions is compared to Q-learning with the same respective learning rates. Results for different horizons and no discount are depicted in Figure 5b. It can be seen that Q-learning with a high learning rate is converging to a wrong action-value and the deviation becomes more significant the longer the horizon of the task. The same holds for Q-learning with a learning rate of 10 −2 , however the deviation becomes smaller. Moreover, Q-learning with the same learning rate as the full Q-function in Composite Q-learning is converging much slower for growing horizons, yet the learning rate for the Truncated Qfunctions is 10 times smaller than for Q-learning. This emphasizes the mutual interplay of Truncated and Shifted Q-functions. Furthermore, Composite Q-learning is converging to the true optimal action-value. Q-learning with a small learning rate of 10 −3 is far from achieving a satisfying level of convergence within the time frame of 2 • 10 7 updates. Shifted Q-functions thus allow for higher learning rates which in combination with a more cautious fitting of short-term predictions leads to increased data efficiency while achieving a smaller deviation from the true action-value. Most importantly, the difference in performance between Q-learning and Composite Q-learning (better value estimation w.r.t. to a high learning rate and better convergence properties comparing to a small learning rate) becomes larger for longer horizons. The performance for different rollout lengths are shown in Figure 6.\n\nMentions:\n- The performance for different rollout lengths are shown in Figure 6.\n\nOCR Text: \"F]u Uuda\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6: Performance of Composite Q-learning applied to the MDP in Figure 5a with different rollout lengths n."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Consider using the dataflow graph shown in figure 2  A sample dataflow graph If we use a multi-agent system to perform this operation, we can implement each of the nodes (operations) with a single agent if we define them at a fine grain level. In this example we use five different agents and each of them runs one single operation as is shown in Figure 3. The agents act in parallel on isolated operations, get information (data) from the environment, and send the results back to the environment. As Figure 4 shows, in this structure, Agent1, Agent2, and Agent3 receive the input tokens from the enviroment, process them and send the results to Agent4 and Agent5. Finally Agent5 sends the overall result to the environment. A signal from the environment activates this multi-agent system and when each agent completes the operation on its input tokens it will set its done signal and send the value of that result to the agent in the next level. It will inform that agent by sending its done signal to the strobe signal of its successive agent. In this model, hardware agents use done and strobe signals for handshaking.\n- To show the power gained by reconfiguring hardware agents and also to demonstrate how hardware agents can provide support for both fine grain and coarse grain abstractions [15], we implement the data flow graph of Figure 5 using the same multi-agent system of Figure 3. As we see in Figure 5, Agent1 and Agent5 are coarse grained agents, with several data flow operations implemented in the same agent,while Agent3, Agent4, and Agent5 are fine grained agents, with only a single data flow operation implemented per agent. They act in parallel in those operations which are not dependent on the other operations, and also cooperate together to find the outputs. The major reason for combining some operations in a single agent is to reduce the hardware interface and the amount of inter-agent communication. This is analogous to the grain packing problem associated with traditional parallel processing problems [16].\n\nMentions:\n- In this example we use five different agents and each of them runs one single operation as is shown in Figure 3.\n- To show the power gained by reconfiguring hardware agents and also to demonstrate how hardware agents can provide support for both fine grain and coarse grain abstractions [15], we implement the data flow graph of Figure 5 using the same multi-agent system of Figure 3.\n\nOCR Text: \"Agente Agent? MAgen} Agent4 Agcnts\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3. A multi-agent system to implement the data flow operations of Figure 2 By implementing this data flow graph with hardware agents we benefit from the speed of the specialized hardware in processing the input tokens and producing the results. Five hardware agents cooperate together to form a multi-agent architecture for this data flow graph. As Figure 4 shows, in this structure, Agent1, Agent2, and Agent3 receive the input tokens from the enviroment, process them and send the results to Agent4 and Agent5. Finally Agent5 sends the overall result to the environment. A signal from the environment activates this multi-agent system and when each agent completes the operation on its input tokens it will set its done signal and send the value of that result to the agent in the next level. It will inform that agent by sending its done signal to the strobe signal of its successive agent. In this model, hardware agents use done and strobe signals for handshaking."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The group structure can be visualised as a graph. The elements are vertices, and edges are transpositions connecting the vertices. If all group elements are exhaustively enumerated, the graph is a circle (e.g. Fig. 4) with q! vertices. There exist more compact representations of the group, such as the complete graph K q . Without affecting the generality, the exhaustive representation is preferred in this work.\n- A circle is obtained as follows: a) only the CNOTs are kept from the circuit, and other gates are discarded (e.g. the T gates from Fig. 4), b) the wire endpoints corresponding to input and output are joined together. Fig. 5 is an example of obtaining a circular CNOT circuit. Pairs of adjacent vertices in the chain represent the qubit allocation configurations before and after a remote CNOT was compiled.\n\nMentions:\n- If all group elements are exhaustively enumerated, the graph is a circle (e.g. Fig. 4) with q! vertices.\n- A circle is obtained as follows: a) only the CNOTs are kept from the circuit, and other gates are discarded (e.g. the T gates from Fig. 4), b) the wire endpoints corresponding to input and output are joined together.\n\nOCR Text: \"[0,1,2] [0,2,1]4 [1,0,2] [2,0,1] J[1,2,0] [2,1,0]\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 4. Circular arrangement of 6 = 3! configurations. In order to avoid confusion with a coupling graph, vertices are polygons. The permutation vector associated to each vertex is placed next to the vertex. For example, [0, 1, 2] is next to p0. The edge represents transpositions. For example, the permutation p0 is transformed into p1 by transposing 0 and 1. The edges and vertices are red in order highlight the correspondence to Fig. 1. The weights along the red circles are zero (Sec. II D)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Electronics components of robot were chosen it to be affordable as it is aimed at one of the main objectives. Block diagram of the electronics circuit is shown in Figure 4. An ATMega2560 microcontroller together with a L298 motor driver were used as the main control unit. It got the velocity feedback from the quadrature encoder integrated with the DC motor. The encoder had 34:1 gear ratio and 48 CPR, hence the total counts per revolution was 1632. That brought precision to the measurement since the encoder is quite sensitive. Besides the quadrature encoder, a 9DOF IMU was placed onto the robot in order to acquire orientation and acceleration. Additionally, an XBee Pro module was used for remote operation and parameter setting. Although an S2B model having a 120m range was used in this experiment, it's easy to change it to a long range model since they're pin compatible.\n\nMentions:\n- Block diagram of the electronics circuit is shown in Figure 4.\n\nOCR Text: \"YM: W Kuo Ve Jle E@dMs U: Quue koze Erc  DCro\"\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4. Block diagram of the electronics circuit."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- As a consequence, we have to keep track of domain/co-domain information when we define the syntax and the reification environments. The corresponding definitions are given in Fig. 6. We assume an arbitrary Kleene algebra (in the previous example, it would be the algebra of heterogeneous binary relations) and two functions src and tgt associating a domain and a co-domain to each variable (label is an alias for positive, the type of positive numbers, which we use to index variables). The reified inductive type corresponds to the typed reification syntax: it has dependently typed constructors for all operations of Kleene algebras, and an additional constructor for variables, which is typed according to functions src and tgt. To define the evaluation function, we furthermore assume an assignation env from variables to elements of the Kleene algebra with domain and co-domain as specified by src and tgt. Reifying a goal using this typed syntax is relatively easy: thanks to the typeclass framework, it suffices to parse the goal, looking for typeclass projections to detect operations of interest (recall for example that a starred sub-term is always of the form @star _ _ _ _, regardless of the current model-this model is given in the first two placeholders). At first, we implemented this step as a simple Ltac tactic. For efficiency reasons, we finally moved to an OCaml implementation in a small plugin: this allows one to use efficient data structures like hash-tables to compute the reification environment, and to avoid type-checking the reified terms at each step of their construction.\n- Finally note that as it is stated here, theorem erase_faithful requires the axiom Eqdep.eq_rect_eq from Coq standard library. This comes from the inductive type reified from Fig. 6, which has dependent parameters in an arbitrary type (more precisely, the field T of an arbitrary graph G). We get rid of this axiom in the library at the price of an indirection: we actually make this inductive type depend on positive numbers and we use an additional map to enumerate the elements of T that are actually used (since terms are finite, there are only finitely many such elements in a given goal). Since the type of positive numbers has decidable equality, we can eventually avoid using axiom Eqdep.eq_rect_eq [30].\n\nMentions:\n- The corresponding definitions are given in Fig. 6.\n- This comes from the inductive type reified from Fig. 6, which has dependent parameters in an arbitrary type (more precisely, the field T of an arbitrary graph G).\n\nOCR Text: \"Ihks\" ' XLIllgoya hrxMuaw Ji_: ! =cj_U6\" hcslks;% l + Zw.x\" e texzisnyK {xz = blutsif | +1+ _ ZNXTU (6p kd , Hdj xiili-%te|-Eikip V,l GF 6I [IKIL VI.?M: (GKI Mirul:\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6. Typed syntax for reification and evaluation function."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- For a low background experiment, a larger number of stopped muons usually leads to a better experimental sensitivity, however, results of the search for µ − A → e + A conversion on an Au target have not been published. Nevertheless, along with the momentum spectrum of electrons, SINDRUM-II also shows the positron data [2]-see Figure 1. The positron momentum distribution in Figure 1 has a bump near the spectrum endpoint. This note presents an attempt to understand the origin of the bump and its implications for upcoming searches for µ − A → e + A conversion by the Mu2e and COMET experiments.\n- We start by developing a parameterized model of the SINDRUM-II detector response and tuning the model parameters to describe the electron data from [2]. The tuning procedure assumes that the measured electron spectrum is dominated by electrons produced in muon decays in orbit (DIO). This assumption doesn't take into account electrons from radiative muon capture (RMC). Those electrons are definitely present in the data, and Figure 1 shows that in the vicinity of 90 MeV/c their contribution is non-negligible and needs to be taken into account.\n- Finally, we discuss if the observed excess is consistent with the signal expected from the exotic µ − Au → e + Ir transition. We make a simplifying assumption that the detector momentum response function is symmetric and can be described by a single Gaussian with a mean of 0. To choose the optimal value of the resolution parameter, σ P , we vary its value in the range [1, 3.5] MeV/c, convolve the theoretical DIO spectrum with the res-olution function, and use the resulting distribution to fit the SINDRUM-II electron spectrum. The fit has one parameter -the normalization. The fit χ 2 dependence on σ P is shown in Figure ??. The best value of the resolution parameter, σ P = 2.0 ± 0.1 MeV/c, is significantly higher than the SINDRUM-II momentum resolution, which is about 0.5 MeV/c. The difference is most likely due to the contribution of RMC electrons, unaccounted for in the fitting procedure. As one can see from Figure 1, for momenta close to 90 MeV/c, the RMC contribution can not be ignored. As the RMC spectrum falls less steeply than the DIO spectrum, ignoring the RMC contribution should result in a larger value of σ P returned by the fit.\n\nMentions:\n- Nevertheless, along with the momentum spectrum of electrons, SINDRUM-II also shows the positron data [2]-see Figure 1.\n- The positron momentum distribution in Figure 1 has a bump near the spectrum endpoint.\n- Those electrons are definitely present in the data, and Figure 1 shows that in the vicinity of 90 MeV/c their contribution is non-negligible and needs to be taken into account.\n- As one can see from Figure 1, for momenta close to 90 MeV/c, the RMC contribution can not be ignored.\n\nOCR Text: \"ireasurgirel Icisl erzr; YIC sirualion JC smalicr\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: SINDRUM-II e− and e+ spectra on an 197Au target from [2]."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- As previously described, if a student demonstrated any of the behaviors in the screencast coding scheme, the entire screencast was given the associated code. Figure 9 shows the percentage of screencasts in which each of the social moves appeared. Each screencast included, on average, 1.8 social moves. As figure 8 shows, the most common social move was using group pronouns (e.g. us, we, and our) when explaining the steps to solving their physics problem. For example, the problem was \"our question,\" each step was something that \"we do,\" and the results were \"our results.\"\n\nMentions:\n- Figure 9 shows the percentage of screencasts in which each of the social moves appeared.\n\nOCR Text: \"2 1 7 [ 1 INX 1 W J I3 1\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIGURE 9. Percentage of screencasts that exhibit social moves."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 1 illustrates with a simple example the standard syntax of PLEXIL. In this particular example, the plan tasks are represented by the root node SafeDrive, the interior node Loop, and the leaf nodes OneMeter, TakePic and Counter. OneMeter and TakePic are, for example, nodes of type Command. The node Counter has two different conditions: Start is a gate condition constraining the execution of the assignment to start only when the node TakePic is in state Finished, while Pre is a check condition for the number of pictures to be less than 10. The internal state of the plan at a particular moment is represented by the set of all nodes of the plan, plus the value of the variable pictures, while the external state of the plan contains the (external) variable WheelStuck.\n- ---Initial and actual values of a variable node ... endfm Using the infrastructure in INTERNAL-STATE-SYNTAX, the internal state of SafeDrive in Figure 1, is represented by the configuration in Figure 2. Observe that the sort Qualified provides qualified names by means of the operator . : Qualified × Qualified −→ Qualified, which we use to maintain the hierarchical structure of the plans. The dots at the end of each object represent the object's attributes that are not explicitly defined by the plan but that are always present in each node such as the status or the outcome. There is a \"compilation procedure\" from PLEXIL plans to their corresponding representation in Maude, that we do not discuss in this paper, which includes all implicit elements of a node as attributes of the object representation of the node. We adopt the syntax Γ π to represent the execution state of the plans, where Γ and π are the external and internal states, respectively.\n\nMentions:\n- Figure 1 illustrates with a simple example the standard syntax of PLEXIL.\n- ---Initial and actual values of a variable node ... endfm Using the infrastructure in INTERNAL-STATE-SYNTAX, the internal state of SafeDrive in Figure 1, is represented by the configuration in Figure 2.\n\nOCR Text: \"FaltDii?e par4t Losu tCxenze \" toa Slucis? Mailet Icje2t-En-Je 5aril01Dated  Vneali ;Jtk: ~Cin] Whchicer LUco Di ~Ci] Tie1 Mceear ILalt F StI Fclat Crzalu Taattelta| 0**-0renl 6uLe : Takakc StS ~JNLSHEA; Prt: pi:turtz Knee M' [Wal Maeca 3X\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: SafeDrive: A PLEXIL Plan Example"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In this section, we provide a global Kirby diagram for two families of knot surgery 4-manifolds in order to explain almost complete decomposability. The first example is the Dolgachev surface E(1) 2,3 , whose almost complete decomposability was already known before. We explain this fact using an argument in the proof of main theorem. Note that the Dolgachev surface E(1) 2,3 is diffeomorphic to a knot surgery 4-manifold E(1) 31 (equivalently, E(1) 3 * 1 ), where 3 1 and 3 * 1 denote a leftand right-handed trefoil knot in S 3 respectively. Hence it suffices to see how E(1) 3 * 1 is untwisted after a connected sum with CP 2 . E(1) 3 * 1 case: A Kirby diagram of E(1) 3 * 1 can be drawn as in Figure 13 (up to 3handles and a 4-handle). If we take a connected sum with CP 2 , then we get a disjoint (+1)-framed 2-handle on unknot. This Kirby diagram satisfies all conditions in Theorem 1.1 above so that we can apply all operations used in the proof of it. Therefore the tangle part is changed as in Figure 12 and a curve corresponding to the 0-framed longitude goes to the 0-framed longitude of a newly generated knot which is the unknot with two positive crossings and one negative crossing. The other parts coming from E(1) − ν(F ) remain unchanged, so that E(1) 3 * 1 CP 2 is diffeomorphic to E(1) U CP 2 , where U is the unknot. Here is a detailed proof: Let us first consider a portion of the Kirby diagram in Figure 13 and an extra (+1)framed 2-handle coming from CP 2 in Figure 14. We slide this (+1)-framed 2-handle over two (−1)-framed 2-handles as in Figure 5, so that we get Figure 15. Now we apply the operation in Figure 7 to get Figure 16. We slide the (−1)-framed 2-handle over a 0-framed 2-handle to get Figure 17, and we slide again this 2-handle over two 0-framed 2-handles passing through the bottom dotted 1-handle in Figure 13 to get Figure 18 and Figure 19. And then we slide (−1)-framed 2-handle along a 0-framed longitude of K over this (−1)-framed 2-handle twice to get Figure 20. Finally, by using an isotopy and a reversed operation of Figure 5, we get Figure 21 and Figure 22. It gives a Kirby    E(1) Kn case: Let us consider the Stallings knot K n which is obtained from 3 1 3 * 1 by applying Stallings twist n times. A Kirby diagram of E(1) K0 is drawn in Figure 23 (up to 3-handles and a 4-handle). By an operation explained in Figure 5, we get a (−1)-framed 2-handle along a simple closed curve γ 1 or γ 2 in Figure 23. By using this (−1)-framed 2-handle and by operations explained in proof of Theorem 1.1, we can show that E(1) K0 CP 2 is diffeomorphic to E(1) Kn CP 2 for any integer n. Here is a sketch of proof: For a positive integer n, we perform the following sequence of Kirby moves n times repeatedly. (For a negative integer n, we put a (−1)-framed 2-handle along γ 1 and we perform all operations from right to left direction.)\n\nMentions:\n- A Kirby diagram of E(1) K0 is drawn in Figure 23 (up to 3-handles and a 4-handle).\n- By an operation explained in Figure 5, we get a (−1)-framed 2-handle along a simple closed curve γ 1 or γ 2 in Figure 23.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 23. A Kirby diagram of E(1)K0"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To further clarify what drove changes in effective compute over this period, we split the AI and Compute trend into Moore's Law and increased spending/parallelization 5 . We graph an estimate for the effective compute trends in terms of these two components as well as progress in algorithmic efficiency in figure 5 below.\n- Directly commenting on the likelihood of any of the 3 macro trends in figure 5 continuing in the future is out of scope for this work. Making credible forecasts on such topics is a substantial enterprise, we'd rather avoid here than give insufficient treatment. Rather we present the evidence we see as relevant for a reader who'd like to form their own expectations about extrapolating trends in algorithmic efficiency.\n- Figure 5 The notion of effective compute allows us to combine AI and Compute trend and this result in a single graph. These trends multiply as in addition to being able to do more with a fixed amount of compute now, researchers have more of it. The AI and Compute trend is decomposed into a hardware efficiency gain estimate (original Moore's Law) and money/parallelization [Moore, 1965,Amodei &amp; Hernandez, 2018. This estimate, as discussed in the body of this section, is more speculative than the rest of the paper, but we think it's important to explore the potential implications of our efficiency measurements.\n\nMentions:\n- We graph an estimate for the effective compute trends in terms of these two components as well as progress in algorithmic efficiency in figure 5 below.\n- Directly commenting on the likelihood of any of the 3 macro trends in figure 5 continuing in the future is out of scope for this work.\n- Figure 5 The notion of effective compute allows us to combine AI and Compute trend and this result in a single graph.\n\nOCR Text: \"fiecziv? €CMpU: lable ~ \"gest excerimerts lestiate} Toet anuadmniaekan Ta4p Oomt #am\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5 The notion of effective compute allows us to combine AI and Compute trend and this result in a single graph. These trends multiply as in addition to being able to do more with a fixed amount of compute now, researchers have more of it. The AI and Compute trend is decomposed into a hardware efficiency gain estimate (original Moore’s Law) and money/parallelization [Moore, 1965,Amodei & Hernandez, 2018]. This estimate, as discussed in the body of this section, is more speculative than the rest of the paper, but we think it’s important to explore the potential implications of our efficiency measurements."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We now consider a length of lossless transmission line with one end open (Z L → ∞) and the opposite end connected to a resistance R g ≫ Z c . A square pulse of height V 0 is applied to the free end of R g . The width of pulse is chosen to be very long compared to the time ℓ/v 0 that it takes signals to travel the length of the transmission line. After the pulse is applied, the time evolution of the voltage V g at the junction between R g and the transmission line is measured. The experimental setup is shown schematically in Fig. 2(a). To ensure a well-defined pulse shape, R g is chosen to be much larger than the output impedance of the pulse generator. In our experiments, we used R g = 1 kΩ and an HP 8011A pulse generator with a 50-Ω output impedance. A list of the equipment needed to perform all of the measurements described in this paper along with possible vendors and cost estimates is given in the appendix.\n- Figures 3(a) and (b) show results of the analysis of the transient voltage steps for the semi-rigid coaxial cable. The slopes of the linear fits and the corresponding values of v 0 and Z c are given in Table I. The table also includes determinations of C, L, and the dielectric constant ε ′ = (c/v 0 ) 2 of the insulator separating the inner and outer conductors of the coaxial cables. Although not shown, the transient responses of an RG-58 BNC coaxial cable and a high-quality (HQ) sma coaxial cable were also measured. As will be shown in Sec. IV B, the conductor and dielectric losses of the HQ cable are low compared to those of the other cables measured. This cable, of unknown make and model, was donated to the undergraduate lab and has 9-mm diameter, a cloth-woven jacket, and is fitted with specialty sma connectors. The RG58C/U coaxial cable specifications from Pasternack give a velocity of propagation that is 0.659c and a capacitance of 101.05 pF/m, both of which are in reasonably good agreement with our results. 21 On the other hand, Pasternack specifies a characteristic impedance of 50 Ω which is about 3.5 standard deviations away from our measurement. We will discuss a possible reason for this difference in Sec. IV B. The specifications for the UT-141-HA-M17 semi-rigid coaxial cable from Micro-Coax are v 0 = 0.7c, C = 98.1 pF/m, and Z c = 50 ± 1 Ω, all of which are in reasonable agreement with the results given in Table I. 22 Figure 2(b) shows a rounding of the steps that becomes more pronounced as time evolves. When the transient responses of the three cables are compared, there are no obvious differences in the observed rounding. This effect may be due to the onset of higher-order modes in the coaxial transmission lines. The TE 11 mode has the lowest cutoff frequency given by f c ≈ 0.340v 0 /(r 2 + r 1 ) where r 1 is the radius of the center conductor and r 2 is the inner radius of the outer conductor. 3 For the semi-rigid coaxial cable, with r 1 = 0.460 mm and r 2 = 1.493 mm, the TE 11 cutoff frequency is approximately 35 GHz.\n\nMentions:\n- The experimental setup is shown schematically in Fig. 2(a).\n- The specifications for the UT-141-HA-M17 semi-rigid coaxial cable from Micro-Coax are v 0 = 0.7c, C = 98.1 pF/m, and Z c = 50 ± 1 Ω, all of which are in reasonable agreement with the results given in Table I. 22 Figure 2(b) shows a rounding of the steps that becomes more pronounced as time evolves.\n\nOCR Text: \"Tm\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 2. (a) Schematic diagram of the experimental setup used to measure the transient response of a transmission line. (b) The transient response of a long semi-rigid UT-141 coaxial cable. The voltage Vg(t) has been scaled by the maximum voltage reached a long time after the pulse is applied. Although data was collected up to 10 µs, only the data up to 2µs are shown in order to highlight the step-like features in Vg(t)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- for certain means g a and variances V a . Therefore, if g 0 = g 1 the distribution ofḠ with respect to the output state |Ψ n ABC (θ) has variance of the order n 2 , and we are in the Heisenberg scaling regime, cf. Figure 3. We now investigate what happens when the ancillas in row C are measured, as described in our environment monitoring scheme. Note that the measurement data c on its own, carries no information about θ, i.e. F cl (θ) = 0 since the unitary rotation is applied at the end, and only on the B row. However, as in the toy example of section II, the results do contribute to a larger quantum Fisher information, by identifying the pure components |Ψ n AB (θ|c) of the mixed probe state ρ n θ . These conditional states have a similar phase decomposition\n\nMentions:\n- Therefore, if g 0 = g 1 the distribution ofḠ with respect to the output state |Ψ n ABC (θ) has variance of the order n 2 , and we are in the Heisenberg scaling regime, cf. Figure 3.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 3. (Color online) The distribution of the ‘total generator’ Ḡ in |ΨnABC〉 is a mixture of (approximately) Gaussian distributions centred at ng0 and ng1, cf. (4). When g0 6= g1 the distance between the two peaks is n(g1−g0) and the quantum Fisher information (F = 4Var(Ḡ)) scales as n2. If phase purification occurs due to ‘which phase’ information leaking to the environment, one of the peaks decays and the variance scales as n."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where Φ min and Φ max are the angles between the substellar meridian and temperature minimum and maxi- mum, respectively. SinceT min is typically much less than unity, Φ min ≈ −π/2 in most situations (see Figure 2), whileT max is close to unity so small differences in this maximum temperature correspond to significant changes in the phase offset of the maximum, as shown in Figure 3. Note that the the diurnal heating pattern has a longitudinal asymmetry: parcels of gas are heated faster than they cool, so parcels East of the hot-spot tend to be warmer than those West of the hot-spot. This asymmetry manifests itself in the disc-integrated thermal phase curve of the planet: the offset in the peak of the lightcurve tends to be larger than Φ max .\n\nMentions:\n- SinceT min is typically much less than unity, Φ min ≈ −π/2 in most situations (see Figure 2), whileT max is close to unity so small differences in this maximum temperature correspond to significant changes in the phase offset of the maximum, as shown in Figure 3.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2.— The solid line shows the minimum temperature reached by a parcel of gas, T̃min, plotted against the longitude at which the minimum is reached (Φmin = −π/2 at the dawn terminator, Φmin = 0 at the sub-stellar point). The dashed line is the maximum value of T̃min = (1/π) 1/4, reached in the ǫ → ∞ limit. We show the rest of the functional form of Equation 11 as a dotted line for completeness."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- 2) Ranking versus direct binary classification: We also performed experiments to evaluate the contribution of our learning-to-rank algorithm. The results reveal the strength of our ranking mechanism relative to other CNN-based methods. We first removed our proposed ranking model, and then employed a softmax layer to replace the last fully connected layer in Figure 3, with the other layers left unchanged. In this way, the deep network was used to assess whether two input images belonged to the same person. In other words, we performed direct binary classification instead of learning to rank for the person re-identification task like FPNN [30]. The experiments were conducted on the CUHK-01 dataset with a positivenegative ratio of 1:1. No pre-training was used. The CMC curves in Figure 11 show that learning to rank consistently surpasses direct binary classification, thereby demonstrating   that the good performance of our method stems from both deep representation learning and the ranking algorithm because of the intrinsic difference between image classification and ranking tasks. A good network for image classification is not optimal for ranking [42]. Our learning-to-rank algorithm is based on a relative similarity comparison rather than an absolute decision for each pair, which better accords with the spirit of person re-identification.\n\nMentions:\n- The CMC curves in Figure 11 show that learning to rank consistently surpasses direct binary classification, thereby demonstrating   that the good performance of our method stems from both deep representation learning and the ranking algorithm because of the intrinsic difference between image classification and ranking tasks.\n\nOCR Text: \"CMC = CUFKOI 31,390 Rarking 28,495 Binary cls; Rak Score\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 11. Comparison of ranking and direct binary classification."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In the first testing case, the 1D torsion potential [6] of H2O2 that has been discussed in Fig. 2   In the second testing case, the 2D torsional mode of 1-pentyl radical [6] with 15 conformational structures is used. Figure 6 shows the comparison of partition function Q MS-ASB /Q MS-AS due to the lack of exact value. The calculation results by using the MS-ASCB method is however provided neither in [6] nor subsequent studies [15,20,21]. The ratio Q MS-ASB /Q MS-AS is smaller than the unity, which is consistent with the theoretical analysis that the MS-AS method tend to predict a larger value of PF if there are many non-uniformly distributed local minima. This is because, geometrically, the weighted Voronoi tessellation in the MS-ASB method can modify the original \"banded\" Voronoi subspace in the MS-AS method to a more \"round\" subspace with the local minima located centrally, as shown in Fig. 7. Voronoi diagram in the MS-ASB method for the 2D torsional mode of 1-pentyl radical.\n- In the third testing case, the 3D torsional mode of 1-butanol radical [6] with 29 conformational structures is used. It is seen that, as show in Fig. 8(a), the ratio Q MS-ASB /Q MS-AS is close to the unity although there are more coupled torsions than that in Fig. 6. This is probably owing to the relatively uniform distribution of the local energy minima shown in Fig. 8(b). It implies that the improvement of the MS-ASB method mainly relies on the largely non-uniform distribution of the local minima as discussed in Fig. 7 rather than by considering more coupled torsions. In summary, both the theoretical analysis and numerical results show that the MS-AS method tends to predict a larger value of PF, but the proposed MS-ASB method can reduce the possible overestimation by a factor that depends on the specific chemical systems. It is also worthy to mention that, for the 2D torsional mode of ethanol radical in the Supplemental Material, the ratio Q MS-ASB /Q MS-AS is nearly the unity because the conformational structures are highly symmetric, which indicates that the MS-ASB method can be degenerated to the MS-AS method for the evenly distributed local minima so that to further consolidate the validations.\n\nMentions:\n- Figure 6 shows the comparison of partition function Q MS-ASB /Q MS-AS due to the lack of exact value.\n- It is seen that, as show in Fig. 8(a), the ratio Q MS-ASB /Q MS-AS is close to the unity although there are more coupled torsions than that in Fig. 6.\n\nOCR Text: \"0,20 015 005 Q0O J00 IU L500 200\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6. Comparison of partition function QMS-ASB/QMS-AS of the 2D torsional mode of 1-pentyl radical."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The ultimate security is qualified by average error rate P E averaged over ten 500/500 database splits, and larger P E means stronger security. We also realize other steganographic methods to show the effectiveness of the selected steganalysis features. LSB matching [42] and AACbased [43] algorithms are chosen, where the former is the conventional and the latter is content-adaptive. LSB matching means that if the message bit does not match the LSB of the cover element, then one is randomly either added or subtracted from the value of the cover pixel element. Otherwise, there is no need for modification. AACbased algorithm is simulated at its payload-distortion bound. The distortion of AACbased is defined as the reciprocal of the difference between the original audio and the reconstructed audio through compression and decompression by advanced audio coding. Fig. 10 and Fig. 11 show the average detection error rate P E as a function of payload in bps for steganographic algorithm payloads ranging from 0.1-0.5 bps against CTM and D-MC. It can be observed in Fig. 10 and Fig. 11 that the P E of AACbased decreases with the increment of payload and turns to be nearly 0%, and that of LSB matching is always nearly 0%, showing that the steganalysis is effective with respect to the generated audio. Accordingly, P E of the proposed scheme is nearly 50%, which means the proposed scheme is nearly perfectly secure. The experimental results verify the security performance as proved in Section IV-D.\n\nMentions:\n- Fig. 10 and Fig. 11 show the average detection error rate P E as a function of payload in bps for steganographic algorithm payloads ranging from 0.1-0.5 bps against CTM and D-MC.\n- It can be observed in Fig. 10 and Fig. 11 that the P E of AACbased decreases with the increment of payload and turns to be nearly 0%, and that of LSB matching is always nearly 0%, showing that the steganalysis is effective with respect to the generated audio.\n\nOCR Text: \"Proposerl scheme AACbased LSB Matchiug Payload (bps) , against D-MC'\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 11. The average detection error rate PE as a function of payload in bits per sample (bps) for steganographic algorithm payloads ranging from 0.1-0.5 bps against D-MC."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- A comparison with the MD data in Fig. 7 shows that the two continuum theory curves are very close together for a high angle misorientation of about 45 • , but still quantitatively different from the atomistic results. The reasons for this disagreement will be elucidated in more detail in the next section.\n- As seen in Fig. 7, although qualitatively similar, there is a large difference in the range of interaction between MD and PFC.\n- As seen in Fig. 7 there is a profound difference in the range of the disjoining potential for MD and PFC with = 0.0923. Above, we attempted to fix this difference through a rescaling of the parameter . Another possible route is to look at the role of fluctuations in PFC. Intuitively spoken, the underlying idea is that the thermal fluctuations lead to a broadening of the melt layer due to the asymmetric nature of the disjoining potential V (W ). We quantitatively incorporate fluctuations into our calculation of the disjoining potential for one high angle boundary. As discussed above in Section II B we choose to cutoff the noise on the scale of one unit cell. In Fig. 16 histograms of the width distributions for three different undercoolings for a misorientation of θ = 43.6 • are shown. Using the procedure presented above in Section IV B, we can extract the disjoining potential. Fig. 17 shows a comparison of MD disjoining potentials, PFC without noise, and noise based PFC for two different values of λ min . Akin to renormalizing , the fluctuation effects clearly increase the range of the disjoining potential. However, the strength of this renormalization is dependent on λ min . The introduction of fluctuations has the additional effect of renormalizing the equilibrium chemical potential (µ eq ). Without noise = 0.0923 and µ eq ≈ −0.19609. The renormalized value of the equilibrium chemical potential is calculated by running cyrstal-liquid coexistence simulations, and we determine that for λ min = 1a, µ eq ≈ −0.18721 and for λ min = 2a, µ eq ≈ −0.19565, where a is one lattice spacing.\n\nMentions:\n- A comparison with the MD data in Fig. 7 shows that the two continuum theory curves are very close together for a high angle misorientation of about 45 • , but still quantitatively different from the atomistic results.\n- As seen in Fig. 7, although qualitatively similar, there is a large difference in the range of interaction between MD and PFC.\n- As seen in Fig. 7 there is a profound difference in the range of the disjoining potential for MD and PFC with = 0.0923.\n\nOCR Text: \"(=426 , 530.0g23 (PFC} 0-45,2 , #0.93 J-416,4\" ID; (=416\" , 580329 (PFC} 0-45,2 FWiZ]  AE} 15 1 va Int]\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 7. (Color online) A comparison of disjoining potentials between PFC, AE and MD for a misorientation of θ ≈ 44◦, with = 0.0923 and = 0.0329. The MD results represent statistics generated from three different undercoolings."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We start the presentation of our results with the high-field INS data obtained with the magnetic field B [110], because this field direction has been most extensively studied in the past, and in particular in our earlier work [43] we already presented both the field dependence for fields up to 7 T and the momentum dependence of multipolar excitations for certain values of the field in this direction. We found perfect quantitative agreement between the energy of the single resonance at the Γ point and the ferromagnetic resonance observed in ESR (resonance A) within the measured field range. At the same time, a second ESR resonance (resonance B) was seen at fields above 12 T [41], which we did not cover in our initial experiment. We therefore decided to extend our INS measurements up to higher fields, expecting to find the emergence of the resonance B at lower energies relative to the main resonance, as suggested by the ESR data. However, the corresponding energy scans measured with setup 1 at the base temperature of 1.7 K, which are presented in Fig. 1, show no signatures of such an additional peak in the expected energy range of 0.9-1.1 meV up to the maximal field of 14.5 T accessible with this particular experimental setup. The background in our range of interest is clean and essentially constant, and the energy resolution is sufficiently narrow so that the tail of the peak corresponding to resonance A does not overlap with the expected position of resonance B at such high magnetic fields. For instance, at 14.5 T the expected peak splitting is 0.35 meV, while the instrument resolution is less than 0.1 meV [52]. Therefore, we can conclude that resonance B is not visible to neutrons, possibly due to certain selection rules that are different from those of ESR. To confirm this unexpected result, we repeated the experiment at the high-flux spectrometer ThALES (setup 3) [55], the corresponding data are presented in Fig. 2 (a). The absence of the peak associated with resonance B was reproduced, but extending the spectrum to higher energies revealed a new peak at a higher energy of ∼2.25 meV in the 14.5 T dataset that rapidly vanished when the field was decreased below its maximal value. This result suggests that an additional resonance (resonance C) forms above ∼14 T, at approximately the same field as resonance B, but on the opposite side of the intense peak corresponding to resonance A. Because 14.5 T was the highest field available on ThALES, we performed one more experiment at the FLEXX spectrometer using the 15 T magnet equipped with a dysprosium \"booster\" (setup 2), which has a much smaller sample volume, so the sample mass and, consequently, the count rate were reduced (which was partly compensated by choosing a larger value of k f = 1.5 Å −1 ). The resulting spectra, shown in Fig. 2 (b), had to be counted up to 2 h per data point. In this configuration, we could reach fields up to 16.5 T, following a shift in the energy of resonance C with approximately the same slope (g-factor) as that of the more intense resonance A. This leaves no doubt about the magnetic origin of the second peak.\n\nMentions:\n- However, the corresponding energy scans measured with setup 1 at the base temperature of 1.7 K, which are presented in Fig. 1, show no signatures of such an additional peak in the expected energy range of 0.9-1.1 meV up to the maximal field of 14.5 T accessible with this particular experimental setup.\n\nOCR Text: \"\"RIC Q-Iu!4:A93W B/I7 {-19 4 3 FS 790J4 (me\"\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1. Unprocessed INS spectra measured near the zone center Γ ′′(110) for B ‖ [110] using setup 1."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- IoT utility often relies on the same function that poses a threat to journalists, such as the need for voiceactivated devices to be constantly listening [19]. Only two interviewees (I1, I4) explicitly mentioned or alluded to the knowledge that IoT cloud databases (for instance) were points of vulnerability, either for information loss, or because they could divulge locational metadata from selftracking services. This was surprising given the number of journalists who use cloud services, as shown in Figure 1; one interviewee chose not to answer these questions. Various smart technologies are also already in use (e.g., wearables, smart TVs and location tracking devices), even in this set of journalists who engage in high-risk topics.\n\nMentions:\n- This was surprising given the number of journalists who use cloud services, as shown in Figure 1; one interviewee chose not to answer these questions.\n\nOCR Text: \"LAl:#  lujL4 Ate {nimt' Wenznmn l-ciWujLsij Sthde 7enar-dFALIL Uahecutenloar RveJnin Konnttitdn7 Wttineieiiineteat Wlahkeuldt Munbc ornmamin 0 haeul Jogac gmg Sletae\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1. Devices that interviewees and AIB survey respondents use."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In nearby late-type galaxies, r2:1 has an average value of ∼ 0.7 (Leroy et al. 2013;Casasola et al. 2015;Saintonge et al. 2017). However, it is also known that r2:1 varies significantly with galactocentric radius (Casoli et al. 1991;Sawada et al. 2001;Leroy et al. 2009). As such, accurately inferring the CO(1-0) distribution in M 74 using the HERACLES CO(2-1) map required a radially-dependent r2:1. To produce this, we used the data presented in Figure 34 (lower-right panel) of Leroy et al. (2009), where they compare the HERACLES I CO(2−1) maps to literature I CO(1−0) maps of the same galaxies produced by several other telescopes (with appropriate corrections applied to account for differences in spatial and velocity resolution). This yielded ≈ 450 directly-measured r2:1 values, spanning radii from 0-0.55 R25, for 9 of the HER-ACLES galaxies. Leroy et al. (2009) simply binned these points to trace the radial variation in r2:1; however, we chose to take a fully probabilistic approach, and use GPR to infer the underlying radial trend in r2:1. In Figure 7, we plot all of the r2:1 points from Figure 34 (lower-right panel) of Leroy et al. (2009). We applied a GPR to this data, using a Mátern covariance kernel. Because r2:1 is a ratio, we constructed the regression so that the output uncertainties are symmetric in logarithmic space; otherwise, output uncertainties symmetric in linear space would extend to unphysical values of r2:1 &lt; 0 at larger radii. The resulting regression is shown in black in Figure 7. It is in excellent agreement with the radial trend that Leroy et al. (2009) traced by binning the data, with r2:1 elevated to ∼ 1 in the galaxies' centres, falling to 0.7-0.8 over the rest of the sampled region -but our approach has the added benefit over binning of providing wellconstrained uncertainties on r2:1 values produced using the regression. The uncertainty associated with the regression is a factor of ≈ 1.3 over the 0 &lt; R/R25 &lt; 0.55 range in radius sampled by the HERACLES measurements, reflecting the intrinsic scatter present in the datapoints; beyond this, the uncertainty steadily increases, reaching a factor of ≈ 2 at R = R25. Given the uncertainty on αCO, this does not represent a large addition to the total uncertainty on the molecular gas masses we calculated.\n\nMentions:\n- In Figure 7, we plot all of the r2:1 points from Figure 34 (lower-right panel) of Leroy et al. (2009).\n- The resulting regression is shown in black in Figure 7.\n\nOCR Text: \"Galaztocentric Rajius (csl\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 7. r2:1 values from Figure 34 (lower-right panel) of Leroy et al. (2009), plotted against galactocentric radius in terms of R25. The black line shows our Gaussian process regression to this data, with the grey shaded area indicating the 1σ uncertainty."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The ) , i.e., the lamplighter moves from the position a to the position a + 1, and the value of the configuration at the arrival point a + 1 changes from Φ(a + 1) to Φ(a + 1) • b. In the same way, the right multiplication by (−1, δ b 0 ) ∈ B − amounts to changing the value of the configuration at the departure point a from Φ(a) to Φ(a) • b and moving from a to a − 1. Therefore, the generators from B + (resp., from B − ) can be called walk-right-switch (resp., switch-walk-left) generators (this nomenclature for various kinds of elements of lamplighter groups was, in the context of random walks, coined by Wolfgang Woess, and apparently first appeared in print in [LNW08]). Now, in order to make the picture more symmetric, it is convenient to \"mark\" the active group A with the width 2 slider (sliding window) over the positions a and a + 1 (or, equivalently, to distinguish the edge between a and a + 1) rather than to point at lamplighter's position a, see Figure 7.2. The reason for the appearance of the minus sign in formula (7.6) is that the map (7.5) essentially consists in passing from the \"fixed coordinate system\" to the \"lamplighter coordinate system\" (it is the position of the lamplighter that becomes the reference point), and therefore, if the lamplighter moves in the clockwise direction (say, if a = 1) in the fixed coordinate system, then the lamp configuration moves in the opposite anticlockwise direction with respect to the lamplighter (cf. Figure 2.2 and Figure 2.3).\n\nMentions:\n- Now, in order to make the picture more symmetric, it is convenient to \"mark\" the active group A with the width 2 slider (sliding window) over the positions a and a + 1 (or, equivalently, to distinguish the edge between a and a + 1) rather than to point at lamplighter's position a, see Figure 7.2.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 7.2. Lamplighter’s window (slider)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- faster-evolving star clusters, as illustrated by fig. 1 of Parmentier (2010). In that example, the constant radius also implies that more massive CFRgs have deeper gravitational potential wells hence slower gas-expulsion time-scales. This allows high-mass clusters to survive despite a low SFE of 0.20, while low-mass ones are disrupted. The mass-dependent gas-expulsion time-scale thereby carves a turnover in the initially power-law cluster mass function. The same figure also shows that, as clusters evolve after gas expulsion, transient patterns sets in the cluster mass function due to the mass-dependent evolutionary rate. At an intermediate age of 15 Myr, clusters less massive than 3000 M ⊙ (i.e. low-density clusters) have hardly evolved and their mass function sticks to the initial one. That is, none of their stars have yet crossed their tidal boundary. Conversely, clusters more massive than 10 4 M ⊙ (i.e. high density clusters) have completed their violent relaxation, and the bellshaped cluster mass function already shows up in the high-mass regime.\n- We consider a power-law CFRg mass function of slope −2, dN CFRg ∝ m −2 CFRg dm CFRg , totalizing 10 8 M ⊙ of dense star-forming gas. Our aim is to highlight the impact of the sole evolutionary rate on the cluster mass function, that is, the impact of t/τ cross in Eq. 1. To that purpose, we assume all other parameters (SFE, τ gexp /τ cross , r h /r t ) to be mass-independent. We take SFE = 0.35 and τ gexp /τ cross = 0.01 (explosive gas expulsion). Note that a longer gas-expulsion time-scale leaves F b unaffected as long as τ gexp /τ cross 0.3 (see fig. 1 in Parmentier et al. 2008).\n- Figure 1 illustrates the CFRg mass-radius relations encompassed by our simulations. We refer to them as the Σ CFRg (constant mean surface density), ρ CFRg (constant mean volume density) and r CFRg (constant radius) models. They all lie below the line r h /r t = 0.01. As just explained, this cancels any tidally-induced mass-dependent effect upon F b despite Σ CFRg and r CFRg models having mass-varying r h /r t . Figure 1 also shows lines of isoτ cross (see Eq. 3). By virtue of their mass-dependent volume densities, the r CFRg and Σ CFRg models give rise to crossing-times shorter and longer, respectively, towards higher masses. Model properties, along with their corresponding crossing-time in dependence of mass, are given in Table 1. The adopted mass-radius relations are representative of star-forming molecular clumps (see middle and bottom panels of fig. 1 in Parmentier &amp; Kroupa 2011). We stress, however, that molecular clump sizes are often FWHM measurements which, therefore, do not necessarily represent the outer boundary of the star clusters formed by these clumps (see Parmentier (2011) and  for applications of this concept). Other normalizations of the mass-radius relations would simply affect τ cross hence the age -but not the shapesof the cluster mass functions we obtain below.\n\nMentions:\n- faster-evolving star clusters, as illustrated by fig. 1 of Parmentier (2010).\n- Note that a longer gas-expulsion time-scale leaves F b unaffected as long as τ gexp /τ cross 0.3 (see fig. 1 in Parmentier et al. 2008).\n- Figure 1 illustrates the CFRg mass-radius relations encompassed by our simulations.\n- Figure 1 also shows lines of isoτ cross (see Eq. 3).\n- The adopted mass-radius relations are representative of star-forming molecular clumps (see middle and bottom panels of fig. 1 in Parmentier &amp; Kroupa 2011).\n\nOCR Text: \"@.010 @00_ Tcrjss L,CDMxr g O.10 Myr 001 M;r G 08l CFRQ TcFJa \" USoc . ~0,5 5,5 log; nFRo [M , Fass\" 23E C-2) 444 OW1 -\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1. Mass-radius diagram of cluster-forming regions (CFRg) with lines of constant mean number density (nH2,CFRg= 2.3× 10 4 cm−3, pink dotted line), constant mean surface density (ΣCFRg= 0.5g.cm−2, red solid line), constant radius (rCFRg= 0.5pc, black solid horizontal line), and constant crossing-times (blue dashed lines with symbols; see key). Illustrated are the mass-dependences of CFRg crossing-times in relation with their mass-radius relations, hence cluster-mass-dependent evolu ionary rates."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Figure 11, we report the application accuracy of each of our applications for each of our evaluated systems normalized to the Baseline. We observe that the accuracy results directly correlate with the ISI distortion results we presented in Section V-D. Specifically, the accuracy using SCO is lower than the Baseline by an average 6% due to the 12% increase in ISI distortion. PSOPART increases the accuracy by 7% due to the 17% reduction of ISI distortion. Finally, SpiNeMap achieves the highest accuracy among all our evaluated systems (12% higher average accuracy than the Baseline, 20% higher than SCO, and 5% higher than PSOPART).\n\nMentions:\n- In Figure 11, we report the application accuracy of each of our applications for each of our evaluated systems normalized to the Baseline.\n\nOCR Text: \"FSOPabT S\"tVar 2 8 1 1 1 1 1 1 1 8 1 1 1 1 1 1 1\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 11: Application accuracy normalized to the Baseline."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We first compare the evolution of a blast wave with the GLL-EoS and parameters of the sets EoS2 and EoS3 (Tab. 2). The initial data in Tab. 1 (BW) corresponds to a relativistic strong blast wave such that the left state belongs to the red region (the purely relativistic non-convex region) in the diagram on the right in Fig. 2. The right state belongs to the cyan region, (the pure classical convex region). Fig. 3 shows the numerical approximation of the path connecting the left and right states in the p − ρ plane for the RP at hand. The profiles of the wave structure in rest-mass density, velocity and pressure are shown in the left panels of Fig. 4. These exhibit a path connecting both states, consisting of a shock wave propagating to the right and a rarefaction wave attached to a shock wave (composite wave), separated by a contact wave. Figure 5 shows the profiles of the classical and relativistic fundamental derivatives. The latter shows that the left wave breaks into two pieces at the point where the relativistic fundamental derivative changes sign, forming a composite wave (shock plus rarefaction). We note that should we chose the left state in the classical non-convex region (green region in Fig. 3), the resulting wave pattern would have been qualitative\n\nMentions:\n- Figure 5 shows the profiles of the classical and relativistic fundamental derivatives.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5. Classical (blue circles) versus relativistic (red asterisks) fundamental derivative profiles of the relativistic blast wave RP set according to Tab. 1 with the GGL EoS and the parameters corresponding to the EoS2 model (Tab. 2)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where Li s (z) is polylogarithm function. The coefficient of m 2 is a negative, non-monotonic function of µ with the minimum at µ/T ∼ O(1). Interestingly, we find possible correction proportional to m 2 ln m vanishes in the final result. Turning to σ V , we find its zero temperature limit can be obtained as σ V = 1 2π 2 µ µ 2 − m 2 . In this limit, the contribution related to gravitational anomaly is absent. Unlike σ B , whose µ = 0 limit vanishes by C-symmetry, σ V does have a non-vanishing µ = 0 limit, whose small m expansion can be obtained analytically as The coefficient of m 2 is a negative, monotonically decreasing function of µ. Surprisingly, we find possible correction proportional to m 2 ln m also vanishes in the final result. For more general parameters, we plot the m dependence of ACVE and CSE coefficients in Fig. 1 and Fig. 2 respectively. We find the presence of mass generically suppress both ACVE and CSE coefficients, with less suppression at larger µ. At small m, we find the correction given by m 2 . At large m, ACVE and CSE coefficients are expected to be exponentially suppressed.\n\nMentions:\n- For more general parameters, we plot the m dependence of ACVE and CSE coefficients in Fig. 1 and Fig. 2 respectively.\n\nOCR Text: \"0poh Mleli ple] pT2\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 2. Mass dependence of σB/sB0 at µ/T = 1/2 (blue solid), µ/T = 1 (red dashed) and µ/T = 2 (green dotted). σB0 is the CSE coefficient in massless limit (1). The presence of mass generically suppress CSE coefficient with less suppression at larger µ."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The time-dependence of the instantaneous population in the computational basis is shown in Fig. 17. Recall that the system is initialized in the singular computational  ground state, as indicated by maximum probability at time t = 0. As time progresses, the population remains in the instantaneous ground state until t ≈ 0.9T At this point in the program schedule, the energy gap between the ground state and the lowest lying excited states has narrowed sufficiently to permit population transfer, thereby violating the adiabatic condition. At this point in the dynamics, however, the lowestlying excited states represent instantaneous states that will terminate in the ground state at time t = T . There are 16 such states participating in the apparent convergence to approximately 15/16 of the total probability and, as shown in Table 1. The 17-th ground state is not visible in this plot, due to the scale of its contribution, however it undergoes a similar behavior and contains approximately 1/16 2 of the population. Approximately 15/256 of probability is distributed over the remaining 239 excited states.\n\nMentions:\n- The time-dependence of the instantaneous population in the computational basis is shown in Fig. 17.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 17. Time-dependence of the population in the computational basis. The resolution is 11 points over the the range [0, T ]."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- For swarming events we decide the orientation of each camera independently; we find the interesting swarm, we fix the baseline and then we rotate each camera in order to center the swarm in the image. We measure the baseline but we do not directly measure the mutual orientation of the stereometric cameras. Instead we retrieve the 5 angles α, β, γ, δ and making use of a post calibration procedure. Two targets, 2 × 2 checkerboard, are mounted on a bar and their distance is accurately measured. 25 pictures of the targets are taken in different positions, moving the bar in the 3d volume where the event of interest take place. A montecarlo algorithm is then used to find the 5 angles minimizing the error in the reconstruction of the distances between the postcalibration targets. In addition we take some pictures of the targets on the bar, which are not used for the calibration procedure but only to check the reconstruction error. Typical reconstruction errors for the targets used during the calibration process are shown in Fig. 6, orange circles, and compared with the reconstruction error on the control targets not used in the calibration process, green circles. The errors on the two sets of targets are comparable and in both cases the relative errors are lower than 0.01. This guarantees the reliability of our retrieved trajectories.\n\nMentions:\n- Typical reconstruction errors for the targets used during the calibration process are shown in Fig. 6, orange circles, and compared with the reconstruction error on the control targets not used in the calibration process, green circles.\n\nOCR Text: \"Rcccc\" Foszalbralion targels | Cartcl targzts Ia-C5\" 2t\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 6. Midges swarms. Relative error on mutual distances between targets. Orange circles represent relative errors on the targets used in the extrinsic parameters postcalibration process, while green circles represent the relative errors on targets not used in the calibration. For both the sets of measures the relative error is lower than 0.01."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Full-search approach. The approach applied by System A and System B seems to search over alternative pre-processing primitives as well as estimators. Fig. 9 shows the merged graph for the top-5 pipelines derived by System A. Note that these pipelines differ both in the structure and primitives used. Pipelines derived System B display a similar behavior.\n\nMentions:\n- Fig. 9 shows the merged graph for the top-5 pipelines derived by System A. Note that these pipelines differ both in the structure and primitives used.\n\nOCR Text: \"\"x:\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 9. A comparison of pipelines produced by System A shows that these pipelines vary both in structure and in the primitives used."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The SOM S C is generated and trained using as inputs the Ne-MuS constant space. The entire NeMuS structure is generated when a knowledge base is compiled. The codes for each logical element are inserted in a very efficient hashed corpus so that they are uniquely identified with space, instance, and the attribute (if it is the case) they belong to. For detailed description, we refer to [Mota and Diniz, 2016]. Our script just selects the NeMuS constant space to feed SOM training phase and it yields a map as shown in Figure 2. The circles represent the f ather predicate instances and the ×'s represent the mother predicate instances. A. let β 1 ∪ β 2 be a copy of all bindings in N , in which a occurs as 1st argument and b occurs as 2nd.\n\nMentions:\n- Our script just selects the NeMuS constant space to feed SOM training phase and it yields a map as shown in Figure 2.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: The 20x20 SOM grid trained using only with the NeMuS constant space."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where θ crit is the angular scale of the source, corresponding to the causal boundary of the collision event, z 0 and z crit are the amplitudes at the center and edge of the template, Θ is the Heaviside step function, and we have centered the template on the Galactic north pole. In the limit of small amplitude, this is an additive contribution to the CMB temperature anisotropies as in Eq. 1. Theoretical work [29,30] which appeared subsequent to the previous analysis suggests that there is no discontinuity in temperature at the causal boundary, and we therefore restrict our attention to z crit = 0. The bubble collision model contains only one global parameter, the expected number of detectable sourcesN s . This is partially a function of the properties of the potential sourcing inflation, and as such is impossible to predict without a model for the potential. In the context of an inflationary landscape,N s can be considered as a continuous parameter with some prior distribution reflecting the typical vacua produced, but without a measure for the landscape it is difficult to estimate even an order of magnitude for this number. It may even be quite likely thatN s 1, in which case the approximation of looking for widely separated sources is not valid. Ref. [31] has derived the observational signatures in this case (see their Fig. 7), and has used WMAP measurements of the low-CMB temperature power spectrum to constrain a combination of the number of observable collisions, their average amplitude and the current energy density in curvature. We shall concentrate on the small-N s limit and, in the absence of a detailed theory, assume thatN s can take values of O(1), and setN s to be uniform in the range 0 ≤N s ≤ 10. As we shall see, this parameter is constrained by data, and the precise choice of upper limit has no effect on the analysis. In this model, ΛCDM corresponds toN s = 0.\n- The CMB, as a correlated random field on the sphere, is most simply defined in harmonic space by its power spectrum, C . At full resolution, the CMB power is smoothed by the instrumental beam, which we approximate in this analysis with a Gaussian of FWHM f WMAP . In patches that are processed at reduced resolution, the CMB signal is also smoothed by the anti-aliasing beam (another Gaussian, of FWHM f deg ) and further by the pixel window function of the degraded resolution map. This final effect, shown for the relevant beams in Fig. 7, is small but non-negligible. Ignoring the degraded pixel window function means that the covariance contains an overestimate of the CMB power -in our analysis, a noise term -and log-evidences can be underestimated by as much as 1 when degrading to N side,deg = 64.\n\nMentions:\n- [31] has derived the observational signatures in this case (see their Fig. 7), and has used WMAP measurements of the low-CMB temperature power spectrum to constrain a combination of the number of observable collisions, their average amplitude and the current energy density in curvature.\n- This final effect, shown for the relevant beams in Fig. 7, is small but non-negligible.\n\nOCR Text: \"Smoothing _Kernels (Naide 128` 200 400 600 800 1000\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 7. Window functions of the various smoothing kernels appearing in the adaptive-resolution analysis. Shown are the WMAP7 W-band beam (approximated as a Gaussian, solid line), the Gaussian smoothing applied before degradation to Nside,deg = 128 (dashed), and the pixel window function at this resolution (dotted). Note that the pixel window function is only defined up to ` = 4Nside,deg, the maximum multipole allowed by the HEALPix software."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In fact, shift register-based circuits like that in [6,Fig. 4] can be used to perform encoding of an LDPCC code, while decoding can be performed through iterative message passing algorithms working on a window sliding over the received sequence [6]. Complexity of these encoding and decoding techniques increases linearly with the syndrome former constraint length of the code. Therefore, designing codes with small constraint length is a valuable target from the complexity standpoint.\n- From Fig. 2 we observe that, for the cases with w = 2 and g = 6, the matching between the theoretical bound and  the values found through exhaustive searches is perfect for all the considered values of c. Indeed, in this situation, all the practical cases are modeled by the bound, therefore it is always possible to find a solution achieving the bound. Instead, when we have larger row weights of H s , the theoretical bound may not be achievable in practical terms. This results from Fig. 3 for w = 3. However, we also observe that the deviations of the experimental values from the theoretical curves are rather small. The results of exhaustive searches are well matched with the theoretical bounds also for the case with w = 2 and g = 8, as we observe from Fig. 4. In this case, we note that the gap to the bound increases for increasing values of c.\n\nMentions:\n- In fact, shift register-based circuits like that in [6,Fig. 4] can be used to perform encoding of an LDPCC code, while decoding can be performed through iterative message passing algorithms working on a window sliding over the received sequence [6].\n- The results of exhaustive searches are well matched with the theoretical bounds also for the case with w = 2 and g = 8, as we observe from Fig. 4.\n\nOCR Text: \"Hm Il  H\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4. Bounds onLh and values found through exhaustive searches as a function of a, for w = 2, g = 8 and some values ofc."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The model architecture introduced in this paper consists of two neural network modules: a policy network and an instinctual network (Fig. 2). The policy network is a neural network module that is trained to solve a specific task through reinforcement learning, while the instinctual network is kept fixed during task adaptation. The goal of the instinctual network is to safeguard the policy network from potentially dangerous actions during exploration, by modulating its outputs. The specific architecture described here is suitable for reinforcement learning problems with continuous action spaces.\n- Figure 2: The topology of the policy network with instinct module. Both networks receive the same input from the environment. The instinctual network outputs an instinctual action a i and a suppression signal s. The suppression signal is a vector of values between 0 and 1 that determines the magnitude of instinctual action that will be mixed in the policy action. The suppression signal s is multiplied with policy action a p and the opposite suppression signal 1 − s is multiplied with the instinctual action a i . Two action values are finally added resulting in the final action a f . 5. final action vector a f is the sum of two modulated action vectors, a f = a * p + a * i .\n\nMentions:\n- The model architecture introduced in this paper consists of two neural network modules: a policy network and an instinctual network (Fig. 2).\n- Figure 2: The topology of the policy network with instinct module.\n\nOCR Text: \"environmental input instinctual policy network network\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: The topology of the policy network with instinct module. Both networks receive the same input from the environment. The instinctual network outputs an instinctual action ~ai and a suppression signal ~s. The suppression signal is a vector of values between 0 and 1 that determines the magnitude of instinctual action that will be mixed in the policy action. The suppression signal ~s is multiplied with policy action ~ap and the opposite suppression signal ~1−~s is multiplied with the instinctual action ~ai. Two action values are finally added resulting in the final action ~af ."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- It may be noted that recently some researchers [12] interpreted their micro-indentation creep tests during the initial few seconds as approximately logarithmic, which would correspond to n → 0; but their data were later found to be fitted slightly better by a power law with n = 0.08 (see Fig. 5 in [15]), although visually the difference is small.\n- To satisfy these limits, a discrete optimization procedure is pursued as follows (Fig. 5):\n\nMentions:\n- It may be noted that recently some researchers [12] interpreted their micro-indentation creep tests during the initial few seconds as approximately logarithmic, which would correspond to n → 0; but their data were later found to be fitted slightly better by a power law with n = 0.08 (see Fig. 5 in [15]), although visually the difference is small.\n- To satisfy these limits, a discrete optimization procedure is pursued as follows (Fig. 5):\n\nOCR Text: \"jkt,t') J(,t')\" XXXKXX t-t' +$\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: Schematic of constrained optimization showing various possible locations of coordinate origin for various horizontal shifts s and vertical shifts ∆ of the compliance curve"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We divide the matched stars into 10 categories according to their spectral types: O stars, early (B0−B2.5) and late (B3−B9) B stars (the majority of these have supergiant or giant luminosity classifications), spectral type A, F and G type (AFG) supergiants, K and M red supergiants (RSGs), WR stars, supergiant B[e] (sgB[e]) stars, confirmed LBVs, Be/X-ray binaries and stars with composite features of both A and F stars (AFcomp; Evans et al. 2004a, see also Section 4.4). Note, that 813 stars not included in any of the above categories are classified as \"other\" (and only shown in certain plots) 20 . Most are A stars with luminosity class II or F/G stars without a luminosity classification from Evans et al. (2004a), although with v radial &gt; 100 km s −1 . To allow for a direct comparison with the LMC results of Paper I, we present the same color-magnitude and two-color diagrams (CMDs and TCDs), identifying stars in the 10 categories described above. 21 Infrared [3.6] versus [3.6]− [4.5] and J − [3.6] CMDs for all the stars in the catalog are shown in Figures 2 and 3. The locations of all the SAGE-SMC catalog detections are represented by the gray two dimensional histogram (Hess diagram). The red giants form the clump at [3.6] &gt; 15 mag, while the vertical blue extension contains late-type SMC and foreground stars (free-free emission causes the OB stars to have redder colors). The asymptotic giant branch stars are located at [3.6] ∼ 11 mag, and J − [3.6] ∼ 2 mag. The RSGs, sgB[e], and LBVs are among the brightest stars at 3.6 µm and occupy distinct regions in the diagrams (as in the LMC). Most of the O and B stars are located along a vertical line at [3.6] − [4.5] ∼ 0, as expected. The RSGs have \"blue\" colors because of the depression at [4.5] due to the CO band (see e.g. Verhoelst et al. 2009). The sgB[e] stars are 0.6-0.8 mag redder and can have similar brightnesses to the RSGs. The late-B stars are brighter than the early-B stars because most of the former are luminous supergiants. A reddening vector for E(B − V ) = 0.2 mag (corresponding to some of the largest values for color excess found by Massey et al. 1995;Larsen et al. 2000) is shown in Figure 3 to illustrate the small effect of reddening, which decreases at longer wavelengths. We note that the reddest of the bright AFG supergiants is the F0I eclipsing binary R47 (Prieto et al. 2008). In Figure 3 ] ∼ 3.5 mag corresponds to background galaxies. We detect dusty LBVs, sgB[e], RSGs, yellow supergiants and some dusty early-B stars in these CMDs. The detection of the following stars -labeled as \"other\" -at [24] (in order of increasing magnitude at [24]): 2dFS3528 22 (F0), 2dFS0712 (G8), and 2dFS1829 (G2), provides further evidence for their supergiant luminosity class and thus membership to the SMC. The early B-type stars appearing in these diagrams, in order of increasing magnitude at [24] are as follows: 2dFS0402, 2dFS2673 (both of type B0-5V; discussed above), AzV 9 (B0III), AzV 201 (B1(Be-Fe)), AzV 409 (or 2dFS2201; B0.5Ib), AzV 230 (B0Ib) and AzV 390 (B2Iab:). Of these, both AzV 409 and AzV 230 have spectral energy distributions (SEDs) with disk-like characteristics in all Spitzer passbands (i.e. similar to the Be star SEDs shown in Paper I, Figure 17), although the former has a slight upturn at [24]. The nature of these objects is discussed in Section 4.5. Finally, the late-B and AFG supergiants present in these diagrams are hypergiants whose spectra and SEDs indicate the presence of substantial stellar winds. A comparison of the colors of various types of massive stars in the LMC versus SMC CMDs reveals that the colors of the RSGs differ the most (see Section 4.5.3).\n- Of the 12 WR stars in our spectral-type catalog, 10 were matched in the SAGE-SMC database. HD 5980 (SMC-WR5;AzV 229 WN6h;LBV binary) is labelled as an LBV in all diagrams, and is the only WR star with a detection at [24]. We compare the colors of the WR stars to those of their LMC counterparts (see Paper I) and find that their infrared excesses are much lower, possibly indicating that their winds are much weaker than those of LMC WR stars. Specifically, we find the SMC WR stars to have \"bluer\" colors: −0.2 &lt; J − [3.6] &lt; 0.5 (versus 0.2 − 1.7 for the LMC sample), −0.2 &lt; J − K s &lt; 0.1 (versus 0 − 0.6), 0.4 &lt; K s − [8.0] &lt; 1.2 mag (versus 0.4 − 1.8). The origin of the hydrogen absorption lines seen in 10 of the 12 WR stars (classified as \"WN+abs\" or \"WNha\") remains unclear, since radial velocity surveys have only confirmed 4 as binary systems (Foellmi et al. 2003). The absorption in the other stars could alternatively be explained as photospheric absorption (due to weak winds; . The infrared photometry made available for the WR stars in this paper, will enable modeling of their SEDs to determine the properties of their stellar winds and perhaps the origin of the absorption lines. The infrared colors of the AF composite stars provide further clues to their nature and confirm their astrophysical peculiarity. In Figure 2, their [3.6] − [4.5] colors are indistinguishable from AFG supergiants 24 , while in Figure 3 they clearly have red colors, with 0.5 &lt; J − [3.6] &lt; 1 mag. None are detected at [24]. In Figure 6, all 4 AF composite stars with [8.0] detections show near-infrared colors in the range 0.3 &lt; J − K s &lt; 0.9 mag; thus, intermediate between blue and red supergiants, possibly indicating these stars are on blue loops or in interacting binaries. The SEDs indeed are quite \"flat\"; thus, not consistent with normal single stars, while the multiple V I and JHK s measurements reveal variability for several sources, which in some cases can be confirmed by light curves available from the OGLE database (Udalski et al. 1997;Szymański 2005). We note that 2dFS2945 (HV 11519) is a known δ Cep pulsator.\n- In the SMC photometric catalog, we have detected 7 luminous sources with colors typical of sgB[e] stars (see Paper I for an introduction), i.e. M 3.6 &lt; −8, [3.6] − [4.5] &gt; 0.7, and J − [3.6] &gt; 2 mag (see Figures 2  and 3). Five of these are previously known sgB[e] stars (with R50; B2-3[e] being the brightest in all IRAC and MIPS bands), while R4 (AzV 16) is classified as an LBV with a sgB[e] spectral type. In addition to these, we find that 2dFS1804 (AFA3kF0/B[e]) has a very similar SED (and therefore infrared colors) to the known sgB[e] 2dFS2837 (AFA5kF0/B[e]). Evans et al. (2004a) also remarked on the similarity of their spectra. We therefore confirm the supergiant nature of 2dFS1804 and present its SED in Figure 14 along with those of other sgB[e] and LBV stars. We note that both are photometrically variable. The similarity of the SEDs of these sgB[e] stars, despite the various optical spectral classifications, implies that all are the same class of object. The cooler, composite spectral types indicate a lower mass and perhaps a transitional stage to or from the sgB[e] phenomenon. The only difference we find between the sgB[e] stars in the SMC versus the LMC is that on average they are ∼1-2 mag fainter (in absolute terms).\n\nMentions:\n- To allow for a direct comparison with the LMC results of Paper I, we present the same color-magnitude and two-color diagrams (CMDs and TCDs), identifying stars in the 10 categories described above. 21 Infrared [3.6] versus [3.6]− [4.5] and J − [3.6] CMDs for all the stars in the catalog are shown in Figures 2 and 3.\n- In Figure 2, their [3.6] − [4.5] colors are indistinguishable from AFG supergiants 24 , while in Figure 3 they clearly have red colors, with 0.5 &lt; J − [3.6] &lt; 1 mag.\n- In the SMC photometric catalog, we have detected 7 luminous sources with colors typical of sgB[e] stars (see Paper I for an introduction), i.e. M 3.6 &lt; −8, [3.6] − [4.5] &gt; 0.7, and J − [3.6] &gt; 2 mag (see Figures 2  and 3).\n\nOCR Text: \"DnCoDn 1 Z]-[40\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2.— [3.6] versus [3.6] − [4.5] color magnitude diagram for massive stars with IRAC counterparts in the SAGE database. The conversion to absolute magnitudes is based on a true SMC distance modulus of 18.91 mag (Hilditch et al. 2005). Different symbols denote different spectral types. The locations of all the SAGE detections are shown in gray as a Hess diagram. The RSGs, sgB[e] and LBVs are among the most luminous stars at 3.6 µm."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The resulting detected instants, t m1 stall (ic) and t m1 attach (ic) are compared to the reference instants extracted from the instantaneous velocity fields of section4.1, t ref stall (ic) and t ref attach (ic) (see figure 14). The first observation is that the stall and reattachment instants are detected earlier in average, −8.6c U∞ and −15c U∞ chord time respectively (or 2.5 to 4 time steps), when using this first detection method.\n\nMentions:\n- The resulting detected instants, t m1 stall (ic) and t m1 attach (ic) are compared to the reference instants extracted from the instantaneous velocity fields of section4.1, t ref stall (ic) and t ref attach (ic) (see figure 14).\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 14: Normalized delay of the stall and reattachment detected instants using Unorm(t) (method 1)"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Object-oriented programming (OOP) is one of the ways in organizing and developing software. The foundation of OOP languages goes back to Simula and Smalltalk. In Simula, a program is a collection of objects. The notion of objects was first introduced in the Simula language designed in the late 60s (Micallef, 1988). However, OOP did not emerge as a new programming paradigm until Smalltalk came along in the late 70s. In object-oriented approach, everything that can be touched is considered as an object. For example, a person, a car, a doll and a house can be considered as objects. An object is an entity. It contains attributes and provides services. Figure 1 shows an example of an object.\n- Before an object can be used, it needs to be created (instantiated). An object is created from a class. A class is a template from which objects are instantiated. A class consists of fields and methods. Fields are data for class instances and methods are operations that access the data. Diagram 1 shows a class Car of an object MyCar from Figure 1.\n\nMentions:\n- Figure 1 shows an example of an object.\n- Diagram 1 shows a class Car of an object MyCar from Figure 1.\n\nOCR Text: \"Nice Ote Emy Kuribulcs Snjc;\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1. Example of an Object - MyCar"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where H is the star's 2MASS Survey H-band magnitude. Stars with RPM J ≤ y are regarded as RPM J -dwarfs and stars with RPM J &gt; y as RPM J -giants. With MARVELS data this method has a giant contamination rate of ∼4% and subgiants are mixed in with the \"dwarf\" sample at a level of 20%-40% (Paegert et al. 2015). Figure 3 shows the RPM J cut on our sample of 3,257 stars with ≥10 RV observations, resulting in 2,378 stars classified as dwarf/subgiant. Our final stellar sample consists of 2,340 stars that have ≥10 RV observations, are RPM J dwarf/subgiants, and have a time baseline longer than 300 days. These 2,340 stars have a mean baseline of ∼582 days and a median of 26 RV observations.\n\nMentions:\n- Figure 3 shows the RPM J cut on our sample of 3,257 stars with ≥10 RV observations, resulting in 2,378 stars classified as dwarf/subgiant.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3. J-band reduced proper motion (RPMJ) vs J-H magnitude for the 3,257 stars in the MARVELS survey that have ≥10 valid RV observations in the UF2D pipeline. The red line shows the RPMJ cut to determine giant or dwarf/subgiant designation. Stars above the red line are designated as RPMJ giants and those below as dwarf/subgiants. In this sample we classify 2,378 stars as dwarf/subgiants and 879 as giants."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We note that the convergence rate given by (4) yields a significant improvement over that given by (2) when the dynamic range of many residuals is large, i.e. when γ k ≪ m for many iterations k. In Figure 2, we present the convergence of Motzkin and RK on a random system which before normalization is defined by matrix A ∈ R 5000×100 with a i j ∼ N (1, 0.5) and b = A1 + ε where 1 1 We use the convention that an empty sum or product equates to one.\n\nMentions:\n- In Figure 2, we present the convergence of Motzkin and RK on a random system which before normalization is defined by matrix A ∈ R 5000×100 with a i j ∼ N (1, 0.5) and b = A1 + ε where 1 1 We use the convention that an empty sum or product equates to one.\n\nOCR Text: \"@030 ;uzW Aler 4030 \"-7= Zuju sarcna Jk;\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIGURE 2. Convergence of Motzkin’s method and RK on correlated system with corresponding theoretical bounds."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- 2) Simulation scenario and comparison results: The size of the surveillance area is 100 pixels by 100 pixels and the size of the image cell is 1. Image data for the true tracks (shown in Fig. 5) is generated according the observation model ( 32)- (33). Sample snap shots of image sequence are displayed in Fig. 6 together with the true number of objects and the description of detection results (by hardthresholding) for each snapshot. Fig. 6 illustrates that low SNR images are prone to mis-detections, and that merged detections occur in mutual occlusions.\n\nMentions:\n- Image data for the true tracks (shown in Fig. 5) is generated according the observation model ( 32)- (33).\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 5: True tracks in the x y plane. Start/Stop positions are shown with ◦/4."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Our results for WMAP are shown in Fig. 7. We show results for 10 example values of c b , from our set of 100 logarithmically spaced sampling points between c b = 0.0001 and c b = 1.0. The c b values have been picked within the interesting range found by examining the Fisher matrix contributions of different N as illustrated in Fig. 6. The estimated amplitudes M in Fig. 7 span a wide range of magnitudes. Numbers above one would mean a dense production of particles, with a low aplitude c b , so that individual profiles do not stand out of the noise. On the high c b end we estimated amplitudes as low as 10 −9 . Such values should not be taken at face value but are an artifact of our mathematical description in terms of correlation functions, which does not take into account the discrete number of events (i.e. an experiment might see exactly zero events). As descussed in the beginning of Sec. IV, we cannot constrain M below ∼ −2 max ∼ 10 −6 , as this is the number of available pixels. In addition, for such high values of c b , one would see the profile in a pixel histogram or even by eye. By comparing at what c b we get unphysical constraints (c b ∼ 0.004) with the Fisher matrix contributions at this c b in Fig. 6, we see that the largest N -point functions of interest are around N 40 in this model.\n\nMentions:\n- Our results for WMAP are shown in Fig. 7.\n- The estimated amplitudes M in Fig. 7 span a wide range of magnitudes.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 7: Estimated amplitudes M for the resummed estimator defined in Eq. (28) for WMAP, as a function of frequency. The different curves show 10 log-spaced cb values between cb = 0.0001 and cb = 1.0. The estimator for each ω has been maximized over φ. The Gaussian mean was subtracted and the Fisher matrix estimated from Monte Carlo. The amplitudes give an impression of the theoretical constraints we obtain, as none of the values is significant within the ensemble of Monte Carlo maps."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Sketched Proof. The crucial part of the construction (a full proof of Lemma 4.4 is given in Appendix B) concerns the definition of two ATL * formulas ϕ conf and ϕ fair satisfying the following for each good check-tree T, Lab of an accepting strategy tree of the PMS S of Lemma 4.3: T, Lab is a model of ϕ conf (resp., ϕ fair ) iff the ATM configuration codes in T, Lab are well-formed (resp., T, Lab satisfies the fairness requirement). We focus on the definition of the formula ϕ fair . By construction and the goodness requirement, for ensuring the fairness requirement, it suffices to require that for each (well-formed) check 3 -marked 3-block bl 3 in T, Lab which does not belong to the first configuration code, denoted by bl ′ 3 the unique (well-formed) check 3 -marked 3block in the subtree rooted at the s 3 -node of bl 3 and by (u p , u, u s ) (resp., (u ′ p , u ′ , u ′ s )) the content of bl 3 (resp., bl ′ 3 ), the following holds: (i) bl 3 and bl ′ 3 have the same number, and (ii) u = next l (u ′ p , u ′ , u ′ s ) if l marks the ATM configuration code of bl 3 , and u = next r (u ′ p , u ′ , u ′ s ) otherwise. Here, we define the ATL * formula ϕ = ensuring that bl 3 and bl ′ 3 have the same number (see Figure 3). For this, we exploit the auxiliary formula ψ = in the definition of ϕ = for requiring from the current e 2 -node x of the current 2-subblock bl 2 of bl 3 that the 2-sub-block bl ′ 2 of bl ′ 3 having the same number as bl 2 has the same content as bl 2 too. Recall that in a good check-tree, the unique nodes controlled by the systems are the check 3 -branching nodes and the {e 2 }-nodes, and each strategy of the system selects exactly one child for each node controlled by the system. Thus, the formula ψ = asserts the existence of a a strategy f x of the player system such that the following holds: 1. each outcome of f x from node x visits a node marked by check 2 whose parent (e 2 -node) belongs to a check 3marked 3-block. This ensures that all the outcomes get trapped in the same 2-block check-tree associated with some 2-block bl ′ 2 of bl ′ 3 . Moreover, bl 2 and bl ′ 2 have the same content. 2. For each outcome π ′ of f x from x which leads to a marked 1-sub-block bl ′ 1 (hence, a marked copy of a 1-subblock of bl ′\n- 2 ), the 1-sub-block of bl 2 having the same number as bl ′ 1 has the same content as bl ′ 1 too. This ensures that bl 2 and bl ′ 2 have the same number. The first (resp., second) condition is implemented by the first (resp., second) conjunct in the argument of the strategic quantifier sys in Figure 3.\n\nMentions:\n- Here, we define the ATL * formula ϕ = ensuring that bl 3 and bl ′ 3 have the same number (see Figure 3).\n- The first (resp., second) condition is implemented by the first (resp., second) conjunct in the argument of the strategic quantifier sys in Figure 3.\n\nOCR Text: \"My vU-M#' 0+6/wk| J-Kock\" 'H\"-Ykx\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Definition of the ATL∗ formula ϕ= in the proof of Lemma 4.4."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We consider a distributed multiuser massive MIMO setup, as shown in Fig. 1, where K user equipments (UEs) transmit uplink radio signals to M remote radio heads (RRHs) simultaneously on the same time-frequency resource. For simplicity, we assume that the RRHs are all singleantenna units and refer to them interchangeably as BS antennas. We also assume that all the UEs in the system are single-antenna units and refer to them as users. The RRHs are connected\n\nMentions:\n- We consider a distributed multiuser massive MIMO setup, as shown in Fig. 1, where K user equipments (UEs) transmit uplink radio signals to M remote radio heads (RRHs) simultaneously on the same time-frequency resource.\n\nOCR Text: \"Cumpuiio Rtt;-[ R-ia Uter Colipmen Uril (CU} Heaa'RRH) (UE} Frcnmnauinc wcink Trans Mnisaion\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1: Setup for user positioning in distributed massive MIMO: K single-antenna UEs transmit uplink signals simultaneously to M RRHs on the same time-frequency resource. Each RRH records its own RSS value and forwards it to the CU via high-speed fronthaul. The CU hosts a machine learning model which takes the RSS vectors as input so as to predict the transmitting user’s location."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The CR pattern for STPA (Fig. 6) works similar to the one for FTA (Fig. 4). However, it is more elaborate in terms of the concepts used to decompose the chain of evidence in causal reasoning and the goals. Based on an identified accident A, Strategy 2 argues over the elimination of all unsafe control actions U CA and corresponding hazards H (Goals 2.1.2 to 2.1.4) by design measures (Goal 2.1.5).\n\nMentions:\n- The CR pattern for STPA (Fig. 6) works similar to the one for FTA (Fig. 4).\n\nOCR Text: \"BAt {rantu #Mac IN; MStJATFesn 74t4u Ftn Sne Fm Lneulky Lanan ( [t Jerata atar8\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6: Module CR argument pattern for STPA"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We define objectives using our modified GQM+Strategies formalisation template [26], as shown in Figure 1. Our modifications to the textual template attempt to improve integration with visual GRL diagrams through:\n\nMentions:\n- We define objectives using our modified GQM+Strategies formalisation template [26], as shown in Figure 1.\n\nOCR Text: \"Aclwily |Reduced Object TSRD Fabrcated Stucure Manuactrng Focus |Lead Time Vegntude |Bmontrs Scale |time monbs   requed \"0 have   Mts Maruacrured hom (he inceplonakanew engne Timekane |i year ater system deployenk Scope |Transmssiors Struchures & Drives (TSRD) SCU Author | John Srn (Conponen Engnees; TS8d)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1:Example GQM+Strategies Formalisation"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We first tested the running time of FixedPoint algorithm in random BNs with a fixed transversal number τ and with different numbers of components n and positive transversal numbers τ + . The tested networks were constructed in such a way that a minimum FVS and a minimum PFVS were both known a priori, i.e. we did not use PFVS algorithm in these cases. The average times obtained in milliseconds with one hundred networks tested in each case, are shown in Figures 7 to 9. We can see that the time performance of FixedPoint algorithm grows exponentially with the value of τ + and polynomially with the size of the network. The results of PFVS algorithm using different heuristic to chose the order of the vertices are presented in Figure 10. We can see that running time of the algorithm depends mainly on the size of the network and not on the sizes of the positive feedback vertex sets. Besides, in random networks as well as in networks from the literature described in Table 1 the performance of the min-order heuristic is better than the random one.\n\nMentions:\n- The average times obtained in milliseconds with one hundred networks tested in each case, are shown in Figures 7 to 9.\n\nOCR Text: \"Aee (zr difafer Gteikaiu= 0 &\" % 3 9 1 # { Hrtsdzz;\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 7: Results of FixedPoint algorithm ."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 4 shows the performance of the code when running the same simulation with a variable number of parallel tasks. The simulation discretizes phase space with 2 13 cells in both the x and the v x direction. Electrons and protons in the simulation are represented by nine marker particles per cell which leads to a grand total of 1.2 • 10 9 particles in the simulation that are updated for 1500 time steps. Given that the field solver incurs negligible computational cost the code should exhibit a linear performance increase when adding additional CPU cores. The code shows good scaling efficiency to at least 256 CPUs.\n\nMentions:\n- Figure 4 shows the performance of the code when running the same simulation with a variable number of parallel tasks.\n\nOCR Text: \"SJt; #0\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4: The strong scaling test simulates a phase space grid with 8192 cells in x direction and 8192 in vx direction using electrons and protons represented by nine particles per cell each. The simulation is performed using a variable number of MPI threads and the average performance over 1500 time steps is given, excluding output."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In figure 4 the SFH of PM2GC and WINGS galaxies is plotted according to the mass of their parent dark matter halo. In the PM2GC, only galaxies in systems with halo masses &lt; 10 14 M are considered, while in WINGS only galaxies in more massive systems are taken into account. The number of galaxies in each halo mass interval is listed in Table 3. The SFRs on the y-axis are normalized so to be equal to that of galaxies in the lowest mass haloes in the lowest redshift bin. As a consequence, only the redshift dependence of the SFHs of galaxies hosted in different halos is compared, while absolute values of SFR are not.\n\nMentions:\n- In figure 4 the SFH of PM2GC and WINGS galaxies is plotted according to the mass of their parent dark matter halo.\n\nOCR Text: \"lc3id IcaiM iL\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4. The SFH of galaxies divided according to the mass of their host halo. Galaxies are from the PM2GC sample until halo masses of log(MHalo)/M =14 and from the WINGS cluster sample for more massive halos. The SFRs are normalized so to coincide in the lowest redshift bin, as indicated by the large black filled circle. The halo mass ranges considered are shown in the legend."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Figure 12, we plot (r /r) max versus U 2 rms T p Pe p for one simulation (T p = 0.3, W s = 0.1, Pe p = 1000, and Re = 1000). Note that each data point represents an instant in time for which the full velocity and particle fields are available. Points start from the lower left corner and move up to the right as U rms increases with time during the development of the Comparisons between (r /r) max and U 2 rms T p Pe p are next shown in Figure 13 for all available simulations that have Re = 1000, and Pe p = 1000. Here, the color of the points represents T p , the shape of the points represent W s , and the size of the points corresponds to the dimensionality (2D vs. 3D), see legend for detail. For a given simulation, each point corresponds to a particular instant in time selected after the onset of the convective instability, but before the bulk of the particle layer has traveled more than one domain height (to avoid it interacting with itself). The solid line shows the relationship (r /r) max = (1/4)U 2 rms T p Pe p , where the proportionality constant 1/4 was selected to fit (approximately) the 2D data in the higher T p runs.\n\nMentions:\n- In Figure 12, we plot (r /r) max versus U 2 rms T p Pe p for one simulation (T p = 0.3, W s = 0.1, Pe p = 1000, and Re = 1000).\n\nOCR Text: \"U;ES\" pPep\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 12. Maximum particle concentration enhancement over the mean as function of U2rmsTpPep for a simulation with parameters Tp = 0.3,Ws = 0.1, Rρ = 0.5,Pr = 1,Pep = 1000, and Re = 1000. Each dot represents an instant in time, with points moving from the bottom-left corner to the top-right corner over time. The black solid line shows (r′/r̄)max = (1/4)U 2 rmsTpPep."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The system is initiated at x = [1, 1] and the simulation lasts for 5s. Fig. 8 compares the time evolution of the inter-event times of the region-based STC, the STC proposed in [11] and ETC. In total, ETC triggered 383 times, the region-based STC triggered 554 times, whereas the STC of [11] triggered 2082 times. Given Fig. 8 and the number of total updates for each technique we can conclude that: 1) the region-based STC scheme highly outperforms the STC of [11] and 2) the performance of the region-based STC scheme follows closely the ideal performance of ETC, while reducing the computational load in the controller.\n\nMentions:\n- Fig. 8 compares the time evolution of the inter-event times of the region-based STC, the STC proposed in [11] and ETC.\n- Given Fig. 8 and the number of total updates for each technique we can conclude that: 1) the region-based STC scheme highly outperforms the STC of [11] and 2) the performance of the region-based STC scheme follows closely the ideal performance of ETC, while reducing the computational load in the controller.\n\nOCR Text: \"ETC updates Recict-E GeC STC upda ES STC  uo lates Hmeis\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 8: The time evolution of region-based STC, STC of [11] and ETC inter-event times along the trajectory of (29)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The observations we have made on dgemm performance can be explained by figures 3 and 4 which respectively show the evolution of the core frequencies over time and the distributions of these frequencies. There is a clear correlation between the frequencies and dgemm performance: the random initialization produces lower frequencies whereas the constant initialization gives higher frequencies. Similar temporal patterns can also be distinguished: the oscillation on the CPU 0 of dahu-1 and dahu-25 as well as the huge drops for the CPU 1 of dahu-15. This experiment has been repeated on other Grid'5000 clusters. Table 1 gives a summary of our observations. Four other clusters show a similar behavior, the performance of dgemm is higher when the matrices are generated with a constant value. However, for four other clusters, this phenomenon could not be observed, the matrix content had no impact on the performance.\n\nMentions:\n- The observations we have made on dgemm performance can be explained by figures 3 and 4 which respectively show the evolution of the core frequencies over time and the distributions of these frequencies.\n\nOCR Text: \"#lttau r Mlji\" Juui*3:4M:IU Wai- te \"p Ehatu| ht2 \"ua Cttuea 0-huornn WZAI 41A 4i\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Core frequencies are higher with constant values in the matrices"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- A subset (∼ 13) of LMXB show ms pulsations in their persistent emission. The rest (11) show the pulsations only during X-ray bursts [106,117] or in QPO behavior. During the X-ray bursts, oscillations have been observed, usually during the decay phase (see Fig 8), whose frequency rises to an asymptotic limit as the intensity drops [107]. The interpretation of this behavior is that the rotation of the NS is modulating the burst hot spot which expands and rises in altitude, then drops back onto the surface. The true rotation frequency of the NS is then the observed asymptotic frequency. The NS spin frequency can also appear as the frequency difference between pairs of QPO that move up and down in frequency together. Since the difference in the frequencies of the QPO pair stays the same and is in the range 200 -400 Hz, it is thought to be the NS rotation frequency. In some cases though, the frequency difference appears to be closer to 1/2 of the spin frequency, and the relation between QPO and spin frequency has been questioned [83].\n\nMentions:\n- During the X-ray bursts, oscillations have been observed, usually during the decay phase (see Fig 8), whose frequency rises to an asymptotic limit as the intensity drops [107].\n\nOCR Text: \"T\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 8. X-ray burst oscillations in 4U 1702-43 (from [107]). The histogram shows the X-ray intensity of the X-ray burst. The contours show the Fourier power level as a function of frequency and time, indicating a drifting oscillation starting at 328 Hz (t = 7 s) and ending at 330.5 Hz."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- From the above relations and from Fig. 5 and Fig. 6, it is clear that as g → 1 the function becomes more like a Dirac-δ function (δ 0−res (0) → ∞ and ∆x → 0, but δ 0−res (0)∆x is a finite number), so with the normalization given by the integral over Eq. (31), we have\n- now Contents 13 Fig. 6. An illustration of the S-wave phase shift derivative for low momenta at a resonance.\n\nMentions:\n- From the above relations and from Fig. 5 and Fig. 6, it is clear that as g → 1 the function becomes more like a Dirac-δ function (δ 0−res (0) → ∞ and ∆x → 0, but δ 0−res (0)∆x is a finite number), so with the normalization given by the integral over Eq. (31), we have\n- now Contents 13 Fig. 6.\n\nOCR Text: \"oaci 3 approxinate 2/3 XekR\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 6. An illustration of the S-wave phase shift derivative for low momenta at a resonance."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 11: Storage cost of Scalasca [7], HPCToolkit [8], and SCALANA running with 128 processes\n- Storage Cost Figure 11 shows the storage costs of SCALANA, HPCToolkit, and Scalasca running with 128 processes (121 for BT and SP) on Gorgon. SCALANA only incurs storage costs in the order of Kilobytes, while Scalasca and HPCToolkit generate Megabytes to Gigabytes of data. Besides, for SCALANA, the average storage cost of the NPB benchmark with 2,048 processes on the Tianhe-2 supercomputer is 4.72MB. 155 do j=js,je+1 156 if(abs(niib23(j,k)) .eq. 1)then 157 v2b3(is-1,j,k) = v2b3(is ,j,k) 148 v2b3(is-2,j,k) = v2b3(is+1,j,k)\n\nMentions:\n- Fig. 11: Storage cost of Scalasca [7], HPCToolkit [8], and SCALANA running with 128 processes\n- Storage Cost Figure 11 shows the storage costs of SCALANA, HPCToolkit, and Scalasca running with 128 processes (121 for BT and SP) on Gorgon.\n\nOCR Text: \"Ip6 jls 106 HRCTocle SOn ICU} J0 Jouk | 4 1 1\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 11: Storage cost of Scalasca [7], HPCToolkit [8], and SCALANA running with 128 processes"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Tested applications are hosted and distributed using IBM® Worklight. Applications securely communicate with Cast Iron® via Worklight server. This is illustrated in Figure 2. Cast Iron® in its turn brings together the data from various sources.\n- At design time the application developer is binding the services to the application. For this purpose the NitroGen platform should be able to provide a set of services that can be used by the application. However, the topology of backend systems may comprise more than one system that provides services each with potentially different technology. A discovery component is required to be able to communicate with each of the systems, using its own protocol, to fetch a list of services definitions. The service definitions should be a structured document that describes the available operations, input and outputs message formats as well as additional required information for the service invocation. The Cast Iron, for instance, is a backend system that provides SOAP based Application Program Interface (API) to query for available services and description of each service in a Web Service Definition Language (WSDL) format (see Figure 2). The result of the discovery component is a set of structured service description documents that contains the relevant information needed for the service binding. An instance of a description document consists of the service invocation protocol, system identifiers, input and output parameters (names and types), name, description and so forth. NitroGen platform use these service descriptors at design time to create the mapping dialog where developer is able to select the service and map fields to and from the service.\n\nMentions:\n- This is illustrated in Figure 2.\n- The Cast Iron, for instance, is a backend system that provides SOAP based Application Program Interface (API) to query for available services and description of each service in a Web Service Definition Language (WSDL) format (see Figure 2).\n\nOCR Text: \"CNST LRCN OIac 5atct ()Worklicht | Hcst, Dstr oute, Manzce; 0aa @cld Senic $ Se-ure, Analyze 6J3lh255 AA Laper\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: Runtime components"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 1 shows the power requirements of an Nvidia C2075 GPU during execution of the single precision Runge-Kutta differential equation test case (see Section 2.1.4) over time. This example shows the results of both the internal vendor specific power profiling and external primary current measurement. As explained in Section 2, the externally measured power can vary from the internal profiling measurements. As overhead due to networking, storage, cooling or the host CPU which is not considered by the internal profiling solutions and can not fully be removed by subtracting the idle current. Furthermore, external measurements also track changes in power due to interconnect link state changes and other automatic workload dependent power saving features. The power supply efficiency (∼90%) was also not considered for this figure, to illustrate the difference. Additionally, the high-capacitance smoothing and filtering capacitors present in modern switch-mode voltage converters cause the slower rise and subsequently part of the slower decay in power after the computational task was completed. These capacitors are also the primary reason why the internal profiling measurements show higher frequency variations, which get filtered out and are therefore not present in the external measurements. This is not the case with internal profiling, as it either uses direct internal measurements of the computational device or a heuristic approach, which are completely independent of other peripherals or components. To minimize the influence of these external factors, which can significantly affect the comparability of the results, internal profiling was used for this study whenever possible. Only for older devices without power profiling support external measurements were performed an 10% of the power subtracted to account for the average difference between internal profiling and external current measurement. The ARM devices lack support for power profiling as well, but as these self contained system-on-chip platforms are supplied by low voltage direct current (typically 5 V or 12 V) and lack high-speed interconnect buses or storage systems, external power measurements are more representative compared to traditional x86 systems.\n\nMentions:\n- Fig. 1 shows the power requirements of an Nvidia C2075 GPU during execution of the single precision Runge-Kutta differential equation test case (see Section 2.1.4) over time.\n\nOCR Text: \"220 160 2 3 200 140 & 1 180 120 8 160 100 L 140 120 5 10 15 25 Time (s)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Power requirements of a Tesla C2075 GPU during execution of the single precision Runge–Kutta differential equation testcase, determined by external primary current measurement (blue) and internal vendor specific power profiling (black)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The results presented in Figure 2 show that this year the performance of systems in the yes/no questions, has clearly improved. In batch 5 for example, presented in Table 8, some systems outperformed the strong baseline based on previous versions of the OAQA system, with the top system achieving almost double the score of the baseline. Some improvement is also observed in the performance of the top systems for factoid and list questions in the preliminary results. However, there is even more room for improvement in these types of question as can be seen in Figure 2.\n- In this paper, an overview of the seventh BioASQ challenge is presented. The challenge consisted of two tasks: semantic indexing and question answering. Overall, as in previous years, the best systems were able to outperform the strong baselines provided by the organizers. This suggests that advances over the state of the art were achieved through the BioASQ challenge but also that the benchmark in itself is challenging. Moreover, the shift towards systems that incorporate ideas based on deep learning models observed in the previous year, is even more clear. Novel ideas have been tested and state-of-the-art deep learning methodologies have been adapted to biomedical question answering with great results. Specifically, the breakthroughs in different NLP tasks using clever Fig. 2. The performance achieved by systems in exact answer generation part of Task B, Phase B, across different years of the BioASQ challenge. For each test set the performance of the best performing system (Top) is presented based on the official evaluation measures. Since BioASQ6 the macro-averaged F1 score (macro F1) is the official measure for Yes/No questions, but accuracy (Acc), the former official measure, is also presented. The results for BioASQ7 are preliminary. The final results for Task 7b, phase B will be available after the manual assessment of the system responses.\n\nMentions:\n- The results presented in Figure 2 show that this year the performance of systems in the yes/no questions, has clearly improved.\n- However, there is even more room for improvement in these types of question as can be seen in Figure 2.\n- Specifically, the breakthroughs in different NLP tasks using clever Fig. 2.\n\nOCR Text: \"K RElNE Rralk Fccnaem kik Le Gakt - Fleltrte Aaanje\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2. The performance achieved by systems in exact answer generation part of Task B, Phase B, across different years of the BioASQ challenge. For each test set the performance of the best performing system (Top) is presented based on the official evaluation measures. Since BioASQ6 the macro-averaged F1 score (macro F1) is the official measure for Yes/No questions, but accuracy (Acc), the former official measure, is also presented. The results for BioASQ7 are preliminary. The final results for Task 7b, phase B will be available after the manual assessment of the system responses."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Let us now provide some results on the comparison between the proposed algorithm and the TNC algorithm. Figures 3, 4 and 5 show a comparison with the TNC approach without shadow edges; results are the average of 50 runs. Figure 3 shows the percentage of localized nodes obtained by Algorithm 2 with respect to total generated nodes, plotted against the distance radius ρ and the network size. As shown by the Figure, and just as expected, for a fixed network size there is a threshold on the distance radius, and the nodes are almost all localized for valued of ρ above such a threshold. Note further that such a threshold value tends to decrease when the network size increases. 3. Specifically, note that the percentage of the proposed algorithm are always greater or equal than the TNC algorithm; moreover there are up to 25% more localized nodes (Figure 4), that is about twice the percentage of localized nodes of the TNC algorithm (Figure 5). Figure 6 shows that, the range of values of ρ and N where the proposed algorithm enhances the localization of nodes, is indeed the range where the shadow edges are created. Note that, in such a range, the amount of shadow edges is about 10 − 12% of the total number of edges created. Percentage of localized nodes obtained by Algorithm 2 with respect to total generated nodes, plotted against the distance radius ρ and the network size. Results are the average of 50 runs. Fig. 4. Difference between the percentage of localized nodes with respect to total generated nodes, obtained by Algorithm 2 and by the TNC algorithm, plotted against the distance radius ρ (the same for all nodes) and the network size. Results are the average of 50 runs.\n\nMentions:\n- Specifically, note that the percentage of the proposed algorithm are always greater or equal than the TNC algorithm; moreover there are up to 25% more localized nodes (Figure 4), that is about twice the percentage of localized nodes of the TNC algorithm (Figure 5).\n\nOCR Text: \"Weaeae\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 5. Ratio between the percentage of localized nodes with respect to total generated nodes, obtained by Algorithm 2 and by the TNC algorithm, plotted against the distance radius ρ and the network size. Results are the average of 50 runs."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The 887 µm continuum images and 13 CO intensity maps for each detection are shown in Appendix B1. The radial profiles in the stacked continuum and 13 CO intensity maps from the full sample, as shown in Figure 7, indicate that 13 CO emission is slightly more extended than dust emission, with a radial profile FWHM 30% wider in 13 CO emission. Similarly, many well-known disks, such as HD163296, V4046 Sgr and Elias 2-27, have 13 CO emission that is 2-3 times extended than the sub-mm dust emission (e.g., Isella et al. 2007;Rosenfeld et al. 2013;Pérez et al. 2016). The 13 CO emission for the tentative detections is typically unresolved in our beam with FWHM of ∼ 0. 6. The brightest 13 CO disk, 2MASS J11095340-7634255 has emission on 2 scales, and would be a factor of two fainter if baselines shorter than 40 kλ were excluded.\n\nMentions:\n- The radial profiles in the stacked continuum and 13 CO intensity maps from the full sample, as shown in Figure 7, indicate that 13 CO emission is slightly more extended than dust emission, with a radial profile FWHM 30% wider in 13 CO emission.\n\nOCR Text: \"Cont Radius\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 7. Azimuthally averaged 13CO J=3-2 and continuum emission normalized to the peak. The typical beam size is shown at the right corner."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- It is continuous even if the primary emission is discrete. The time-integrated signal associated with the primary emission can easily be analytically shown to lead to a differential spectrum scaling as E −3 . We have performed the numerical integration of the secondary component. The very interesting point is that, as shown in Fig. 2, the time-integrated signal is nearly the same for both components. This is quite unexpected as the physics involved depends on the details of nongravitational processes (subtleties of the hadronization, cross sections for decays into gamma-rays, etc.). At a given BH temperature -and, therefore, at a given mass -the number of secondary photons is much higher than the number of primary photons. However, the mean energy of the secondary component is much smaller than for the primary component. This means that the primary emission was peaked at this energy when the BH mass was higher and, due the dynamics of the process (dM/dt ∝ −M −2 ), it has spent a \"longer time\" in this mass region (between M and M + dM ). Those phenomena compensate each other and the neat result is that both components have the same order of magnitude. The important consequence of this calculation is that, even in this case, the interesting quantum gravity line structure remains, in principle, detectable. It is not \"diluted\" in a huge continuous signal due to the secondary component. The secondary component only induces a reasonable \"self-background\" and as soon as the detector resolution is better than the line spacing, the phenomenon is easy to identify, if it exists at all. The relative energy difference between lines can easily be shown to be ∆E/E ∼ πγ ∼ 1. As a typical detector resolution is of the order of 10% − 20%, the structure is clearly visible, even when taking into account this secondary component.\n\nMentions:\n- The very interesting point is that, as shown in Fig. 2, the time-integrated signal is nearly the same for both components.\n\nOCR Text: \"J81) J,801 :0y =\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 2. Time integrated primary (upper curve) and secondary (lower curve) gamma-ray emission from an evaporating BH."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The resulting distributions are shown in Figure 9. As can be seen from the figure, the fact that the cost distributions for higher savings are stochastically dominated by the cost for lower savings at r ≤ 1 translates into larger pensions for those with higher savings. This has an important implication on the utility of the retirees as we discuss in the next section. Note. Conditional distributions of maximum pensions for retiree with median savings within each quintile.\n\nMentions:\n- The resulting distributions are shown in Figure 9.\n\nOCR Text: \"Quintile Quintile uintile uintile Quintile 5 I002 Iz70 1400 1503 cersion\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 9: Distributions of Maximum Pension Pmax"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In a system with dynamical heterogeneities where slow and fast trajectories coexist, P(K) will exhibit two peaks, of small and large activity, or at least an asymmetric shape biased towards small K, especially if the peaks are not well separated. As the system becomes more and more glassy, the width of P(K), as well as the asymmetry of P(K), is expected to increase. In the two following paragraphs, we will then focus on the variance and the skewness of the activity with varying u to investigate such a behaviour, and detect the emergence of slow trajectories (low activity) for u &gt; 1. Unlike what occurs at T = 29 • C, asymmetry is slightly more marked at a = Σ(∆t) and this is what we will quantify in the next subsection. This is visible with the naked eye only at u &lt; 1. † The effective dimension of the tracer dynamics is d = 2 and not d = 3 since the tracking procedure selects trajectories which remain in the focus plane during the experimental duration of the measurements. Fig. 5 From left to right, histogram of the activity at T = 27 • C normalised by its average for a = 0.021σ , a = 0.085σ and a = 0.41σ , respectively. The corresponding values of u are then respectively: 0.37, 1.50 and 7.35, and those of K : 32.8, 24.1 and 4.71 . The time lapse ∆t is 7.5 s. The most symmetric histogram (centre) is at the value of a of the same order as Σ(∆t). We compare in Fig. 6 the rescaled variances K 2 c /M of P(K) in the dense suspension at temperatures T = 27 • C and T = 29 • C, and the variance in the Brownian case (equation ( 9)), for the same value of the time lapse ∆t = 15s. In Fig. 7, the variances at each temperature are also plotted for different values of the time lapse ∆t. Although the experimental data can not be directly compared for the same ∆t (except for ∆t = 15 s), the variance in the suspension at low temperature T = 27 • C is found to be larger than at high temperature T = 29 • C (closer to the Brownian one). These results show that the variance increases in a significant way when approaching the glassy regime, indicating P(K) displays a significant broadening, consistent with the increase of dynamical heterogeneities.\n- In Fig. 13 and Fig. 14, we illustrate (as for our experimental data in Fig. 6 and Fig. 7) that more glassy (denser) system display an increase in the variance. The Brownian result (9) (in dimension d = 3) serves as a comparison, and is much smaller in all cases.\n\nMentions:\n- We compare in Fig. 6 the rescaled variances K 2 c /M of P(K) in the dense suspension at temperatures T = 27 • C and T = 29 • C, and the variance in the Brownian case (equation ( 9)), for the same value of the time lapse ∆t = 15s.\n- In Fig. 13 and Fig. 14, we illustrate (as for our experimental data in Fig. 6 and Fig. 7) that more glassy (denser) system display an increase in the variance.\n\nOCR Text: \"T-29C   T-2C (Jf-15} 202 V_Tut\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 6 Mean variance 〈K2〉c/M as a function of u = a/Σ(∆t) at temperatures T = 29◦C (red) and T = 27◦C (blue) for the same time lapse ∆t = 15s. The dashed green curve stands for the Brownian motion counterpart Eq. (9) (in dimension d = 2)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In this section, we present experimental results on simulated synthetic data and a real-world dataset 9 : the trip records for Jan 2013 of New York yellow cabs. We aim to experimentally study the convergence properties of objectives in Equations ( 1) and (2) for various algorithms. We consider three kinds of algorithms; Samp(α = 0.8) and Rand-Perm(φ) described in the main section of the paper, where φ is chosen as the clockwise permutation and the Greedy algorithm whose action is defined as follows. When a request r arrives at time t with the system in state X (t) , we choose the neighbors such that the number of cars in state X (t) are in decreasing order. Where appropriate, we also compare against a standard RL [32] algorithm, namely, Deep-Q-Learning [26], with agent and environment adapted from OpenAI Gym [7] for 5 episodes of 2500 steps and experience replay. 10 Fig. 3 represents exploratory analysis using optimal policy obtained via value iteration. 11 Preprocessing. The records are pre-processed by restricting to trips whose starting and ending coordinates lie in the range of (40. Synthetic experiments using learnt parameters. Here, we \"learn\" the parameters from the processed taxi dataset, but simulate the requests from these learnt parameters. In particular, we first learn the arrival rates of each of the requests using the data from Jan 2013 using a standard unbiased estimator. We then set the weight for each request as the distance between the starting and ending location. At each time step t, a request r is drawn from this learnt distribution. The grid size and the number of cars are identical to the real-world experiments (described below). Figs. 6 and 7 show the convergence of Obj. (1) and (2) while Figs. 9 and 10 show the corresponding absolute error bound. In Fig. 9 we fit a statistical best-fit exponential curve using least squares and in Fig. 10 we fit a statistical best-fit linear function. This corroborates our Main Theorem 1 as we see the same asymptotic behavior. Note that the parameters in Figs. 8 and 7 are the same except the arrival assumption, thus the sharp difference does shed light on the importance of arrival assumption for convergence.\n\nMentions:\n- Figs. 6 and 7 show the convergence of Obj. (1) and (2) while Figs. 9 and 10 show the corresponding absolute error bound.\n- In Fig. 9 we fit a statistical best-fit exponential curve using least squares and in Fig. 10 we fit a statistical best-fit linear function.\n\nOCR Text: \"Sunaet[ GJ e-(Pelldd-cininu) caMciai Carchet-Inn occo het @ternnncnthl\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 9: Error bound of Obj. (1) on synthetic data"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The ǫ for paired and control galaxies are plotted in Figure 6. The enhancement for spirals in S-S pairs increases as the stellar mass of galaxies becomes higher, while the enhancement of spirals in S-E pairs shows the opposite trend. These trends can be quantitatively described by the linear regressions    The trend for spirals in S-E pairs still needs further tests because, as mentioned above, there are only nine galaxies in this sample so the result may not be representative. In the rest of this section, we focus on spirals in S-S pairs.\n\nMentions:\n- The ǫ for paired and control galaxies are plotted in Figure 6.\n\nOCR Text: \"Depenclence OXFuv paired galaxies the separa\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 6. Dependence ofAFUV enhancement on galaxy stellar mass for spirals in S-S pairs (dots) and S-E pairs (diamonds). The dashed line is the linear regression for spirals in S-S pairs."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The goal of this subsection is to divide the set of triples T into classes according to the value of |∆(u, v; w)|. Since we do not know how to compute exactly this value efficiently, we actually need to define the classification based on an approximation of |∆(u, v; w)| that can be computed efficiently. In Figure 2 we describe a classical algorithm called IdentifyClass that either aborts or assign a nonnegative integer c uvw to each node (u, v, w) ∈ T . Note that the complexity of the algorithm is O(log n) rounds: Step 1 can obviously be implemented in 20 log n rounds and Step 3 does not require any communication. In the case where the algorithm does not abort, we write T α the set of all triples (u, v, w) ∈ T such that c uvw = α, for each integer α ≥ 0. This defines a partition of the set T . We now show that with high probability the algorithm does not abort and the partition indeed classifies the triples according to the value of |∆(u, v; w)|.\n\nMentions:\n- In Figure 2 we describe a classical algorithm called IdentifyClass that either aborts or assign a nonnegative integer c uvw to each node (u, v, w) ∈ T .\n\nOCR Text: \"Euc #d?, = ~d%\"rzi ke\"l 9, %ck ~tait; \"llugek4/ dzo#stahE=o3 >Ekz;ic # Jask: i 4 tbn keagcihc &uc: Ourrzix; %M bjr&lcubat (i: stWpatDct LresvileR-4y'8} velu Exind:> ;ui:fhssl; %qptbs Li: !lu [Xw;eVxtin? Wewat #x/#enxli tinkn6} Jl] oLplb: ' dib_# kesillt c2 Dzll Eks; LrS i duo '\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: Algorithm IdentifyClass."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- For the sake of comparison with Ref. [2], we chose our parameters to match those in their work. Fig. 2 compares the two results -represented by the J/J BB vs d curves, J BB being the heat current density corresponding to standard black body radiation -head on. Fig. 2: Heat transfer enhancement vs distance for parallel surface system compared with Ref. [2]. µ 1 = µ 2 = 0.1 eV, T 1 = 300 K, T 2 = 1000 K. Fig. 3: Heat transfer enhancement vs distance for parallel surface system under temperature variation. µ 1 = µ 2 = 0.1 eV, T 1 = 300 K.\n- Following Ref. [2], the temperature variation is also analyzed, this is shown in Fig. 3. Similar conclusions are drawn: remarkable agreement are achieved on the two extreme ends while the peak anomaly is observed in the neighbourhood of d ≈ 5 nm. On the other hand, the effect of increasing temperature itself appears to diminish the heat transfer by a constant (logarithmic) value throughout the linear regime; it however does not affect the curve's slope. Note that this adverse relationship implies that the Coulomb force NFHT is less sensitive to temperature than that of the black body radiation.\n\nMentions:\n- For the sake of comparison with Ref. [2], we chose our parameters to match those in their work. Fig. 2 compares the two results -represented by the J/J BB vs d curves, J BB being the heat current density corresponding to standard black body radiation -head on. Fig. 2: Heat transfer enhancement vs distance for parallel surface system compared with Ref. [2]. µ 1 = µ 2 = 0.1 eV, T 1 = 300 K, T 2 = 1000 K. Fig. 3: Heat transfer enhancement vs distance for parallel surface system under temperature variation. µ 1 = µ 2 = 0.1 eV, T 1 = 300 K.\n- Following Ref. [2], the temperature variation is also analyzed, this is shown in Fig. 3.\n\nOCR Text: \"Emc Le E7i 6a3 1\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3: Heat transfer enhancement vs distance for parallel surface system under temperature variation. µ1 = µ2 = 0.1 eV, T1 = 300 K."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Finally, assume that |R| = |L| + 1 and lw(∆ (1) ) ∈ {4, 6}. In this case, we have that |L ∩ R| = 2, hence there are only two lattice points of ∆ (1) on the lines at height 1 and lw(∆ (1) ) + 1. Given a possible L, convexity considerations provide a region in which ∆ (1) must strictly lie (here, 'strict' means that no boundary point of the region is in ∆ (1) ). In Figure 9, the boundaries of the regions are indicated by the blue normal/dashed lines, so each lattice point of ∆ (1) must be one of the black dots.\n- Figure 9. Regions for ∆ (1) So either ∆ (1) is too small (i.e., it doesn't have the correct lattice width), or it is of the form kΥ with k ∈ {2, 3}, but then |∂∆ (1) ∩ Z 2 | = 3k ≥ 2k + 2 = lw(∆ (1) ) + 2, a contradiction.\n\nMentions:\n- In Figure 9, the boundaries of the regions are indicated by the blue normal/dashed lines, so each lattice point of ∆ (1) must be one of the black dots.\n- Figure 9.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 9. Regions for ∆(1)"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Figure 23, we compare various hybrid action spaces with shape selection. We study different ways of placing a shape: dropping at a fixed location, or deciding x-position, or deciding (x, y)-positions.\n- Figure 23 demonstrates performance on different shape placement strategies in Shape Stacking using our frame-work. In No Place, the shapes are dropped at the center of the table, and the agent only selects which shape to drop from the available set. Since there are two randomly placed cylinders on the table, this setting of drop- Comparison of an alternative auxiliary network architecture that is conditioned on the selected discrete action. The \"Main\" results are the default results that do not condition the auxiliary policy on the selected action (Figure 4).\n- Figure 23 also shows the results of our method trained and evaluated on Full Split which was introduced in Table 2.\n\nMentions:\n- In Figure 23, we compare various hybrid action spaces with shape selection.\n- Figure 23 demonstrates performance on different shape placement strategies in Shape Stacking using our frame-work.\n- Figure 23 also shows the results of our method trained and evaluated on Full Split which was introduced in Table 2.\n\nOCR Text: \"Skare Sta kng ID *ceMan KoPa ? @Fax Fus\"|\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 23. Comparing different placement strategies in shape stacking and showing performance on the Full Split action split. Results are using our method with the standard evaluation details."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where V is the survey volume, n g is the number of galaxies, and d sep /2 is the radius of the volume associated with each galaxy. We measure d sep for the two fields in six redshift bins with edges at z = [0.55, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90] (see Fig. 3). We observe that the mean separation is not constant but increases significantly in the two highest redshift bins (z = 0.8 − 0.85 and z = 0.85 − 0.9). The trend can be explained by the survey completeness that was found to decrease at fainter apparent magnitude, i AB &gt; 20.5, due to the degradation of the redshift measurement success rate (Guzzo et al., 2014). This incompleteness of galaxies with the faintest apparent magnitudes predominantly affects the highest redshift bins in a volume-limited sample. We account for this effect when constructing the mock catalogues, as discussed in Sec. 3.\n- We matched the galaxy number density in the mocks to the VIPERS data taking into account the apparent magnitudedependent sampling rate described in Guzzo et al. (2014). The mocks well reproduce the trends observed in the real data. This is shown in Fig. 3 where the mean inter-galaxy separation measurements in the VIPERS-like mocks have been over-plotted with thin blue lines. The adopted apparent magnitude dependent sampling rates are:\n- where N rnd (≥ r) is the number of maximal spheres found in the random sample with radius equal to or greater than r, and N rnd,tot is the total number of maximal spheres in the random sample. The closer P(r) is to 1, the less likely a maximal sphere is expected to occur in a random distribution. The number density of galaxies in the VIPERS data is not constant with redshift (see Fig. 3). Therefore, the significance Fig. 8. Significant radius limit for maximal spheres as a function of the mean inter-galaxy separation for confidence levels 2, 3, and 3.5 σ.\n\nMentions:\n- We measure d sep for the two fields in six redshift bins with edges at z = [0.55, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90] (see Fig. 3).\n- This is shown in Fig. 3 where the mean inter-galaxy separation measurements in the VIPERS-like mocks have been over-plotted with thin blue lines.\n- The number density of galaxies in the VIPERS data is not constant with redshift (see Fig. 3).\n- The trend for d sep is obtained by fitting the combined data from the two fields W1 and W4 where isolated galaxies have been removed (similar to what was shown in Fig. 3, where the isolated galaxies have not yet been identified), and can be described as:\n\nOCR Text: \"MkcCAn itctcalt s~pairarion, M\",n \"1+7- YipkkIIc neeshif\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3. Mean inter-galaxy separation for the W1 and W4 volume-limited samples (red and blue lines, combined value in black) and for mock survey mask samples (light blue lines, individual values). The green dashed line is the fit for the combined mean inter-galaxy separation of W1 and W4 fields."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 18 and Figure 19 show the global convergence for N = 100000: when K and N are larger, the analysis be-    comes much more complex: we can observe significant and sudden slope modification during the iteration. See for instance Unif+DYN or CB curves for K = 512 in Figure 19. One of very visible effect is the impact of the fluid exchange cost which is increased for larger value of K. The above results show in particular (and this is not surprising) that it is not necessarily better to increase K and an optimal K need to be applied (for a given vector size N ). This may suggest the possibility of considering a further adaptive scheme where we could also dynamically adjust the number of PIDs: we hope to address this issue in a future work. What we propose here is a first simple candidate to highlight the potential of the approach. From this first step, one may explore a lot of variants (for instance, we should favour partition sets such that there are more links inside the Ω k sets; we could also define the number of nodes to be re-affected, when modification required, based on its CB evaluation, etc).\n\nMentions:\n- Figure 18 and Figure 19 show the global convergence for N = 100000: when K and N are larger, the analysis be-    comes much more complex: we can observe significant and sudden slope modification during the iteration.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 18: Global convergence: N = 100000. For K = 16, 32, 64."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- with τ tot c = (n e /n tot e )τ c . The qualitative scaling of the analytic growth rate is confirmed in figure 4, where the growth rate is numerically calculated using code [34,35], which directly solves the kinetic equation (1). These simulations employed the general fieldparticle knock-on operator of [36,37,38] and a Boltzmann operator for partially screened bremsstrahlung losses as described in section 2.2. The vertical lines denote the analytic prediction in (23) for when one would expect the transition between growth and decay of an existing runaway population. Radiation losses affect where this threshold lies and the analytic model E eff c accurately and robustly captures this effect. In particular, we note that the mean-force bremsstrahlung model employed in the analytical derivation of E eff c agrees with the Boltzmann-type bremsstrahlung operator used in the simulations within a few percent.\n- With the estimate of Γ ′ (E) from the numerical results of figure 4 (at B = 0 T) and estimating R/a ≈ 5 we find that the minimum required current for E ≈ E eff c is approximately\n\nMentions:\n- The qualitative scaling of the analytic growth rate is confirmed in figure 4, where the growth rate is numerically calculated using code [34,35], which directly solves the kinetic equation (1).\n- With the estimate of Γ ′ (E) from the numerical results of figure 4 (at B = 0 T) and estimating R/a ≈ 5 we find that the minimum required current for E ≈ E eff c is approximately\n\nOCR Text: \"Ac tad Jst 'Kieles , 6 =UT Biplis , 6 =57 EYEU\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4. Steady-state runaway growth rate as a function of electric field normalized to the critical electric field Eeffc,0 without radiation losses. The solid black line is without radiation losses; the dash-dotted blue line includes bremsstrahlung and the dashed green line includes both bremsstrahlung and synchrotron losses corresponding to B = 5T. The vertical lines denote the analytical prediction E = Eeffc . Parameters: nD = 10 20 m−3, a density of Ar+ given by nAr = 4nD and T = 10 eV."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Unlike task-oriented bots, most neural response generation models developed for open-domain dialog systems are not grounded in real world, which prevents these systems from effectively conversing about anything that relates to the user's environment. Only recently have researchers begun to explore how to ground open-domain dialog systems in real-world entities and knowledge [36,88,104]. Knowledge grounding is also crucial for the system to provide interpersonal responses. For instance, the conversations between friends are quite different from those between strangers. So the system needs to be grounded in the personas of the speaker and addressee, respectively [62]. The tone of system responses needs to be adjusted according to user's emotional states and affect by grounding in affect or emotion of the user [52,154,162]. topic that users are interested in. However, these neural-based systems often fail to establish long-term emotional connections with users due to many issues as illustrated by the examples in Table 1 and summarized in Figure 1. In what follows, we elaborate three key issues which we will discuss in the rest of the paper 5 . 1. Examples for demonstrating the semantics, consistency, and interactiveness issues. The first two examples show semantic conflicts between user post and machine response, the third session shows bad consistency within the dialog context due to the lack of a coherent personality, and the last session has bad interactiveness due to the lack of grounding. The results in the first two blocks are from a standard Seq2Seq model with an attention mechanism, and the last session is from a commercial system.\n- We summarize the techniques required to address the three issues in Figure 1, including named entity recognition, entity linking, domain/topic/intent detection, and sentiment/emotion detection. As demonstrated in the Alexa Prize challenge which targets at developing dialog systems for conversing coherently and engagingly with humans on various popular topics, the winning dialog systems [16,29] are composed of different modules that are developed based on these techniques, including language understanding, dialog management, and natural language generation. In such modular designs, the semantic issue is mainly related to the understanding module which is intended to understand the dialog (e.g., content, entity, topic, etc.) and user (e.g., opinion, personality, emotional needs). The other two issues are mainly related to the dialog management and generation modules, aiming to generate responses that are not only consistent in content and personality, but also interactive so as to increase the long-term user engagement. These issues are highly interleaved. For example, understanding dialog and user (semantics) is fundamental to generating consistent and interactive responses.\n\nMentions:\n- However, these neural-based systems often fail to establish long-term emotional connections with users due to many issues as illustrated by the examples in Table 1 and summarized in Figure 1.\n- We summarize the techniques required to address the three issues in Figure 1, including named entity recognition, entity linking, domain/topic/intent detection, and sentiment/emotion detection.\n\nOCR Text: \"Gcnbam; Pg %nam\" Dmqudi Be-auior & Conex, Personz Sentment Statejy Scen: Semantics Consistency Irteractivencss Wimed Enlny @mzin}Tcpt;' SenticemYE %0 Kcoaenay5 Recoznfion FentWeecm Fon Detertion Sniet[ Ferne Klo)Lmz;n Corlest 'Acdeire $ Ext Lnking Enlit 01 Peron leabn Cib; Rimtive\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1. Key issues (the middle block), key factors (the upper block), and key technologies (the bottom block) in open-domain dialog systems."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Some of these points are illustrated in the following figures. Figure 4 shows the simulated heat load per meter length in the LHC arcs for the nominal 25-ns bunch spacing as a function of bunch population (design value N b =1.15x10 11 ). Figure 5 presents details of the LHC beam screen in the cold arcs, including the location of the \"sawtooth\", cooling tubes, pumping slots, and electroncloud shields. Figure 6 shows a cut through a prototype sawtooth surface. The conditioning of a colaminated Cu surface with 500 eV electrons as measured in the laboratory can be seen in Fig. 7, and the \"scrubbing\" of LHC sawtooth Cu chamber in situ with photo-electrons of critical energy 194 eV and a 100-V biasing voltage at EPA in Fig. 8. The conditioning by the electron cloud itself has been verified with Cu samples in the SPS (See Fig. 9). Also the conditioning at cryogenic temperatures was demonstrated, in the laboratory (Fig. 10). It is interesting to note that in Fig. 7 the primary energy at which the secondary emission yield is maximum, called ε max , increases with surface conditioning, while in Figs. 8-9 it decreases and in Fig. 10 its evolution is nonmonotonic.\n- A further unresolved aspect pertains to the probability R that an incident low-energy electron is elastic reflected when it hits the surface. In Fig. 10 the probability of elastic e-reflection R for copper at cryogenic temperature seems to approach 1 and to be independent of δ * max . Another value for the low-energy elastic reflection probability R, namely R~0.2-0.5 was obtained in a different way, by benchmarking electron cloud simulations against the electron flux seen on SPS e-cloud monitors, observed with different beam filling patterns [27]. These SPS measurements were done using an unbiased \"strip detector\" with copper strips located below a perforated stainless steel vacuum chamber. One goal of ECLOUD12 was a critical review and a summary of the experimental observations (machine and laboratory) with respect to R and ε max values and their evolution during conditioning.\n\nMentions:\n- Also the conditioning at cryogenic temperatures was demonstrated, in the laboratory (Fig. 10).\n- It is interesting to note that in Fig. 7 the primary energy at which the secondary emission yield is maximum, called ε max , increases with surface conditioning, while in Figs. 8-9 it decreases and in Fig. 10 its evolution is nonmonotonic.\n- In Fig. 10 the probability of elastic e-reflection R for copper at cryogenic temperature seems to approach 1 and to be independent of δ * max .\n\nOCR Text: \"Cu aS vield receivcd scrummed Cu |oky\" scrubbed AC) ev e\" ACcne CortribuluI Ofsccondarics Conurihutiom Ofrcfected clectrons to Primary (eV) 'partial Energy (\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 10: Conditioning of Cu sample at 10 K in special laboratory test [26]."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 5: Evolution process of 2 species where one is aging and the other one is non-aging, in both cases a childhood impairment is assumed. The graph shows in detail the first 80 iterations and is then extended to the 800th iteration, where the range from the 80th to the 800th iteration is plotted in the same horizontal range at which the first 80 iterations are depicted. In the case of the aging species extension shows a slow but exponential convergence towards the target value.\n- where age i &lt; 5 results again in a boolean expression and is 1 if true. See Figure 5 that depict the results of this simulation. Initially (for the first 30 or so iterations) both species show a very similar performance. However, one can see that the aging species overcomes the limit that was induced by the childhood impairment and continues to approximate exponentially the target value. c.\n\nMentions:\n- Figure 5: Evolution process of 2 species where one is aging and the other one is non-aging, in both cases a childhood impairment is assumed.\n- See Figure 5 that depict the results of this simulation.\n\nOCR Text: \"F4p:; Din@: k; Din@: 1\" Tahirn\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: Evolution process of 2 species where one is aging and the other one is non-aging, in both cases a childhood impairment is assumed. The graph shows in detail the first 80 iterations and is then extended to the 800th iteration, where the range from the 80th to the 800th iteration is plotted in the same horizontal range at which the first 80 iterations are depicted. In the case of the aging species extension shows a slow but exponential convergence towards the target value."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- For the purpose of proving the universality of the Σ − D law for SNRs, regardless of which samples are used, we constructed a composite sample containing available extragalactic SNR populations at the same frequency of 5 GHz (214 SNRs in total), which is shown in Figure 21. Properties of the 10 extragalactic samples considered in this paper are listed in Table 4. The entire sample has a compact appearance for the presented variable range, with an overall slope of β = 3.60 ± 0.15, very close to that of the LMC sample in this study. The resulting slope indicates that the observed extragalactic SNRs are mainly in the Sedov phase of evolution, as it was predicted by the values of slopes which are theoretically derived (Duric &amp; Seaquist 1986;Berezhko &amp; Völk 2004).\n\nMentions:\n- For the purpose of proving the universality of the Σ − D law for SNRs, regardless of which samples are used, we constructed a composite sample containing available extragalactic SNR populations at the same frequency of 5 GHz (214 SNRs in total), which is shown in Figure 21.\n\nOCR Text: \"svGill NGe_LUGu 4212 2ui*(-4f9ii MWMSLJ Lvgi-U MU MPu-ZoW I3u28 E Entire campic XNHS M-KUA\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 21. The Σ − D graph of LMC SNRs alongside other 9 nearby galaxies. This composite sample contains 214 SNRs. Orthogonal fitting has been applied to the 5 GHz data for the galaxies. The youngest known SNR in the LMC, 1987A, is also shown (circled plus) but it is not included in the Σ−D fit, being in the early free expansion phase of evolution. The solid black line represents the best orthogonal fit to the data (β = 3.60 ± 0.15)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Before presenting our approach, we describe a general training data selection process, which consists of three main steps: TDS (training data set) setup, ranking, and duplicate removal. TDS setup: For each target project with little historical data, we need to set up an initial TDS where training data are collected from other projects. To simulate this scenario of CPDP, in this paper any defect data from the target project must be excluded from the initial TDS. Note that different release versions of a project actually belong to the same project. A simple example is visualized in Fig. 3.\n- Fig. 3 shows the overall structure of the proposed approach to training data selection named TDSelector. Before selecting appropriate training data for CPDP, we have to set up a test set and its corresponding initial TDS. For a given project treated as the test set, all the other projects (except the target project) available at hand are used as the initial TDS. This is the so-called many-to-one (M2O) scenario for CPDP [13]. It is quite different from the typical O2O (one-to-one) scenario, where only one randomly selected project is treated as the training set for a given target project (viz. test set).\n\nMentions:\n- A simple example is visualized in Fig. 3.\n- Fig. 3 shows the overall structure of the proposed approach to training data selection named TDSelector.\n\nOCR Text: \"Udmy Eants (Dty 0a Iri[4 Ii \"wl Ti $ HpcM sMAD , Sanato FepcBa Ati Fxa Col Kece T4x\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3. The overall structure of TDSelector for CPDP."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 9 shows how the BHAR/SFR ratio and the BHAR/ M * ,in ratio scale with the total stellar mass of the host galaxies. Here, M * ,in refers to the rate of change of stellar mass within R 0 and we show results for the A series of simulations. The average behaviour of the BHAR/SFR ratio at 4 ≥ z ≥ 1 as predicted by our simulations has a similar slope (slope= 0.79) as predicted by Delvecchio et al. (2019) (slope= 0.73) based on an empirical model. However, the normalizations differ by up to an order of magnitude. Our predicted BHAR/ M * ,in ratios agree well with the results presented in Delvecchio et al. (2019).\n\nMentions:\n- Figure 9 shows how the BHAR/SFR ratio and the BHAR/ M * ,in ratio scale with the total stellar mass of the host galaxies.\n\nOCR Text: \"CcBH\"RH: Cc BHGR EFRI Deveccnid S 3 0 KURIqniF ~12 33 6 {axlnnin L6> log(M; {Mo)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 9. Comparison of the relations between the ratios of BHAR/SFR and BHAR/M∗, in in MassiveFIRE and the empirical relation from Delvecchio et al. (2019) for SMBHs located in density maximum (MAX) and few mergers model. Diamonds show the ratio of BHAR to stellar mass change in the inner region (< 100 pc), ÛM∗, in. Circles show the ratio of BHAR calculated within 100 pc to SFR of the host galaxy. Red, green, blue and magenta are for redshift z=4,3,2, and 1, respectively. The solid black line shows the fit line from Delvecchio et al. (2019), while the dashed and the dotted dashed black lines show the best fit for the BHAR/M∗, in and BHAR/SFR, respectively."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To demonstrate that SAVE is applicable to more standard environments, we also evaluated it on a subset of Atari games (Bellemare et al., 2013). We implemented SAVE on top of R2D2, a distributed Q-learning agent that achieves state-of-the-art results on Atari (Kapturowski et al., 2018). To allow for a fair comparison 2 between purely model-free R2D2 and a version with SAVE, we controlled R2D2 to have the same replay ratio as SAVE and then tuned its hyperparameters to have approximately the same level of performance as the baseline version of R2D2 (see Appendix E). We find that SAVE outperforms or equals this controlled version of R2D2 in all games, with particularly high performance on Frostbite, Alien, and Zaxxon (shown in Figure 5). SAVE also outperforms the baseline version of R2D2 (see  We introduced SAVE, a method for combining model-free Q-learning with MCTS. During training, SAVE leverages MCTS to infer a set of Q-values, and then uses a combination of real experience plus the estimated Q-values to fit a Qfunction, thus amortizing the value computation of previous searches via a neural network. The Q-function is used as a prior to guide future searches, enabling even stronger search performance, which in turn is further amortized via the Qfunction. At test time, SAVE can be used to achieve high levels of reward with only very small search budgets, which we demonstrate across four distinct domains: Tightrope, Construction (Bapst et al., 2019), Marble Run, and Atari (Bellemare et al., 2013;Kapturowski et al., 2018). These results suggest that SAVEing the experience generated by search in an explicit Q-function, and initializing future searches with that information, offers important advantages for model-based RL.\n\nMentions:\n- We find that SAVE outperforms or equals this controlled version of R2D2 in all games, with particularly high performance on Frostbite, Alien, and Zaxxon (shown in Figure 5).\n\nOCR Text: \"change relalive RZDZ 6002. I0o0* 200k = 100\"- L' l J J 2 M JI 8 1 1\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: Results on Atari."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We now present some numerical simulations that compare the performance of our Insensetive Mechanism with the state of the art. As discussed earlier, past literature focuses on a non-colluding setting, and here we compare with BEACON (Zheng et al., 2017) which is the well-known mechanism for the non-colluding setting. Figure 1 compares the regret β of our Insensetive Mechanism with BEACON for a 5-round 5-agent game in the heterogeneous setting where agents have true thresholds as 20, 40, 50, 70, 100. The budget ranges from 20 to 3, 000. Overall, our mechanism collects 82.94% of the total number of data points compared to α˚, whereas BEACON attains only 46.63% of α˚on average. Compared to BEACON, our Insensetive Mechanism achieves an improvement of 36.31%.\n\nMentions:\n- Figure 1 compares the regret β of our Insensetive Mechanism with BEACON for a 5-round 5-agent game in the heterogeneous setting where agents have true thresholds as 20, 40, 50, 70, 100.\n\nOCR Text: \"setive Wechansm BEACON JO0) 20C0 3036 Budget\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: A heterogeneous threshold scenario where the game has 5 rounds and 5 vehicles with real thresholds of 20, 40, 50, 70 and 100. The regret β of our Insensetive Mechanism (solid blue line) is consistently and significantly smaller as compared to the prior state-of-the-art BEACON (dotted brown line)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- As illustrated in Figure 2, our method consists of the teacher, student and discriminator networks. We denote the teacher network (require pre-train) as T , and the student network as S. Both the teacher and student network are built by a backbone-subnetwork (e.g., VGG, ResNet) T /S (i.e., the input vector of the softmax layer) as the input. The detailed structure of the task-driven attention estimator is presented in Figure 3. As the channel number of g b T /S might differ from that of L i T /S , the channel alignment convolution is introduced to achieve the feature vectorĝ b,i T /S . Then, the element-wise sum betweenĝ b,i T /S and L i T /S is conducted along the dimensionality of channel. Finally, the squeezed knowledge descriptor g a,i T /S for the i-th block can be gained by the following equation:\n\nMentions:\n- The detailed structure of the task-driven attention estimator is presented in Figure 3.\n\nOCR Text: \"bask ANen amenton estakor aFwh Fku {rel JWH H Wh FM La FWh F [Fu\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: The structure of designed task-driven attention estimator for i-th block, where the backbone intermediate feature LiT/S and the global descriptor g b T/S are taken as input to produce the squeezed knowledge descriptor g̃a,iT/S ."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Figure 11 we show the total equivalent width of the Na I(8183,8195Å) doublet as a function of the observed V-J color 2 for 3441 stars: 1025 TTS TTS (dark green dots) and 2416 field stars (light blue dots). For every object the equivalent widths of each of the Na I lines were measured both with SPTCLASS (Hernandez et al. 2017) and interactively with the splot utility in IRAF, which allowed us to obtain an estimate of the measurement uncertainty, indicated by the errorbar in the figure.\n- Despite the large scatter of Na I equivalent widths at any given V-J, it is readily apparent that the bulk of the TTS lie distinctly below the distribution of field stars. To quantify this effect, we show in Figure 11 isochrones calculated by Schlieder et al. (2012), using the Siess et al. (2000) evolutionary tracks and the PHOENIX model atmospheres (Hauschildt et al. 1999;Rice et al. 2010). The mean value for the field stars in Figure 11 matches very well the 100Myr isochrone. 99.9% of the TTS fall below this lines, and most of the spread seen in the field stars for spectral types later than ∼M0 is encompassed within the 1 Gyr and 50 Myr isochrones. It is also noticeable that the TTS and field stars tend to separate in the W(Na I) vs Teff plane, only for V-J 3, corresponding to spectral types later than ∼ M1.5.\n\nMentions:\n- In Figure 11 we show the total equivalent width of the Na I(8183,8195Å) doublet as a function of the observed V-J color 2 for 3441 stars: 1025 TTS TTS (dark green dots) and 2416 field stars (light blue dots).\n- To quantify this effect, we show in Figure 11 isochrones calculated by Schlieder et al. (2012), using the Siess et al. (2000) evolutionary tracks and the PHOENIX model atmospheres (Hauschildt et al. 1999;Rice et al. 2010).\n- The mean value for the field stars in Figure 11 matches very well the 100Myr isochrone. 99.9% of the TTS fall below this lines, and most of the spread seen in the field stars for spectral types later than ∼M0 is encompassed within the 1 Gyr and 50 Myr isochrones.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 11. Total equivalent width of the Na I 8183,8195Å lines for the new Orion OB1 TTS confirmed with Hectospec spectra, shown as dark green dots. The small light blue dots correspond to stars classified by us as field stars. The blue straight line is the least square fit to the distribution of those field stars. The dashed red lines are, from top to bottom, the 1 Gyr, 100 Myr, 50 Myr and 10 Myr isochrones from Schlieder et al. (2012). A typical errorbar is indicated."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Homogeneous nucleation refers to classical nucleation theory in the bulk, which can be derived from the surface energy (proportional to area) and the energy of phase transformation (volumetric). Meanwhile, heterogeneous nucleation refers to nucleation at interfaces, including on surfaces and particles in the bulk. The equation above shows that heterogeneous nucleation is favored, as its energy barrier is much smaller. Figure 12 shows the approximate Gibbs free energy barrier for homogeneous nucleation on the superhydrophobic membranes from this study using the equation above, and heterogeneous nucleation on the same membranes. This equation and figure show that for larger contact angle, the Gibbs free energy required to nucleate on the membrane surface is increases, approaching that required for homogeneous nucleation in the bulk liquid. Physically, this means that MD membranes with large contact angles are not really favored as a location for nucleation. Heterogeneous nucleation, however, can still occur on any small foreign particles in the bulk liquid, and it would be favored over either homogeneous nucleation in the bulk or heterogeneous nucleation on the membrane surface. While this contact angle of substrate-crystal-liquid is hard to measure, it is closely linked with the substrate-liquid-air contact angle that defines hydrophobicity. Both angles depend on the magnitude of the surface free energy term of the substrate: for a lower surface free energy, the higher both the nucleation energy barrier ‫ܩ∆‬ ௧௨௦ * and hydrophobicity are.\n\nMentions:\n- Figure 12 shows the approximate Gibbs free energy barrier for homogeneous nucleation on the superhydrophobic membranes from this study using the equation above, and heterogeneous nucleation on the same membranes.\n\nOCR Text: \"IOCPO 808 EoX 0,25 403 3 0,75 20X 3 6 =0,90 110 Contact Angle 9 [radians]\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 12. Plot of the energy of formation of a critical nucleus versus substrate-crystal-liquid contact angle (equation 8.3) with curves for variable porosity ε."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 2 shows the laser configuration. The microring resonator was simply embedded in a fiber loop cavity that contained a short erbium doped fiber amplifier, acting as the gain medium. The loop also contained a passband filter with a bandwidth large enough to pass all of the oscillating lines, with the main purpose of controlling the central wavelength λ 0 , as well as an optical isolator, a polarization controller, and an output beam sampler embedded in a delay line of length L d . The total main cavity length was 3m yielding an FSR of 65MHz. The delay line was employed to control the phase of the main cavity modes with respect to the ring modes. By adjusting the delay line of our system we observed three main operation regimes: unstable, stable and dual mode, described in detail in the next section.\n\nMentions:\n- Figure 2 shows the laser configuration.\n\nOCR Text: \"EDFA iteee Fantasn F:El79ne Ebua Frt: 3FSR, uzul\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2. Laser configuration. The free space delay line was used to adjust the spectral position of the main cavity modes relatively to the ring resonator modes."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To measure the noise introduced by thermal effects, the body of the autocollimator and the CCD were in-  Difference FIG. 8: Difference angle data for the noise run shown in Figure 7. Daily variation is believed to be due to 0.25-0.5 K daily variation in lab temperature.\n- Adding heat sinks to the CCD and the LED, increasing their thermal time constants to &gt;1000 s, was sufficient to reduce the effect of temperature variation to below the 1 nrad/ √ Hz noise floor from 10 mHz to 1 Hz. Figure 7(a) shows the sensitivity of the autocollimator at low frequencies and Figure 7(b) shows the sensitivity at high frequencies. The noise between 1 Hz and 10 Hz is consistent with the levels seen in Figure 7(a) and Figure 7(b). The autocollimator has ∼1 nrad/ √ Hz sensitivity at all frequencies down to ∼5 mHz where the noise begins to rise as 1/f in amplitude. Figure 7(c) shows the sensitivity of the reference and target patterns compared to the difference signal. As expected, at high frequencies the difference noise floor is inherently a factor √ 2 higher than the noise floors of the reference and target patterns due to subtraction. The differential measurement effectively suppresses noise at frequencies below ∼0.2 Hz by an order of magnitude. As hinted by Figure 7(c), alignment adjustments can give single-mirror noise floors as low as 0.5 nrad/ √ Hz, with a differential noise floor of 0.7 nrad/ √ Hz. When the CCD was immobilized for lowfrequency tests presented here, the reference signal was noisier than the target signal, limiting differential performance to 1 nrad/ √ Hz. After internal optical alignment, our autocollimator is robust and practical. It operates in both single-mirror and reference-mirror modes with ease. It can be removed from an apparatus and reattached/aligned in minutes. The autocollimator operates in ambient room light with a slightly elevated noise floor. External connections to the instrument are a USB cable and an optical fiber; no analog electrical or ground connections are required. The instrument's optimal working point was adjusted for a 25 cm separation between the objective and the mirrors; but similar performance can be achieved for working dis- tances between 10-40 cm without realignment. The aperture of the main lens restricts the maximum working distance; our first prototype MSA with a 15.25 cm objective lens operated with a ∼1 nrad/ √ Hz differential noise floor at frequencies above 0.1 Hz at a working distance of 1 m. The dynamic measurements discussed below were made with a 7 cm separation in optical path length between the reference and target mirrors.\n\nMentions:\n- To measure the noise introduced by thermal effects, the body of the autocollimator and the CCD were in-  Difference FIG. 8: Difference angle data for the noise run shown in Figure 7.\n- Figure 7(a) shows the sensitivity of the autocollimator at low frequencies and Figure 7(b) shows the sensitivity at high frequencies.\n- The noise between 1 Hz and 10 Hz is consistent with the levels seen in Figure 7(a) and Figure 7(b).\n- Figure 7(c) shows the sensitivity of the reference and target patterns compared to the difference signal.\n- As hinted by Figure 7(c), alignment adjustments can give single-mirror noise floors as low as 0.5 nrad/ √ Hz, with a differential noise floor of 0.7 nrad/ √ Hz.\n\nOCR Text: \"WYHHH Hh\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 7: Sensitivity of the autocollimator for Invar-mounted mirror pair. (a) Power Spectral Amplitude of the difference signal at low frequencies. (b) High frequency sensitivity. Yellow is the full spectrum, green is the averaged spectrum and the black line is 0.5 nrad/ √ Hz. (c) Averaged sensitivity of the"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Although this measurement was designed to study the quenching effect of low-energy sodium recoils, data acquired with large scattering angles could also contain iodine recoils of up to 10 keV nr based on kinematic calculations and simulations. However, we did not observe significant evidence for iodine recoils with the expected rate above 0.65 keV ee , in which region we have over 50% trigger efficiencies. Therefore, we have set an upper limit of 0.065 (&gt; 3 σ) for the iodine quenching factor at 10 keV nr . We note that DAMA uses iodine quenching value of 0.09 [11], and Collar measured a much lower By using low-energy, pulsed neutrons and combining TOF and PSD methods, we obtained the most accurate measurement of Na-recoil quenching factors in NaI(Tl) to date. This result differs significantly from the measurements of DAMA, as illustrated in Figure 8. At high energies, our measurement approximately agrees with previous reports, but the quenching factor values are found to drop significantly as the recoil energy decreases, similar to what was observed in the Collar measurement [18]. At low energies, our measurement falls between, and is statistically consistent with, the recent measurements by Collar and Chagani [17]. This energy-dependent quenching effect may be explained by the increasing ionization density along the Na recoil tracks at low energies, which has a large nuclear component but small electronic component; it is usually believed that the former leads to heat generation and the latter causes scintillation [17].\n\nMentions:\n- This result differs significantly from the measurements of DAMA, as illustrated in Figure 8.\n\nOCR Text: \"Simon 2003 DAMA Jageminn 20C6 Spconer I924 2CC8 Tcvey' 1928 Colla: 2013 Crbier 1999 This Enerzy (keVnr) (4arini ely a\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 8. The Na-recoil quenching factors measured in this work in comparison with previous results [12–18]. With much improved accuracy, this measurement approximately agrees with previous measurements at high energies and may reconcile the low-energy discrepancy. We comment that these measurements were calibrated to different gamma ray energies, and this direct comparison of quenching factors may include an uncertainty from the possible non-linearity of NaI(Tl) scintillation [27, 28]."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- For the training subset, we chose to use a more stringent quality threshold of SNR λ &gt; 250 (see Figure 3). These choices of SNR λ thresholds are described more fully in Section 3.2.\n\nMentions:\n- For the training subset, we chose to use a more stringent quality threshold of SNR λ &gt; 250 (see Figure 3).\n\nOCR Text: \"Xk?\"j IMcImlTet I-I E 20101 Si,J (SNR Rel;\"\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3. Signal to noise ratio measured in ugriz bands (SNRλ) for both survey and training sets. Cutoff minimums chosen were SNRλ>250 for the training subset and >50 for the validation and survey subset (see Section 3.3)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- It is also of interest to consider the impact of directly • cm modifying the outflow velocity. This is because different wind-driving mechanisms can yield vastly different outflow rates (Veilleux et al. 2005). To ensure a reasonably selfconsistent estimate, the density profile must be scaled in conjunction with the velocity: from equation 1, the product ρ v(h) h 2 is conserved. So, for an equivalent mass injection rate, a velocity scaled by a factor Ψ would require a corresponding scaling of the density profile by factor Ψ −1 . We note that this also results in a change in the associated mechanical energy of the wind, which is also scaled by a factor of Ψ as follows from ρ v 2 ∞ . The effect of scaling the system in this way is shown in Fig. 13, with the result essentially following directly from the initial condition for the CR energy density at the base of the outflow combined with the variation in the outflow density profile (which results from scaled velocity profile). In equation 45 it can be seen that the CR number density, and hence any heating effect the CRs may drive, is inversely proportional to v ∞ . Thus, if the velocity is reduced by an order of magnitude, the CR number density increases by an order of magnitude. However, the density of the outflow would also increase by an order of magnitude in such a scaling. Overall, this would increase the CR heating power in the advection limit by two orders of magnitude as it is proportional to both the CR number density and the density of the outflow wind fluid (which provides the target hadrons for the CR heating effect). In the diffusion limit, the picture is more straightforward: this time there is no dependence of the CR number density on the scaling, so the the CR heating power would only increase by one order of magnitude. We find that there is little bearing on the fraction of CRs absorbed in the outflow cone when a scaling is applied: when reducing the flow velocity by a factor of 10, the absorbed fraction falls slightly to 11.9% (compared to 12.0% in the baseline case -see section 4.1.4), while the fraction increases to 13.7% if increasing the outflow velocity by the same factor. The most directly influential parameter is the SN event rate, R SN , which essentially specifies the energy budget of the system. The CR heating rates scale with R 2 SN , as seen in Fig. 14, where a plausible range of values from 0.01 yr −1 (fairly quiescent, barely star-bursting system) to 10.0 yr −1 (extremely violent star-forming environment, perhaps possible in a system undergoing a major merger 12 ) are explored. follows from this. The 12% fraction of CR proton energy absorbed in the outflow by hadronic interactions (as calculated in section 4.1.4) is independent of SN event rate, as is the outflow velocity profile. In both cases, this is because the contributing quantities which are dependent on R SN scale antagonistically.\n\nMentions:\n- The effect of scaling the system in this way is shown in Fig. 13, with the result essentially following directly from the initial condition for the CR energy density at the base of the outflow combined with the variation in the outflow density profile (which results from scaled velocity profile).\n\nOCR Text: \"\"Mxl : I6-*- F W-s 10r-*_ Kxktu:: Ji #\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 13. Heating profiles in the diffusion (dashed lines) and advection (solid lines) limits, with outflow profile according to standard parameter choice, yielding a terminal velocity of v∞ ≈ 290 km s−1 (black line), with velocity increased by a factor of 10, with corresponding decrease in the density profile (red line) and with the velocity decreased by a factor of 10 and corresponding change in density profile (blue line)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 5 con rms this result with a 5-site deployment (two from Europe, one from North America, one from South America and one from Australia) in our other testbed (Tangled). X axis is presented with the number of prepends applied to each site. e number zero (0) represents the baseline, the positive numbers (1)(2)(3)(4)(5)   and the negative numbers represent negative prepends. As depicted, each site can capture at most 55-65% of blocks, and can shed at most 95% of blocks, even with up to 5 prepends. We can also see that we do not get a granular control as only three points are between the minimum and maximum.\n\nMentions:\n- Figure 5 con rms this result with a 5-site deployment (two from Europe, one from North America, one from South America and one from Australia) in our other testbed (Tangled).\n\nOCR Text: \"100% 80% 60% 40% atvract trathc pet tic W tr  = 20% 0% -5 -4 -3 -2 -] prepends\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: Tangled: catchment distribution in respective BGP prepend policy."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- However, as noted section III B, the transition matrices {P τ } τ ≥0 do not necessarily inherit from the semigroup property (6) of the transfer operators {L τ } τ ≥0 . Thus, no generator may exist and the rates r i in ( 17) may depend on the lag τ . Nevertheless, calculating the rates FIG. 4: Rates r i (τ ) corresponding to the 10 leading eigenvalues different from unity of eachP τ , with the lag τ as abscissa and the (cyclic) coloring giving the rank of the rate. A complex pair of eigenvalues is represented by one square for the two conjugates. The error bars represent 99% confidence intervals estimated from a thousand surrogate transition matrices by applying the bootstrap method described in Appendix A.\n- For this purpose, we calculated the rates r i (τ ) by solving the eigenvalue problem (16) and applying (17) for τ ranging from 1 to 39 days. The leading rate equals 0, since it is associated to the unit-eigenvalue. The rates corresponding to the 10 leading eigenvalues different from unity of eachP τ are represented figure 4 with the lag τ as abscissa and the (cyclic) coloring distinguishing the rank of the rate. A complex pair of conjugate eigenvalues is represented by one square for the two conjugates. The error bars represent 99% confidence intervals estimated from a thousand surrogate transition matrices by applying the bootstrap method described in Appendix A.\n- Being confident on the robustness of the rates to the sampling as well as to the grid resolution, we now turn to a detailed analysis of the results in figure 4. First, we can see that the two rates closest to zero, the leading rates in red and green, are well separated from the rest of the spectrum after a lag between 5 and 10 days. One of these rates derives from a real eigenvalue (the circle), the other represents a pair of complex conjugate eigenvalues (the square); they exchange rank at a lag of 8 days. We can calculate an indicative time scale associated with each rate as the inverse of the rate. Whether this time scale corresponds, to a good approximation, to a decorrelation time is not the matter of this study. For a lag of 15 days, the first rate (in green) and the second rate (in red) correspond to a time scale of 40 and 23 days respectively. Furthermore, the second rate is separated from the third (in cyan) by a gap of 13 days. This time-scale separation suggests that the reduced dynamics are slowly mixing due to the presence of meta-stable regimes responsible for low-frequency variability [49,79,80]. This confirms the work on meta-stable atmospheric regimes in this model by Crommelin [28]; we will see in section IV C how such regimes can be more objectively detected.\n\nMentions:\n- Nevertheless, calculating the rates FIG. 4: Rates r i (τ ) corresponding to the 10 leading eigenvalues different from unity of eachP τ , with the lag τ as abscissa and the (cyclic) coloring giving the rank of the rate.\n- The rates corresponding to the 10 leading eigenvalues different from unity of eachP τ are represented figure 4 with the lag τ as abscissa and the (cyclic) coloring distinguishing the rank of the rate.\n- Being confident on the robustness of the rates to the sampling as well as to the grid resolution, we now turn to a detailed analysis of the results in figure 4.\n- We have seen, section IV B, that the two leading rates of figure 4 are close to zero and that a large spectral gap separates them from the rest of the rates, a configuration indicative of the presence of meta-stable regimes.\n- for the rates represented in figure 4.\n\nOCR Text: \"0,25 020 5015 010 20,0 0.05 0c 20   25 #\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 4: Rates ri(τ) corresponding to the 10 leading eigenvalues different from unity of each P̂τ , with the lag τ as abscissa and the (cyclic) coloring giving the rank of the rate. A complex pair of eigenvalues is represented by one square for the two conjugates. The error bars represent 99% confidence intervals estimated from a thousand surrogate transition matrices by applying the bootstrap method described in Appendix A."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- for f &gt; 0. Note that this implies that the expected value of the default generation time (denoted here by τ 0,C ) is multiplied by f : E(τ 0,C ) = f (5 days) . Fig. 4 depicts the relation between E(τ 0,C ) and Eff ∞ , as f varies. As expected, the isolation measures become more effective as the time taken by the infection to be transmitted increases.\n\nMentions:\n- Fig. 4 depicts the relation between E(τ 0,C ) and Eff ∞ , as f varies.\n\nOCR Text: \":oii\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4. Eff∞ for some rescalings of the distribution of τ 0,C."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In figure 4, we plot the loss values as a function of the number of epochs, for the critic (blue), generator (green), and discriminator (red) losses. Even though the generator loss continues improving, it is difficult to compute the best parameters for either generator loss or critic loss without a visual inspection of their graphs. The discriminator loss provides a more stable signal. Not all generated human poses are smooth, especially when we train on a subset of the actions. Figure 5 shows a discontinuity between the last pose of the input and the first pose of the prediction. This can be mitigated by increasing the effect of the consistency loss in the generator loss, or by reducing the capacity of the network, or by feeding the actual last input directly into the first decoder node.\n\nMentions:\n- In figure 4, we plot the loss values as a function of the number of epochs, for the critic (blue), generator (green), and discriminator (red) losses.\n\nOCR Text: \"Giic Iqeremtog Idiadminalc \"4\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4. Critic versus Generator versus Discriminator loss."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In this subsection we consider the perfect reporting scenario. In Fig. 2, we consider a three-RX cooperative system and plot the average global error probability versus the decision threshold at the RXs for the OR rule, AND rule, and majority rule. The expected curves for the three rules are obtained from (8)-( 13) with (5) and (6). The Gaussian approximation curves for the three rules are obtained from ( 8)-( 13) with (20) and (21). The upper bound curves for the OR rule, AND rule, and majority rule are obtained from ( 10) and ( 26), ( 13) and (28), and ( 31) and (32), respectively, with (20) and (21). The value of ξ • RX for the OR rule, AND rule, and majority rule is obtained by solving (27), (29), and (33), respectively, with (20) and (21).\n- In Fig. 2, we first observe that the simulated points accurately match the expected curves, validating our analysis of the expected results. Second, we observe that ξ • RX is almost identical to ξ * RX for each fusion rule, confirming the accuracy of ξ • RX . Third, we observe that the Gaussian approximation curves well approximate the expected curves. Fourth, we observe that the convex upper bound curve for the OR rule is lower than its expected curve. This can be explained as follows: In the single-RX system, the Gaussian approximations give an upper bound on P md [j] and a lower bound on P fa [j]. For the OR rule, Q md [j] is the product of P md [j] and Q fa [j] is the sum of P fa [j]. Since the Gaussian approximation of Q md [j] is tighter than that of Q fa [j], the Gaussian approximation of the global error probability for the OR rule is lower than the expected curve. Finally, observing the expected curves, we find that the majority rule outperforms the OR rule and the OR rule outperforms the AND rule at their corresponding optimal decision thresholds.\n- , and refer to it as the instantaneous approximated error probabilities. Finally, we calculate the mean of all the instantaneous approximated error probabilities for all realizations of W j−1 TX . In Fig. 3, we first observe that for the OR rule and majority rule, the approximated curves match the expected curves, which confirms the accuracy of ξ • RX . Second, we observe that for the AND rule, the approximated curve deviates from the expected curve when K = 5 and K = 6. This is due to the fact that ξ • RX is outside the feasible set restricted by all the constraints for all realizations of W j−1 TX . Third, we observe an accurate match between the simulated points and the expected curves. Fourth, we observe that for the three fusion rules, the error performance clearly improves when the optimization is performed for each realization of W j−1 TX . However, as previously explained, this performance gain may not be feasible in practice and thus, we consider the average approximated curves as the best performance bound of our considered system. Fifth, we observe from the expected curves that the majority rule outperforms the OR rule and AND rule, which is consistent with that in Fig. 2. Lastly, we observe that the cooperative MC system outperforms the baseline case for all fusion rules, even though the distance of the baseline case is shorter than that of the cooperative MC system. Importantly, we see that the system error performance significantly improves as K increases. This is due to fact that an increasing number of cooperative RXs enables more independent observations of the transmitted information symbol.\n\nMentions:\n- In Fig. 2, we consider a three-RX cooperative system and plot the average global error probability versus the decision threshold at the RXs for the OR rule, AND rule, and majority rule.\n- In Fig. 2, we first observe that the simulated points accurately match the expected curves, validating our analysis of the expected results.\n- Fifth, we observe from the expected curves that the majority rule outperforms the OR rule and AND rule, which is consistent with that in Fig. 2.\n- Similar to Fig. 2, we observe that ξ • RX is almost identical to ξ * RX .\n- By comparing Fig. 2 with Fig. 4, we also observe that the expected error probabilities in Fig. 2   slightly lower than those in Fig. 4.\n\nOCR Text: \"ne Cue KNM Ful: Lpnt Huj_ JCSuI AMIMtl un Siing; Rul: Suluktod\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2. Average global error probability QFC of different fusion rules versus the decision threshold at RXs ξRX with K=3 in the perfect reporting scenario."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The results in figure 6 indicate that again the force applied in the unmodified enhanced drag implementation, in Fluidity and MIKE reduces significantly with increasing mesh resolution. A modification to the enhanced bottom drag c t was derived in this section, solving for c t in ( 26) and (30) for MIKE and Fluidity respectively, that is shown here to lead to a force that remains close to the desired value. The correction in MIKE was implemented by first finding the value for c t from (26) and then working back from (6) to compute what value of C t should be entered in the GUI to achieve this value in MIKE.  for the square case, the force applied decreases significantly with increasing mesh resolution.\n\nMentions:\n- The results in figure 6 indicate that again the force applied in the unmodified enhanced drag implementation, in Fluidity and MIKE reduces significantly with increasing mesh resolution.\n\nOCR Text: \"ralac Loce trce MIk: Lice F Ucg Lrc? MIKE #Uh :Qiec)on Lrce Fucry #Ih coneciot] Un Em UElw F\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6: Results for the enhanced drag formulation with the drag applied in a single triangular cell as implemented in both Fluidity and MIKE 21. As in figure 4, which show the results for the square case, the force applied decreases significantly with increasing mesh resolution. Applying the correction for ct however, given by solving (26) (MIKE 21) or (30) (Fluidity), the force can be kept more or less constant and much closer to the desired value."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We retrieved all available spectra of the 97 W-R stars listed in Table 1 and Table 2 from MAST. The spectra were obtained with IUE's SWP camera in the low-dispersion mode at a resolution of approximately 6 Å. The 10''  20'' entrance aperture was used for all considered spectra in order to maximize the photometric accuracy. Instrumental details of the IUE satellite and its spectrograph can be found in Boggess et al. (1978a, b). Spectra whose region around He II 1640 was flagged as having saturated pixels were eliminated from the sample and not considered. In cases when more than one spectrum was available for the same program star, we used the spectrum with the highest S/N for the measurements. Column 6 of Table 1 and Table 2 identifies the SWP spectrum used for the Galactic and LMC stars, respectively. We measured the equivalent widths EW(1640) and line fluxes F(1640) by integrating the observed profiles of He II 1640 over a wavelength range of 30 Å. The central wavelength was 1640.4 Å for the Galactic stars, and 1641.8 Å for the LMC stars (corresponding to an LMC recession velocity of 262 km s 1 ). The measurements are in columns 7 and 8 of Table 1 and Table 2. The measured values of EW( 1640) range between about 5 and 150 Å. The principal uncertainty is the placement of the adopted continuum level. We determined the continua by inspecting the full wavelength range, manually choosing regions between emission lines as representative of the stellar continuum, and interpolating between these regions to fit the full continuum. Conti &amp; Morris (1990) estimate errors of 30% for their measurements. Since their data and ours are similar, we assume the same errors for our measurements as well. As a test of the accuracy of our measurements, we performed a comparison between our results and those of Conti &amp; Morris, who have 50 stars in common with our sample. In Figure 1 we show the comparison of EW(1640) between the measurements obtained here and those of Conti &amp; Morris for the 50 Galactic and LMC stars. The error bar in this figure corresponds to a 30% uncertainty in both our and the Conti &amp; Morris data set. We find excellent agreement with a mean difference of (0.04  0.02) Å between log(EW(1640)) measured by us and by those authors. In order to identify offsets between the Galactic and the LMC sample we show in Figure 2 the ratio of the two measurements as a function of the measurements in this work. The mean differences for the Galactic and the LMC stars are the same: (0.04  0.02) Å for either stellar group. The corresponding tests for F(1640) are in Figure 3 and Figure 4. In this case the logarithmic difference is (0.01  0.02) erg s 1 cm 2 for the full sample. The mean difference for the Galactic stars is (0.02  0.02) erg s 1 cm 2 , and for the LMC stars it is (-0.01  0.02) erg s 1 cm 2 . The comparison suggests no systematic offset between the results of Conti &amp; Morris and those derived here.\n\nMentions:\n- The corresponding tests for F(1640) are in Figure 3 and Figure 4.\n\nOCR Text: \"5 \"efmo\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3. Same as Figure 1, but for the line flux measurements."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Let l p be the label of the first block of p (where procedure execution begins). Let blocks(p) be the set of block labels in p. Then, V C(p) is b lp ∧ l∈blocks(p) ϕ l . If the VC is satisfiable, then one can read-off the counterexample trace from a satisfying assignment by simply looking at the model for f . As an example, the VC of procedure foo of Figure 1 is given in Figure 2.\n\nMentions:\n- As an example, the VC of procedure foo of Figure 1 is given in Figure 2.\n\nOCR Text: \"Vw; U; V HA-Nubl:# Vs dhpzot|abale| 63 -hzi-IahMW=& Vs = Vuc\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2: The VC of procedure foo from Figure 1"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- After that, it is good to analyze the number of news in each month of the year. By considering Fig 5, the first and last months of year do not include considerable news in the corpus. Months eight and ten are the months of approval of budget in Iran and therefore, there are many economic and political news in these two months. Also month six stands in the third place since it is the last month of summer followed by fall in which the schools start. As the Fig 5 demonstrates, as we move forward from the first of summer to the end of it (month four to month six), the number of news increase. On the contrary, as we move forward from the start of winter to the end of it (month ten to month twelve), the number of news decrease significantly. The last month of winter is also the last month of year. Next, we need to analyze the length of each document. According to Fig 7, the majority of documents have less than 2K characters and a document which have more than 5K characters is rarely found in the corpus. The biggest set is the group of 415 news which contains 500-1000 tokens. For the number of characters more than 1k, as this number increases, the number of documents decreases most of the time. of Documents { \"body\": \"Tehran (ISNA) -Iran's Economic Development Committee chairman …\", \"code\": \"96030302144\", \"sentiment\": { \"body_s\": -3, \"title_s\": 0 }, \"tags\": \"Imad Khamis, Irans economic development committee, Saeed Ohadi, Syrian PM\", \"entity\": { \"ORDINAL\": \"0\", … }, \"pos_tagging\": { \"ADV\": \"1\", … }, \"date\": \"24 May 2017\", \"category\": \"Politics\", \"datetimestamp\": 1495668060, \"title\": \"Iranian officials meet Syrian PM\", \"url\": \"http://en.isna.ir/news/96030302144/\", \"journalist\": \"71477\" }\n\nMentions:\n- According to Fig 7, the majority of documents have less than 2K characters and a document which have more than 5K characters is rarely found in the corpus.\n\nOCR Text: \"YI Kn\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig 7. The histogram of ISNA-Set based on the number of characters"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Now, we compare the performance of NOMA and OMA in quasi-static Rayleigh fading channel. First, we evaluate the throughput as a function of the number of information bits k. The result is shown in Fig. 5 for n \" 500 and P 1 \" P 2 \" 10 dB. From the figure, we notice that the non-orthogonal scheme outperforms OMA in the case of both users. When the number of information bits is large enough, we notice that for both users NOMA achieves higher throughput than OMA. Again the optimum number of information bits k which maximizes the throughput differs from NOMA to OMA. To decrease the outage for the user we can increase the transmit power and apply ARQ with M transmissions or decrease the rate as will be shown below.\n\nMentions:\n- The result is shown in Fig. 5 for n \" 500 and P 1 \" P 2 \" 10 dB.\n\nOCR Text: \"SQulx Ier SOM: Utt J: Jhx Wsr !\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: Throughput as a function of the number of information bits in Rayleigh fading channel, considering n “ 500 channel uses and P1 “ P2 “ 10 dB."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The samples magnetization dependency on the temperature, with a fixed H applied, is obtained with a SQUID magnetometer (Quantum Design MPMS-XL). The measurement procedure is the following: i) each sample is initially brought to 350 K with H = 0; ii) the sample is cooled down to 100 K and then a magnetic field H = 1 T is applied; iii) the magnetic moment is recorded continuously during the temperature variation from 100 to 350 K, In the inset, the magnetization relative to saturation close to the transition region is shown. and then back to 100 K, with a sweep rate of ±2 K/min. The results are presented in Fig. 5, where (M(T ), T ) curves corresponding to an irradiated and the reference samples are shown. At low temperature, the saturation magnetization values M sat of the reference and irradiated samples are comparable within the experimental uncertainty of 1-2%. Similarly, the transition temperature T C , defined here as the temperature for which M(T C ) = M sat /2, is for both samples around 305 K. In contrast, differently from the reference sample, characterized by ∆T hys ≈ 5 K for H = 1 T, the thermal hysteresis disappears in the irradiated samples.\n\nMentions:\n- The results are presented in Fig. 5, where (M(T ), T ) curves corresponding to an irradiated and the reference samples are shown.\n\nOCR Text: \"Fel\"ic  rukS ICrsicM 1 300 pntenae\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 5. (Color online) Magnetization as a function of temperature for the reference (solid lines) and for the irradiated samples (dashed lines). Data obtained by a temperature increase (from colder temperatures) and decrease (from hotter temperatures) are presented in blue and red, respectively. In the inset, the magnetization relative to saturation close to the transition region is shown."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Since the model can convert the unrecognizable IR evaluation index into a function that can be derived by transforming the loss function, the specific situation of this method is shown in Fig 2\n\nMentions:\n- Since the model can convert the unrecognizable IR evaluation index into a function that can be derived by transforming the loss function, the specific situation of this method is shown in Fig 2\n\nOCR Text: \"quety Q&A paits Question relevance Q4A0n quality model model Question relevanc? QA4 on qualily Fz Fy. F LulbdaMARL' soling leaing model\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2. Fusion Q&A on the quality of sorting learning model framework"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In the following simulations, we select the function of ( ) 2sin t as the signal ( ) v t . Though the signal tracking output x 1 is smooth (See Fig. 3), derivative estimation output x 2 is continuous but non-smooth (See Fig. 4-b). The intensive chattering phenomenon happen due to discontinuous differentiator structure.\n\nMentions:\n- Though the signal tracking output x 1 is smooth (See Fig. 3), derivative estimation output x 2 is continuous but non-smooth (See Fig. 4-b).\n\nOCR Text: \"HJeal cerivaline sigral tx Levant TD bmneds} ~-bThe neqnified fiqure of (ah\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4 Derivative estimation by Levant differentiator Though the signal tracking output x1 is smooth (See Fig. 3), derivative estimation output x2 is continuous but non-smooth (See Fig. 4-b). The intensive chattering phenomenon happen due to discontinuous differentiator structure."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Our weak head evaluation strategy will postpone propagation of substitutions into an expression until necessary. Treating substitutions lazily seems to be beneficial as also supported by the experimental analysis on lazy vs eager reduction strategies for substitutions by Nadathur and his collaborators [9]. We present the algorithm for weak head normalization in Figure 4. We define a function whnf L where L is either a variable x n or a proper closure [ρ][[η]]E. The function whnf is then defined recursively on E.\n\nMentions:\n- We present the algorithm for weak head normalization in Figure 4.\n\nOCR Text: \"W U I ll H\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4: Weak head evaluation"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The 44 sources in our sample with known redshifts span the redshift range z = 0.22 − 2.7. The 10-40 keV luminosity range of the 19 of these 44 that are significantly detected in the 8-24 keV band (from which we calculated 10-40 keV luminosities) span the range ∼ 8 × 10 42 ergs s −1 to ∼ 3 × 10 45 ergs s −1 . The redshift-luminosity plane for our sample, together with comparison samples from the Swift-BAT survey (Baumgartner et al. 2013) and the NuSTAR serendipitous survey (Alexander et al. 2013) is shown in Figure 12 and highlights the complementary regions of parameter space that these various surveys cover, which is the focus of the following subsection.\n- The AGNs detected and characterized as part of the all-sky Swift-BAT survey form the local sample that is most directly comparable to the NuSTAR-ECDFS AGNs reported here. Figure 12 shows that, while the range in luminosities of both samples are large (i.e., each spanning roughly 3 orders of magnitude or more) and there is some overlap, the AGNs in the ECDFS survey are typically an order of magnitude more luminous than those in the Swift-BAT sample. As expected, the difference in the redshift distributions of the two samples is also large, with the ECDFS sample centered around z ∼ 1, compared to z ∼ 0.03 for the Swift-BAT sample.\n- To help place the two samples in a cosmological context, we include a line in Figure 12 showing the evolution in the position of the knee (i.e., L * ) of the AGN luminosity function (converted to 10-40 keV from the 2-10 keV luminosity functions of Aird et al. 2010 assuming an intrinsic photon index of Γ = 1.8). The position of the two samples relative to this line demonstrates the complementary nature of these samples, with the Swift-BAT sample probing the knee of the luminosity function at z ∼ 0.015 and the ECDFS sample probing it at z ∼ 0.8 − 1. Crucially, this means that due to the evolution of the SMBH accretion rate density, the power of NuSTAR and Swift-BAT combined will enable us to probe roughly 25% of the accretion rate density of the Universe with hard X-rays, compared to just 0.5% with Swift-BAT alone. 38  Lutz et al. 2011) surveys. Only one of these, NuSTAR -ID: 37 shows any evidence of hosting an AGN at any wavelength other than hard X-rays, which comes from mid to far-infrared SED fitting (see Figure 14).\n\nMentions:\n- The redshift-luminosity plane for our sample, together with comparison samples from the Swift-BAT survey (Baumgartner et al. 2013) and the NuSTAR serendipitous survey (Alexander et al. 2013) is shown in Figure 12 and highlights the complementary regions of parameter space that these various surveys cover, which is the focus of the following subsection.\n- Figure 12 shows that, while the range in luminosities of both samples are large (i.e., each spanning roughly 3 orders of magnitude or more) and there is some overlap, the AGNs in the ECDFS survey are typically an order of magnitude more luminous than those in the Swift-BAT sample.\n- To help place the two samples in a cosmological context, we include a line in Figure 12 showing the evolution in the position of the knee (i.e., L * ) of the AGN luminosity function (converted to 10-40 keV from the 2-10 keV luminosity functions of Aird et al. 2010 assuming an intrinsic photon index of Γ = 1.8).\n\nOCR Text: \"1040 ~Svifl _ BAT ONuSTAR ECDFS (> 30) RNUSTAR ECDFS (<Jo) Nustar Serendlp 10*5 10*] 0.0O1 0.010 00d padeniri {0* {0\"\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 12.— X-ray luminosity in the rest-frame 10-40 keV band versus redshift for several hard X-ray selected samples. We show the NuSTAR-ECDFS sources (i.e., our sample), the first ten sources from the NuSTAR serendipitous survey (Alexander et al. 2013) and the sources from the Swift-BAT survey (Baumgartner et al. 2013). In the case of the NuSTAR-ECDFS sources, the 10-40 keV luminosity is derived from the observed 8-24 keV flux, assuming either the measured photon index (Γ) if the source is significantly detected in both the 3-8 keV and 8-24 keV bands or Γ = 1.8 otherwise. Upper limits are shown for NuSTAR-ECDFS sources when the measured flux in the 8-24 keV band is < 3σ. The dotted line indicates the evolution of the position of the knee of the X-ray luminosity function, extrapolated from the 2-10 keV luminosity functions of Aird et al. (2010) assuming Γ = 1.8. The NuSTAR sources in the ECDFS probe to this “knee” out to z ∼ 1."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- 4. split each subsample into two parts, RA, DEC on one side and z, R on the other. Shuffle the two parts separately and recombine them. Fig. 6 shows the correlation functions using the proper random catalog constructed by the four steps described above, comparing with the ones constructed by skipping step 2 or step 3. In addition, we also take random angular positions from the galaxy random catalogues instead of stacking mocks and show the corresponding CF in Fig. 6. One can see that the CF is boosted if we ignore step 3 (shuffling within radius bins) or we use galaxy random catalogues. Our results are not sensitive to redshift dependency because we always keep the radius together with corresponding redshift as one part when constructed the random catalog. The boosting should due to the boundary effect described earlier (see Fig. 5). The boosting is significant while using galaxy random catalogue because of the complex geometry of the real survey. In other words, it is critical to use the proper mock catalogues to build the random catalogue.\n\nMentions:\n- Shuffle the two parts separately and recombine them. Fig. 6 shows the correlation functions using the proper random catalog constructed by the four steps described above, comparing with the ones constructed by skipping step 2 or step 3.\n- In addition, we also take random angular positions from the galaxy random catalogues instead of stacking mocks and show the corresponding CF in Fig. 6.\n\nOCR Text: \"Doncrducnalon tejshil _ DE_ ttcr cJijz\" Cnjon\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6. Comparison of the void correlation function with different random catalogs including 1. the proper random catalog (blue curve), 2. the random catalogue constructed without shuffling within radius bins (cyan curve), 3. the random catalog constructed without shuffling within redshift bins (red curve), and 4. random catalog constructed using mock galaxy randoms instead of stacking mocks as the angular random positions."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- It is also worthwhile to show the results at a lower resolution (here 100 pixels in each direction) with only three points resolving the boundary layer (r 1 −r 0 ). In this case we have chosen a strong 3 kG magnetic field strength for which the density in the flux-tube has reduced by between 98% Fig. 4 Scattered pressure perturbations (snapshot) from a strong, 3 kG, vertical flux tube with axis x = y = 0. The bottom panel shows the exact result, the top panel the numerical. The calculation was performed at a lower resolution of 100 by 100 grid points with only 3 points across the tube boundary. This shows the type of artifacts which can be expected from using a too low resolution. and 99%. The results are shown in Figures 4 and 5. Magnetic fields of this strength, and this level of evacuation, will presumably be important in calculations of wave propagation through sunspots. Given the resolution, the agreement between the numerical and analytic results are reasonable, however artifacts near the now poorly resolved boundary layer are clearly visible.\n- The above test involves several terms related to background inhomogeneities in the magnetic field, the density and the pressure fields. An obvious remaining class of perturbations involve the velocity field, and we have tested the code here by comparing against an analytic solution for uniform flow Fig. 5 A cut at y = 0 through the scattered pressure perturbations shown in Figure 4 (vertical 3 kG flux tube, low resolution). The dashed line shows the exact solution and the solid line the numerical. The units are such that the incoming pressure perturbations have an amplitude of 1. in a cylinder embedded in a stationary uniform media. We assume that the density and temperature are the same inside and outside the cylinder, so that the only defining property is the velocity which we assume is directed along the axis of the cylinder. The analytic solution was obtained in a similar manner to that in Gizon et al. (2006).\n\nMentions:\n- In this case we have chosen a strong 3 kG magnetic field strength for which the density in the flux-tube has reduced by between 98% Fig. 4 Scattered pressure perturbations (snapshot) from a strong, 3 kG, vertical flux tube with axis x = y = 0.\n- The results are shown in Figures 4 and 5.\n- An obvious remaining class of perturbations involve the velocity field, and we have tested the code here by comparing against an analytic solution for uniform flow Fig. 5 A cut at y = 0 through the scattered pressure perturbations shown in Figure 4 (vertical 3 kG flux tube, low resolution).\n\nOCR Text: \"HUeF aHU OHPGIHOH €\"WAVP HQFAUGHOH HaKe a 8: Anaduonal complraion_s Hallheanalcsdhi bher OorespurOS wik anudsthedpane Wavben; Jlterhanobsacerahepato hevaeTontesh usyypacsedtuhmeglye, Fotecahualoh we haetaken te wae HOuaqaiuh Oter diecONW  Hhe cHerunghuHehu:\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4 Scattered pressure perturbations (snapshot) from a strong, 3 kG, vertical flux tube with axisx = y = 0. The bottom panel shows the exact result, the top panel the numerical. The calculation was performed at a lower resolution of 100 by 100 grid points with only 3 points across the tube boundary. This shows the type of artifacts which can be expected from using a too low resolution."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Assume thatb is constructed byb = Hx * where x * is a vector of all ones. We plot in Figure 1 the curves of (28) for a few representative iteration numbers. This figure shows that the curves of Q n (α) corresponding to steepest descent converge to the limit, as proved in Lemma 2 and Lemma 3.\n- It is observed that ( 23) leads to a straightforward estimation of parameter γ * in (4). From Figure 1 we can deduce that the optimal parameter in HSS could be actually approximated by steepest descent iterations, which is shown in the following theorem.\n\nMentions:\n- We plot in Figure 1 the curves of (28) for a few representative iteration numbers.\n- From Figure 1 we can deduce that the optimal parameter in HSS could be actually approximated by steepest descent iterations, which is shown in the following theorem.\n\nOCR Text: \"nE\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Curves of Qn(α) for a few representative iteration numbers. Steepest descent is used for solving system (6) where H satisfies (29) and b̂ is a vector of all ones."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 18 shows the total number of clusters, in which those IP addresses of the four known organizations are grouped. No matter what value K is set, the maximum number of clusters is always 6. It indicates that there are only 6 attack patterns at the most in the samples with known organization labels.\n\nMentions:\n- Figure 18 shows the total number of clusters, in which those IP addresses of the four known organizations are grouped.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 18: The total number of clusters, in which those IP addresses of the four known organizations are grouped."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- As shown in Fig. 4, considering both average AUC and NRS, we find that Staple CA [37] achieves well balance between the accuracy and blur robustness. Although VI-TAL [49] is slightly worse than ECO [6] on the average AUC, it has much higher NRS than ECO. According to blur robustness plots, we find that the rank of trackers has great difference on 5 subsets. For example, VITAL [49] obtains smaller AUC score than ECO [6] and STRCF [31] on S 1 while being the best one on S 16 . We can find similar results on BACF [15], CSRDCF [34], DSST [7] and CSK [20].\n\nMentions:\n- As shown in Fig. 4, considering both average AUC and NRS, we find that Staple CA [37] achieves well balance between the accuracy and blur robustness.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4. Normalized robustness score (NRS) and average AUC of 23 trackers. The legend is the same with that of Fig. 3."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- This allows the contamination of two photoelectrons in the single photon electron spectrum to be estimated: if the ratio of one photoelectron counts to pedestal counts is less than 1%, then the ratio of two photoelectron counts to one photoelectron counts is less than 0.5%. An example of a single photoelectron spectrum taken with the setup presented in this paper is shown in Fig. 4. On the left hand side of the spectrum is the region of the spectrum, known as the pedestal, corresponding to gates during which no photoelectrons were collected into the electron multiplier of the PMT. In theory, the pedestal will be centered on zero charge, but in practice the pedestal has a non-zero mean due to leakage current from the PMT and the current offset of the read-out electronics. The area of the pedestal gives the number of gates in which no photoelectrons were collected.\n- Each single photoelectron spectrum is analyzed to extract the number of single photoelectron counts and the position of the single photoelectron peak. In order to work in an efficient way with a large number of MAPMTs this analysis was automated and built into the DAQ. The example spectrum shown in Fig. 4 was analyzed by this routine.\n- Each spectrum is first smoothed using the 353QH algorithm 2 [10] to reduce statistical fluctuations, after which the valley, shown by the downwardsfacing triangle in Fig. 4, is found using a simple peak searching routine. No attempt was made to perform any fits in the analysis as neither the single photoelectron peak, nor the pedestal, are properly described by a simple Gaussian distribution. A discussion of the deconvolution of PMT spectra is given in [6], but such multi-parameter analyses are difficult, model dependent, and not needed in this case.\n\nMentions:\n- An example of a single photoelectron spectrum taken with the setup presented in this paper is shown in Fig. 4.\n- The example spectrum shown in Fig. 4 was analyzed by this routine.\n- Each spectrum is first smoothed using the 353QH algorithm 2 [10] to reduce statistical fluctuations, after which the valley, shown by the downwardsfacing triangle in Fig. 4, is found using a simple peak searching routine.\n- Once the valley is found, the mean of the photoelectron peak is determined, as shown by the circle in Fig. 4.\n\nOCR Text: \"Ci? JMCd ncc crannes LLv\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4: An example of a single photoelectron spectrum measured using the setup presented in this article. The mean of the zero photoelectron (upward-facing triangle) and one photoelectron peaks (circle) are marked, as are the valley (downward-facing triangle) and 1/3 of a photoelectron (square)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 8 shows that in the case of Gaussian interuser channels, the loss compared to perfect interuser channels is very small. Furthermore, the performance of the I-JNCC has improved a lot in comparison with Sec. VII-A where H GLNC only was used. The degree distributions causing the poor coding gain of the I-JNCC in Sec. VII-A, are considerably improved by the point-to-point codes.\n\nMentions:\n- Fig. 8 shows that in the case of Gaussian interuser channels, the loss compared to perfect interuser channels is very small.\n\nOCR Text: \"Ieieece Varc_Incc HJNCC EreIcbi\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 8. The word error rate of the SMARC-JNCC is compared to that of the I-JNCC, assuming Gaussian source-relay channels. The reference curve is the performance of the SMARC-JNCC assuming perfect source-relay channels (Sec. VII-A)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- After the direct simulation in the global domain has been completed and the sub-sampled data stored, data at a specific spatial and temporal location (x, t) may be required, for example to examine local flow states in particularly interesting sub-regions of the flow or to track particles through the flow. In general these locations do not correspond to stored data, and the data must be evaluated by re-evaluating the flow evolution in the host sub-volume and time interval (Figure 2).\n- Figure 2. When data is required on gridpoints that are not stored in the database, a re-simulation of a small region which includes the queried grid point is performed to obtain the data.\n- To fix notation, in the rest of this work the \"global domain\" refers to the domain of the original simulation (the black enclosing box in figure 2); a \"sub-domain\" refers to the much smaller region containing a queried point or sets of points (the yellow region in figure 2); and \"re-simulation\" refers to numerical solution of the governing equation in this sub-domain using initial and boundary conditions extracted during the original computation and stored in the STSR database.\n\nMentions:\n- In general these locations do not correspond to stored data, and the data must be evaluated by re-evaluating the flow evolution in the host sub-volume and time interval (Figure 2).\n- Figure 2.\n- To fix notation, in the rest of this work the \"global domain\" refers to the domain of the original simulation (the black enclosing box in figure 2); a \"sub-domain\" refers to the much smaller region containing a queried point or sets of points (the yellow region in figure 2); and \"re-simulation\" refers to numerical solution of the governing equation in this sub-domain using initial and boundary conditions extracted during the original computation and stored in the STSR database.\n\nOCR Text: \"Queried point A simulation of this small region will be performed. grid _\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2. When data is required on gridpoints that are not stored in the database, a re-simulation of a small region which includes the queried grid point is performed to obtain the data."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The proposed PV distribution projection method was applied to optimize the PV distribution in the projected high PV scenarios in the EI system. The horizon featured an interconnection-level PV growth primarily driven by high carbon-emission prices projected in [56]. Input parameters in PV expansion optimization included solar radiation, PV prices, land prices, as well as associated renewable portfolio standards. A summary of input data sources of the PLEXOS model is shown in Table IV. As the main dataset in optimization, the Eastern Interconnection Planning Collaborative (EIPC) Phase I dataset was created by Charles River Associates (CRA) in an effort supported by U.S. Department of Energy involving major stakeholders and planning coordinators in the Eastern Interconnection [56]. It is also compatible with the MMWG dynamic model in representing the current-stage generationtransmission infrastructures. This dataset was translated into PLEXOS Energy Exemplar for multiple EI expansion studies [60,61]. It uses a bubble/pipe model representing 24 EI regions (as shown in Figure 5) and the interfaces between them [62]. The colored regions in Figure 5 represent utilities, regional transmission operators, coordinating authorities, independent system operators or other natural groupings based on the structure of the EI. The commercial mix-integer programming (MIP) solver Xpress-XP was adopted to solve the MIP problem. The PLEXOS optimization result included the generation mix and projected PV capacity distribution for each year in the study horizon, representing multiple PV penetration levels. Based on the PV expansion optimization results, the PV growth in the EI system over the study horizon is shown in Figure 6. It shows a close-to-linearwise increase before some regions' PV has replaced the majority of thermal and nuclear generation. This increase pattern is consistent with some regional PV forecast studies conducted by ISOs and RTOs [63]. Base on the PV growth curve, the PV distribution in four years: 2020, 2024, 2030, and 2035, were selected as the four target penetration scenarios.\n\nMentions:\n- Based on the PV expansion optimization results, the PV growth in the EI system over the study horizon is shown in Figure 6.\n\nOCR Text: \"3 [ 7 2015 2018 2020.2022.2024 2025 2028 2030 2032 20034\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6. The increase of installed PV capacity in EI during 2015-2035"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In order to assess the effectiveness of our immersion cell we measured the resistance of the carbon thermometer mounted onto the heatsinks as a function of mixing chamber temperature in two different configurations. First, we measured the resistance when the cell was intentionally left empty. In this configuration the carbon resistor is cooled only through the copper electrical leads. This configuration mimicks a traditional setup of samplein-vacuum. As shown in Fig. 4, we found that the resistance of the carbon thermometer saturates at 3kΩ below about 30mK. We conclude that in this configuration we are unable to cool our thermometer below 30mK even though our mixing chamber was at 5mK.\n\nMentions:\n- As shown in Fig. 4, we found that the resistance of the carbon thermometer saturates at 3kΩ below about 30mK.\n\nOCR Text: \"cell empty filled with He-3 1 100 [mK]\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 4. Filling of the immersion cell with liquid He-3 causes a large change in the resistance of a carbon thermometer soldered onto the heatsinks from 3kΩ to 30kΩ at the mixing chamber temperature of 5mK."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 6 compares the maximum transmitted power achieved after 50 iterations as a function of thickness L/λ for the iterative, phase-only modulated steepest descent and gradient descent methods and the non-iterative SVD and SDP methods. The non-iterative methods increase transmission by 8.3% relative to the steepest descent method. The gradient descent method performs poorly relative to the steepest descent method but still achieves increased transmission relative to the non-adaptive 'equal-phase' wavefront. Fig. 7 plots the average number of iterations required to reach 95% of the respective optimas for the phase-only modulated steepest descent and gradient descent algorithms as a function of the thickness L/λ of the scattering system. On average the steepest descent algorithm converges in about in about 15−20 iterations while the gradient descent algorithm converges in about 35 − 45 iterations.\n- or, equivalently, Eq. (A1). and M = 395. The system was generated so that when L = 3.4 × 10 5 λ, N c = 430, 000 and l = 6.69λ, where l is the average distance to the nearest scatterer. The empirical average and the one-standard-deviation error bars were computed over 1700 random realizations of the scattering medium. Fig. 5. The average transmitted power versus the number of iterations is shown for steepest descent algorithm, the phase-only steepest descent algorithm and the phase-only gradient descent algorithm for setup described in Fig. 3. Here L = 3.4×10 5 λ. For each of these algorithms the optimal step size µ was chosen by a line search. Phase-only SD Phase-only GD Fig. 6. The average transmitted power obtained after 50 iterations of the phaseonly steepest descent (SD) and gradient descent (GD) methods as a function of L/λ for the setup described in Fig. 3. For comparison, we plot the transmitted power realized by a opt,sdp .\n\nMentions:\n- Fig. 6 compares the maximum transmitted power achieved after 50 iterations as a function of thickness L/λ for the iterative, phase-only modulated steepest descent and gradient descent methods and the non-iterative SVD and SDP methods.\n- Phase-only SD Phase-only GD Fig. 6.\n\nOCR Text: \"x Mrl\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 6. The average transmitted power obtained after 50 iterations of the phaseonly steepest descent (SD) and gradient descent (GD) methods as a function of L/λ for the setup described in Fig. 3. For comparison, we plot the transmitted power realized by aopt,sdp."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- After performing the address vector space mapping, we use Transformer-IPv6 to learn the address vector. To illustrate the effectiveness of Transformer-IPv6, we performed attention visualization work to show the model performance. Figure 6 shows the model's focus on IPv6 addresses. In IPv6 address words, each word has a clear object of attention in the entire address, which enables the associated information in the address to be effectively mined. For example, 1i, 1j, 0k, and 0l may respectively have strong correlations in Figure 6. In addition, the multi-head Fig. 7. The predicted address results with different softmax temperature t. Increasing temperature t will generate targets with more random address words.\n- In our model, softmax temperature is a key parameter that can control the quality of the addresses generated by the model. When selecting a high temperature t, the model tends to sample randomly and the generated address contains more creative sequences. The model is required to sample greedily and the generated address is more close to the seed set when keeping a low temperature t. Figure 7 shows the generation results corresponding to different temperature t. The increase of temperature promotes the generation of address words more diversified. In order to find the equilibrium point, we measured the generation performance corresponding to different t values. The model keeps the highest active hit rate r hit and active generation rate r gen when t = 0.01. We recommend t value between 0-0.05 to ensure the model performance.\n\nMentions:\n- In addition, the multi-head Fig. 7.\n- Figure 7 shows the generation results corresponding to different temperature t.\n\nOCR Text: \"Mitc ai Yeemn mmin Jaln f-auo e WICYRCMT DIR; WitiXECIMT DEih WItYECMUI Drit Tatait ` 5afete nut7au WX\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 7. The predicted address results with different softmax temperature t. Increasing temperature t will generate targets with more random address words."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Since we do not have an exact solution to compare with, we perform a three-level convergence check, i.e., we compute the ratio of the differences in the strain Dh +,e between the three resolutions FIG. 9: Comparison of quadrupole to CCE waveforms for model A3B3G3. As for model A1B3G3 (Fig. 7), the PV variant leads to the smallest difference from CCE (∼ 5%).\n\nMentions:\n- Since we do not have an exact solution to compare with, we perform a three-level convergence check, i.e., we compute the ratio of the differences in the strain Dh +,e between the three resolutions FIG. 9: Comparison of quadrupole to CCE waveforms for model A3B3G3.\n\nOCR Text: \"43J1 E E 59 5 5 [[ TaF Ti\"i Amsi\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 9: Comparison of quadrupole to CCE waveforms for model A3B3G3. As for model A1B3G3 (Fig. 7), the PV variant leads to the smallest difference from CCE (∼ 5%)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Example 15. Let ϕ[x, y] be the BDD shown in figure 4, and let {a, b} be the domain of I σ , P Iσ = {b}, R Iσ = {} and Q Iσ = {(b, b)}.\n\nMentions:\n- Let ϕ[x, y] be the BDD shown in figure 4, and let {a, b} be the domain of I σ , P Iσ = {b}, R Iσ = {} and Q Iσ = {(b, b)}.\n\nOCR Text: \"P(x) RC y) 1\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4: A BDD representing the formula (P (x) ∧Q(x, y)) ∨ (¬P (x) ∧R(x))"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The equatorial slice for the radial pressure p ⊥ is shown in more detail in Fig. 17 for the solution at the turning point and a solution close to it located on the upper branch. The figure illustrates that, when both rotation and radial excitation are present, some equilibrium configurations exhibit (albeit very sightly) a negative radial pressure in the equatorial plane, an effect not seen in the nodeless rotating case.\n\nMentions:\n- The equatorial slice for the radial pressure p ⊥ is shown in more detail in Fig. 17 for the solution at the turning point and a solution close to it located on the upper branch.\n\nOCR Text: \"IUIj ~=1573616 0566765 ~iI\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 17: Radial pressure pr as measured by an observer at rest with respect to the scalar fluid in the equatorial plane for the rotating radially excited solution at the turning point and a solution on the upper branch close to it (one node, m = 1, κ = 0.05). The inset highlights that the radial pressure in the equatorial plane can become slightly negative in the outer region of the outer shell."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 2 shows the monopole and quadrupole of the QPM mock correlation functions pre-reconstruction (blue) and post-reconstruction (red) for the CMASS and LOWZ samples. The shaded region represents the standard deviation of the mock correlation functions around their average, which is displayed with a dashed line. For reference we include the observed correlation functions from Data Release 12 as dotted lines. We note that as in Anderson et al. (2014), the covariance matrix computed from the mock catalogues has been corrected using the corresponding Hartlap factors (Hartlap, Simon &amp; Schneider 2007) for our number of mocks, number of fitting parameters, and number of bins (see Percival et al. 2014).\n\nMentions:\n- Figure 2 shows the monopole and quadrupole of the QPM mock correlation functions pre-reconstruction (blue) and post-reconstruction (red) for the CMASS and LOWZ samples.\n\nOCR Text: \"C\"iSS Feej Jowz dic-iccut (ross corMFco Acst-ercn (RasS [re-efjr 0w? AtF-[Fco1 C#iSS Fos-IECO\" OWZ jcst-erzn\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2. The correlation function of QPM mocks, pre-reconstruction (blue) and post-reconstruction (red). The upper panels show the average and standard deviation (dashed line and shaded regions) of the monopole of the CMASS (left) and LOWZ (right) correlation function of the QPM mocks. Bottom panels display the average and standard deviation of the quadrupole of the QPM correlation functions for CMASS (left) and LOWZ (right). A dotted line shows the clustering of the data for comparison."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- . Note that, in this formulation, we have n(e i , t k ) ≡ 1 for Freebase and YAGO, since the evidence in Freebase and YAGO is deterministic. We also want to leverage the information provided by the document or corpus. Therefore, we can also replace n(e i , t k ) with the frequency n dj (e i , t k ) or document frequency j I dj (e i , t k ) used in the previous two methods. In this case, we mean we construct a sub-knowledge base with the edges being weighted by the evidence shown in the document or the corpus. The probability P (t k |E) is used to rank the entity types and the largest ones are selected. In this case, different entities in a document can be used to disambiguate each other. For each cluster, only the common types are retained, and concepts with conflicts are filtered out. Here we also assume that the type that can best fit Fig. 7: Heterogeneous information network schema. The specified knowledge is represented in the form of heterogeneous information network. The schema contains multiple entity types: document D, word W, named entities {E I } T I=1 , and the relation types connecting the entity types.\n- Then for our world knowledge dependent network, we use the network schema shown in Figure 7 to represent the data. The network contains multiple entity types: document D, word W, named entities {E I } T I=1 , and a few relation types connecting the entity types. Notice that, we use \"entity type\" to represent the node type in HIN, as Definition 4.1 showed. We use \"named entity type\" to represent the type of the name mentioned in text (widely used in NLP community), e.g., person, location, and organization names. The entities in HIN do not have to be named entities, e.g., the categories of animals or diseases. We denote the document set as D = {d 1 , d 2 , . . . , d M }, where M is the size of D, the word set as W = {w 1 , w 2 , . . . , w N }, where N is the size of W, and the Fig. 8: Document based heterogeneous information network example. The network G contains five entity types: document, word, date, person and location, which are represented with gray rectangle, gray round, green square, blue round, and yellow triangle, respectively. entity set as E t = {e t\n- Motivated by ITCC, according to the network schema shown in Figure 7, our problem of HIN clustering is formulated as\n\nMentions:\n- Here we also assume that the type that can best fit Fig. 7: Heterogeneous information network schema.\n- Then for our world knowledge dependent network, we use the network schema shown in Figure 7 to represent the data.\n- Motivated by ITCC, according to the network schema shown in Figure 7, our problem of HIN clustering is formulated as\n- where the equation aims to leverage the link and type information (documents, words, types of named entities) in the HIN shown in Figure 7 to estimate the cluster label of each document.\n- The first part of the problem, D KL (p(D, W)||q(D, W)), according to Eq. 3, is exactly the original ITCC on the document-word bipartite graph (the top portion of the Figure 7 with two types of entities, document and word).\n\nOCR Text: \"Document Word Named Named Engity Tpe _ Typ? = Named Named Entity Enety Type Type Entzy\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 7: Heterogeneous information network schema. The specified knowledge is represented in the form of heterogeneous information network. The schema contains multiple entity types: document D, wordW , named entities {E I}TI=1, and the relation types connecting the entity types."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Finally, we consider the setting where the scatterers are absorptive with a refractive index given by n d = 1.3 − jκ. Here, backscatter minimization as a general principle for increasing transmission is clearly sub-optimal since an input with significant absorption can also minimize backscatter. In Fig. 10, we compare the gain, relative to τ equal , of the transmitted power achieved by the iterative phase-only steepest descent algorithm and non-iterative algorithms that assume knowledge of the S 21 matrix. Specifically, we compare the transmissions achieved by the wavefront produced by the backscatter analysis based steepest descent algorithm, the optimal transmission maximizing wavefront v 1 which requires amplitude and phase modulation and the wavefront a opt,sdp obtained as in Eq. ( 15), except with A sdp defined as the solution of the optimization problem\n- Fig. 10 shows that the iterative method realizes a significant increase in transmission even when the scatterers are weakly absorptive. The iterative algorithm converges rapidly, in about as many iterations as in the lossless setting for the same range of stepsizes\n\nMentions:\n- In Fig. 10, we compare the gain, relative to τ equal , of the transmitted power achieved by the iterative phase-only steepest descent algorithm and non-iterative algorithms that assume knowledge of the S 21 matrix.\n- Fig. 10 shows that the iterative method realizes a significant increase in transmission even when the scatterers are weakly absorptive.\n\nOCR Text: \"eLTi 7m5Fi: FWF-l ~Q1L Gutel {Q)L SY ms WL Fice-45\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 10. Gain in transmitted power relative to aequal versus thickness L/λ for a system setup as described in Fig. 3 except with nd = 1.3 − jκ, where κ is the extinction coefficient."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Apart from the KI scheme, each KC flavor with a variational energy functional is characterized by a different energy with respect to LDA or PBE. This energy change can be both positive or negative, but our results on the G2-1 set show that the energy change is mostly negative in the case of PBE, while positive in the case of LDA, as shown by Figs. 8 and 9. A justification for this fact in the case of the PZ functional comes from the plot of Fig. 10, which shows that, for a hydrogen-1s shaped orbital, the PZ orbital-density-dependent correction for PBE is positive if the orbital has an effective Bohr radius larger than the Bohr radius of the hydrogen atom, while the PZ correction on top of LDA is negative for all radii smaller than approximately 2.5Å. Although the PZ correction is applied on molecular rather than atomic orbitals, the above remark tells us that we should expect the PZ correction on top of PBE to be mostly positive, while the one on top of LDA to be mostly negative, at least for second and third-row elements, all having Bohr radii with values between 0.5 and 2.5Å. An argument similar to the above can be applied to the KIPZ functional, which, for screening coefficient α = 1, has exactly the same energy, for integer particle numbers, as the PZ functional.\n\nMentions:\n- A justification for this fact in the case of the PZ functional comes from the plot of Fig. 10, which shows that, for a hydrogen-1s shaped orbital, the PZ orbital-density-dependent correction for PBE is positive if the orbital has an effective Bohr radius larger than the Bohr radius of the hydrogen atom, while the PZ correction on top of LDA is negative for all radii smaller than approximately 2.5Å.\n\nOCR Text: \"2 1 PZILDA] PZIPBEE\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 10. (Color online) Plot of PZ orbital-densitydependent correction for a 1s orbital with effective Bohr radius aeff . Lines are linear interpolations of a mesh of data. The black line shows the result for LDA, while the blue line for PBE. The two vertical dashed lines mark the values of 0.529 Å(the Bohr radius) and 0.7 Å(approximately equal to the bond length of H2). The correction on top of LDA is negative for aeff / 2.5 Å, while the correction on top of PBE is positive whenever aeff ' aBohr. The black LDA curve is analogous to the one plotted for Gaussian orbitals in the work of Körzdörfer et al.56."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Here, we use the fact that the coherent state decomposition ( 7) is an expansion that displays a hierarchical structure. Indeed, our simulations demonstrate that the weight p M of a newly added coherent state is typically smaller than the majority of the weights p m of the preceding states. This hierarchical structure is clearly apparent in Fig. 8. This feature is exploited as follows in our numerical implementation. The algorithm starts with the solution for a single coherent state (the so-called Silbey-Harris Ansatz) with M cs = 1, which is reliably obtained by a local routine, providing a first estimate of f (1) k . Then the energy is minimized for M cs = 2 with two coherent states, using the previously determined f (1) k as an initial guess, f\n\nMentions:\n- This hierarchical structure is clearly apparent in Fig. 8.\n\nOCR Text: \"s=16 s=U 20\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 8. (Color online) Weights pm of the coherent state |f (m) 〉 in the coherent state expansion (7), as a function of index m, for s = 0.3 (triangles) and s = 0.8 (circles), with the same parameters as in Fig. 1. The fast exponential decay of the weights illustrates the hierarchical structure of the CSE."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To present our results in comparison with the state-of-the-art, we have performed the following experiment: Given a fixed girth g (g = 10, 12), for each size m × n of the exponent matrix, we start by the smallest value of N reported in the literature as providing for a QC-LDPC code obtained from a cyclic lifting of degree N of the fully-connected m × n exponent matrix. For this value of N , we apply the proposed search algorithm to see if we can find an exponent matrix of the form (2) for a QC-LDPC code of girth g. If we succeed, we then reduce the value of N into the nearest smaller integer value for which Z × N contains at least one eligible cyclic subgroup to form P 1 , and repeat the same experiment. We continue until the proposed algorithm fails to provide an answer. At that point, we report the previous value of N along with the corresponding exponent matrix found by the algorithm. These results are presented in Tables IV-VII for values of (m, g) = (3, 10), (3,12), (4,10), and (4, 12), respectively. To present the exponent matrices, we have only provided the second row along with the generator element a of the corresponding cyclic subgroup. In the tables, we have also reported the d c = n, rate and the minimum found lifting degree N . Although our proposed algorithm has the capability to find very high rate codes with girth g = 10, 12, Tables IV-VII contain the codes with lengths below 100K bits. This is because most of the implemented LDPC code in the literature have lengths below 100K bits. In the tables, we have additionally provided the best available results (in terms of minimum N ) in the literature even for the search-based results or the explicit (i.e., deterministic) constructions for comparison. Note that, due to the lack of published results for exponent matrices with a large row degree d c , we apply search algorithm 1 either by considering some proposed lower bounds (of lifting degree N ) in the literature or with our conjecture of lifting degree N as a primary input of this algorithm. If input parameter N is considered as a lower bound then algorithm 1 has to test N every time and moves upward up to the point that it achieves the first success. Otherwise (i.e., if there is no lower bound or upper bound), we need to guess the starting point of N . This conjecture of N comes from studying the general trend of the lifting degree growth rate of previous N s of smaller exponent matrices with the same girth. Here we used nonlinear regression to predict the new input values of N where \"cubic polynomial\" is considered as to be the regressions model (RM). As an example of former situation with a lower bound, we can look at girth g = 10 exponent matrices. When g = 10 and the exponent matrix is of size m × n, there is a lower bound equal to (m 2 −m)(n 2 −n) 2 + 1 for the lifting degree  to the point that it encounters first success. We limited the running time of our search program to 72 hours, and the smallest successful lifting degree was N = 2689 using a core i7 desktop computer with a 3.5 GHz CPU and 8 GB RAM running in parallel. As an example of the later situation where there is no bound for N we can look at girth g = 12 exponent matrices. We   So, if our underlined exponent matrix is of size 3 × n (resp., 4 × n) and there is no bound for the size of N , we estimate it with N RM g=12 m=3 (n) (resp., N RM g=12 m=4 (n) ). As we are not sure if this approximated value of N is a lower bound or upper bound, our search program would be run for two cases in parallel: 1) upward check and 2) downward check. During this process and at a time when the program sees a success by decreasing N , it will terminate the upward manner and will focus only on downward movement. This process is continued until the processing time is over. As a result, for a girth 12 exponent matrix of size (m, n) = (3, 14), we could not find an accurate bound for its lifting degree (see Table V); however, we estimated it as N RM g=12 m=3 (14) = 5040. We ran our search program for it, and after 24 hours of running, the smallest successful N was 4953. The point-to-point growth rate curves to all the values of N found by our search program, by proposed bounds, and by estimations are included in Fig. 10 for further comparison and investigation.\n\nMentions:\n- The point-to-point growth rate curves to all the values of N found by our search program, by proposed bounds, and by estimations are included in Fig. 10 for further comparison and investigation.\n\nOCR Text: \"L o\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 10. Minimum lifting degree N growth rate of new constructed codes versus dc = n for dv = m = 3, 4."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where, CR i (l) is the 'significance bin' i corresponding to the loudest observed candidate in the l-th frequency band, and S h ( f l ) is the average amplitude spectral density of the data in that band, measured in Hz −1/2 . The uncertainties on the h 90% 0 upper-limit values introduced by this procedure amount to roughly 10% of the nominal h 90% 0 upper-limit value. The final h 90% 0 upper-limit values for this search, including an additional 10% calibration uncertainty, are given in table 2, and shown in figure 9.\n- Note that we do not set upper limits in 0.5 Hz bands where the results are entirely produced with fake Gaussian data inserted by the cleaning procedure described in section 4; h 90% 0 upper-limit values for such bands do not appear either in table 2, or in figure 9.\n- Moreover, there also exist 50 mHz bands that contain results contributed by entirely fake data as a result of the cleaning procedure, or that have been excluded from the analysis because they are marked as 'disturbed' by the visual inspection method described in section 4. We mark the 0.5 Hz bands which host these particular 50 mHz bands with empty circles in figure 9. In table 5, we provide a complete list of such 50 mHz bands, highlighting that the upper-limit values do not apply to these bands. Finally, we note that, because of the cleaning procedure, there exist signal-frequency bands where the search results may have contributions from some fake data. We list these signal-frequency ranges in table 4. In line with the remarks in section 4, and for the sake of completeness, table 4 also contains the cleaned bands that featured under Type C in table 5, under the column header \"All Fake Data\".\n\nMentions:\n- The final h 90% 0 upper-limit values for this search, including an additional 10% calibration uncertainty, are given in table 2, and shown in figure 9.\n- Note that we do not set upper limits in 0.5 Hz bands where the results are entirely produced with fake Gaussian data inserted by the cleaning procedure described in section 4; h 90% 0 upper-limit values for such bands do not appear either in table 2, or in figure 9.\n- We mark the 0.5 Hz bands which host these particular 50 mHz bands with empty circles in figure 9.\n- We show in figure 9 that these h 90% 0 upper-limits are about 33% higher than the upper-limits1 set [10] in the same frequency range but using S6 data .\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 9: 90%-confidence upper-limits on the gravitational wave amplitude for signals with frequency within 0.5Hz bands, over the entire sky, and within the spin-down range of the search described in section 3. The empty circular markers denote 0.5Hz bands where the upper-limit value does not hold for all frequencies in that interval; the list of corresponding excluded frequencies is given in table 4. For reference, we also plot the upper-limit results (with non-circular markers) from the only other high-frequency search [10], on significantly more sensitive S6 data. It should be noted that the upper-limits from the PowerFlux search [10] are set at 95%-confidence rather than 90%-confidence level as in this search, but refer to 0.25Hz bands rather than 0.5Hz bands."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Our algorithm relies on the obvious fact that the pair (λ 1 , v 1 ), corresponding to τ = 1, is an eigenpair of A. We therefore want to find an approximation of this pair and to do so, the idea is to start with the eigenpair (λ, v) = (λ 0 , v 0 ) of M = Q 0 and numerically follow the path (Q τ , λ τ , v τ ). This is done by subdividing the interval [0, 1] into subintervals with extremities at 0 = τ 0 &lt; τ 1 &lt; • • • &lt; τ K = 1 and by successively computing approximations (ζ i , w i ) of (λ τ i , v τ i ) by Newton's method. To ensure that these are good approximations, we actually want to ensure that for all i, (ζ i , w i ) is an approximate eigenpair of Q τ i+1 . Figure 2 attempts to convey the general idea. The following pseudocode gives a precise description of how this is done. The letter ξ denotes a constant, namely ξ = 0.001461.\n\nMentions:\n- Figure 2 attempts to convey the general idea.\n\nOCR Text: \"E- 1i,u| 2-0\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: The continuation of the solution path."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Charged-current scattering on electrons in matter creates an effective potential V eff = √ 2G F n e for ν e (but not for ν µ ). This changes the effective squared mass of ν e to grow linearly with the electron number density n e , which in the Sun and the Earth is proportional to the total matter density ρ. In vacuum, the propagation eigenstates -i. e. the eigenstates of the Hamiltonian -are equal to the mass eigenstates. In matter, the additional term V eff modifies the propagation eigenstates as shown in figure 3.3.\n\nMentions:\n- In matter, the additional term V eff modifies the propagation eigenstates as shown in figure 3.3.\n\nOCR Text: \"pumt aa hcme\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3.3: Impact of the presence of matter with density ρ on the neutrino eigenstates. The thin lines show the effective masses of the flavor eigenstates νe (diagonal) and νµ (horizontal), while the thick lines show the evolution of the propagation eigenstates. As discussed in the text, we use a simplified two-flavor picture and ignore the effect of neutral-current scattering on neutrons, which is common to both flavors. (Figure taken from reference [111].)"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- As a universal and simplest oscillatory spiking model in the absence of the feedback, we consider the phase oscillator dϕ/dt = ω, where ϕ ∈ R ( mod 1), and ω = 1 without loss of generality. When the oscillator reaches ϕ = 1 at some moment t, the phase is reset to zero and the oscillator produces a pulse signal. If this signal is sent into a delayed feedback loop [Fig. 1(a)] the emitted pulses affect the oscillator after a delay τ at the time instant t * = t+τ . When the pulse is received, the phase of the oscillator undergoes an instantaneous shift by an amount ∆ϕ = Z(ϕ(t * − 0)), where Z(ϕ) is the phase resetting curve (PRC).\n- is the phase at the moment of the pulse arrival t * j = t j−P + τ [Fig. 1(c)]. Here, P is the number of ISIs between the emission time and the arrival time.\n- where P = [τ /T ], and hence τ − P T = τ (modT ). Figure 1(d) shows the period T as a function of τ for Z ex (ϕ) and q = 5.\n\nMentions:\n- If this signal is sent into a delayed feedback loop [Fig. 1(a)] the emitted pulses affect the oscillator after a delay τ at the time instant t * = t+τ .\n- is the phase at the moment of the pulse arrival t * j = t j−P + τ [Fig. 1(c)].\n- Figure 1(d) shows the period T as a function of τ for Z ex (ϕ) and q = 5.\n- The first scenario takes place at α = 1/P when the multiplier λ = 1 appears, which indicates a saddle-node bifurcation [diamonds in Fig. 1(d)].\n\nOCR Text: \"@ pnase\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. C.1. PRC of the Hodgkin-Huxley model (C.1). The region, where Z ′ (ϕ) < −1 is indicated by red."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 7. Ratio between HCN(5-4) and [CI](1-0), tracing the dense-gas fraction, as a function of the ratio between the infrared and the [CI](1-0) luminosity, tracing the star formation efficiency. The data comes from the same references as in Fig. 6. The red solid line is a linear relation between the dense-gas fraction and the star formation efficiency (DGF ∝ SFE), which normalization has been set to match the mean value of our sample (red star).\n- relation between the DGF and the SFE (with respect to the total gas mass). In Fig. 7, we performed a similar diagnostic using [CI](1-0) instead of CO(1-0) and HCN(5-4) instead of HCN(1-0). Our SPT sources are consistent with the trend of DGF versus SFE found in our local reference sample described in Sect. 4.1.\n- The relation linking our local reference sample and the SPT sources suggests that DGF ∝ SFE (solid red line in Fig. 7). This result agrees with the suggestion by Gao et al. (2007) and Daddi et al. (2010) that the high SFEs in starbursts are directly connected to their DGF. The link between these two quantities is not surprising, if, as suggested by the various physical models cited previously, the amount of dense gas, rather than the total gas reservoir, drives the SFR. Unfortunately, our observations do not allow us to form a conclusion about the mechanism that causes these high DGFs. The sources in our sample are at the high end of the SFE and DGF distribution of our local reference sample. Their impressive SFRs are thus caused by a combination of large gas reservoirs (Aravena et al. 2016) coupled with a high DGF.\n\nMentions:\n- Fig. 7.\n- In Fig. 7, we performed a similar diagnostic using [CI](1-0) instead of CO(1-0) and HCN(5-4) instead of HCN(1-0).\n- The relation linking our local reference sample and the SPT sources suggests that DGF ∝ SFE (solid red line in Fig. 7).\n- In Fig. C.1, we present a version of Fig. 7 using CO(4-3) instead of [CI](1-0).\n- In Sect. 4.2 and Fig. 7, we discussed the link between the star formation efficiency and the dense-gas fraction using a diagram of the form A/C versus B/C.\n\nOCR Text: \"ee 1 I-iuil eacen Kt #achmoll Vrejn [ je e07is SFE reijoem Cmnet Cemamm araameal 3 Sa/Loi: S-ar format &r efficien-y\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 7. Ratio between HCN(5-4) and [CI](1-0), tracing the dense-gas fraction, as a function of the ratio between the infrared and the [CI](1-0) luminosity, tracing the star formation efficiency. The data comes from the same references as in Fig. 6. The red solid line is a linear relation between the dense-gas fraction and the star formation efficiency (DGF ∝ SFE), which normalization has been set to match the mean value of our sample (red star)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Fig. 5, we plot each contribution to the SI effective coupling of DM with a proton. The solid line shows the contribution of the Higgs boson exchanging diagrams, the dashed line indicates the twist-2 type contribution, the dash-dotted line corresponds to the scalar-type contribution (except for the Higgs-exchanging contribution), and the dotted line represents the gluon contribution (except for the Higgs-exchanging contribution). Here we set the Higgs boson mass 3 equal to 125 GeV, and the mass difference between the DM and the first KK quark to be 10 % by hand. We find that all of the contributions have the same sign thus they are constructive. The twist-2 contribution is dominant when M 800 GeV, while that of Higgs boson exchanging process becomes dominant above it. Moreover, although the tree-level quark contributions are dominant, it is found that the gluon contribution is not negligible at all. By using the effective couplings obtained above, we evaluate the SI scattering cross sections. In Fig. 6, we plot the SI cross section of KK photon DM with a proton as a function of the DM mass. Here again, the Higgs boson mass is set to be 125 GeV. We take the cut-off scale Λ = 5/R, 20/R, 50/R from top to bottom. We find that the scattering cross sections reduce as the cut-off scale is taken to be large, as expected. As a result, we obtain the SI scattering cross section which is larger than those obtained by the previous calculations. Note that the DM mass with which the thermal relic abundance is preferred by the WMAP observation [2] is around 1300 GeV [20]. It corresponds to σ SI N =(2.5-5)×10 −47 cm 2 , so the direct detection experiments with ton-scale detectors might be able to probe the DM in the future.\n\nMentions:\n- In Fig. 6, we plot the SI cross section of KK photon DM with a proton as a function of the DM mass.\n\nOCR Text: \"Mn =125 GeV E 10*45 I 37 AC 6Q0 801 0O0 1200 1400 130 DM Mass (GeV) 1 50 R\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6. Spin-independent cross section with proton for mh = 125 GeV. Each line corresponds to Λ = 5/R, 20/R, and 50/R, respectively."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- It is evident that parametric fitting does not remove the foreground completely, thus resulting in foreground leaking into the reconstructed cosmological signal. To highlight the comparison between parametric fitting and PCA, we show in Fig. 8 the dispersion, pixel by pixel, between the recovered HI signal as a function of the input HI signal, obtained from parametric fitting and from PCA. This plot shows that parametric fitting is much less effective than the PCA method and induces a bias in the reconstructed HI signal.\n\nMentions:\n- To highlight the comparison between parametric fitting and PCA, we show in Fig. 8 the dispersion, pixel by pixel, between the recovered HI signal as a function of the input HI signal, obtained from parametric fitting and from PCA.\n\nOCR Text: \"rM-Ic hlli-o\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 8. Scatter plot of the recovered HI signal from parametric fitting (blue markers) and from principal component analysis (red markers) as a function of the input HI signal. The parametric fitting is more noisy than the principal component analysis and also appears to be biased. The black dashed line represents the perfect correlation between the recovered HI signal and the true signal."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In the sequel, the \"instantaneous\" SNR estimation results are presented for the first subcarrier only, but they actually  hold the same irrespectively of the subcarrier index. Moreover, all the results are obtained for complex channels since, in practice, the baseband-equivalent representation of the channel coefficients in the discrete model ( 1) is always complex. We will also consider QPSK and 16-QAM as representative examples for constant-envelope and non-constant-envelope constellations, respectively. First, we verify from Fig. 2 that the analytical variance of the unbiased ML DA estimator which we developed in (29) coincides with its NMSE computed empirically using Monte-Carlo computer simulations. The small discrepancies between them is due to lack of averaging.\n\nMentions:\n- First, we verify from Fig. 2 that the analytical variance of the unbiased ML DA estimator which we developed in (29) coincides with its NMSE computed empirically using Monte-Carlo computer simulations.\n\nOCR Text: \"Jn ne en Jnid j09 Maane Id iteal 4673\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2. NMSE (empirical) and normalized variance (analytical) of the unbiasedDA ML estimator vs. the average SNRγ, with Nr = 2, N = 112, N̄DA = 112, N̄NDA = N/2 = 56, FDTs = 7× 10−3 andL = 4, 16-QAM."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The rest of the paper is organized as follows. Section 2 presents background on neural networks and SMT solvers. Section 3 introduces the theory of local and global robustness to parameter perturbations and Section 4 details the corresponding SMT problem formulations. Section 5 presents the case studies and their corresponding trained neural networks. Section 6 presents robustness analysis on the neural networks. Section 7 reviews related work and Section 8 presents our conclusions and the directions for future work. Every node of an NN performs two operations: weighted averaging of the inputs, and a nonlinear transformation of the weighted sum using a so-called activation function, see Fig. 2. Some of the commonly used activation functions are depicted in Fig. 3.\n\nMentions:\n- Every node of an NN performs two operations: weighted averaging of the inputs, and a nonlinear transformation of the weighted sum using a so-called activation function, see Fig. 2.\n\nOCR Text: \"Inputs   Weights Met input Actvation Fung tion Hunchoh Oluut\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2. Weighted averaging, followed by nonlinear activation."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- which was already detected in the first two-particle cut. Therefore (3.6) alone comprises the full result for this cut. We now move on to investigate three-particle cuts. The first case we want to consider is shown in Figure 2. This three-particle cut is given by\n\nMentions:\n- The first case we want to consider is shown in Figure 2.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: A possible three-particle cut of F (2)3,3 ."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We begin with Fig. 1. A finite sized 1D conductor with N atomic sites is attached to two semi-infinite 1D metallic electrodes, viz, source and drain. At much low temperature and bias voltage, conductance g of the conductor can be written by using Landauer conductance formula,\n- is the creation (annihilation) operator of an electron at site i, ǫ i is the site energy of an electron at the i-th site and t corresponds to the nearest-neighbor hopping integral. A similar kind of tight-binding Hamiltonian is also used for the description of electrodes where the Hamiltonian is parametrized by constant on-site potential ǫ 0 and nearest-neighbor hopping integral v. In Eq. 4, Σ S = h † Sc g S h Sc and Σ D = h Dc g D h † Dc are the self-energy operators due to the two electrodes, where g S and g D are the Green's functions for the source and drain, respectively. h Sc and h Dc are the coupling matrices and they will be non-zero only for the adjacent points in the conductor, 1 and N as shown in Fig. 1, and the electrodes respectively. The coupling terms Γ S and Γ D of the conductor can be calculated from the following expression,\n- Since the coupling matrices h Sc and h Dc are non-zero only for the adjacent points in the conductor, 1 and N as shown in Fig. 1, the transmission probability becomes,\n\nMentions:\n- We begin with Fig. 1.\n- In Eq. 4, Σ S = h † Sc g S h Sc and Σ D = h Dc g D h † Dc are the self-energy operators due to the two electrodes, where g S and g D are the Green's functions for the source and drain, respectively. h Sc and h Dc are the coupling matrices and they will be non-zero only for the adjacent points in the conductor, 1 and N as shown in Fig. 1, and the electrodes respectively.\n- Since the coupling matrices h Sc and h Dc are non-zero only for the adjacent points in the conductor, 1 and N as shown in Fig. 1, the transmission probability becomes,\n\nOCR Text: \"Mu; (HMsy Le\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 1: Schematic view of a one-dimensional conductor with N number of atomic sites attached to two electrodes, namely, source and drain. The atomic sites at the two extreme ends of the conductor are labeled as 1 and N , respectively."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- As a last consideration let us now turn off the quantum dissipation. Since quantum dissipation is described by the term ∝ e ik•x/2 ik•∇ 4MT e ik•x/2 , we can switch it off by taking the M → ∞ limit in the QSD equation everywhere, except in the Hamiltonian part. In Fig. 6, we show the corresponding time evolution of the momentum distribution as given by the QSD equation without dissipation. Clearly, the distribution does not approach the equilibrium Boltzmann distribution. Instead the energy gained by the heavy quark from thermal fluctuations is not dissipated back to the system and the heavy quark overheats. In Fig. 7, we show the time evolution of the eigenstate occupation as given by the QSD equation without dissipation. The lowest three levels become equally occupied regardless of the energy gaps 2 . It is expected that not only these levels but all the levels are eventually occupied equally if the quantum dissipation is neglected. We also observe that the effect of quantum dissipation at early time strongly depends on the external potential. This dependence can be understood by analyzing the Lindblad equation as follows. The initial decay rate of an eigenstate ψ i of the Hamiltonian is given by\n\nMentions:\n- In Fig. 7, we show the time evolution of the eigenstate occupation as given by the QSD equation without dissipation.\n\nOCR Text: \"JCU 120@   '408 repulzed Cosemb;0-02 who Jies W Jiss , MX 10C_ 2030 3C00 480 S_D_\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 7. Time evolution of the occupation number of the eigenstates without dissipation. For comparison, the time evolution with dissipation is also plotted. The bars denote statistical errors."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- C. How we solve for p and pi. To solve for p in Equation 7and pi in Equation 11, as we do for the examples in Equation 8and Equation 12we developed a software tool to compile Equation 7and Equation 11 to constraints in the SMT-2 format. We then use the dReal SMT solver (13) to solve for p in Equation 8or pi in Equation 12. Fig. 6. Empirical probability mass functions (i.e., f V (v) from Figure 7) for 29 978 samples (25 minutes of sensor data sampled at 20 Hz) from the three axes of an accelerometer worn by a person engaged in a variety of activities (14).\n- Figure 7 shows the analytical formulation for fM (m) when the channel induces single bit errors (Equation 13) and multiple bit errors (Equation 14). Our goal is to obtain the function f t i which satis es a given constraint FM (m) on channel integer distortion.\n- [14] Fig. 7. Analytic relations between the probability distribution of error-free values (f V (v)), the distribution of channel errors (f t i (k)), and the distribution of value deviations (f M (m)), from (15), for the cases of a singly-occurring error (i.e., k = 1) assumption (Equation 13) and a multiply-occurring error (i.e., k ≥ 1) assumption (Equation 14).\n\nMentions:\n- Empirical probability mass functions (i.e., f V (v) from Figure 7) for 29 978 samples (25 minutes of sensor data sampled at 20 Hz) from the three axes of an accelerometer worn by a person engaged in a variety of activities (14).\n- Figure 7 shows the analytical formulation for fM (m) when the channel induces single bit errors (Equation 13) and multiple bit errors (Equation 14).\n- [14] Fig. 7.\n\nOCR Text: \"Xl: EL, luscissux EIoos! [~o Mxti' ~KWk\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 7. Analytic relations between the probability distribution of error-free values (fV (v)), the distribution of channel errors (ft〈i〉(k)), and the distribution of value deviations (fM (m)), from (15), for the cases of a singly-occurring error (i.e., k = 1) assumption (Equation 13) and a multiply-occurring error (i.e., k ≥ 1) assumption (Equation 14)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- to give 1[x &gt; i] (as in the proof of Lemma 5.2); this is split into F (i + 1) − F (i) copies, while the output (x−R) + of the last delayer is fed into a network emulating G, and all the results are added. See Figure 10 for an example. 6.2. Two-layer functions. We now proceed with a simple case of the inductive step, which provides a prototype for the main argument, and which will also be used as a step in the main argument. Lemma 6.2. Let P be a k-ary abelian processor that computes a function F , and suppose that F (y, z) = F (y, z ) if z, z ≥ 1.\n\nMentions:\n- See Figure 10 for an example. 6.2.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 10. Emulating a transient unary processor. (In this example, the difference F (i + 1) − F (i) takes values 0, 3, . . . , 1 for i = 0, 1, . . . , R− 1)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We only consider the O(α 3 α s ) corrections in our analysis. At this order, the process (3.23) receives a contribution only from the interference of EW with QCD amplitudes, as illustrated in Fig. 8. Owing to colour conservation, this interference term is only non-zero for identical quark flavours and for σ ′ = σ ′′ . It is non-singular over the full phase space defined by the event-selection cuts.\n\nMentions:\n- At this order, the process (3.23) receives a contribution only from the interference of EW with QCD amplitudes, as illustrated in Fig. 8.\n\nOCR Text: \"V 3Z\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 8: Sample diagram for a non-trivial interference between EW and QCD diagrams in e+e− → qq̄qq̄."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The most important practical result is the raw accuracy of the filter: for each trajectory, which proportion of the paths or of the points was correctly identified? These results are presented in Figure 12 and Figure 13. As expected, the error rate is 0 for high frequencies (low sampling period): all the points are correctly identified by all the algorithms. In the low frequencies (high sampling periods), the error is still low (around 10%) for the trained models, and also for the greedy model (\"Hard closest point\"). For sampling rates between 10 seconds and 90 seconds, trained models (\"Simple\" and \"Complex\") show a much higher performance compared to untrained models (\"Hard closest point\", \"Closest point\" and \"Shortest path\").\n\nMentions:\n- These results are presented in Figure 12 and Figure 13.\n\nOCR Text: \"0.01 70 Sampling ocriod (scconds) 1\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 13. Path misses using the Viterbi reconstruction for different models and different sampling rates, as a percentage on each trajectory (lower is better). The solid line denotes the median, the squares denote the mean and the dashed lines denote the 98% percentiles. The error rate is close to 0 for high frequencies: the paths are correctly identified. In higher sampling regions, there are many more paths to consider and the error increases substantially. Nevertheless, the probabilistic models still perform very well: even at 2 minute intervals, they are able to recover about 75% of the true paths. In particular, in these regions the shortest path becomes a viable choice for most paths. Note how the greedy path reconstruction fails rapidly as the sampling increases. Also note how the shortest path heuristic performs poorly."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 2 shows the runtimes of the original algorithm (when applied to the unabstracted game) and our proposed approach. We can observe that while for small instances, the original approach is competitive and can even outperform our proposed approach, for larger instances with large state spaces (the number of states in the game is up to 2 |V |−2 + 1) the compact representation used in our approach turns out to be beneficial. Moreover, the original approach has been unable to solve 50% of the 11-vertex instances due to its memory requirements. In contrast, our approach relying on the summarized abstraction based on marginal probabilities has not exceeded 1GB of memory consumption even when applied to the most challenging 17-vertex instances.\n- In this section, we focus on analyzing the abstraction quality. In general, the summarized abstraction may lose important (in %) 0.8 0.9 1.0 0.7 0.7 0.5 information needed to derive the optimal behavior. We show, however, that with properly designed characteristic vectors the quality loss can be minimal. The scalability issues of the original approach (see Figure 2) make it hard to determine the value V * (b 0 ) of the original, unabstracted game. Therefore we present the quality analysis only for graphs with 5 ≤ |V | ≤ 10 vertices where the original approach solved all instances considered. In Table 1, we present the empirical bound on the relative distance from the equilibrium of the unabstracted bound. We compare the upper bound V (b 0 ) computed by the original approach for the unabstracted game with the lower boundṼ LB (χ 0 ) on the quality of the strategy when the abstraction is used. We depict the maximum relative distance based on the 100 randomly generated instances for each size of a graph. Observe that in all of the cases, the empirical upper bound on the relative distance (i.e., the worst-case possible quality loss due to abstraction) is below 1.0%. Note, that in all of the instances the quality of the strategy found by our novel algorithm is within the bounds of the original approach (i.e., the empirical bounds are likely to be overestimated).\n\nMentions:\n- Figure 2 shows the runtimes of the original algorithm (when applied to the unabstracted game) and our proposed approach.\n- The scalability issues of the original approach (see Figure 2) make it hard to determine the value V * (b 0 ) of the original, unabstracted game.\n\nOCR Text: \"2 10\" 104 1 10\" I0-1 3 Original (unabstracted) LU\" Our approach (abstracted) 10 11 12 13 1415 I6 17 Numbet of vertices V\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: Comparison of the runtime of the original (unabstracted) approach with the proposed one (using summarized abstraction). Confidence intervals mark standard error."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Top-K Ingest Index: To keep recall high, Focus indexes each object using the top K object classes from CheapCNN i 's output, instead of using just the top-most class. Recall from §2.1 that the output of the CNN is a list of object classes in descending order of confidence. We empirically observe that the top-most output of the expensive GT-CNN is often contained within the top-K classes output by the cheap CNN (for a small value of K relative to the 1, 000 classes recognized by the CNNs). Figure 5 plots the effect of K on recall on one of our video streams, lausanne (see §6.1). The three models in the figure are ResNet18 [38], and ResNet18 with 3 and 5 layers removed; additionally, the input images were rescaled to 224, 112, and 56 pixels, respectively. All models were retrained on their original training data (Im-ageNet [63]). We make two observations. First, we observe steady increase in recall with increasing K, for all three CheapCNNs. As the figure shows, CheapCNN 1 , CheapCNN 2 , and CheapCNN 3 reach 90% recall when K ≥ 60, K ≥ 100, and K ≥ 200, respectively. Note that all these models recognize 1000 classes, so even K = 200 represents only 20% of the possible classes. Second, there is a trade-off between different models -the cheaper they are, the lower their recall with the same K. Overall, we conclude that by selecting the appropriate K, Focus can achieve the target recall.\n- Since the specialized CNN classifies across fewer classes, they are more accurate, which allows Focus to select a much smaller K (for the top-K ingest index) to meet the desired recall. We find that specialized models can use K = 2 or 4, much smaller than the typical K = 60~200 for the generic cheap CNNs (Figure 5). Smaller K directly translates to fewer objects that have to be classified by GT-CNN at query time, thus reducing latency. Model Retraining: On each video stream Focus periodically obtains a small sample of video frames and classifies their objects using GT-CNN to estimate the ground truth of distribution of object classes for the video (similar to Figure 3). From this distribution, Focus selects the most frequently occurring L s object classes to retrain new specialized models. As we saw in §2.2.2, there is usually a \"power law\" in the distribution of classes -just a handful of classes account for a dominant majority of the objectsthus, low values of L s usually suffice. 1 Specialization is also based off a family of CNN architectures (such as ResNet [38], AlexNet [45], and VGG [66]) with different number of convolution layers, similar to §4.1. Specialization adds to the set of options available for ingest CNNs ({CheapCNN 1 , ..., CheapCNN n } in §4.1), and Focus picks the best model (CheapCNN i ) and the corresponding K for the index. \"OTHER\" class: While Focus specializes the CNN towards the most frequently occurring L s classes, we also want to support querying of the less frequent classes. For this purpose, Focus includes an additional class called \"OTHER\" in the specialized model. 2 Being classified as OTHER simply means not being one of the L s classes. At query time, if the queried class is among the OTHER classes of the ingest CNN's index, Focus extracts all the clusters that match the OTHER class and classifies their centroids through the GT-CNN model. The parameter L s (for each stream) exposes the following trade-off. Using a small L s allows us to train a simpler model with cheaper ingest cost and lower query-time latency for the popular classes, however, it also leads to a larger fraction of objects falling in the OTHER class; querying for them will be expensive because all those objects will have to be classified by the GT-CNN. Using a larger value of L s , on the other hand, leads to a more expensive ingest and query-time models, but cheaper querying for the OTHER classes. We select L s next in §4.4.\n\nMentions:\n- Figure 5 plots the effect of K on recall on one of our video streams, lausanne (see §6.1).\n- We find that specialized models can use K = 2 or 4, much smaller than the typical K = 60~200 for the generic cheap CNNs (Figure 5).\n\nOCR Text: \"]GheaCNR L47w  € gheaCWN2 288  @Chez_CHH3 588 VX 8k Hu 2 4K 208 0 Hutaer ofsele\"teu resu ts K\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: Effect of K on recall for three cheap CNNs. The number within the parenthesis indicates how much cheaper the model is compared to our GT-CNN, ResNet152."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Co-AMC, a base CNN is first trained by signals from each receive antenna, and then the decision on the modulation type is cooperatively made based on outputs of trained CNNs from all branches. Direct averaging (DA) method is used due to the identical signal distribution of each receive antenna in our experiment, where each branch of predicted distribution is averaged to make a cooperative prediction as Fig. 4. Also, these CNNs share the same parameters and have the same structure.\n\nMentions:\n- Direct averaging (DA) method is used due to the identical signal distribution of each receive antenna in our experiment, where each branch of predicted distribution is averaged to make a cooperative prediction as Fig. 4.\n\nOCR Text: \"PeFmoessg = Trana| CNK PeFmzesrg = TrarcoCH Conporave Dejison {PwFwewrg Tune_CKN\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4. The decision process of cooperative automatic modulation classification (Co-AMC) for multiple-antenna modulation recognition."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- As an example, the final reflection coefficient at the structure input port for a seven-cell X-Band structure is reported in Fig. 23. The coupler in this example has been tuned at 11.424 GHz. The finite number of reflection coefficient minima is given by the resonant SW patterns generated in the structure by the reflections at the input and output couplers. In fact, the coupler has a finite bandwidth (typically 50 MHz at 11 GH Z ) and, over this, there are reflections of the travelling wave from the input and output couplers. The minima are located in the pass-band of the periodic structure and their number is equal to the number of cells. By increasing the number of cells we progressively increase the number of minima in the pass-band.\n\nMentions:\n- As an example, the final reflection coefficient at the structure input port for a seven-cell X-Band structure is reported in Fig. 23.\n\nOCR Text: \"Gokrd rccucrc; {requency ICHz]\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 23: Reflection coefficient at the coupler as a function of frequency in the case of a seven-cell structure operating at 11 GHz"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- if i == 1 then end if 27: end for To compare the accuracy of this method with DEIM, we use Algorithm 5 to compute DEIM and modify line 22 to use Algorithm 6 with n g = 2n deim for a range of values of n deim , so that there are two indices selected for each v i in the gappy algorithm. We use both methods to approximate the nonlinear component and solve the resulting models. Figure 4.4 shows the error indicators as functions of n deim for both methods. It is evident that for smaller number of basis vectors the gappy POD provides additional accuracy. However, for larger number of basis vectors, the additional accuracy provided by gappy POD is small. Thus, the gappy POD method can be used to improve the accuracy when the number of basis vectors is limited. Since S is generated using the Full(k s ) described in Section 4.1, no additional accuracy is gained for the DEIM method when n deim &gt; 102. 5. Iterative methods. We have seen that the DEIM generates reduced-order models that produce solutions essentially as accurate as the reduced solution. In addition, Table 4.1 and Figure 4.3 illustrate that as expected, the DEIM model significantly decreases the online time spent constructing the nonlinear component of the reduced model. Since Q T u N ( u)Q u has been replaced by a cheap approximation, L T P T N ( u)Q u , the remaining cost of the nonlinear iteration in the DEIM is that of the linear system solve in line 2 of Algorithm 4.\n\nMentions:\n- Figure 4.4 shows the error indicators as functions of n deim for both methods.\n\nOCR Text: \"Can- *C Ted.ed\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4.4: Average error indicator as a function of basis vectors for reduced, DEIM, and gappy POD methods. For n = 32, m = 4, τ = 10−4, k = 306, ndeim varies, and ng = 2ndeim. Averaged over ns = 100 samples."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 1 illustrates the two frames that represent a triangular formation created by three UAVs, the formation and the inertial frames. The formation frame, {x F , y F , z F }, is determined such that its origin P F is selected to be coincident with the triangle centroid. Denoting P n = {x n , y n , z n }, n = 1, 2, 3, as the position of UAV n and d n as the distance between UAV n and P F . The formation is then represented by the position P F computed as:\n\nMentions:\n- Figure 1 illustrates the two frames that represent a triangular formation created by three UAVs, the formation and the inertial frames.\n\nOCR Text: \"UAVZ UAV3 inertiol Kramie UAVI\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1: UAV formation frames"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The primary motivation for this work is the excess of low-energy electron-neutrino-like events observed by the MiniBooNE experiment [1]. MiniBooNE is a mineral oil Cherenkov detector in the Booster Neutrino Beam (BNB) at Fermilab. The BNB is a primarily ν µ beam with a small intrinsic ν e component and has an average neutrino energy of about 800 MeV [2]. As shown in Figure 1, the MiniBooNE experiment saw an approximately 3σ excess of electron-neutrino-like events with a reconstructed neutrino energy of 200 to 600 MeV. However, this excess is in tension with global fits to a 3 + 1 sterile neutrino model [3].\n- MicroBooNE is well-positioned to investigate this anomaly. The MicroBooNE detector, a liquid argon time projection chamber (LArTPC), is in the same beam and is at a similar baseline. However, the LArTPC detector technology provides significantly better ability to distinguish photons from electrons and to reject other backgrounds. This will reduce the level of the backgrounds in a MicroBooNE electron neutrino appearance plot comparable to Figure 1, especially the contributions that are related to photon misidentification in a Cherenkov detector (shown in red and yellow in the stacked background histogram). Figure 1: Plot of the number of electron-neutrino-like events observed by the MiniBooNE experiment as a function of reconstructed neutrino energy [1]. The excess in the data compared to the stacked backgrounds in the 200 to 600 MeV energy range is highlighted.\n\nMentions:\n- As shown in Figure 1, the MiniBooNE experiment saw an approximately 3σ excess of electron-neutrino-like events with a reconstructed neutrino energy of 200 to 600 MeV.\n- This will reduce the level of the backgrounds in a MicroBooNE electron neutrino appearance plot comparable to Figure 1, especially the contributions that are related to photon misidentification in a Cherenkov detector (shown in red and yellow in the stacked background histogram).\n- Figure 1: Plot of the number of electron-neutrino-like events observed by the MiniBooNE experiment as a function of reconstructed neutrino energy [1].\n\nOCR Text: \"HIcM 4y6 Mit Cotsl %es1. Encr (GcV)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Plot of the number of electron-neutrino-like events observed by the MiniBooNE experiment as a function of reconstructed neutrino energy [1]. The excess in the data compared to the stacked backgrounds in the 200 to 600 MeV energy range is highlighted."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 2: The graphical model of the EigenNet.\n- Finally we can assign Gamma priors on all the hyperparameters, λ 1 , λ 2 , and λ 3 . The whole model is depicted in the graphical model in Figure 2.\n\nMentions:\n- Figure 2: The graphical model of the EigenNet.\n- The whole model is depicted in the graphical model in Figure 2.\n\nOCR Text: \"W;W F;,0 F3L,\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: The graphical model of the EigenNet."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where W is the line FWHM obtained from the fit, W th is the thermal width of the line calculated by using the atomic parameters from CHIANTI v7.1, W I is the instrumental FWHM, λ is the rest wavelength and c is the speed of light. Fig. 10 summarizes the results of the Doppler shifts observed by IRIS and EIS during the impulsive phase of the flare. In particular, the line position of Fe XXI (orange), Fe XXIII (blue, triangle) and Si IV (green) are plotted as a function of time. The Fe XXIII velocity values refer to the fit results of the most blue-shifted component in the EIS spectra shown in Fig. 9. The Fe XXI and Si IV centroid positions have been obtained by fitting the spectra averaged over each detector slice in Fig. 8 over time.\n- The atmospheric responses to the heating are shown in Fig. 19, showing the electron density, electron temperature, bulk flow velocity, and energy deposition as functions of position, for a few selected times. The majority of the heating begins around 1000 seconds into the simulation, when the temperature begins to rapidly rise from an initial coronal temperature of less than 1 MK to well over 20 MK. As the chromosphere is heated up to chromospheric values, chromospheric evaporation begins to cause strong flows of material into the corona. These flows quickly raise the electron density by more than two orders of magnitude in the corona, to above 10 11 cm −3 . Fig. 20 shows the electron density and temperature at the apex of the loop as functions of time. The temperature has two distinct peaks, one around 22 MK and the other around 19 MK, corresponding to the two heating events seen in the X-ray light-curves. The density increases to above 10 11 cm −3 , and remains roughly constant during the duration of the flare, until heating ceases and the loop begins to cool and drain at late time. After approximately 4000 seconds, the loop cools catastrophically . From this simulation, three spectral lines were forward modeled using the method of Bradshaw &amp; Klimchuk (2011) with atomic data from CHIANTI v7.1: Fe XXIII 263.77Å as might be seen by EIS, Fe XXI 1354.08Å and Si IV 1402.77Å as might be seen by IRIS. The lines were fitted with Gaussians at every point in time, from which Doppler shifts were measured. Fig. 21 shows synthesized Doppler shifts for the 3 spectral, which can be compared directly with the observed quantities in Fig 10 . We emphasize that the basic duration of evaporation and magnitude of the flows are consistent with the observations. 6.3. Comparison of the derived plasma parameters with those in the HYDRAD simulation In this section we compare the plasma parameters derived from Sects. 4 and 5 with the outcomes of the HYDRAD 1D hydrodynamic simulations of a flare loop heated by an electron beam. and then the evaporation further increases to ∼ 200 km s −1 before gradually decreasing, as shown in Fig. 10. This trend would suggest that our results are in agreement with the HYDRAD simulation obtained assuming ionization equilibrium (Fig. 21, top panel). However, as explained in Sect. 4, we are not able to observe a Fe XXI profile before 14:18:37 UT, due to the strong FUV continuum dominating the O I spectral window. Hence, we cannot definitively confirm whether the Fe XXI blueshifts trend is consistent with the ionization equilibrium rather than the non-equilibrium simulation. We note that the time of the maximum upflows in the simulation is predicted at ∼ 1400 s after the start of the heating at 14:00 UT (Fig. 21), while the maximum Fe XXI and Fe XXIII blueshifts are observed at around 1100-1200 s after 14:00 UT (Fig. 10). Nevertheless, the amplitude of the flows and the trends are in very good agreement, and this is particularly pleasing considering that we are modelling a complex flare geometry with a single one-dimensional loop.\n- (b) The Si IV 1402.77Å sythesized velocities in Fig. 10 (green) show that the transition region line is red- shifted by less than 20 km s −1 during the impulsive phase. This is consistent with the IRIS observation that the only significant Si IV redshift is ∼ 16 km s −1 at around 14:18 UT (see Fig. 10).\n\nMentions:\n- Fig. 10 summarizes the results of the Doppler shifts observed by IRIS and EIS during the impulsive phase of the flare.\n- Fig. 21 shows synthesized Doppler shifts for the 3 spectral, which can be compared directly with the observed quantities in Fig 10 .\n- Comparison of the derived plasma parameters with those in the HYDRAD simulation In this section we compare the plasma parameters derived from Sects. 4 and 5 with the outcomes of the HYDRAD 1D hydrodynamic simulations of a flare loop heated by an electron beam. and then the evaporation further increases to ∼ 200 km s −1 before gradually decreasing, as shown in Fig. 10.\n- We note that the time of the maximum upflows in the simulation is predicted at ∼ 1400 s after the start of the heating at 14:00 UT (Fig. 21), while the maximum Fe XXI and Fe XXIII blueshifts are observed at around 1100-1200 s after 14:00 UT (Fig. 10).\n- (b) The Si IV 1402.77Å sythesized velocities in Fig. 10 (green) show that the transition region line is red- shifted by less than 20 km s −1 during the impulsive phase.\n\nOCR Text: \"EXDl 1 \"F5 2 Tala' Iron 41l\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 10. Doppler shift as a function of time of IRIS 1354.08 Å, Si IV 1402.77 Å and EIS Fe XXIII 263.77 Å. In Sect. 6.3, we compare the values of velocity derived from observations with the simulated results of Fig.21."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- It is known then when the size of the dataset is too small for training our AlexNet-style convolutional network, it will result in over-fitted problem. To prove that the network is over-fitted, we first used 198 128x128 images to show the test accuracy of the network in Fig 3   In order to further compensate the potential problems associated with small training datasets for image classification and pattern detection, we use Generative adversarial networks (GANs) for data augmentation. GANs is a generative framework that consists of two competing networks: a generator network and a discriminator network. We use a particular form of GAN, called Wasserstein GAN. We use 3 convolutional layer alex-style network structure for both the generator and the discriminator. The discriminator network has the same network structure as our classifier shown in Fig 2.\n\nMentions:\n- To prove that the network is over-fitted, we first used 198 128x128 images to show the test accuracy of the network in Fig 3   In order to further compensate the potential problems associated with small training datasets for image classification and pattern detection, we use Generative adversarial networks (GANs) for data augmentation.\n\nOCR Text: \"Wc 20 DD EdED Wd Trer Qd\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig 3. Overfitting problem. Accuracy improved with smaller network (green)"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The coupling parameter ( ) changeswith the coupler length 2 and also the mismatch coefficient variesfrom −Δ 0 to +Δ 0 . With both Δ and being z dependent, the coupler design results in tapered structure ofthe waveguides. Moreover the variation of Δ( ) should be slowenough to accomplish adiabatic evolution. Also for thechoices in Eq. ( 7) and Eq. ( 8), the adiabatic conditionis given by 0 ≫ and hence it demands thecoupler length to be large.The schematic of the proposed coupler is shown in Fig. 1.\n\nMentions:\n- ( 7) and Eq. ( 8), the adiabatic conditionis given by 0 ≫ and hence it demands thecoupler length to be large.The schematic of the proposed coupler is shown in Fig. 1.\n\nOCR Text: \"Guide Cuide Counlino dettieem quidcs\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 1. (Color online) Schematic for adiabatic directional coupler with Allen-Eberly scheme. 𝛽1 and 𝛽2are propagation constants for waveguide one and two respectively. Coupler length is 2𝐿. Maximum of the coupling occurs at 𝐿 = 0."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- For short GRB pulses intrinsic lags are in general considered smaller that those of long GRB pulses. Looking for eventual Lorentz invariance violation (LIV), the authors in [43] have analyzed 15 short pulses (on the energy bands 50-100 and 150-200 KeV) concluding that there is no evidence 6 for energy dependence of the light propagation speed. Here the same analyzing technique as described above has been applied to the same data with a different conclusion, as shown in Fig. 5\n\nMentions:\n- Here the same analyzing technique as described above has been applied to the same data with a different conclusion, as shown in Fig. 5\n\nOCR Text: \"La-in\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: Lags/(1+z) versus I2(z) (1+z)3 for the 15 short GRB’s in [43] and fitting lines with 3 (red) or one intersect (green)"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To be able to work with any kind of application scenario, NViz allows user to define their packet formats in a packet specification file. Fig. 5 represents an example of packet specification file.\n\nMentions:\n- To be able to work with any kind of application scenario, NViz allows user to define their packet formats in a packet specification file. Fig. 5 represents an example of packet specification file.\n\nOCR Text: \"{PacketSpec ticutica RyctetICLerg: {Kackot Ts'] ceacripeion=\" Ab8ociate (Eleld Fype-\"Bode\"> Scxcendlreda</Field (Fleld Tn-'\" expe-\"Vate\" > Deseinaticaiclres:< /Field 4eld [D=t1 3og= Linc Llarsttonethti;-0Ld3 (Flalc Fipe-\" Fode\" > cdeFunccion<{Field {Rl:ked {PackrC T-' 7 Concrioeian-\"LjilerTnnoiraturo\"> rhleld ID=\"0 ' Fxpe=\" Eode\" >Ncdenddreas< /Fielo (Fleld ID-\"3\"' eype-\"Bode\" > VBes< {Eleld> ealo MeJim Jcnrcraturc< Ficld> {ipicka?) {PackeC Cegcripelon-\"Cpiace Function ' (Fleld Iadc\"  cdaidiras< Ficlo; €held [oz'L 7ipe Eane\"uecticn<ifielo {pinker ptacet cejcripilan= Ljiec Wimiecoant teld 37o0 LT\" ~Chinralt?-0_d> (Fleld EAVI\" > PAIDG Z1e-D {nliked 4hckeleancificicions Ft'\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5. An example Packet Specification XML file"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To calculate the state transition matrix, we convert HMM-N to an equivalent first-order HMM [10], [24]. Figure 4 shows a partial example of the equivalent first-order HMM for HMM-2 (for clarity, only state transitions starting from bite are shown). Logically, transitions between some states are impossible. For example, state BB cannot transition to DB because the former state's most recently recognized gesture is B, which does not match the memory of the latter, which is D. In total HMM-2 has only 5 × 25 possible state transitions. Formally, each state in HMM-N is s i = {g 1 g 2 ...g N }. The state transition matrix A is calculated as:\n\nMentions:\n- Figure 4 shows a partial example of the equivalent first-order HMM for HMM-2 (for clarity, only state transitions starting from bite are shown).\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4: State transitions in HMM-2. For clarity, only the 25 transitions starting from bite are displayed. B = bite, D = drink, R = rest, U = utensiling, O = other."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Obviously, the reduction of the mass flow growth rate causes a decrease of C d as shown in Fig. 11, where the discharge coefficient as a function of the square root of the Nurick cavitation number of Eq. ( 18) is given (here C n is determined considering the liquid branch spinodal pressure instead of the vapor pressure in Eq. ( 18)). Indeed, except for the case with kinematic viscosity equal to 0.8, we can identify a critical cavitation number below which there is an abrupt reduction of C d . In the non-cavitating regime, for cavitation numbers higher than the critical value (filled circles in Fig. 11), C d slightly increases by decreasing C n while it reaches a plateau at large C n . In the cavitating regime, moving towards the critical value of the cavitation number, for kinematic viscosity equal to 0.5 and 0.6, C d shows a small undershoot before reaching the critical point. The undershoot is not present when the kinematic viscosity is 0.71. These findings are qualitatively in good agreement with the experimental ones 11 since it is possible to identify a critical point below which the discharge coefficient abruptly decreases. In this region C d is a linear function of C 1/2 n , as suggested by Eq. ( 19) only for the case with ν = 0.71, whereas Eq. ( 19) is not met when the kinematic viscosity is equal to 0.5 and 0.6.\n- The partial disagreement with the Nurick law of Eq. ( 19) can be due to the effects of the Reynolds number which are not taken into account in the derivation of Eq. (19). We show the dependence of C d and C n on Re. In Fig. 12, where the discharge coefficient is given as a function of Re, one sees that C d reaches a maximum and then it abruptly decreases due to cavitation effects. This result is qualitatively in good agreement with the experimental one by Payri et. al 11 . Fig. 13 shows that C n decreases with Re in both cavitating and non-cavitating cases. Given the dependence of C d and C n on the Reynolds number, it is reasonable to expect, coming back to discuss the cavitation regime in Fig. 11, that Reynolds number effects come into play also where cavitation is occurring so that the linear behavior in terms of C 1/2 n of Eq. ( 19) is not completely fulfilled. On the other hand, in the non-cavitating regime of Fig. 11, the reduction of C d with C n is mainly triggered by the reduction of Re, since C n decreases and C d consistently increases with increasing Re.\n\nMentions:\n- Obviously, the reduction of the mass flow growth rate causes a decrease of C d as shown in Fig. 11, where the discharge coefficient as a function of the square root of the Nurick cavitation number of Eq.\n- In the non-cavitating regime, for cavitation numbers higher than the critical value (filled circles in Fig. 11), C d slightly increases by decreasing C n while it reaches a plateau at large C n .\n- Given the dependence of C d and C n on the Reynolds number, it is reasonable to expect, coming back to discuss the cavitation regime in Fig. 11, that Reynolds number effects come into play also where cavitation is occurring so that the linear behavior in terms of C 1/2 n of Eq.\n- On the other hand, in the non-cavitating regime of Fig. 11, the reduction of C d with C n is mainly triggered by the reduction of Re, since C n decreases and C d consistently increases with increasing Re.\n\nOCR Text: \"LhhUM Fo-anialil _\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 11. Discharge coefficient Cd as a function of the square root of the Nurick 63 cavitation number C 1/2 n for different values of the kinematic viscosity. Simulation parameters: lattice sites 601×401 lu with fixed-pressure BC at θ = 0.85, κ = 0.1 and total inlet pressure pus = 0.4."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 4 shows the cumulative distribution function for the delay of critical messages for different delay thresholds D th with λ = 2, p 11 = 0.9, and m = 5 bits. Fig. 4 shows that the delay threshold is satisfied, which means that the achieved delay is less than or equal to D th , with probability 0.80 for D th = 0.75 ms, 0.87 for D th = 1.25 ms, 0.95 for D th = 2.5 ms, and 0.99 for D th = 5 ms. Moreover, the average achieved delay is 0.59 ms for D th = 0.75 ms, 0.72 ms for D th = 1.25 ms, 0.99 ms for D th = 2.5 ms, and 1.35 ms for D th = 5 ms. Although the largest D th = 5 ms has the longest average delay, it has the highest probability of satisfying the delay threshold. This is because the IoT devices with critical messages have more time available to satisfy D th for larger D th , and more IoT devices can learn β with more time available. Moreover, with more time, the IoT devices with critical messages are more likely to be allocated contention-free RAP by the BS for successful RA request. Here, it is important to note that D th may not be satisfied, because t and m are finite. Fig. 5 shows the cumulative distribution function for the delay of critical messages for different network densities λ with D th = 2.5 ms, p 11 = 0.9, and m = 5 bits. As our proposed learning method relies on the IoT devices communicating and propagating information, it is important that the IoT devices have at least one neighboring device to be able to learn β. Therefore, a massive IoT network with higher values of λ is better for learning. However, with smaller value of λ, T min is smaller, and, thus, D f is smaller. The value of D f is 19.75 ms for λ = 1, 39.63 ms for λ = 2, and 79.25 ms for λ = 4. Therefore, a small value of λ increases the delay by increasing the number of isolated IoT devices and decreases the delay by decreasing the value of D f . From Fig. 5, we can see that the probability of achieving a delay less than or equal to D th is 0.97 for λ = 1, 0.95 for λ = 2 and 0.92 for λ = 4. Therefore, it is more likely to satisfy D th with lower λ, and, thus, the effect of having smaller D f is more significant than having more isolated IoT devices. Furthermore, this shows the effectiveness of our learning method in a massive network with devices that have limited resources and capabilities. Fig. 6 shows the cumulative distribution function for the delay of critical messages for different probabilities of detection p 11 with D th = 2.5 ms, λ = 2, and m = 5 bits. Since the observation of an IoT device affects its learning as well as that of all IoT devices later in the sequence, the performance of our proposed method significantly depends on the quality of observations. For low p 11 , the IoT devices are prone to false alarms as p 01 = 1 − p 11 , and, thus, they do not learn N a correctly, which causes higher critical message delays. The delay threshold is satisfied with probability 0.98 for p 11 = 0.99, 0.96 for p 11 = 0.9, 0.85 for p 11 = 0.7, and 0.57 for p 11 = 0.5. Unlike D th and λ, which indirectly affect the performance of learning, p 11 affects the performance of learning method directly and more significantly, because the probability of a successful RA request achieves 0.99 after 11 time slots. Fig. 7 shows the cumulative distribution function for the delay of critical messages for different memory sizes m with D th = 2.5 ms, λ = 2, and p 11 = 0.9. The effect of having larger values of m is not significant in reducing the delay of critical messages even though the memory size m is critical for the convergence of proposed learning method. In our learning framework, the memory size m extends the detection range r d such that the devices that are located far from the abnormal event can learn accurately. In other words, with larger m, the IoT devices located outside of r d have more observations of previous devices to compute more accurate private belief x i for the second phase of sequential learning. However, having large m is less effective for the IoT devices within r d to learn more accurately. Since the RA request is successfully done during the first few time slots as shown in Fig. 7 when the devices within r d are learning, having larger m has negligible effects on decreasing the delay of the critical messages. The effects of m on the performance and the convergence of the learning method will be more pronounced when the devices outside of r d are learning as later shown in Fig. 8. The convergence of the proposed learning technique is analyzed via simulations by studying the percentage of IoT devices that learn correctly after all devices had a chance to learn N a . The parameters that affect the performance of the proposed learning technique include memory size m, network density λ, and probability of detection p 11 within r d . The network density λ affects the number of IoT devices that can learn such that an IoT device is more likely to have no neighboring device within r c to get necessary information for learning for lower values of λ. Such isolated device will not be able to learn using our proposed learning method as it cannot receive necessary information for learning from its neighboring devices. However, if the network is dense enough, the performance of learning is limited not by λ but by memory size m and probability of detection p 11 . The memory size m determines the effective detection radius r d and p 11 controls how accurately the devices can observe within r d . For the subsequent simulations, we take D th = 3, while varying m and p 11 . Fig. 8 shows the average percentage of devices that learned N a correctly out of all N IoT devices for different memory sizes m in bits with λ = 2. As the IoT devices can learn N a correctly within r d and r d depends on m, the average percentage of devices that learned correctly is clearly limited for finite memories m = {3, 10, 15, 30} bits, while the average percentage approaches 100% for infinite memory m = ∞ bits. Moreover, the average percentage of devices that learned correctly is an increasing function of m in bits as r d is an increasing function of m. The maximum average percentage of devices that learned correctly is 4.67% for m = 3 bits, 18.79% for m = 10 bits, 29.84% for m = 15 bits, and 66.07% for m = 30 bits. This is because as the learning progresses further, the IoT devices outside of r d will learn incorrectly, and, thus, the percentage of devices that learn correctly out of all N will no longer increase. Moreover, the maximum average percentages for different values of m are achieved at different time slots since a larger m requires more time to achieve the maximum average percentage. Furthermore, the results in Fig. 8 differ from the results shown in Fig. 3 in terms of the percentage of devices that learned N a correctly, because the results in Fig. 3 are done in smaller-scale network with different values of w, l, r c , and r d . Fig. 9 shows the average percentage of devices that learned N a correctly out of all N IoT devices for different probabilities of detection p 11 with m = 15 bits and λ = 2. For lower p 11 , the devices are more likely to observe and learn N a incorrectly. Learning incorrectly will affect subsequent devices in their learning as the proposed approach relies on the observation of previous IoT devices. For p 11 = 0.5, less than 1% of IoT devices learn correctly, and about 17.5% of IoT devices learn correctly even for p 11 = 0.9. Even for a p 11 close to 1, Fig. 9 shows that the average percentage of devices that learn N a correctly does not approach 100%. Hence, we can conclude that m is much more critical in increasing the percentage of devices that learned correctly than p 11 .\n- Even with high probability of detection p 11 , the average percentage of devices that learned correctly is about 30%. This is because the memory size m is not large enough to extend r d to circumscribe the entire deployment region. For small m, some IoT devices do not satisfy (8) due to their limited observation range. In other words, for those devices, the effective detection range r d is not extended enough to include the IoT devices located far away from the abnormal event, and the IoT devices located outside of r d will not learn correctly. Moreover, high p 11 is only applicable within the detection radius of r d , and the probability of detection p 11 outside of r d is still very low. Therefore, it is necessary for both m and p 11 to be large to achieve high percentage of devices that learned correctly. However, it is unnecessary to have all IoT devices learn correctly to satisfy a given delay threshold as shown in Fig. 4, Fig. 5, and Fig. 6. Furthermore, since the memory size m will be finite or even small in practical IoT systems, it is important to analyze the convergence of learning method with finite values of m as shown in Fig. 8.\n\nMentions:\n- Fig. 5 shows the cumulative distribution function for the delay of critical messages for different network densities λ with D th = 2.5 ms, p 11 = 0.9, and m = 5 bits.\n- From Fig. 5, we can see that the probability of achieving a delay less than or equal to D th is 0.97 for λ = 1, 0.95 for λ = 2 and 0.92 for λ = 4.\n- However, it is unnecessary to have all IoT devices learn correctly to satisfy a given delay threshold as shown in Fig. 4, Fig. 5, and Fig. 6.\n\nOCR Text: \"Tsecir U5i1l0y MHeci deu5ty Cheee Dela\" TS\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5. Cumulative distribution function of delay of critical messages for different λ."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Using the same large laboratory dataset, the ability of the system to integrate down according to the radiometer equation was tested. Because spectra collected on the terminated antenna input are indistinguishable from those collected on the reference, odd integrations were calibrated with the even ones. Such calibrated data were later averaged and a constant room temperature was fitted in the 155-190 MHz sub-band to the resulting average spectrum. The Gaussian distribution of residuals of this fit yielded a statistical error δT ref ≈1.7 mK of the mean integration resulting from averaging all the integrations, which is within 0.1 mK of what is expected from equation 13 and τ ≈ (475/2) hours and B=117.2 kHz. The same procedure of fitting Gaussian to a distribution of residuals (Fig. 19) led to determination of the error as a function of integration time which is shown in Figure 20.\n\nMentions:\n- The same procedure of fitting Gaussian to a distribution of residuals (Fig. 19) led to determination of the error as a function of integration time which is shown in Figure 20.\n\nOCR Text: \"X\" Ymi L12le-06 ' 155 382 3 _ 0.6421 rteoratizn _m? Rirs\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 20. The error δTref (τ) in a single frequency bin of 117.2 kHz within 155-190 MHz frequency band (derived as described in the text) as a function of integration time τ . Every point in this figure was obtained from the Gaussian fit to a distribution of residuals (Fig. 19)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where σ max (A) and σ min (A) are maximal and minimal singular values of A respectively. For our Deep-ESN, we compute the condition number of the echo-state collected matrix X (i) res ∈ R (N (i) ×T ) in the i-th reservoir, and of the encodedstate collected matrix X (j) enc ∈ R (M (j) ×T ) in the j-th encoder. For the MESM, we only compute its condition number of X (i) res . The results of collinearity analysis on three 8-layer Deep-ESNs with different encoders and an 8-layer MESM are shown in Fig. 9.\n- As seen in Fig. 9, for all the methods, the largest redundancy occurs in the first reservoir. With the multiple state transitions in its hierarchical direction, MESM does not reduce redundancy any further after two layers. A high condition number in MESM lowers the accuracy of the linear regression. Compared with MESM, our Deep-ESN works well with its encoders. We can see that the higher the layer, the redundancy on reservoirs will be less, especially with PCA, although this flattens out after E2 or E3. In fact, after R4, the condition number appears to increase for ELM and RP, which suggests why the PCA encoder is more effective.\n\nMentions:\n- The results of collinearity analysis on three 8-layer Deep-ESNs with different encoders and an 8-layer MESM are shown in Fig. 9.\n- As seen in Fig. 9, for all the methods, the largest redundancy occurs in the first reservoir.\n\nOCR Text: \"Dezp-ESMKmh FcX DopESMKMELN AE De-p ESMKin RJ RI F R? FZ   R3E3 R4 E4 Re E5 R5 E6 R7 Mebuk lartr MES'\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 9. The results of collinearity analysis on three 8-layer Deep-ESNs and a 8-layer MESM for the MGS time series, where Ri denotes the i-th reservoir and Ej denotes the j-th encoder (i = 1, . . . , 8, j = 1, . . . , 7). In order to facilitate visualization, the logarithmic form of condition numbers are given in this plot (Y-axis)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We then convolve both the cluster-lensed CMB and foreground maps by a Gaussian instrumental beam with FWHM = 1 . We store three combinations of these maps for each realization. The first combination map is representative of what a telescope would see; it has the sum of the CMB, foregrounds, and a realization of 5 µK-arcmin white noise. The second combination map is used as a 'truth' set in training the first network; it has only the CMB. The third combination map is the input for training the second network; this map has the CMB and a realization of white noise. We randomly draw the white noise level for this third map from a uniform distribution between 0 to 5 µK-arcmin. Varying noise levels is designed to diversify the training set to better handle the non-Gaussian statistics in the output map from stage I (see Fig. 3). We do not use the maps from stage I for training the second stage in order to ensure that the network is not trained to recover masses using information besides lensing, for example from some residual SZ signal. The κ maps are used as the 'truth' sets in training the second network. All maps are then trimmed to 10 × 10 and apodized as described above. We have tested training the networks after trimming the maps to larger (20 × 20 ) sizes at a single cluster mass; we found a small improvement for 20 × 20 maps. The uncertainty decreased by a factor of 0.93 to 0.98 for 20 ×20 maps, but consumed more computer resources. All results shown in this work use 10 × 10 maps.\n- Figure 3 shows the distributions of the residual temperature calculated as the difference between the true and the recovered CMB-only maps. The green solid and black dotted contours show the distribution of all image pixels and central pixels, respectively, for 800 realizations of the cluster with M 200c = 4 × 10 14 M . This indicates that the noise in the recovered CMB-only maps is non-Gaussian and resembles a Lorentz distribution with γ = 0.5.\n- The above comparisons show that the trained deep learning models put competitive constraints on the cluster masses. Note that we do not use the reconstructed CMB-only maps with suppressed foregrounds and noise to derive the MLE mass uncertainties. This is mainly due to the non-Gaussian nature of the residual noise in these maps (see Fig. 3). The application of MLE to the reconstructed CMB-only maps will require new developments and we plan to modify the MLE in our future work.\n\nMentions:\n- Varying noise levels is designed to diversify the training set to better handle the non-Gaussian statistics in the output map from stage I (see Fig. 3).\n- Figure 3 shows the distributions of the residual temperature calculated as the difference between the true and the recovered CMB-only maps.\n- This is mainly due to the non-Gaussian nature of the residual noise in these maps (see Fig. 3).\n\nOCR Text: \"AlI pixels Central pixels 3 (pki\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3. The distribution of the residual temperature that is the difference between the true and the recovered CMBonly maps (T̃FF) as shown in the fourth column of Figure 2. The green solid and the black dotted contours show the distribution of all image pixels and central image pixels for 800 realizations of a cluster with M200c = 4 × 1014 M , respectively. The distribution is clearly non-Gaussian, resembling a Lorentz distribution with γ = 0.5. This indicates that the noise in the recovered T̃FF maps is non-Gaussian."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Therefore, for small finite θ, the confinement-deconfinement transition must also be a second order transition. Given that we have already argued that CP symmetry is spontaneously broken at θ = π at zero temperature and given that the phase diagram has to be periodic in θ, we can infer that the phase diagram is as in fig. 1. The lines on the diagram have to meet in some way, which we will soon discuss. The reflection symmetry around θ = π is due to CP invariance at θ = π.\n- lines) are now good local operators on R 3 . On the other hand, all space-like loops are now attached to surfaces. This is very convenient since we can discuss more uniformly the different phases of the theory in fig. 1.\n\nMentions:\n- Given that we have already argued that CP symmetry is spontaneously broken at θ = π at zero temperature and given that the phase diagram has to be periodic in θ, we can infer that the phase diagram is as in fig. 1.\n- This is very convenient since we can discuss more uniformly the different phases of the theory in fig. 1.\n\nOCR Text: \"B 2t\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1: Known information about the phase diagram of SU(2) gauge theory as a function of θ and temperature β−1. The smooth line is a second order transition in the Ising 3d universality class. The zigzag line is expected to be a first order transition. There are 2 vacua on the zigzag line and one vacuum on each side of it. Later in this paper we will discuss the region with question marks."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We tested Alg. 2 to retrain costs c on the class of Fourier shapes (see fig 7g(right)) so as the total cost assigned by the model would match better to the true cost. It is initialized with patterns (w, c) which were learned using Alg. 1. Here we kept the weights w fixed and only update the weights c in step 2. Figure 15 shows the progress of Alg. 2. Figure 16 evaluats the new trained model on independent samples of circles and Fourier shapes. We see that the bias of the model to overestimate the total cost was reduced, compared to fig. 7(d,e). As expected, this is especially true for the Fourier shapes, since only Fourier shapes were used in training. Note, since retraining with Alg 2 gave only a mild improvement in approximation error, we did not use these patterns, but rather the ones obtained after Alg. 1 for all of our experiments. Figure 16: Results after applying Alg. 2 to refit constant terms. We show approximate total cost / length vs. true total cost / length, for circles (a) and Fourier shapes (b) respectively. As can be seen teh bias is removed\n\nMentions:\n- Figure 15 shows the progress of Alg. 2.\n\nOCR Text: \"8 I 9 Iteralion\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 15: Objective of (24) during iterations of Alg. 2. Red curve is evaluated on training data and green dashed curve is evaluated on the independent test data."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- FIG. 1: The trilinear W W γ vertex in presence of the background field b αβ .\n- Using the notation and conventions shown in Fig. 1, the corresponding vertex function can be written as follows:\n\nMentions:\n- FIG. 1: The trilinear W W γ vertex in presence of the background field b αβ .\n- Using the notation and conventions shown in Fig. 1, the corresponding vertex function can be written as follows:\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 1: The trilinear WWγ vertex in presence of the background field bαβ ."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- 3 is shown in Fig 1 . We see that it becomes negative at low energies. Here we consider the case that the 3rd generation right-handed slepton doublet acquires the negative mass squared. The potential forL c\n\nMentions:\n- 3 is shown in Fig 1 .\n\nOCR Text: \"Ln#c\"\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 1: The RGE evolution of the soft mass squared for L̃c3, which becomes negative at low energies."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Note that in past studies on 1-bit CS, reconstruction performance was evaluated through directional MSE, which is defined by |x |x| − x 0 |x 0 | | 2 as scale information is lost. We solved the saddle point equations for signal sparsity ρ = 0.25 and variance σ 2 0 = 1 when ratio α = 3. The curve in Fig. 1 denotes the theoretical predictions of MSE as evaluated by ( 13)-( 18) (strategy 1) and ( 22) plotted against the threshold λ. Fig. 2 represents the theoretical predictions of MSE evaluated by ( 19)-( 21), ( 16)-(18) (strategy 2), and ( 22) plotted against the standard deviation σ λ of the threshold. Figures 1 and 2 show that there is an optimal threshold distribution (red circle symbol) that minimizes MSE for each set of parameters. Similar features hold for various sets of values of α, ρ, σ 2 0 for both strategies 1 and 2. To compare the optimal MSE (changing threshold distribution) of strategy 1 and strategy 2, we plot the optimal MSE for the same signal distribution in Fig. 1 and Fig. 2 against α in Fig. 3, which is referred to the envelope curve of MSE. The blue and red curves represent the envelope curves for strategies 1 and 2, respectively. From Fig. 3, we can see that strategy 1 outperforms strategy 2 when parameters are optimally tuned. Therefore, we hereafter focus on strategy 1, for which the thresholds are fixed.\n\nMentions:\n- The curve in Fig. 1 denotes the theoretical predictions of MSE as evaluated by ( 13)-( 18) (strategy 1) and ( 22) plotted against the threshold λ. Fig. 2 represents the theoretical predictions of MSE evaluated by ( 19)-( 21), ( 16)-(18) (strategy 2), and ( 22) plotted against the standard deviation σ λ of the threshold.\n- Figures 1 and 2 show that there is an optimal threshold distribution (red circle symbol) that minimizes MSE for each set of parameters.\n- To compare the optimal MSE (changing threshold distribution) of strategy 1 and strategy 2, we plot the optimal MSE for the same signal distribution in Fig. 1 and Fig. 2 against α in Fig. 3, which is referred to the envelope curve of MSE.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1. Replica prediction of MSE (in decibel) versus fixed threshold λ for signal distribution ρ = 0.25, σ2 0 = 1, and ratio α = 3."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Concomitantly, the normal force estimate agrees well with the truth, as seen in Fig. 12. The transition from a symmetric cylinder wake to a periodic vortex shedding causes some challenge to the discrete vortex models, as highlighted by the temporary growth of the uncertainty envelope around t = 4.0. The envelope remains small for both filters after the periodic wake behavior has been established. The history of the LESPc estimate in Fig. 13 reveals the same growth and decay of this critical value observed in the previous example; here, they represent a response to individual cylinder wake vortices passing the leading edge of the plate. These variations are essential to our inviscid framework to control the leading-edge vortex shedding in the presence of flow perturbations. As in the previous example, the weak correlation between the LESPc and measured pressure causes significant volatility in the estimated LESPc among the different realizations. The vortex element population remains small, O(60), after 12 convective time units. Without aggregation, we would have to track 2400 vortex elements.\n\nMentions:\n- The history of the LESPc estimate in Fig. 13 reveals the same growth and decay of this critical value observed in the previous example; here, they represent a response to individual cylinder wake vortices passing the leading edge of the plate.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 13. Time history of the ensemble mean value of the LESPc of an impulsively translating plate at 20◦ in a cylinder wake, averaged over 100 realizations, from the inviscid vortex model with sEnKF and the inviscid vortex model with ETKF . Shaded areas show the 95% confidence interval for the inviscid vortex model with sEnKF and ETKF."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Proof. Observe from Figure 4.1 that if a path P starts at the NE point of T and goes leftward, it will go through the rightmost box A m precisely once. In particular, once a path exits from A m , it cannot return to A m . Thus both strands of T must go through A m .\n\nMentions:\n- Observe from Figure 4.1 that if a path P starts at the NE point of T and goes leftward, it will go through the rightmost box A m precisely once.\n\nOCR Text: \"NE SW SE\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4.1. The 3-braid form of a rational tangle."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 9 exhibits H 4 with a graceful labeling. Theorem 15 characterizes the graceful game for helms and its proof uses the following lemma. Lemma 14. Given a helm H n , for j ∈ {1, . . . , n}, Alice can label a cycle vertex v j of H n in only two cases: when v j 's respective pendent vertex is already labeled or, when the colors 0 or 3n (or both) have already been assigned to a vertex.\n\nMentions:\n- Figure 9 exhibits H 4 with a graceful labeling.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 9: Helm H4 with a graceful labeling."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The temperature dependences of D i , I, and the time resolution were evaluated with a thermostat chamber ES-PEC SH-641 [21] for a range of the chamber temperature    from −10 • C to 60 • C. The data were acquired after the temperature of FPGA became stable during the operation. The temperature of FPGA during operation was about 8 • C higher than that of the chamber. Figure 10 shows the relation between the deviation σ of D i and the temperature. Figure 11 shows the relation between the slope parameter A from the linear fit to I and the temperature. Figure 12 shows the relation between the time resolution and the temperature. The temperature dependence is small in the investigated temperature range.\n\nMentions:\n- Figure 11 shows the relation between the slope parameter A from the linear fit to I and the temperature.\n\nOCR Text: \"{ 5 Temperature of Thermostat Chamba)C\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 11: Temperature dependence on the slope of the linear fit for the integral nonlinearity for the channel 8. The values are shown for a bin size of 0.28 ns."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Note that as explained in Section 1, y t is an observable quantity, while P E * and z t are unobservable, i.e. they are hidden state or latent variables. Note that these are two different types of latent variables, i.e. P E * is constant and z t is time-varying. Thus, Eq. ( 7) is different from standard state-space and graphical models such as Hidden Markov Models or Linear State Space Model [7]. The graphical model of our proposed stock price dynamic has three layers as represented in Figure 2. In our case, where the model is temporal, the graphical model framework is also called dynamic Bayesian network (DBN).\n- Figure 2: The proposed model represented by dynamic Bayesian network (DBN). y t is an observable quantity, while P E * and {z t } are unobservable.\n- 3 Bayesian inference on the DBN of stock price dynamic As explained in previous sections, our goal is to make an inference on PE* so that we can estimate the fundamental price of a stock. In Section 4, we will show that estimations of {z t } is also useful in investment. To infer the values of these two latent variables, similar to Hidden Markov Models (HMM) and Linear State Space Model (LSSM) [7,15], we need to derive equations in two steps. First, the inference algorithms with known parameters, second, the parameter estimation algorithms given that parameters are unknown. However, because there are two types of latent states as explained in the previous section, our graphical model shown in Figure 2 is more sophisticated than HMM and LSSM. In this section, we show the new equations for both inference tasks. To simplify the notation, we use notation x T 1 to denote{x 1 , ..., x T } .\n\nMentions:\n- The graphical model of our proposed stock price dynamic has three layers as represented in Figure 2.\n- Figure 2: The proposed model represented by dynamic Bayesian network (DBN). y t is an observable quantity, while P E * and {z t } are unobservable.\n- However, because there are two types of latent states as explained in the previous section, our graphical model shown in Figure 2 is more sophisticated than HMM and LSSM.\n- In the above derivation, Bayes's rule, conditional independent properties [15] with respect to DBN shown in Figure 2 and sum rule are applied consecutively to get the above result, similar to the filtering equation of HMM.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: The proposed model represented by dynamic Bayesian network (DBN). yt is an observable quantity, while PE∗ and {zt} are unobservable."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 6 shows the DG of the basic block b 2 in the running example from Figure 2, where i 6 is the entry instruction and i 9 is the exit instruction. Data dependencies are represented with solid arcs labeled with their latency (and corresponding temporary, for clarity). All instructions are assumed to have unit latency, except i 6 (load) which is assumed to have a latency of two cycles. (i 6 , i 8 ) is called an anti-dependency: i 6 uses t 1 which is then redefined by i 8 . Due to the structure of instruction pipelines, instructions related by anti-dependencies can typically run simultaneously in multiple-issue processors (see the Instruction bundling paragraph in this section). Hence, such dependencies are often labeled with a latency of zero cycles. Branch dependencies maintain the original control flow (for example forcing i 7 to be scheduled before i 9 ) and are represented with dashed arcs. Branch dependencies that are redundant from a latency perspective such as (i 6 , i 9 ) are kept in the DG for uniformity.\n- Early approaches. An early combinatorial approach to local instruction scheduling is introduced by Arya [10]. This approach uses IP to compute optimal schedules for single-issue vector processors such as the early Cray-1 [141]. Arya's model can be seen as an application of Manne's RCPSP model [116] with general (that is, non 0-1) integer variables s(i) (T i in the original notation) representing the issue cycle of each instruction i. This model allows dependency constraints to be formulated with simple inequalities on scheduling variables. For example, the dependency (i 6 , i 7 ) in Figure 6 yields s(i 7 ) ≥ s(i 6 ) + 2. However, the use of general integer variables precludes multi-capacity resources and requires auxiliary 0-1 variables p(i, j) to indicate whether instruction i precedes instruction j or vice versa. A constraint is added for each pair of instructions i, j to relate the auxiliary and the scheduling variables. For example, the corresponding constraint for i 7 and i 8 is 1 ≤ M × p(i 7 , i 8 ) + s(i 7 ) − s(i 8 ) ≤ M − 1, where M is a large positive constant. Each value of p(i 7 , i 8 ) activates one of the sides of the compound inequality and enforces a precedence among i 7 and i 8 in one or the opposite direction. Arya shows on three basic blocks of up to 36 instructions that the approach is feasible and improves the makespan of hand-optimized code significantly.\n- Ertl and Krall introduce the first local instruction scheduling approach based on constraint programming [55]. In particular, the approach applies constraint logic programming [90] which embeds constraint propagation into logic programming. Their model targets single-issue, pipelined RISC processors where the resource consumption of an instruction must be considered at each stage of the pipeline. The model includes, for each instruction i and pipeline stage k, a finite integer domain variable s(i, k) representing the cycle in which i resides in k (the original notation uses a set of variables {D i ,W i , . . . } for each instruction i where each variable corresponds to a pipeline stage such as decode and writeback). Dependencies among instructions are modeled as in Arya's approach. Equality constraints link the different stage scheduling variables of an instruction. An all-different global constraint [156] ensures that only one instruction resides in each pipeline stage k at a time by forcing all variables {s(i 1 , k), s(i 2 , k), . . . , s(i n , k)} to take different values. The use of all-different yields a compact model as it removes the need for a quadratic number of auxiliary variables present in Arya's approach. For example, enforcing that only one of the four instructions in the DG of Figure 6 can be executed at a time (at the pipeline stage called EX ) is captured by the constraint all-different({s(i 6 , EX ), s(i 7 , EX ), s(i 8 , EX ), s(i 9 , EX )}). Ertl and Krall's experiments on a few programs for the Motorola 88100 processor [3] show that their approach improves 8.5% and 19% of the basic blocks scheduled by the Harris C compiler and GCC 1.3 [62] respectively. However, the richness of the resource model comes at the cost of low scalability: their experiments show that the approach cannot handle larger problems than Arya's scheduler despite the six-year gap between them.\n\nMentions:\n- Figure 6 shows the DG of the basic block b 2 in the running example from Figure 2, where i 6 is the entry instruction and i 9 is the exit instruction.\n- For example, the dependency (i 6 , i 7 ) in Figure 6 yields s(i 7 ) ≥ s(i 6 ) + 2.\n- For example, enforcing that only one of the four instructions in the DG of Figure 6 can be executed at a time (at the pipeline stage called EX ) is captured by the constraint all-different({s(i 6 , EX ), s(i 7 , EX ), s(i 8 , EX ), s(i 9 , EX )}).\n- For example (assuming that only one version v 1 is available for each instruction), the dependency (i 6 , i 7 ) in Figure 6 yields the constraint for each cycle k that if s(v 1 , i 6 , k) holds then one of s(v 1 , i 7 , k + 2), s(v 1 , i 7 , k + 3), . . . must hold.\n- The DG transformations include: partitioning to decompose DGs while preserving optimality, redundant dependency elimination to discard dependencies that do not affect the solution but incur overhead in the IP model (such as the branch dependency (i 6 , i 9 ) in Figure 6), and region linearization to determine the order in which instructions from specific components of the DG called regions must appear in the optimal solution.\n\nOCR Text: \"16 2(t5 O(t1) 18 i9 1(t1)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 6. DG of b2"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- For the five images shown in Fig. 3, the corresponding mean opinion scores (MOS) increase from left to right, which means that the right one has the highest subjective score given by human. Using Eq. 2, we can easily obtain the corresponding ge s scores: 0.1457, 0.5364, 0.5562, 0.6404 and 0.7735 respectively, which are illustrated in Fig. 4. It can be realized that ge s is monotonically related to the MOS score. Further, we exhibit the scatter plots of MOS/DMOS versus ge s on the whole CCID2014 (including 655 images) and CSIQ (including 116 contrast changed images) database [34] in Fig. 5. Clearly, the score of image similarity shows higher linearity and monotonicity on CCID2014 database, but the performance becomes worse on CSIQ database. So, only one feature is not enough to handle all the contrast distortion models and levels, we should explore more contrast-aware features.\n\nMentions:\n- Using Eq. 2, we can easily obtain the corresponding ge s scores: 0.1457, 0.5364, 0.5562, 0.6404 and 0.7735 respectively, which are illustrated in Fig. 4.\n\nOCR Text: \"\"04 2   2 W 26 23 1 32 M VCS\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4. Comparison between Sge and subjective MOS of five images from CCID2014"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- CatBoostLSS also allows to investigate feature importance for all distributional parameters. Looking at the top 10 features with the highest Shapley values for both the conditional mean and variance in Figure 9 indicates that both yearc and area are considered as being the most important variables. To get a more detailed overview of which features are most important for our model, we can also plot the SHAP values of every feature for every sample. The plot below sorts features by the sum of SHAP value magnitudes over all samples and uses SHAP values to show the distribution of the impacts each feature has on the model output. The colour represents the feature value (red high, blue low). This reveals for example that newer flats increase rents on average. We can also visualize all predictions and assess the attribute importance. Besides the global attribute importance, the user might also be interested in local attribute importance for each single prediction individually. This allows to answer questions like 'How did the feature values of a single data point affect its prediction?' For illustration purposes, we select the first predicted rent of the test data set and present the local attribute importance for E(Y |X = x) . As we have modelled all parameters of the Normal distribution, CatBoostLSS provides a probabilistic forecast, from which any quantity of interest can be derived. Figure 13 shows a random subset of 50 predictions only for ease of readability. The red dots show the actual out of sample rents, while the boxplots visualise the predicted distributions. We can also plot a subset of the forecasted densities and cumulative distributions as shown in Figure 14.\n\nMentions:\n- Figure 13 shows a random subset of 50 predictions only for ease of readability.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 13: Boxplots of Probabilistic Forecasts of Munich Rents."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Here we show the results for 50 independent runs in which 1,000 IID Gaussian data are generated and the experimental curve generated and h taken at the curve minimum (black curves in Figure 8). While there is a large vertical spread in the curves, the locations of the minima are fairly tightly constrained. The mean of 50 values of h 0 is 0.335 (compared to the exact value of 0.341), and the estimated h 0 have a standard deviation of 0.0238. Several other characteristic functions (Fourier transforms of PDFs) are easily integrated and illustrate the effect of data distribution on estimation of h 0 . For example, the standard Cauchy density, defined by\n\nMentions:\n- Here we show the results for 50 independent runs in which 1,000 IID Gaussian data are generated and the experimental curve generated and h taken at the curve minimum (black curves in Figure 8).\n\nOCR Text: \"Lnulu Wmis\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 8: Plots of unbiased estimator for Fourier-transformed MISE (denoted εn(h)) as a function of global bandwidth parameter h. Plots either assume or use Gaussian data with σ2 = 1 and 1,000 data points. The exact expression Eq. (19) is plotted with a thick red curve; the minimum (shown with a + sign) is found at h0 = 0.3406. The estimate of h0 using Taylor series is 0.266 and is denoted by a circle above the curves. Also shown is an ensemble of 50 curves (in black) wherein for each curve 1,000 IID Gaussian data are generated and the density function is estimated by Fourier transform Eq. (14). The ensemble statistics of the estimated h0 were calculated with mean h0 = 0.335, with standard deviation of σh0 = 0.0238 (box above curves denotes mean ±σh0)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Example 5. Let us look at the example of Fig. 7. The only possible situation for temporary disabling an action by another one is the execution of a that disables b. And then b could be enabled again after the execution of the sequence cde, so after 3 steps. Hence, the net is e/l-3-persistent, and obviously not e/l-2persistent.\n- Fig. 7. A p/t-net (for Ex.5) that is e/l-3 persistent but not e/l-2 persistent\n\nMentions:\n- Let us look at the example of Fig. 7.\n- Fig. 7.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 7. A p/t-net (for Ex.5) that is e/l-3 persistent but not e/l-2 persistent"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- A monolayer of APTES was assembled on Si/SiO 2 (Figure 1) as described elsewhere. 46 AFM measurements verify smooth and continuous monolayer formation (SI, Figure S1). This step results in a layer containing both alkylamonium (pKa~10) and alkylamine functionalities. The ratio between the ammonium and amine groups grows with decreasing pH. The change in the work function was found to be reversible, implying that after washing with water the work function returns to its original value. It is important to notice that the ammonium groups have a dipole pointing towards the surface while the amine groups have a dipole pointing away from the surface, 36 as illustrated in Figure 1. This fact can be used for work function tuning by protonation with different pH. It should be expected that higher degree of protonation inducing more dipoles pointing toward the surface should result in the decrease of the WF. However, the CPD measurements have shown an opposite behavior resulting in increasing of the WF upon the protonation as shown in Figure 3 (see also control experiments with bare Si: SI, Figure S2). These results can be explained by the counter-ion dipole (Cl -), coupled to the propylammonium positively charged monolayer. The position of the Cl with respect to the PrN + H 3 group can create an ionic dipole with an opposite direction, which points away from the surface resulting in increasing of the WF (Figure 3). In this case the ionic dipole leads to an increase of work function from 4.24eV (Si/SiO 2 -PrNH 2 , ethanol) to 4.73 eV (Si/SiO 2 -PrNH 2 , pH=1 HCl). In order to confirm this assumption, XPS measurements were conducted in several pH values and indeed XPS result verified the quaternization reaction by the presence of Cl -(Cl -[2p] 198.2 eV) at pH = 1. (SI, Table S1 ) At this point, it seems of interest to explore other anions as well. A change in the counter anion was conducted by using the appropriate acids (HBr, or HI) in order to check the influence of the anions alteration on the work function of the substrates. For this purpose, some of the amine terminated substrates was immersed in hydrobromic acid, and some in hydroiodic acid. The base peak of hydrobromic acid was found in XPS measurements (Br-[3d] at 0.65eV), but no indication for the I presence was obtained by XPS (due to its volatility under the x-ray radiation). CPD measurements have shown increasing of the work function with increasing of atomic radius/polarizability of the bases (Fig. 4).\n\nMentions:\n- However, the CPD measurements have shown an opposite behavior resulting in increasing of the WF upon the protonation as shown in Figure 3 (see also control experiments with bare Si: SI, Figure S2).\n\nOCR Text: \"PrNH SiPH] SiFihh\" pk-l J46   344 Kinding E F (eV) Energy (\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2. XPS of bare silicon, Si/SiO2-PrNH2 and Si/SiO2PrN + H3 after immersing in HCl at pH=1."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where E γ1 and E γ2 are the energy of the 2 photon showers and θ γγ is the opening angle. The invariant mass distribution shows good agreement between data and MC (Figure 1) with the mass peak matching the known π 0 mass (134MeV). Cuts are applied on the invariant mass distribution to choose only the peak region to reduce non-π 0 background. The coherent contribution to the NC π 0 sample is small compared to other interaction modes. The background dominantly comes from neutral current resonance (RES) and deep-inelastic scattering (DIS), with small contribution from diffractive (DFR) π 0 production and charged current interactions.\n\nMentions:\n- The invariant mass distribution shows good agreement between data and MC (Figure 1) with the mass peak matching the known π 0 mass (134MeV).\n\nOCR Text: \"NOvA Preliminary ; T-[9z SC 1 'GeV; 1\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Data and MC π0 invariant mass distribution of the selected 2-prong NC π0 sample."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figs. 4 and 5 consider the MuMIMO-CR-SWIPT systems for a system with K I = N I = 2 and P T = 10dBm, and show the rate-energy tradeoff performance in a 3-D plot and in its 2-D intersection at E th,1 = 30µW, respectively. Here, we employed the WSR utility as an objective function of (P-1) to acquire the maximum WSR. The name of \"SuMIMO outerbound\" amounts to the case of perfect collaboration among the multiple S-ID users, which results in a single macro S-ID user with N I = 4. Unlike the case of K I = 1, it is generally difficult to identify whether the resulting solution is optimal or not due to the lack of knowledge on the global optimal solution. Nevertheless, we can carefully infer that the proposed solution achieves the optimal boundary points based on the observation that all the tradeoff regions exhibit a nice convex shape and are close to their single-user outer bounds. At any information rate, there exists an unachievable energy region, which shows that the feasibility check in Section III is important before solving the problem. Obviously, as I th decreases, the achievable tradeoff region will shrink to meet tighter CR constraints. One interesting observation in Fig. 5 is that as the energy threshold approaches its maximum value, the achievable rate converges to its outerbound. This confirms our previous statement in Proposition 1 that the maximum energy  of an S-EH user is achievable via a single beam vector that is pointing in one direction irrespective of the S-ID user topology. Fig. 6 shows a snapshot of the ellipsoid process in Algorithm 1 in terms of the harvested energy and the interference power for a system with M = 4 and K I = N I = 2. Here, we set the threshold values as E th,1 = 30µW, E th,2 = 20µW and I th,1 = I th,2 = 0.1µW. The figure confirms that the proposed algorithm achieves all the required constraints. The interference power is kept below the threshold attributed to the power normalizing factor γ in (18).\n\nMentions:\n- One interesting observation in Fig. 5 is that as the energy threshold approaches its maximum value, the achievable rate converges to its outerbound.\n\nOCR Text: \"Fic Mudo Silo cueuua (W 0]r\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 5. Rate-energy tradeoff performance of the proposed WSR design with PT = 10dBm, M = 4, KI = NI = 2, and Eth,1 = 30µW"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- As the measure of performance, we consider the mean square error (MSE) between the original and the recovered signals. For each value of k, we perform simulations until we obtain 100 \"successful\" recovery instances. The results for the 3 algorithms are reported in Fig. 1, where for each algorithm, MSE averaged over all simulated cases for a given value of k is shown.\n\nMentions:\n- The results for the 3 algorithms are reported in Fig. 1, where for each algorithm, MSE averaged over all simulated cases for a given value of k is shown.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1. Comparison between SBB, `1 minimization, and modified `1 minimization in terms of MS reconstruction error in the presence of Gaussian measurement noise with zero mean and variance σ2 = 0.25 (n = 1000, m=500)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We show V LO in Fig. 1. At LO, the potential is flat along the φ axis, which composes the minima of this potential. § Thus, at LO, the vacuum is not determined uniquely. At every vacuum the Higgs boson is massless, while the singlet scalers are massive. § If λ S is small and of the same order as λ H , λ S ϕ 4 should be counted as NLO. In this case, both φ and ϕ axes become the flat minima of V LO . Other features, especially the results presented in the next section, are hardly affected, if the global minimum of V LO + V NLO is on the φ axis. The NLO contributions are given by\n\nMentions:\n- We show V LO in Fig. 1.\n\nOCR Text: \"II8\" [ c 3 /%\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: The LO effective potential VLO as a function of φ and ϕ. The red line shows the φ axis, which composes the minima of the potential."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The above problem could be modeled as a graph partitioning problem, where entities and result pages would be connected by weighted edges representing their similarities. Figure 1 shows an example graph, where edges indicate differently weighted similarity relationships. Note that there is no edge between e1 and e2 (e1, e2 ∈ E ), because we apply a disjointness constraint between all entities from E . The objective then would be to partition the graph into |E | components while minimizing the total weight of the edges between separate components. This problem is known to be NP-hard [35]; hence we follow a relaxed version of this model, where we assign each document to the entity partition for which it exhibits the largest similarity or probability score. To this end, we propose a suite of vector space and probabilistic models, which can be analogously applied to address the above problem.\n\nMentions:\n- Figure 1 shows an example graph, where edges indicate differently weighted similarity relationships.\n\nOCR Text: \"ds e\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Example graph for the partitioning problem."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In the first experiment, the evaluation of DotDFS versus GridFTP in a single stream over LAN is done. Fig. 15 shows the performance and throughput for transferring files with size of ranging from 200MB to 4000MB. While our current implementation is suffering from the overheads of MONO .NET implementation and .NET Platform Invoke calls (more meaningfully due to the virtual machine nature of the .NET Framework), the proposed DotDFS protocol has reasonable performance over GridFTP. An interesting observation is the reduction of throughput for files larger than 1GB. The measured throughput in this state is rather much less than the speeds of the local read and write over the storage system. This phenomenon is discussed with more detail in section 5.2.\n\nMentions:\n- In the first experiment, the evaluation of DotDFS versus GridFTP in a single stream over LAN is done. Fig. 15 shows the performance and throughput for transferring files with size of ranging from 200MB to 4000MB.\n\nOCR Text: \"1 DoHDFs CrdFt? Alodh 3n J file size\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 15. Single-stream throughput on LAN."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 9 shows the standard success probability M 1 = p s and the variance as a function of θ for α = 3, 4. Since the variance necessarily tends to zero for both θ → 0 and θ → ∞, it assumes a maximum at some finite value of θ. A numerical evaluation shows that for α = 3, the variance is maximized quite exactly at θ = 1, and for both values of α, the success probability at which the variance is maximized is p s = 0.38.\n\nMentions:\n- Fig. 9 shows the standard success probability M 1 = p s and the variance as a function of θ for α = 3, 4.\n\nOCR Text: \"~ann4 Uz4 ~Yaijnt 0e3 lo.os kceI 4348208998s #alee Leedtotiont\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 9. Success probabilityM1 and varianceM2 −M21 for α = 3 andα = 4."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The DNS-ML method was implemented using KBNNs. To show the convergence of the DNS-ML method, the lowest high-fidelity DNS energy computation at each iteration is plotted against the iteration index in Figure 11. The method was considered sufficiently converged after twelve iterations, with a computation time of 16.8 hours. The predicted equilibrium values for the features after twelve iterations of the DNS-ML method are presented in Table 8, along with computational time and resources used. The predicted equilibrium shapes at various iterations of the DNS-ML method are shown in Figure 12. Again, the predicted equilibrium values and shapes are given by the lowest energy high-fidelity DNS computation at each iteration. The FLOP count in Table 8 was estimated using the performance counter tool perf with the event pm flop on the Power8 Minsky nodes in the ConFlux cluster. The count was performed for a low-fidelity DNS, a high-fidelity DNS, one time step of the phase field model, and one round of KBNN training and testing. These values were then used to estimate the total FLOP count. Although the phase field model was run on XSEDE Comet and the TensorFlow training and testing of the KBNN on the ConFlux GPUs, the FLOP count for these computations was done on the Power8 nodes to ensure a consistent count method across all simulations. The FLOP count for KBNN training and testing was on the order of 1 × 10 14 , having a minimal effect on the total count. The equilibrium shapes predicted by the phase field and DNS-ML methods are compared in Figure 13.\n\nMentions:\n- To show the convergence of the DNS-ML method, the lowest high-fidelity DNS energy computation at each iteration is plotted against the iteration index in Figure 11.\n\nOCR Text: \"Convergence ol DNS-ML melhod 3 8 ~20 Ileralon\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 11: Convergence of the DNS-ML method for the single precipitate problem, based on the lowest energy high-fidelity DNS computation at each iteration."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We test our proposed model on a simulated population of 40,000 Plug-in Hybrid EVs (PHEV) that receive home charging. The operation of the aggregator is simulated for one full day, and the effects of shifting the load forward to the next day is captured by extending the duration of this study to 32 hours. Participating EVs in the DLS program arrive randomly, according to a non-homogeneous Poisson arrival process. The arrival rate, as well as the SoC and tolerable delay of each request, is simulated using real-world statistics in [35]. We divide arriving vehicles between 15 different clusters, representing different discrete SoCs between 0 to 4 hours and slack times between 1 to 3 hours (full charge corresponds to 5 hours of charging from a zero SoC). Charge requests are taken to be non-interruptible once started. We add the simulated Fig. 2. The day-ahead marginal price curve used in our case study charging load of this population on top of a typical daily base load scaled to have PHEV demand as 10% of peak load [45].\n- Prices π F (h) reflect those of the PJM day-ahead energy market 10/22/2013 (see Fig. 2). To only observe the effects that are of interest in this paper, we assume that the aggregator has access to perfect forecasts of its uncontrollable load, and the only uncertainty is in the deferrable load. To avoid observing effects of the aggregator trying to trade day-ahead prices against hour-ahead prices, we pick upward adjustment prices to be 20% higher than day-ahead prices, and downward adjustment prices to be 20% lower than day-ahead.\n\nMentions:\n- We add the simulated Fig. 2.\n- Prices π F (h) reflect those of the PJM day-ahead energy market 10/22/2013 (see Fig. 2).\n\nOCR Text: \"Tms\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2. The day-ahead marginal price curve used in our case study"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- It is well known that time domain data with edges and gaps lead to spectral leakage in the Fourier domain. The spectral leakage can be mitigated using window functions, but at the cost of lost data, and artificial nonstationarity. The latter point is not widely appreciated. Even using a Tukey window that is flat across a large fraction on the data results in a non-diagonal noise correlation matrix in the Fourier domain. Figure 10 shows the Fourier domain noise correlation function c k for 16 seconds of stationary white noise, x[i], sampled at 4096 Hz, with a Tukey window applied. The Tukey window had a rise and decay time of 0.4 s to mimic the settings in typical LIGO analyses [8]. The window is flat across 1−α of the data, with α = 0.8/16 = 0.05. The magnitude of the maximum correlation is of order α.\n\nMentions:\n- Figure 10 shows the Fourier domain noise correlation function c k for 16 seconds of stationary white noise, x[i], sampled at 4096 Hz, with a Tukey window applied.\n\nOCR Text: \"02)5 0015 Qj15 4025 0n3\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 10. The off diagonal elements of the Fourier domain noise correlation function ck for stationary white noise with a Tukey window function applied. The index k corresponds to the number of frequency bins between the data samples used to compute the correlation."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In this section we apply the proposed numerical procedure to forecast future expected values of market interest rates. We first consider a window of fixed size m of market interest rates that is rolled through time, each time adding a new rate and taking off the oldest one. The length of this window is the historical period over which we forecast the next expected spot rate value. It is worth noting that in case of large datasets, as with weekly observations, the methodology herein proposed necessarily requires to consider rolling windows of variable size. Thus, the step of the procedure relative to the dataset partition, described in Sections 4.1, is modified as follows. Fixed the initial length m of the historical data sample, say (r 1+h , . . . , r m+h ), h = 0, 1, 2, ..., the Lilliefors or the Kolmogorov-Smirnov test is carried out starting from the last observed rate r m+h and then moving backward in the sample up to detect the smaller interest rate value r m+h−s (s = 1, . . . , m − 1), such that the sub-goup (r m+h−s , . . . , r m+h ) has Normal or non-central Chi-square distribution. The value r m+h−s denotes the change point setting the window of latest interest rates, say (r m+h−s , . . . , r m+h ), which is taken into account to calibrate the parameters of the Vasicek/CIR model and forecast the next expected interest rate value r m+h+1 (the past interest rate values (r 1+h , . . . r m+h−s−1 ) are disregarded). Clearly, when the historical period is rolled through time, the size of the window over which to forecast a future rate may be variable since it depends on the detected change point. We applied the modified numerical procedure to forecast expected future nextweek interest rates. To explain our idea we refer to the first data sample considered in the previous section (n = 308 weekly observed EUR interest rates with maturity T = 30Y). The initial size of the historical data sample was fixed to m = 52 weeks. Steps 1-5 of the numerical procedure were applied to the historical market interest rates. Note that the calibration of the Vasicek/CIR model parameters with sub-groups of size smaller than 12 is not always possible when the optimal estimating function method mentioned in Section 4.3 is applied (see Remark 1). In this case two adiacent sub-groups are joined together. The sequence of forecast next-week expected values computed by formula (7) for both Vasicek and CIR models, has been compared with the sequence of future rates computed by the Exponentially Weighted Moving Average (EWMA) by considering a rolling window of fixed size m = 52 weeks. The EWMA is a weighting scheme to estimate future values averaging on historical data with weights that decrease exponentially at a rate λ throughout as the observations are far in the past (the reader can refer, for example, to Hull (Hull, 2012, Ch.II). The EWMA has been shown to be powerful for prediction over a short horizon and track closely the volatility as it changes. Indeed recent interest rates movement is the best predictor of future movement as it is not conditioned on a mean level of volatility. The forecasted sequences are plotted in Figure 7. The proposed numerical procedure clearly shows a better performance with respect to the EWMA model. Further, an error analysis to all data samples (63 maturities) available from Dataset I and II in Table 1 was carried out. Figure 8 compares the corresponding RMSE values computed by applying our numerical algorithm to the Vasicek and CIR model with the ones computed by the EWMA model. The initial size of the historical data sample was fixed to m = 52 weeks (the vertical black line differentiates samples of Dataset I from samples of Dataset II). In this case, too, a better performance of our procedure with respect to the EWMA model is confirmed. Further, the modified procedure to make predictions based on rolling windows of variable size highlights an improvement in forecast when a partition with non-central Chi-square distribution (CIR model) is considered.   1.\n\nMentions:\n- Figure 8 compares the corresponding RMSE values computed by applying our numerical algorithm to the Vasicek and CIR model with the ones computed by the EWMA model.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 8: Error analysis in the forecast procedure of future next-week expected interest rates. RMSE values computed by the proposed numerical procedure based on a rolling window of variable size: CIR model (blu line), Vasicek model (red line). RMSE values computed by the EWMA model (yellow line) based on a rolling window of fixed size m = 52 weeks. The vertical black line differentiates samples of Dataset I from samples of Dataset II in Table 1."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 1 shows the typical evolution of the correlation length relative to the Hubble length H −1 with a different (relatively large) initial correlation length. Hereafter we set z = 2.3 × 10 7 as initial time. The initial velocity is set to v = 0 except for the bottom line, which represents the standard, always-scaling case with initial velocity v = 0.65. While we chose such initial velocities, we also confirmed that the evolution of the correlation length is almost independent of the initial velocity, since the velocity decreases vanishingly and it loses its initial information quickly. As mentioned above, when L is larger than the horizon scale, it simply evolves in proportion to a. In terms of the redshift z, L/H −1 is proportional to z in the radiation dominated era (z z eq ) and z 1/2 in the matter dominated era (z z eq ), where z eq ≈ 3400 is the redshift at the matter-radiation equality. We can also see that it takes a few orders of redshift for the system to enter the scaling solution completely, which will be important for the observational signatures. Since this result shows the general feature of the evolution of the correlation length on superhorizon scales, it strongly suggests that even if the correlation length is much larger than the Hubble length just after inflation, the system gradually approaches the scaling solution and at a relatively late epoch, say z = 10 3 or later, starts to evolve in accordance with the scaling rule depending on the epoch of the phase transition during inflation. Note that if the phase transition takes place when the present horizon scale exited the horizon during inflation, the correlation length would become the horizon scale again today, since its initial correlation length can be estimated by the horizon scale at that time. Therefore, cosmic strings formed several e-folds after the current Hubble scale went out of the horizon during inflation would enter the scaling regime after the recombination. 3 To evaluate the onset time of scaling, we calculate the redshift when the correlation length L becomes 2L scaling , as well as the redshift when the correlation length falls shorter than the horizon. Here L scaling is the correlation length of the scaling network, which corresponds to the bottom line in Fig. 1. In Fig. 2, we show these redshifts as a function of the initial correlation length. This figure shows that there arises an intermediate epoch before the complete entrance into the scaling regime, which may be important for the CMB anisotropies.\n- Let us discuss first the effect of strings with delayed scaling on the temperature anisotropies to investigate how the constraint on the string tension changes qualitatively. Varying the initial correlation length, we show the angular power spectrum for the CMB temperature anisotropies induced by cosmic strings in the left panel in Fig. 3. For illustrative purposes, from top to bottom, we adopt the string tension Gµ = 3 × 10 −7 with the initial correlation lengths as (L/H −1 ) ini = 1.5, 7.5 × 10 3 , 4.5 × 10 4 , and 1.5 × 10 5 . With the help of Fig. 2, we can estimate the scalingonset time for each initial correlation length. Hence, we can treat the last three cases as the delayed scaling cases, while the first one corresponds to the system which already realizes the scaling by the time of the recombination.\n\nMentions:\n- In Fig. 2, we show these redshifts as a function of the initial correlation length.\n- With the help of Fig. 2, we can estimate the scalingonset time for each initial correlation length.\n\nOCR Text: \"sculing~onsz L-H\" nzumbinaliun ~I initial cornelatjon Jeueth (L'H' J L-ilaA; 0ai' Srh\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 2: The redshift at which the system enters the horizon and approaches the scaling solution as a function of the initial correlation length."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 12 shows the thermodynamic extrapolation of the LLL Coulomb ground state energies for these states. Table I shows the thermodynamic energies of these states as well as the critical Zeeman energies for the transitions among these states.\n\nMentions:\n- Fig. 12 shows the thermodynamic extrapolation of the LLL Coulomb ground state energies for these states.\n\nOCR Text: \"fully polarizedl (exactj partially polarixcd 'cxact; -05i1 spin singlec (exact; -0,55 0lod 0.05 0.10 0.15 1N\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 12: Thermodynamic extrapolation of the lowest Landau level Coulomb ground state energy for different spin polarized states at ν = 6/7."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We will now make the conjecture precise for the case of n = 1 and n = 2. We begin with the case of Spin(5). Here the 6d SCFT is the rank 2 E-string theory 4 . The direct compactification of this model to four dimensions is not known, but a mass deformation of it breaking the SU (2) global symmetry is known [9]. We can consider such a theory corresponding to flux 1 2 breaking E 8 → U (1) × E 7 . Due to the fractional flux the E 7 is broken to SO(8) so the global symmetry of the 4d theory is U (1) × SO (8). See figure 2 with the four dimensional theory. We can now preform an additional deformation by giving a vev to the flip field. This breaks both the U (1) and the R-symmetry which is replaced by a new Rsymmetry under which the symmetric and lower antisymmetric have R-charge 1. Therefore, a mass term for these two fields is consistent with all the symmetries and we expect it to be generated, eliminating these fields from the resulting theory.\n- We now see that the charged matter content of the resulting theory is the same as the one in the Spin(5) theory we studied though the two differ by the flipping field spectrum. As some operators hit the unitary bound in this theory we expect them to decouple. We do Figure 2. The 4d theory conjectured to be the result of the compactification and mass deformation of the rank 2 E-string. Here the gauge group is U Sp(4) and S and AS denote the symmetric, that is the adjoint for U Sp groups, and antisymmetric representations, respectively. Here there is a superpotential coupling the symmetric chiral to the quark bilinear and the middle antisymmetric to the symmetric bilinear. The theory has an SO(8) global symmetry represented by the square as well as a U (1) whose charges we denoted using the fugacity t. The black X on the symmetric chiral stands for an additional chiral field which couples linearly to the symmetric bilinear. In fact, due to the charges of the fields, the flip field also couples linearly to the lower antisymmetric bilinear, and so efficiently flips some combination of the two symmetric bilinears. There is also an anomaly free R symmetry under which the symmetric and lower antisymmetric have R charge 0, the quarks R charge 1 and the middle antisymmetric and flip fields have R charge 2.\n\nMentions:\n- See figure 2 with the four dimensional theory.\n- We do Figure 2.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2. The 4d theory conjectured to be the result of the compactification and mass deformation of the rank 2 E-string. Here the gauge group is USp(4) and S and AS denote the symmetric, that is the adjoint for USp groups, and antisymmetric representations, respectively. Here there is a superpotential coupling the symmetric chiral to the quark bilinear and the middle antisymmetric to the symmetric bilinear. The theory has an SO(8) global symmetry represented by the square as well as a U(1) whose charges we denoted using the fugacity t. The black X on the symmetric chiral stands for an additional chiral field which couples linearly to the symmetric bilinear. In fact, due to the charges of the fields, the flip field also couples linearly to the lower antisymmetric bilinear, and so efficiently flips some combination of the two symmetric bilinears. There is also an anomaly free R symmetry under which the symmetric and lower antisymmetric have R charge 0, the quarks R charge 1 and the middle antisymmetric and flip fields have R charge 2."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Next, we study the impact of the size of the \"hole\"; i.e. N on the expected per packet delay. Under DIVBAR the packets that arrive at node 2 from source 1 are likely to be forwarded and wander between nodes 12, 13 . . . , 12 + (N − 1) before eventually forwarding to 4. In contrast, increasing N has no effect on the performance of D-ORCD. This is because V 7 2 (t) &lt; V 7 12+i (t), i = 0, 2, . . . , N − 1, for all time slots t, in effect, preventing the packets to enter the \"hole\". Fig. 7 provides the expected delay encountered by the source packets under various routing policies, as the size of the \"hole\", N , increases and the arrival rate is set to low value of λ 1 = 200 kbps. The figure shows that the average delay under D-ORCD is significantly lower than other candidate protocols as N increases from 1 to 5.\n\nMentions:\n- Fig. 7 provides the expected delay encountered by the source packets under various routing policies, as the size of the \"hole\", N , increases and the arrival rate is set to low value of λ 1 = 200 kbps.\n\nOCR Text: \"@ OHCD EO3 DIVE4R EJMzAF 3 1 Mtedtce:\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 7. Performance for Canonical Example for λ=200 kbps"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The analysis of keywords associated with the number of persons involved allows to build the distribution function of the number of articles, N k , reporting k people killed. As shown in Figure 7, the distributions for all four newspapers considered can be described by a single power-law:\n- In our case, using the articles where a location could be determined, we built distribution laws for domestic and foreign events. In Figure 7 it is possible to see that the latter have a strong spectral break for 2 ≤ k ≤ 10 (absent or less prominent in the former). Fitting the distribution function with two power laws γ H = γ(k &gt; 10) and γ L = γ(2 ≤ k ≤ 10) we see that ∆γ = γ L − γ H is always negative for foreign events, ranging from ∆γ REP = −0.511 ± 0.005 to ∆γ CDS = −1.09 ± 0.01. For Italian newspapers featuring domestic events, the distributions are closer to a pure power law (highest ∆γ = +0.28 ± 0.02 for CDS). In all cases ∆γ is positive, implying that a higher emphasis is given to low k events. NYT has a smaller discrepancy between foreign (∆γ = −0.672 ± 0.003) and domestic events (∆γ = −0.42 ± 0.01). A plot of the value of γ vs ∆γ (Figure 9) shows that the foreign and domestic categories are clearly separated in all four newspapers (Ta-Newspaper Loc.\n\nMentions:\n- As shown in Figure 7, the distributions for all four newspapers considered can be described by a single power-law:\n- In Figure 7 it is possible to see that the latter have a strong spectral break for 2 ≤ k ≤ 10 (absent or less prominent in the former).\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 7: Number of articles Nk as a function of persons killed k mentioned in each article. The full distributions have been fitted with a single power law (values in Table 2), the domestic/foreign ones with two power laws γL (2 ≤ k ≤ 10) and γH (k > 10). For all foreign events there is a break in the spectral index γL < γH due to missing events not being reported (from 91% for REP to 122% for CDS). In domestic events γL > γH for Italian papers (over-reporting of low k events, from 47% in STA to 71% for CDS). In NYT whereas the decrease in γL for domestic events is lower (over-reporting of 4%) than for foreign ones (under-reporting of 98%), hinting to a higher degree of internationalization of this publication (Table 3)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The 24 µm data are highly sensitive to geometric effects in protostellar cores (see Dunham et al. 2008;Ragan et al. 2012). A proto-star in which outflows have carved a cavity in its natal core will be detectable at 24 µm if our line of sight is aligned with the cavity. In particular the shocked molecular hydrogen, visible as green structures in Figure 1 (right, bottom panel), traces high velocity outflows. They originate from the rotational S9-S11 transition of excited H 2 in the IRAC 4.5 µm band. They are generally referred to as either \"green and fuzzy features\" (Chambers et al. 2009) or \"extended green objects\" (EGOs, Cyganowski et al. 2008). Because the 24 µm flux arises primarily from the warm material immediately surrounding the proto-star (and not the cold outer region of the core), we do not include the 24 µm flux in our SED fit to the PACS data but instead fit a second modified blackbody component (see Figure 6). The best fit to the SED results from the sum to these two components. For Source 8, we used one modified blackbody function. The fit parameters are presented in Table 2.\n\nMentions:\n- Because the 24 µm flux arises primarily from the warm material immediately surrounding the proto-star (and not the cold outer region of the core), we do not include the 24 µm flux in our SED fit to the PACS data but instead fit a second modified blackbody component (see Figure 6).\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 6.— The modified blackbody fits for the detected sources (see Section 3.5)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 5 shows that the per-thread throughput and processor aggregate throughput scales predictably, according to the number of threads active (i.e. in the running state) in the system. The data is taken from [9] and shows the effect of the 4slot pipeline in the XS-1 architecture: per-thread throughput if constant for up-to four threads, then declines linearly. We also see that processor aggregate throughput is maximised when at least four threads are active.\n\nMentions:\n- Fig. 5 shows that the per-thread throughput and processor aggregate throughput scales predictably, according to the number of threads active (i.e. in the running state) in the system.\n\nOCR Text: \"Perthread througrdu Aggregate ~througnou Mber ofachwe threads\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: Scaling of thread throughput with number of threads on the XS-1 architecture (at 500MHz clock speed)"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- • Correlation Test: For vectors with different covariances under the null hypothesis, we show that the LMPIT is given by the Frobenius norm of the sample coherence matrix. The block diagram of the LMPIT is depicted in the lower part of Figure 1, where we can see that the whitening is different for each vector. We should also point out that the Frobenius norm of the coherence matrix, or similar squared-sum test statistics, have been previously proposed as approximations of the GLRT that might be computationally simpler or have some other advantage [18], [19], [24], [25]. Here, we prove for the first time that the Frobenius norm of the coherence matrix is in fact the LMPIT for this problem.\n- • Sphericity Test: When the vector-valued observations under the null hypothesis are independent and identically distributed, we show that the LMPIT is given by the Frobenius norm of a normalized version of the sample covariance matrix. Figure 1 also shows the block diagram of the LMPIT for this problem. However, in this case, the whitening is common for all vectors, and it is given by the inverse square root matrix of the average of the individual sample covariance matrices.\n- LMPIT for Sphericity Fig. 1. Block diagram of the LMPIT for the testing problems considered in this paper. Whiten denotes a whitening step, that is, the input vector is multiplied by the inverse of the square root matrix that enters into the block,R is a block that computes the sample covariance matrix of the input-vectors, whereasR kk computes the covariance matrix of the kth vector, 1/L averages the input matrices, and • 2 denotes the squared Frobenius norm of the input-matrix. As can be seen, the main difference among the detectors is the whitening step. For testing correlation among Gaussian vectors, the whitening step is different for each vector, whereas for testing sphericity the same whitening is applied to each vector. matrix. From a practical point of view, this means that for close hypotheses the spatial structure does not play any role for detection purposes. In other words, the use of subspace-based estimation techniques does not help for detection, at low signal-to-noise ratios (SNR) and/or low sample support.\n\nMentions:\n- The block diagram of the LMPIT is depicted in the lower part of Figure 1, where we can see that the whitening is different for each vector.\n- Figure 1 also shows the block diagram of the LMPIT for this problem.\n- LMPIT for Sphericity Fig. 1.\n\nOCR Text: \"ii Ven\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1. Block diagram of the LMPIT for the testing problems considered in this paper. Whiten denotes a whitening step, that is, the input vector is multiplied by the inverse of the square root matrix that enters into the block, R̂ is a block that computes the sample covariance matrix of the input-vectors, whereas R̂kk computes the covariance matrix of the kth vector, 1/L ∑ averages the input matrices, and ‖ · ‖2 denotes the squared Frobenius norm of the input-matrix. As can be seen, the main difference among the detectors is the whitening step. For testing correlation among Gaussian vectors, the whitening step is different for each vector, whereas for testing sphericity the same whitening is applied to each vector."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In parallel to the MDI analysis, the Global Oscillation Network Group (GONG: Harvey et al., 1996) has done an independent medium-ℓ analysis of dopplergrams taken from six ground-based observatories (the GONG network), using the same spectral line and cadence as MDI. Although the two analyses are generally in good agreement, in certain areas the inferences drawn by the two projects differ by more than their errors. In particular, the above-mentioned bump is absent in the GONG analysis. Likewise, the MDI analysis indicates a polar jet at a latitude of about 75 • , shown in Figure 2, which is not seen in the GONG analysis. Excluding the modes that contribute to the bump does not remove this high-latitude jet. Although the jet may be a real feature, the fact that it is not seen in the full-disk analysis of MDI data makes this questionable (Larson and Schou, 2009). Until such discrepancies can be resolved, the analysis results must remain in doubt, and the issue has been studied at length by several investigators with little success .\n- in perfect analogy with Equation ( 26) (Schou, Christensen-Dalsgaard, and Thompson, 1994). We formed common modesets and averaged them using the same method described above for one-dimensional inversions, and used tradeoff parameters of µ r = 10 −6 and µ θ = 10 −2 for the radial and latitudinal regularization terms respectively. Using this relatively high value for µ θ should dampen variations in latitude (Howe et al., 2000). The results are shown in Figure 19; the jet is more pronounced in this plot than in Figure 2, which can be attributed both to the different modeset and to the smaller errors resulting from averaging. Although in every updated analysis the polar jet actually had a greater magnitude than in the original analysis, the gapfilling resulted in a reduced rotation rate in the lower convection zone, which brings our result closer to agreement with inferences drawn by the GONG analysis .\n\nMentions:\n- Likewise, the MDI analysis indicates a polar jet at a latitude of about 75 • , shown in Figure 2, which is not seen in the GONG analysis.\n- The results are shown in Figure 19; the jet is more pronounced in this plot than in Figure 2, which can be attributed both to the different modeset and to the smaller errors resulting from averaging.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2. Rotation profile for the same time interval shown in Figure 1. The curves correspond to the latitudes indicated."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- As shown by Dysthe and Trulsen (1999), there is a relationship among the breather solutions of the NLS equation. Different parameters are used for explicit expressions of the SFB and the Ma breather in Dysthe and Trulsen (1999) as well as in Grimshaw et. al. (2001). Introducing new parameters ϕ and ϑ ∈ R by the following relations:ν = √ 2 sin ϕ,σ = sin 2ϕ,μ = √ 2 sinh ϑ andρ = sinh 2ϑ, we can write our expressions similar to the ones in these papers. T T r r r r r r r r r r r r r r r j% The SFB becomes the Ma breather if we substitute ν = iµ and it becomes the rational breather if ν → 0. Similarly, the Ma breather becomes the rational breather for µ → 0. Interestingly, the Ma breather becomes the one-soliton solution for µ → ∞ (Akhmediev and Ankiewicz, 1997). Figure 2 explains the schematic diagram of these derivations. These relations can also be seen from the expression in the previous section. For the SFB, we take ζ(τ ) = cos(ντ ), which leads to the Ma breather if we substitute ν = iµ, so that ζ(τ ) = cos(iµτ ) = cosh(µτ ). Taking either ν or µ → 0, we get\n\nMentions:\n- Figure 2 explains the schematic diagram of these derivations.\n\nOCR Text: \"Mr Van aLyk watol Ma Ieatbel \"+0 Earjobal Meaba 7+0\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: The schematic diagram for the derivation of the Ma breather and the rational breather from the SFB."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The results of applying the above procedure to the four DA white dwarfs for which the spectroscopic and photometric data are complete (discarding the discrepant object with the emission components in the Balmer lines) are shown in Table 2 and graphically in Figure 7. Included in the Table are the three fitted parameters, A V , T eff , and log g, intrinsic normalized magnitude before reddening, the extinction in each band, and the net predicted magnitude per band. Both synthetic quantities include the normalization constant, c i . Comparison of column 9 with column 3 (predicted vs. observed magnitudes with WFC3/HST ) corresponds to the fit residuals per band given in column 10. Note that the extreme residuals are +/ − 0.006 with rms = 0.003 mag.\n- If we use the extinction law of O'Donnell (1994) instead of Fitzpatrick (1999), the derived A V values change by 1 milli-mag, thereby indicating that our experiment is insensitive to differences between these forms of the reddening law. Wavelength (Å) Fig. 7.-Extinction curve fits with R V = 3.1 using the reddening law of Fitzpatrick (1999) (solid) and O'Donnell (1994) (dashed) for four objects in our sample, spanning a range from low to high reddening. The largest differences in the derived value of A V using the two different extinction laws is seen for SDSS-J010322.19-002047.7 and SDSS-J120650.40+020142.4 where it is 0.001 mag. The mean residuals between the data and the fit extinction curve are 3 mmag.\n\nMentions:\n- The results of applying the above procedure to the four DA white dwarfs for which the spectroscopic and photometric data are complete (discarding the discrepant object with the emission components in the Balmer lines) are shown in Table 2 and graphically in Figure 7.\n- Wavelength (Å) Fig. 7.-Extinction curve fits with R V = 3.1 using the reddening law of Fitzpatrick (1999) (solid) and O'Donnell (1994) (dashed) for four objects in our sample, spanning a range from low to high reddening.\n\nOCR Text: \"ntirith  ~ W 3Ds3-Ju18322.19-0u2u1?.5 4; SDSS-J1024J0 9)-003207.0 Wu 4ixss-1155Q.40 0130142.4 Aled 30S3-1164000.4#+1mx71R.7 Ma U4--endl\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 7.— Extinction curve fits with RV = 3.1 using the reddening law of Fitzpatrick (1999) (solid) and O’Donnell (1994) (dashed) for four objects in our sample, spanning a range from low to high reddening. The largest differences in the derived value of AV using the two different extinction laws is seen for SDSS-J010322.19-002047.7 and SDSS-J120650.40+020142.4 where it is 0.001 mag. The mean residuals between the data and the fit extinction curve are 3 mmag."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We investigated the C 60 , C 36 , and C 20 fullerenes. We studied a total of five different geometries of C 20 whereas only one conformation for other fullerenes was studied. All HF calculations were performed with wavefunction stability analysis to ensure the local stability of solutions. The pertinent cGHF electronic Hessian is provided in the Appendix. All calculations were carried out with a development version of Q-Chem. 58 All correlation calculations employed the frozen core approximation for the sake of computational efficiency. All the plots were generated with Matplotlib 59 and all the molecular figures were generated with Chemcraft. 60 Obtaining quantitatively accurate answers with CC methods is very computationally intensive for fullerenes so we look for qualitative answers by comparing them to various OOMP2 methods. We will employ the STO-3G basis set in order to exaggerate the effect of strong correlation and discuss the implication within this basis set. We only present the CC data for C 60 and C 36 here; the CC data for C 20 showed the same conclusions as the other analyses based on OOMP2 we present below. C 60 is a well-known electron paramagnetic resonance silent molecule 61 and its geometry is shown in Figure 3. Therefore, it is undoubtedly a molecule with a singlet ground state.\n\nMentions:\n- C 60 is a well-known electron paramagnetic resonance silent molecule 61 and its geometry is shown in Figure 3.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: The Ih molecular structure of C60."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 2. The ratio of the electron temperature upstream and downstream (T e,2 /T i,2 ), resulting from different assumptions. The dashed line corresponds to Eq. 19 (adiabatic heating and thermalisation), the solid line corresponds to Eq. 10. For illustrative purposes we also show the effect of adiabatic compression alone (dotted line) and the upstream and upstream temperature ratios for ions (dot-dashed line).\n- Note that we made use of Eq. 8 in order to introduce the sonic Mach number. One recognises in this equation again the two sources of heating, with the first term being associated with adiabatic heating by the ions and the second term being associated with the thermalisation of the electron kinetic energy, which asymptotically approaches Eq. 2. We illustrate the effects of adiabatic heating and thermalisation of the kinetic energy of the electrons in Fig. 2, which shows the ratio of downstream over upstream electron temperatures as given by Eq. 19 and comparing it to the result of Eq. 10 and adiabatic heating only (i.e. if one would use q e,ad as given in Eq. 17). It shows that at low Mach numbers Eq. 19 results in only adiabatic heating, whereas for very high Mach number Eq. 19 asymptotically approaches Eq. 10.\n\nMentions:\n- Fig. 2.\n- We illustrate the effects of adiabatic heating and thermalisation of the kinetic energy of the electrons in Fig. 2, which shows the ratio of downstream over upstream electron temperatures as given by Eq. 19 and comparing it to the result of Eq. 10 and adiabatic heating only (i.e. if one would use q e,ad as given in Eq. 17).\n\nOCR Text: \"Iharmeliga-inn Eictawns: adabalc Iaercelojn jocojIc Jecti-j\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2. The ratio of the electron temperature upstream and downstream (Te,2/Ti,2), resulting from different assumptions. The dashed line corresponds to Eq. 19 (adiabatic heating and thermalisation), the solid line corresponds to Eq. 10. For illustrative purposes we also show the effect of adiabatic compression alone (dotted line) and the upstream and upstream temperature ratios for ions (dot-dashed line)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The slope dg * /dK is estimated by the finite difference ∆g * over a small coupling interval around K c , and C is an unimportant constant. The resulting finite-size analysis for α = 0.2 is illustrated in Fig. 3, and we find ν = 0.49(1). This is very close to the expected (mean-field) value ν = 1/2 (Ref. 10), which is reasonable given that z ≈ 2. We finally note that, whereas increasing α does not lead to a significant change of z, it certainly does increase the prefactor a of the scaling relation L τ ∼ L z and thereby the peak position L * τ . This reflects the increased anisotropy of the interactions, and can be seen also for α = 0 when K and K τ are allowed to vary freely. At criticality one has a = 1 for K τ = K, with increasing a for increasing anisotropy K τ /K. In fact, for the analytically solvable 2D Ising model there even exists an exact mapping between system size anisotropy (i.e. a) and interaction anisotropy (i.e. K τ /K). 32\n\nMentions:\n- The resulting finite-size analysis for α = 0.2 is illustrated in Fig. 3, and we find ν = 0.49(1).\n\nOCR Text: \"25.35 1/ 45 1.55 46 Jug L\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 3: Finite-size analysis for obtaining 1/ν for the 2D quantum Ising model. Here we have evaluated the slope of the Binder cumulant g∗ around K = 0.160312 for α = 0.2, which yields ν = 0.49(1). The straight line represents a least squares fit to the data points."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The rate of work done by the swimmer divided by that in the Newtonian fluid with identical total viscosity is displayed in Fig. 3 as a function of W e. The power needed to swim decreases for all cases considered and seems to approach a constant asymptotic value at large Weissenberg number. It is important to note that, if scaled with the solvent viscosity, the actual value of the work performed by the microorganism is increasing when decreasing β. However, the work done is significantly less that that of a swimmer in a Newtonian fluids with the same viscosity, similarly to what was observed for the swimming sheet [40]. This relative power saving in a polymeric fluid increases with Weissenberg numbers in all cases.\n\nMentions:\n- The rate of work done by the swimmer divided by that in the Newtonian fluid with identical total viscosity is displayed in Fig. 3 as a function of W e.\n\nOCR Text: \"Je0.1 Je03 J=0,6 1.00_ 0.80- Logl\" 0.60- 0.40- 0.20 0.00_ 0D4\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 3: (Color online) Swimming power P divided by that in the Newtonian fluid with identical total viscosity, versus Weissenberg number, We, for three values of the viscosity ratio, β: 0.1 (red squares), 0.3 (green circles), 0.6 (blue triangles)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- An overview of our rank aggregation proposal is shown in Figure 1, which highlights two stages -offline and online -and four numbered components.\n- . . , V n } be the fusion vector set related to the response set of a certain collection. A fusion vector indexer creates an index of V, in order to allow efficient searches of fusion vectors in response of a query fusion vector, as previously shown in Figure 1.\n- we also analyze the efficiency of our method comprising its alternative formulations, and also compare to FG, the main baseline. We measure the mean time spent per query, for each combination of rankers in each dataset. The elapsed time for query retrieval refers to the sum of the times spent for fusion graph extraction, fusion vector extraction (graph embedding), and object retrieval, as represented by the online stage in Figure 1. The mean time of 5 independent measurements is reported. For the baseline FG, the same steps but the fusion vector extraction (absent) is taken into consideration.\n\nMentions:\n- An overview of our rank aggregation proposal is shown in Figure 1, which highlights two stages -offline and online -and four numbered components.\n- A fusion vector indexer creates an index of V, in order to allow efficient searches of fusion vectors in response of a query fusion vector, as previously shown in Figure 1.\n- The elapsed time for query retrieval refers to the sum of the times spent for fusion graph extraction, fusion vector extraction (graph embedding), and object retrieval, as represented by the online stage in Figure 1.\n\nOCR Text: \"Mfang@.h Han be:le uuizn grand Juii edntin @ Ezroliuiz TpM (ezho s ezbd in | ~ilie Ketel go.h (ecnne imze:dng 2 [Cipto? Jusongo:h5! Tn niz) He:u Ftad\" UEO \"ICEC Eine %( eebraclisa 'l Imsere fnc Ep\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Schematic view of the proposed method."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The difference in time values measured at both ends of the strip gives information on the hit position. The precision of the position measurement can be estimated by the distribution of this time difference. Here two thirds of the speed of light is taken as the speed of signal on the strip. The measured position resolutions for all gas mixtures are shown in figure 10. Depending on the gas mixture types and the rate, they vary from 1 cm to 1.3 cm at 20 kV.\n\nMentions:\n- The measured position resolutions for all gas mixtures are shown in figure 10.\n\nOCR Text: \"4s + LFY| ecde M EcDie Fconiot econioatr ~F-eFCO IC  Wat FCO SFG IA BUYie 5271 HVIkVi\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 10: Position resolution."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Each of the graphs in Fig. 4 and Fig. 7 specifies a particular process and defines a hidden Markov model. While there are still an infinite number of possible processes in the limit of indefinitely long sequences, a kind of order has been imposed. We can, for example, enumerate all the processes over a two-symbol alphabet with just one state. There is but one, and it is shown in Fig. 4c. (Figs. 4a,b are just special cases of Fig. 4c.) For two-state binary processes, there are thirteen [66]. 10 For binary processes, the number of distinct processes up to six states has been tabulated [73]. Thus, chaotic crystallography does for disordered materials much the same service that classical crystallography does for perfectly ordered ones: it organizes and structures the space of possible atomic arrangements. Further, it allows comparison of the hidden Markov models between different materials in much the same way that crystal structures of different materials are compared according to which, for example, crystal system they belong.\n\nMentions:\n- Each of the graphs in Fig. 4 and Fig. 7 specifies a particular process and defines a hidden Markov model.\n- There is but one, and it is shown in Fig. 4c.\n- (Figs. 4a,b are just special cases of Fig. 4c.)\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4. -Machines describing the stacking of ice I. Nodes represent causal states and are connected by arcs labeled s|p, where s is the symbol emitted and p is the probability of making such a transition. (a) The -machine for ice Ih and (b) ice Ic. Models for disordered stacking sequences of close-packed structures were introduced by Jagodzinski [62,63]. The model parameter specifying the range of influence between MLs is called the Reichweite s. (c) The simplest -machine in the hc-notation that gives an ice Isd stacking sequence. (d) The -machine for s= 3 ice Isd and (e) s= 4 ice Isd."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Technically, the proper choice of α can be made by looking at a range of α, in which we obtain the relatively largest values of i while the corresponding R i is around 1. As a rule of thumb, C α should be designed such that R i ≈ 1 occurs, where the values of i are relatively maximum. With this in mind, we adopt C α =cα 2 , wherec = 1500. Figure 5.2 illustrates the variation of ν α versus α ∈ [0, 1] for the specified properties of Case (I) and Case (II) in Tables 5.1 and 5.2 at room temperature.\n\nMentions:\n- Figure 5.2 illustrates the variation of ν α versus α ∈ [0, 1] for the specified properties of Case (I) and Case (II) in Tables 5.1 and 5.2 at room temperature.\n\nOCR Text: \"Case (D) Casc !\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 5.2: να versus α for ν = 1.85 × 10−4 in Case (I) and ν = 10−3 in Case (II)"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We provide evidence that random perturbations in image space induce larger changes in z than comparable natural perturbations in image space. Figure 28 shows a plot of the norm of the change in image space, averaged over 100 test images, as a function of the size of a perturbation in latent space. Natural directions are given by the interpolation between the latent representation of two test images. For the denoising problem, this difference in sensitivity indicates that the optimization algorithm might obtain a larger decrease in z by an image modification that reduces unnatural image components than by a correspondingly large modification in a natural direction. To further illustrate this point, we investigate in Figure 29 the decay of the singular values of the Glow model's Jacobian for random points.\n\nMentions:\n- Figure 28 shows a plot of the norm of the change in image space, averaged over 100 test images, as a function of the size of a perturbation in latent space.\n\nOCR Text: \"rndcm-dircction natu al E 3 direc:'cn\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 28. The magnitude of the change in image space as a function of the size of a perturbation in latent space. Solid lines are the mean behavior and shaded region depicts 95% confidence interval."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The DOS of standard Hubbard model is particle-hole symmetric at half filling. Nevertheless, in future consideration of the NNN hopping integrals, the DOS becomes asymmetric at half filling. Fig. 1 shows the DOS of the twoorbital Hubbard model with K = t ′ 1 /t 1 = t ′ 2 /t 2 = 0.1 for different interactions U, where t 1 and t 2 are the NN hopping for the wide and narrow bands, and t ′ 1 and t ′ 2 represent the NNN hopping accordingly. The particle-hole symmetry of the DOS is broken by the NNN hopping for both the wide and narrow bands, and the asymmetry becomes more distinct in the conditions with weak interactions.\n- Most of the theoretical studies have paid attention to the OSMT in the systems with particle-hole symmetry at half filling [14,15,16,28]. By introducing the NNN hopping, we could find out whether the OSMT exists when the particle-hole symmetry is broken at half filling. As shown in Fig. 1(a) and (d), resonance peaks appear at Fermi level of DOS of both the wide and narrow bands for weak interactions U = 2.0, suggesting that the two orbitals are all metallic. OSMP appears when the onsite interaction increases to U = 3.0, as shown in Fig. 1(b) and 1(e). In Fig. 1(b) the wide band is still metallic with resonance peaks at Fermi level, but a Mott gap opens around the Fermi level of the narrow band (Fig. 1(e)). Further increasing interactions to U = 4.5, both the wide and narrow bands transit to Mott insulating phase. Therefore, OSMT is still found in two-orbital Hubbard model with both NN and NNN hopping, where the DOS is asymmetric at half filling.\n\nMentions:\n- Fig. 1 shows the DOS of the twoorbital Hubbard model with K = t ′ 1 /t 1 = t ′ 2 /t 2 = 0.1 for different interactions U, where t 1 and t 2 are the NN hopping for the wide and narrow bands, and t ′ 1 and t ′ 2 represent the NNN hopping accordingly.\n- As shown in Fig. 1(a) and (d), resonance peaks appear at Fermi level of DOS of both the wide and narrow bands for weak interactions U = 2.0, suggesting that the two orbitals are all metallic.\n- OSMP appears when the onsite interaction increases to U = 3.0, as shown in Fig. 1(b) and 1(e).\n- In Fig. 1(b) the wide band is still metallic with resonance peaks at Fermi level, but a Mott gap opens around the Fermi level of the narrow band (Fig. 1(e)).\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: (Color online) Density of states of wide band (upper panel) and narrow band (lower panel) for different onsite interactions: U = 2.0 ((a) and (d)), U = 3.0 ((b) and (e)), and U = 4.5 ((c) and (f)). The parameters of the two-orbital Hubbard model are: t2/t1 = 0.4, K = t ′ 1 /t1 = t ′ 2 /t2 = 0.1, J = U/4, and U ′ = U − 2J . t1 (t2) and t ′ 1 (t′ 2 ) are nearest neighbor and next-nearest neighbor hopping integrals of the wide (narrow) band. Energies are in unit t1, and the energy broadening factor is ǫ=0.01."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Our automatic express dispatching system is composed of two components, the vision processing system and the manipulator. As for the vision processing system, two Re-alSense D435 cameras are employed to provide color and depth images. The color images are used to detect objects and recognize the barcodes, while the depth images are fed to grasping planning algorithm. As for the manipulator, we use one UR10 robot mounted with an end-effector which support the usage of switching between using two-fingered gripper and a vacuum suction cup. All the programs of the system are implemented with Robot Operating System. Fig. 2 shows the whole system and the flowchart of the information processing pipeline is shown in Fig. 3.\n\nMentions:\n- Fig. 2 shows the whole system and the flowchart of the information processing pipeline is shown in Fig. 3.\n\nOCR Text: \"TAStitnnd G1\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3. The flowchart of the processing in the express package dispatching system"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- For the first test case we fix the left and right boundaries at the same height and apply the surface forcesḡ on the right boundary. Under these circumstances, straight rods with varying thickness that connect the boundaries can be expected as solutions of the bi-criteria shape optimization problem (13). The numerical studies in [11] support this intuition, see Figure 3 for some exemplary results. [11] and the initial shape x 0 . This is the motivation for using a discretized straight rod with constant thickness of 0.2 m as the initial shape x 0 for Pareto front tracing, see Figure 3c, even though this particular shape was not the outcome of any weighted sum scalarization considered in [11]. To determine a corresponding weight λ 0 such that x 0 is J λ 0 -critical the equation ∇J λ (x 0 ) = 0 is solved for λ 0 ∈ (0, 1). The resulting weight has the value λ 0 ≈ 0.813 and we therefore have x 0 ≈ x(0.813). Numerical integration in [λ l , λ u ] = [λ 0 − 0.66, λ 0 + 0.1] with a step size of h = 0.01 resulted in solutions of varying thickness that are also straight rods and hence coincide with the results of [11], see Figure 4 for some exemplary shapes corresponding to those from Figure 3. In Figure 5 the outcome vectors obtained from numerical integration are compared in the ob-jective space with the outcome vectors obtained in [11] from the repeated solution of weighted sum scalarizations using a gradient descent algorithm. The results nicely document that the Pareto tracing approach not only covers the weighted sum solutions, but also approximates a larger part of the (local) Pareto front. Figure 6 shows the results from evaluating the first and the second order optimality conditions during the course of Pareto tracing, validating the statement of Proposition 5(ii). Indeed, the results nicely show that the computed shapes consistently achieve good w.r.t. first and second order optimality tests.\n\nMentions:\n- In Figure 5 the outcome vectors obtained from numerical integration are compared in the ob-jective space with the outcome vectors obtained in [11] from the repeated solution of weighted sum scalarizations using a gradient descent algorithm.\n\nOCR Text: \"FiA Keau;e\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: Comparison of the outcome vectors of Pareto tracing (blue) starting in x0 and the outcome vectors obtained from the repeated application of gradient descent in [11] (green)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Thus, S = 30 particles were used to construct the other plots. Remarkably, with only S = 5 particles, the proposed algorithm captures a significant portion of the maximum possible goodput gain over the FP-RUS. Figure 3 plots average sum-goodput versus the fading rate α. There we see that, at low fading rates (i.e., small α), the proposed greedy scheme achieves an average sum-goodput that is much higher than the FP-RUS and, in fact, not far from the CGG upper bound. For instance, at α = 10 −4 , the sum-goodput attained by the proposed scheme is 92% of the upper bound and 170% of that attained by the FP-RUS. As the fading rate α increases, we see that the sum-goodput attained by the proposed scheme decreases, and eventually converges to that of the FP-RUS. This behavior is due to the fact that, as α increases, it becomes more difficult to predict the SSGs using delayed ACK/NAK feedback, thereby compromising the scheduling-andresource-allocation decisions that are made based on the predicted SSGs. In fact, one can even observe a gap between the CGG and NCGG for large α because, even with delayed perfect-SSG feedback, the current SSGs are difficult to predict. Figure 3 reveals a gap between the proposed scheme and the CGG bound that persists as α → 0. This non-vanishing gap can be attributed-at least in part-to greedy scheduling under ACK/NAK feedback. Intuitively, we have the following explanation. Because the inferred SSGdistributions of not-recently-scheduled users quickly revert to their apriori form, the proposed greedy algorithm will continue to schedule users as long as their SSGs remain better than the apriori value. There may exist, however, not-recently-scheduled users with far better SSGs who remain invisible to the proposed scheme, only because they have not recently been scheduled. Figures 4 and 5 plot average sum-goodput versus the number of subchannels (i.e., total bandwidth) N. In Fig. 4, the total BS power X con is scaled with N such that the per-subchannel SNR remains fixed at 10dB, whereas, in Fig. 5, the total BS power X con remains invariant to the bandwidth N, and is set such that per-subchannel SNR = 10dB for N = 32. In both cases, the average sum-goodput increases with bandwidth N, as expected, since the availability of more subchannels increases not only scheduling flexibility, but also the possibility of stronger subchannels, which can be exploited by the BS. In Fig. 4, where the per-subchannel SNR is fixed, the sum-goodput increases linearly with bandwidth N, as expected. In all cases, the proposed greedy scheme captures about 80% of the sum-goodput gain achievable over the FP-RUS. Figure 6 plots average sum-goodput versus the number of available users K. It shows that, as K increases, the average sum-goodputs achieved by the NCGG, CGG, and the proposed greedy schemes increase, whereas that achieved by the FP-RUS remains constant. This behavior results because, with the former schemes, the availability of more users can be exploited to schedule users with stronger subchannels, whereas with the FP-RUS scheme, this advantage is lost due to the complete lack of information about the users' instantaneous channel conditions. Figure 6 also suggests that, as K increases, the sum-goodput of the proposed greedy scheme saturates. This can be attributed to the fact that the proposed greedy algorithm can only track the channels of recently scheduled users, and thus cannot benefit directly from the growing pool of not-recently-scheduled users.\n\nMentions:\n- In Fig. 4, the total BS power X con is scaled with N such that the per-subchannel SNR remains fixed at 10dB, whereas, in Fig. 5, the total BS power X con remains invariant to the bandwidth N, and is set such that per-subchannel SNR = 10dB for N = 32.\n\nOCR Text: \"NOS Aicdie Glegde\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 5. Average sum-goodput versus number of subchannelsN . Here,K = 8, Xcon does not scale withN and it is chosen such thatSNR = 10dB for N = 32, α = 10−3, andS = 30."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In the next step, we built a bio-entity citation network based on the concept that if paper A cites paper B, then an entity in paper A will be considered to cite an entity in paper B. Entities in the citing paper are paired with entities in all the cited papers (see Figure 3). A hash table is used to store the entity citation associations and their occurance frequency.\n\nMentions:\n- In the next step, we built a bio-entity citation network based on the concept that if paper A cites paper B, then an entity in paper A will be considered to cite an entity in paper B. Entities in the citing paper are paired with entities in all the cited papers (see Figure 3).\n\nOCR Text: \"Erhns G\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3. From paper citation to entity citation."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 4 shows the induction time as a function of shear rate at 250K. There are two regimes, one in which flow has no effect on the induction time, and one where the induction time decreases as a power law in the shear rate. This observation agrees with experimental results 16,75 as well as with the theoretical work by Grizzuti and coworkers 16,76 . Based on the assumption that shear can only affect nucleation if the sheared chains do not have enough time to relax back into their equilibrium structure, the crossover is expected to occur at Weissenberg number τ maxγc ≈ 1, where τ max is the longest relaxation time in the system, andγ c is the critical shear rate, at which the induction time begins to drop. In our simulation dataγ c can be estimated from the intersection of the line (continuous) drawn through the induction time data at high shear rates and a horizontal line (dashed) at the value of the induction time under quiescent conditions (γ = 0). If we assume that the center of mass diffusion of a chain is the slowest relevant process in the system, we find τ maxγc = 0.6, which confirms the assumption. (Here, we have used the time a chain needs to diffuse over the length of its radius of gyration as an estimate of τ max .)\n- In agreement with this interpretation, we find that aṫ γ &lt;γ c the nuclei are oriented in any random direction, while atγ &gt;γ c the nuclei are oriented on average in the direction of flow, i.e. the stems are parallel to the flow field. In Fig. 5, we show the average tilt angle of the critical nucleus with respect to the flow field at different shear rates. With increasing shear rate the alignment becomes stronger (this effect is also know from experiments 77,78 ). In the inset of Fig. 4, the size of the critical nucleus is plotted against the shear rate: there is no effect of shear.\n- In Fig. 9 we show the induction time versus the shear rate. The red circles represent the simulation data points and the blue line is a fit. Again, the critical shear rate can be estimated as the intersection of the fitted line (continuous) at higher shear rate and a horizontal line (dashed) placed at the value of the induction time under quiescent conditions (γ = 0). (The data point atγ = 0 is the same as in Table I.) In the inset of Fig. 9, the size of the critical nucleus is shown versus the shear rate. It is constant within the error bars. And aboveγ c , the crystallites are again aligned with the flow field (fig. 11). Thus all results are qualitatively the same as those shown in Fig. 4 for C20. Quanititatively, however, there is a difference: if we take the time the center of mass of a chain needs to diffuse across its radius of gyration to estimate the Weissenberg number at the critical shear rate, we obtain τ maxγc = 0.007 1. We do not interpret this as a contradiction to the theory, but rather as a finitesize effect. The induction time at zero shear rate has been obtained at constant pressure, while the induction times under shear have been obtained at constant volume. The system sizes that we simulated were relatively small, thus even though the average densities and pressures were matched, there were large fluctuations (much larger than in the case of C20, where we could work with more chains). We therefore assume that our estimate oḟ γ c is not accurate.\n\nMentions:\n- Fig. 4 shows the induction time as a function of shear rate at 250K.\n- In the inset of Fig. 4, the size of the critical nucleus is plotted against the shear rate: there is no effect of shear.\n- Thus all results are qualitatively the same as those shown in Fig. 4 for C20.\n\nOCR Text: \"Joy ;\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 4. Main panel: Induction time versus shear rate for C20 at 250K. Inset: size of critical nucleus versus logarithm of shear rate."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- An interesting question arises at this point about the relative density of closable and typable skeletons. Fig. 4, shows how many typable skeletons are among the closable skeletons for sizes up to 18. We leave as an open problem finding out the asymptotic behavior of the relative density of the typable skeletons in the set of closable ones.\n\nMentions:\n- Fig. 4, shows how many typable skeletons are among the closable skeletons for sizes up to 18.\n\nOCR Text: \"-orablc Atakl\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4. Closable skeletons vs. typable skeletons by increasing sizes"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The positions, periods, and epochs of maximum brightness (HJD 0 ) of RR Lyrae stars observed in this work are listed in Table 1 and their g-band light curves are shown in Figure 2. The periods and epochs of maximum brightness are precise to within a few seconds and minutes, respectively, allowing the phase of pulsation to be determined with a 1% uncertainty (Sesar et al. 2010). The precise estimate of phase is important when subtracting the velocity due to pulsations from the measured radial velocity, as we explain in Section 2.2. Star 343892 (indicated by an arrow in Figure 1) is in common with Kollmeier et al.'s sample and provides a good check on the consistency of both works.\n\nMentions:\n- The positions, periods, and epochs of maximum brightness (HJD 0 ) of RR Lyrae stars observed in this work are listed in Table 1 and their g-band light curves are shown in Figure 2.\n\nOCR Text: \"20.0 20,53 21.0 21,5 22.07 22.51 0.4 0.6 0.8 phase\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2.— The g-band light curves of observed stripe 82 RR Lyrae stars, ordered as in Table 1. The magnitudes are corrected for ithe ISM extinction using the map from Schlegel et al. (1998). The light curves are offset for clarity. The solid lines show best-fit g-band templates from Sesar et al. (2010)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To evaluate the agreement, we use the standard evaluation tool from the LETOR website 2 , which implements the Normalised Discounted Cumulative Gain (NDCG). In fig. 1a, we see that our approach RAGS-performs better than all other methods at any selection size on the MQ2007-agg data set. Indeed, we also perform better than Qin et al. (2010a) where the best result uses a coset-permutation distance based stagewise (CPS) model with Spearman's ρ in a probabilistic model. Recall that our approach considers the multivariate Spearman's ρ whereas Qin et al. (2010a) uses bivariate Spearman's ρ in a pairwise fashion. For MQ2008-agg (fig. 1b), again our approach performs better than all other methods.\n\nMentions:\n- In fig. 1a, we see that our approach RAGS-performs better than all other methods at any selection size on the MQ2007-agg data set.\n- For MQ2008-agg (fig. 1b), again our approach performs better than all other methods.\n\nOCR Text: \"2/ \"elr Hcc33\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Results on MQ2007-agg (a, left) and MQ2008-agg (b, right): NDCG@k. Our method is labelled RAGS- and RAGS- corresponding to top and bottom non-informative imputation respectively. The results for CPS-S was the best reported in Qin et al. (2010a). The results of θ-MPM was the best among the reported results in Volkovs and Zemel (2012) from BordaCount, CPS, SVP, Bradley-Terry model, and Plackett-Luce model. The results of St.Agg was the best among the reported results in Niu et al. (2013) and was the best among MCLK, SVP, Plackett-Luce model, θ-MPM, BordaCount and RRF."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- , y is the rapidity, and k B is the Boltzmann constant. However, this exponential distribution in energy and momentum turned out to be in disagreement with experimental data, as these tend to behave instead as a power-law, cf. Fig. 1. The non-extensive selfconsistent thermodynamics (NESCT) was proposed as an extension of the Hagedorn's theory to non-extensive statistics [10], and it was based on the Tsallis factor\n- instead of the exponential BG factor, P BG (ε) = e −ε/(k B T ) . This extended theory allowed to reproduce the distribution of all the species produced in pp collisions with high accuracy, cf. Fig. 1, leading to q = 1.14 ± 0.01 and T = 62 ± 5 MeV [11,12]. Moreover, the NESCT predicts a power-law behavior for the hadron spec-\n\nMentions:\n- However, this exponential distribution in energy and momentum turned out to be in disagreement with experimental data, as these tend to behave instead as a power-law, cf. Fig. 1.\n- This extended theory allowed to reproduce the distribution of all the species produced in pp collisions with high accuracy, cf. Fig. 1, leading to q = 1.14 ± 0.01 and T = 62 ± 5 MeV [11,12].\n\nOCR Text: \"EdX =Accl lar [A] = Gcl  ? Gev aelWnl \"F [GeVicl Lu 11LA UTL-=\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Fitting of the experimental data for p⊥ distribution of the abundance of different hadron species, by considering the Tsallis factor of Eq. (2). The NESCT fits well data over 15 orders of magnitude."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In our case (Arabic and Latin text printed), the sequential dilation causes a decrease in numbers of connected components thereof (the characters of same word stick), then there is a stabilization, then there is a second decrease (the words stick). The difference between the two phases of reductions is stabilization, which is shown in Figure 5. Stability is the first value where the standard deviation of the variation in the number of related components vanishes.\n\nMentions:\n- The difference between the two phases of reductions is stabilization, which is shown in Figure 5.\n\nOCR Text: \"1200 IJOC 72 % } 4 # Ordler 0* dllatio n\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5. The standard deviation of the impact of dilation on the number of connected components."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Identifying Single-Stock Perturbations. We observe daily stock prices for six large American companies: Apple, Google, Amazon, Intel, Boeing, and FedEx. Generally, large companies are wellestablished in the market, so we would expect their correlation network to change only slightly over time. However, events occasionally occur which cause a sudden shift in the network structure. We look at the dependencies of these companies while using a perturbed node penalty. This enforces a temporal dynamic where single nodes may occasionally reweigh all their connections at once. This typically reflects that something happened to affect just one company, while leaving the rest of the network unchanged. We solve the TVGL optimization problem with a perturbed node penalty and discover that one event stood out as having the largest single-node effect on the dynamics between these stocks. After running our TVGL method, we plot the temporal deviation, ∥Θ i − Θ i−1 ∥ F , in Figure 5. We discover that there is a large spike in the temporal deviation score during the last week of January. This represents a significant \"shift\" in the network at this specific time. We show the network change at this shift in Figure 6(a), where we see that the perturbed stock was Apple. At this timestamp, only Apple's edges were affected, reflecting a local rather than global change in the network. We examined the media to understand what may have caused this shift, and we found that on January 27th, Apple first introduced the original iPad to the public. This corresponds to the exact time that our TVGL method captured a structural change in the network, where it was also able to identify Apple as the cause of this shift.\n\nMentions:\n- After running our TVGL method, we plot the temporal deviation, ∥Θ i − Θ i−1 ∥ F , in Figure 5.\n\nOCR Text: \"Ferubeu Moue Dykecbon Jor Fyance Data\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: Plot of the temporal deviation of our estimated stock network, which detects a local shift in late January."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Excited-state DMC calculations require identifying the conduction band and valence band wavevectors of the excitation. Therefore, we obtained band structures using LDA+U =6 eV to identify VBM and CBM. The band structures of both materials are given in the Supplementary Information [82]. We find that both HS-AFM and LS LCO are indirect band gap materials, where indirect and direct band gap energies differ less than 0.1 eV in the LDA+U calculations. HS-AFM LCO has a well defined valence band maximum at X with a bandwidth of 1 eV at the valence band. However, HS-AFM LCO conduction band and the LS LCO conduction and valence bands are found to be rather flat with bandwidths smaller than 0.2 eV. Therefore, we study the direct band gaps in the DMC calculations, since it provides more flexible choice for the supercell tiling matrices to eliminate the finite size errors. We study HS-AFM direct band gap at X, while the LS- LCO is studied at Γ. The reported DMC band gap values are obtained from the finite-size extrapolation in Fig. 5. We find DMC band gaps of roughly ∼3.7(2) eV for both LS and HS-AFM LCO. This agrees very well with the photoluminescence and UV/Vis [125,126] experiments and also with PBE0, where a band gap of 3.2-3.3 eV is obtained using QE-RRKJ and CRYSTAL-LCAO. DMC band gaps are larger than the hybrid-DFT band gaps as observed previously [32,46,87]. We used a simple single-determinant scheme to obtain the band gaps from DMC; therefore, our results should be treated as an upper bound due to the fixed node bias, which may not fully cancel between the ground and excited states. Multideterminant wavefunctions can be used to optimize the excited-state nodal surface and control fixed node bias, although studies for extended systems are very limited [131][132][133], because of the significant computational resources that would be required using DMC. A very recent work on VMC, however, shows that nodal surface errors can be minimized using orbital rotations on the singledeterminant wavefunction [134]. DMC quasiparticle and optical gaps in Fig. 5 are identical given the statistical uncertainties, suggesting very small exciton binding energies. gle electrons overlap over the intermediate anions (ligands) near linear angles [101,118,119]. However, in broken symmetry states, orbital ordering may emerge, leading to an alternating pattern of localized occupied orbitals [135]. If orbital ordering occurs, the filled orbital at one site can overlap with a vacant orbital in the adjacent site and lead to relatively weaker FM interactions [120]. Orbital ordering in LCO was initially proposed for the formation of the IS-FM state [106]. Co 3+ ions have the t 5 2g e 1 g configuration in the IS-FM state where the ordering is observed on the majority-spin e g orbitals. In HS-FM LCO, however, the majority-spin orbitals on Co 3+ are completely filled. Therefore, the orbital ordering can only form over the minority spin t 2g orbitals as the hopping between the filled parallel spin electrons is forbidden by the Pauli principle. The superexchange mechanism among the minority spin electrons of the Co 3+ ions in the HS-FM phase LCO is analogous to the superexchange of d 1 ions such as Ti 3+ in LaTiO 3 and YTiO 3 .\n\nMentions:\n- The reported DMC band gap values are obtained from the finite-size extrapolation in Fig. 5.\n- DMC quasiparticle and optical gaps in Fig. 5 are identical given the statistical uncertainties, suggesting very small exciton binding energies. gle electrons overlap over the intermediate anions (ligands) near linear angles [101,118,119].\n\nOCR Text: \"Iza jIikLe Mt\" crti:: E:-4FX ! Jssiuaslicl? 1 1;40 1/2\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 5. (Color online) Optical and quasiparticle gaps of LS and AFM LCO using DMC. For LS and AFM states, Γ →F and L→F transitions are investigated, respectively. The xaxis is the inverse number of atoms in the simulation cell, while the y-axis is the gap energies in eV. LS and AFM energies are shown in blue and black, respectively. Optical gaps are connected with dashed lines, while quasiparticle gaps are connected with solid lines."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- x=−L N e Figure 2: X-mode equations in slab geometry: the physical parameters. The electronic density x → Ne(x) is low at the boundary, and increases towards a plateau. The background magnetic field B 0 is taken as constant for simplicity.\n- x=−L x x=H slope −r δ α Figure 3: X-mode equations in slab geometry: parameters of the dielectric tensor deduced from the value of the physical parameters described in figure 2, assuming that ω &gt; ωc. The coefficient α decreases from positive to negative values. It crosses the axis with a slope bounded from below by r. The coefficient δ is positive and bounded. Since the electromagnetic wave is strongly absorbed for x ≥ H, we simplify by taking all coefficients constant for x ≥ H because it does not change the physics of the problem.\n- It expresses the fact that the length of the transition zone between x = 0 and x = H is small with respect to the other parameters of the problem. One can refer to Figures 2 and 3 for a graphical representation. This hypothesis is physically very reasonable. It is known in the physical community that this problem may be highly singular at the origin. With these hypotheses, one can consider as well other coefficients are now normalized ω = c = ε 0 = µ 0 = 1. As explained previously in (1.9), the solutions in the context of the limit absorption principle correspond to adding a complex part to the diagonal coefficient α, that is α is replaced by α + iν.\n\nMentions:\n- x=−L N e Figure 2: X-mode equations in slab geometry: the physical parameters.\n- x=−L x x=H slope −r δ α Figure 3: X-mode equations in slab geometry: parameters of the dielectric tensor deduced from the value of the physical parameters described in figure 2, assuming that ω &gt; ωc.\n- One can refer to Figures 2 and 3 for a graphical representation.\n\nOCR Text: \"B\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: X-mode equations in slab geometry: the physical parameters. The electronic density x 7→ Ne(x) is low at the boundary, and increases towards a plateau. The background magnetic field B0 is taken as constant for simplicity."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Stay-at-home orders also result in trip distance reduction (Figure 4). Hawaii records the lowest miles traveled per person (4•9 mi.), followed by The District of Columbia (15•8 mi.), and Rhode Island (17•2 mi.). Three states with the highest miles traveled are Wyoming (39•0 mi.), Utah (31•4 mi.), and New Mexico (30•1 mi.). The highest decline after the order is observed in Illinois (-9•7 mi.), California (-9•0 mi.), and New Jersey (-8•9 mi.), which are marked with a green box in Figure 4, while the lowest changes are found in Missouri (+1•9 mi.), South Carolina (+0•9 mi.), and Pennsylvania (-0•3 mi.). The stay-at-home order leads to four miles distance reduction on average.\n\nMentions:\n- Stay-at-home orders also result in trip distance reduction (Figure 4).\n- The highest decline after the order is observed in Illinois (-9•7 mi.), California (-9•0 mi.), and New Jersey (-8•9 mi.), which are marked with a green box in Figure 4, while the lowest changes are found in Missouri (+1•9 mi.), South Carolina (+0•9 mi.), and Pennsylvania (-0•3 mi.).\n\nOCR Text: \"BTd4 Qenttt Ke Gatt Koc\" HCageaWaaaiurI NineLdE U#baa €am WDolei s56VIMQatewCim Leeentxwaluttrtic4\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4: State Trends on Miles Traveled per Person"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The reduced turbulence in the Outer Galaxy compared with that in the Inner Galaxy leads us to predict steeper CR energy spectra in the Outer Galaxy although the wide range of latitude ( |b| &lt; 8 • ) means that the far Outer Galaxy is not reached in the used Fermi LAT observations. The published Fermi LAT measurements cover the area of the whole Outer Galaxy with coordinates 80 • &lt; ℓ &lt; 280 • , |b| &lt; 8 • and the collaboration has already noticed that the excess of measured intensities above the model calculations at high energies is smaller in the Outer compared with the Inner Galaxy. It means that the energy spectrum of gamma rays in the Outer Galaxy is indeed steeper than in the Inner Galaxy. Figure 3 shows the measured spectra compiled from Figures 15 and 16 of [6]. Fitting them by power laws at energies above 10 GeV yields values for the differential exponents of 2.401 ± 0.009 for the Inner Galaxy and 2.525 ± 0.005 for the Outer Galaxy. Taken at face value the difference is very significant but it is appreciated that systematic errors degrade the results. Presumably some, at least, of the systematic errors are the same for the measurements towards, and away from, the Galactic Centre so that their effect is not large; nevertheless, it could be appreciable. Thus, the conservative statement: 'this supports the prediction of a steeper gamma ray spectrum in the Outer Galaxy compared with the Inner' is appropriate.\n- Both spectra shown in Figure 3 are for the total gamma ray flux including contributions from discrete sources and the isotropic background. In §2 it was shown that the subtraction of these contributions increases the exponent of the spectrum in the Inner Galaxy by (0.117-0.067)/3 = 0.017. It is not enough to remove the difference between the calculated exponents of the two spectra in the Inner and Outer Galaxy.\n\nMentions:\n- Figure 3 shows the measured spectra compiled from Figures 15 and 16 of [6].\n- Both spectra shown in Figure 3 are for the total gamma ray flux including contributions from discrete sources and the isotropic background.\n\nOCR Text: \"IER 3 i-Ec*< sEC , psE) LCCs #LOr 338k3f tks ; 437 LcF\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Differential energy spectra of total gamma rays in the Inner Galaxy ( open circles, |ℓ| < 80◦, |b| < 8◦ ) and in the Outer Galaxy ( full circles, 80◦ < ℓ < 280◦, |b| < 8◦ ) compiled from Figures 15 and 16 of [6].The dashed lines are the cubic splines drawn to guide the eye. The full lines above logE = 1 are best linear fits, the slope of which confirms the steeper spectrum in the Outer Galaxy."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- By training models of different sizes and then using HAIS to compute their likelihood, we are able to explore how each model behaves in this regard, and find that three have somewhat different characteristics, shown in Figure 5. The POE model with a Laplace expert has relatively poor performance and we have no evidence that it is able to overfit the training data; in fact, due to the relatively weak sparsity of the Laplace prior, we tend to think the only thing it can learn is oriented, band-pass functions that more finely tile the space of orientation and frequency. In contrast, the Student-t expert model rises quickly to a high level of performance, then overfits dramatically. Surprisingly, the mcRBM performs poorly with a number of auxiliary variables that is comparable to the best performing POE model. One explanation for this is that we are testing it in a regime where the major structures de- signed into the model are not of great benefit. That is, the mcRBM is primarily good at capturing long range image structures, which are not sufficiently present in our data because we use only 36 PCA components. Although for computational reasons we do not yet have evidence that the mcRBM can overfit our dataset, it likely does have that power. We expect that it will fare better against other models as we scale up to more sizeable images. Finally, we are excited by the superior performance of the bilinear generative model, which outperforms all other models with only a small number of auxiliary variables. We suspect this is mainly due to the high degree of flexibility of the sparse prior, whose parameters (through Θ and Ψ) are learned from the data. The fact that for a comparable number of \"hidden units\" it outperforms the mcRBM, which can be thought of as the bilinear generative model's 'analysis counterpart', highlights the power of this model.\n- As illustrated in Table 1, we used HAIS to compute the log likelihood of the test data under each of the image models in Section 3. The model sizes are indicated in the table -for both POE models and the mcRBM they were chosen from the best performing datapoints in Figure 5. In linear models, the use of sparse priors or experts leads to a large (&gt; 6 nat) increase in the log likelihood over a Gaussian model. The choice of sparse prior was similarly important, with the POE model with Student's t experts performing more than 7 nats better than the POE or generative model with Laplace prior or expert. Although previous work Ranzato &amp; Hinton (2010); Culpepper et al. (2011) has suggested bilinear models outperform their linear counterparts, our experiments show the Student's t POE performing within the noise of the more complex models. One explanation is the relatively small dimensionality (36 PCA components) of the datathe advantage of bilinear models over linear is expected to increase with dimensionality. Another is that Student's t POE models are in fact better than previously believed.\n\nMentions:\n- By training models of different sizes and then using HAIS to compute their likelihood, we are able to explore how each model behaves in this regard, and find that three have somewhat different characteristics, shown in Figure 5.\n- The model sizes are indicated in the table -for both POE models and the mcRBM they were chosen from the best performing datapoints in Figure 5.\n\nOCR Text: \"AVAMnE Mmmos Aoe Teiot 'JLWsti | mrLarer Rutbc olaxlanWalakot\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: Increasing the number of auxiliary variables in a model increases the likelihood it assigns to the test data until it saturates, or overfits."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- It is interesting to look for this discrepancy in the equation of state as well. On Figure 18 we compare the trace anomaly obtained in this study with the trace anomaly of the \"hotQCD\" collaboration. We plot the N t = 8 data using the \"p4fat\" and \"asqtad\" actions, which we took from References [13] and [14]. As it can be clearly seen, the upward going branch and the peak position are located at ∼ 20 MeV higher temperatures in the simulations of the \"hotQCD\" group. This is the same phenomenon as the one, which was already reported for many other quantities in Reference [22]. We also see, that the peak height is about 50% larger in the \"hotQCD\" case.\n\nMentions:\n- On Figure 18 we compare the trace anomaly obtained in this study with the trace anomaly of the \"hotQCD\" collaboration.\n\nOCR Text: \"htQCD result: FeNJ rccd[ 8 Xl N=3 erout 6 10,12 Wuppertal Budapest results 103 150 200 250 300 350 403 IMevi\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 18: The normalized trace anomaly obtained in our study is compared to recent results from the “hotQCD” collaboration [13, 14]."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Fig. 5 and Fig. 6, we plot the sum rate performance of the proposed GSA transmission scheme at the antenna configurations of M = ⌊ 2N 5 ⌋ for even M and M = ⌊ 2N −1 5 ⌋ for odd M , respectively. The upper bound of the DoF of 4M is also achieved. These results indicate that our proposed GSA transmission scheme is feasible and effective.\n\nMentions:\n- In Fig. 5 and Fig. 6, we plot the sum rate performance of the proposed GSA transmission scheme at the antenna configurations of M = ⌊ 2N 5 ⌋ for even M and M = ⌊ 2N −1 5 ⌋ for odd M , respectively.\n\nOCR Text: \"K-JN-8 MSN-13 M-7N-18 Ezo| 3 6 B -12\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 6. Total DoF for the MIMO two-way X relay channel whenM = 2N−1"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- For N = 3 the coefficients defined in (3.23) simply reduce to the Wigner 3 − j symbol . . L N −3 } factor, defined in (A.24) as a product of certain {j i , j k , j m } factors. Generalizing the picture laid out in [42,69] for the N = 4, 5 cases, 5 this quantity (A.24) can be interpreted as a \"multilateral delta\" in the following way. Being a product of N − 2 triangle deltas, it is non-zero only if all of the involved integers l 1 , . . . , l k and L 1 , . . . , L N −3 form their respective triangles. Moreover, with respect to the triangle ordering in (A.24), one of the edges of two neighboring triangles must have the same length L k , so we can picture the triangles as being connected by common edges. The resulting shape is therefore a multilateral with N edges of lengths l 1 , . . . , l N , while the L 1 , . . . , L N −3 integers correspond to the lengths of the N − 3 diagonals connected to the vertex where the edges l 1 and l N join (see figure 1). Thus, the multilateral delta is not non-zero for any set of l 1 , . . . , l N that can form a multilateral, but only for those whose diagonals also have integer length. This suggests naming the symbols W ... ... defined in (3.23) the \"multilateral Wigner symbols\", with the 3 − j symbols therefore corresponding to the \"triangular\" case.\n\nMentions:\n- , L N −3 integers correspond to the lengths of the N − 3 diagonals connected to the vertex where the edges l 1 and l N join (see figure 1).\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1. A multilateral illustration for the case N = 7."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To further investigate these findings, we leveled the playing field, using a subset of PaSC testing videos successfully Figure 7: Recognition performance on the common handheld PaSC videos (2nd pipeline). KS [23] &amp; OFM slightly exceeded the best performance reported in Fig. 5. The 2D alignment (dashed) curve made a big jump from Fig. 5 suggesting the 2D alignment bin from Table 1 had more difficult frames than other methods due to its higher yield. pre-processed by all methods in the 2nd pipeline. A total of 1070 handheld videos were used for these experiments. The performance results of this experiment can be seen in Fig. 7 and 8. Even with equal datasets, KS [23] and OFM outperform other methods. The increased performance of the 2D alignment network suggests that its higher yield in the previous experiment provided more difficult frames to match, and subsequently hindered performance.\n\nMentions:\n- To further investigate these findings, we leveled the playing field, using a subset of PaSC testing videos successfully Figure 7: Recognition performance on the common handheld PaSC videos (2nd pipeline).\n- The performance results of this experiment can be seen in Fig. 7 and 8.\n\nOCR Text: \"cHRSR {-Toddle imisanan Eannnenn D\"Jwntdi 7 ceotel: Utteine 3XDanal D\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 7: Recognition performance on the common handheld PaSC videos (2nd pipeline). KS [23] & OFM slightly exceeded the best performance reported in Fig. 5. The 2D alignment (dashed) curve made a big jump from Fig. 5 suggesting the 2D alignment bin from Table 1 had more difficult frames than other methods due to its higher yield."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In the Figure 2 we plot the differences in the radial temperature profiles between the three models, taking as a reference the C7 model. We found three heights where the differences in temperature are higher than 6%: around 400 km (6% only the VALC model), 750 km (8.5%), and 1100 km (6%). In these cases the differences with the C7 model are significant but always less than 10%.\n- In the Figure 5 from De la Luz et al. ( 2011), we can observe the brightness temperatures computed with these models and their comparison with the observations (collected by Loukitcheva et al. 2004). This figure show that SEL05 provides the best match (although their approximation in the low chromosphere disagree with the semi-empirical models predictions). The results of C7 and VALC models presents opposite behaviors. At lower frequencies the predicted brightness temperature differs of the observations, C7 show lower and VALC higher brightness temperatures in the same range of frequencies. This situation is partially inverted around 400 GHz where both models shows brightness temperature above the observed but C7 predicts higher brightness temperatures than VALC. If we observe the Figure 2, a direct relation between the differences in the radial temperature models and the final brightness temperatures can not be easy established.\n- De la Luz et al. (2010Luz et al. ( , 2011 reported that the emission profiles, in the 40 to 400 GHz range, have two regions of enhanced local emissivity. Remarkably, no attention has been paid early to this interesting behavior. For instance, in figure 2 from Avrett &amp; Loeser (2008), we note that at 0.5 mm there are also two regions of emission, unfortunately the authors did not elaborate much on this result. In this work we present a detailed study of this region.\n\nMentions:\n- In the Figure 2 we plot the differences in the radial temperature profiles between the three models, taking as a reference the C7 model.\n- If we observe the Figure 2, a direct relation between the differences in the radial temperature models and the final brightness temperatures can not be easy established.\n- For instance, in figure 2 from Avrett &amp; Loeser (2008), we note that at 0.5 mm there are also two regions of emission, unfortunately the authors did not elaborate much on this result.\n\nOCR Text: \"~pntme hero-ktezik--\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2.— Difference in the temperature profile between the C7 model (taking as reference) and the VALC (continuous line) and SEL05 (dashed line) models."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Figure 4, it is interesting to note the behaviour of Aν as a function of Ys for the models with ∆M0 = 5 × 10 −6 M⊙; it decreases slightly instead of increasing as Ys increases. This is because, it turns out that the models with smaller Ys have larger T eff for ∆M0 = 5 × 10 −6 M⊙, as can be seen in Figure 5. The average amplitude is known to rapidly increase as T eff increases (see Figure 6 of Verma et al. 2014b). Hence, the effect of the decrease in Ys on Aν gets compensated by the increase in T eff , resulting in the trend seen in the figure.\n- In Figure 6, we note that the models with ∆M0 = 5 × 10 −5 and 5 × 10 −4 M⊙ put together show a reasonably tight correlation between Aν and Ys. We calibrated the observed Aν against these models of different Ys to infer the surface helium abundance for all three stars. To perform the calibration, we fitted a straight line to the model Aν as a function of Ys. Since the models with ∆M0 = 5 × 10 −5 M⊙ are unlikely to represent the stars and may potentially bias the helium estimate, we gave less weight to these models in In Figure 7, we compare simultaneously the inferred Ys and measured [Fe/H]s of all stars with the corresponding quantities predicted by the representative models with different values of ∆M0. We can see that only the models with ∆M0 = 5 × 10 −4 M⊙ reproduce both quantities together. This led us to conclude that we need turbulent mixing with approximately ∆M0 = 5 × 10 −4 M⊙ to reproduce the observed Aν .\n\nMentions:\n- The average amplitude is known to rapidly increase as T eff increases (see Figure 6 of Verma et al. 2014b).\n- In Figure 6, we note that the models with ∆M0 = 5 × 10 −5 and 5 × 10 −4 M⊙ put together show a reasonably tight correlation between Aν and Ys.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6. Same as the panel for KIC 2837475 in Figure 4, except the models with ∆M0 = 5× 10−6M⊙ are not shown. The dashed line is a straight line fit to the models with the band representing Monte Carlo regression uncertainty."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In the hypothesis that the system is virialised, the velocity differences among QQQ1519A, QQQ1519B and QQQ1519C (estimated from the broad Mg II emission line redshifts) imply a dynamical mass of ∼ 10 13 M⊙, i.e. the system would lie inside a massive structure. In order to test this scenario, we search for galaxies at redshift ∼ 1.5 in the proximity of the system. In Figure 5 we show the observed colourmagnitude diagram of the sources in our field. We compare the photometry of these sources with the apparent magnitude and colours of a M ⋆ (z) galaxy, where M ⋆ (z) is the characteristic luminosity (in absolute magnitudes) of galaxies at a given redshift z in the rest-frame B-band, as computed by Ilbert et al. (2005). For filter and k-correction, we refer to the elliptical galaxy template by Mannucci et al. (2001). We do not find evidence of a strong overdensity of sources with colours consistent with those of a red sequence at z ≈ 1.5. This suggests that QQQ J1519+0627 is not located in a rich galaxy cluster or that the red sequence is not yet formed. We also estimate the photometric redshifts of the sources in the field of QQQ J1519+0627 with HyperZ 3 (Bolzonella, Miralles, &amp; Pelló 2000) relying upon different galaxy templates (i.e., starburst, elliptical, lenticular, different type of spirals, and irregular galaxies) and assuming the extinction law by Calzetti et al. (2000). It is worth noting that this choice of dust extinction model has only marginal effects on our results. This analysis confirms that none of the objects detected within 60 ′′ (that corresponds to ∼ 500 kpc at the triplet redshift) and 120 ′′ (∼ 1 Mpc) have colours consistent with a galaxy at z ∼ 1.5 (see Figure 6). We thus propose that the physical structure, where the three QSOs reside, may be caught in the act of formation (similarly to what reported in e.g. : Decarli, Reynolds, &amp; Dotti 2009;Decarli et al. 2010a;Farina, Falomo, &amp; Treves 2011).\n\nMentions:\n- In Figure 5 we show the observed colourmagnitude diagram of the sources in our field.\n\nOCR Text: \"Imar\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5. Observed colour–magnitude diagram for the objects detected by SExtractor in the field of QQQ J1519+0627(green circles). Sources selected as galaxies (see §2.2) are shown in orange, and those that lie within 60′′ from the triplet are highlighted with red squares. The vertical dotted line marks the 5σ detection limit (note that SExtractor is on average more conservative than the statistical limit magnitude). Typical error bars are shown as a cross in the bottom–right of the figure. The dashed line shows the location of an M⋆(z) galaxy at various redshifts, as derived from Ilbert et al. (2005), and assuming the elliptical galaxy template by Mannucci et al. (2001) for k–corrections. For reference, we show with light violet crosses the location of the galaxies of the galaxy cluster XMMXCS J2215.9-1738 (z = 1.46, velocity dispersion σv = 580 kms−1, Stanford et al. 2006; Hilton et al. 2007, 2009). Dark violet crosses highlight the spectroscopic confirmed associations. We do not find any evidence of an overdensity of galaxies with colours consistent with a red sequence at z = 1.51, thus suggesting that the environment of QQQ J1519+0627 is not a rich cluster."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The purple line in Fig. 1 illustrates this worst-case variance of PM as a function of . Observe that PM's worst-case variance is considerably smaller than that of Duchi et al.'s solution when ≥ 1.29, and is only slightly larger than the latter when &lt; 1.29, where 1.29 is x-coordinate of the point that the Duchi et al.' solution curve intersects that of PM in Fig. 1. Furthermore, it can be proved that PM's worst-case variance is strictly smaller than Laplace mechanism's, regardless of the value of . This makes PM be a more preferable choice than both the Laplace mechanism and Duchi et al.'s solution.\n- The answer turns out to be positive: that we can combine PM and Duchi et al's solution into a new Hybrid Mechanism (HM). Further, the combination used in HM is non-trivial; as a result, the noise variance of HM is often smaller than both PM and Duchi et al's solution, as shown in Fig. 1 on Page 4.\n- The red line in Fig. 1 on Page 4 shows the worst-case noise variance incurred by HM, which is consistently no higher than This full paper appears in the Proceedings of the IEEE 35th Annual International Conference on Data Engineering (ICDE), held in April 2019.\n\nMentions:\n- The purple line in Fig. 1 illustrates this worst-case variance of PM as a function of .\n- Observe that PM's worst-case variance is considerably smaller than that of Duchi et al.'s solution when ≥ 1.29, and is only slightly larger than the latter when &lt; 1.29, where 1.29 is x-coordinate of the point that the Duchi et al.' solution curve intersects that of PM in Fig. 1.\n- Further, the combination used in HM is non-trivial; as a result, the noise variance of HM is often smaller than both PM and Duchi et al's solution, as shown in Fig. 1 on Page 4.\n- The red line in Fig. 1 on Page 4 shows the worst-case noise variance incurred by HM, which is consistently no higher than This full paper appears in the Proceedings of the IEEE 35th Annual International Conference on Data Engineering (ICDE), held in April 2019.\n\nOCR Text: \"-Laplace Mechanism -Duchi et al, $ solution ~Piecewise Mechanism Hvbrid Mechanism ppiIVacy 'budlget €\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1: Different mechanisms’ worst-case noise variances for one-dimensional numeric data versus the privacy budget . Our Piecewise Mechanism and Hybrid Mechanism will be discussed in Sections III-B and III-C, respectively."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- the assumption that all these nuclei are spherical was used to analyze experimental data, [64]. The results taken from [19] for the radii of helium and lithium isotopes are presented in Fig. 6. The significant increase of 11 Li radius in comparison with nearest nuclei is evident and remains valid even after accounting for the criticism presented in [81]. The classical behavior of nuclear radii A-dependences shown in Eq. (1.2) is evidently violated.\n- The general outlook on light nuclei with halo or skin is presented in Fig. 8 taken from [89]. It seems that the skin/halo effects decrease with increase of atomic weight. For example, the cross section or interaction radii measured in the collisions of Na isotopes on a carbon target [90] in a wide range of atomic weights (A=20-23, 25-32) increase with the number of neutrons rather regularly, contrary to the behavior of He and Li isotopes shown in Fig. 6.\n\nMentions:\n- The results taken from [19] for the radii of helium and lithium isotopes are presented in Fig. 6.\n- For example, the cross section or interaction radii measured in the collisions of Na isotopes on a carbon target [90] in a wide range of atomic weights (A=20-23, 25-32) increase with the number of neutrons rather regularly, contrary to the behavior of He and Li isotopes shown in Fig. 6.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 6: Matter rms radii R (m) m . Lines connected isotopes are only guides for the eye. Differences in radii are seen for isobars with A = 6, 8 and 9. The 11Li isotope has a much larger radius than other nuclei."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The reddening towards the direction of 3XMM J014528.9+610729 in V band is given as AV =4.522 mag in NASA/IPAC Extragalactic Database 4 using Galactic extinction from Schlafly &amp; Finkbeiner (2011). The reddening (A λ ) towards the direction of 3XMM J014528.9+610729 for SDSS wavebands u ′ , g ′ , r ′ , i ′ and z ′ are given as 6.990, 5.447, 3.768, 2.800 and 2.083 mag, respectively. For 2MASS J, H and Ks bands, the A λ are given as 1.169, 0.740 and 0.498 mag, respectively. The reddening in WISE wavebands are derived using the relations given in Gandhi et al. (2011) 5 in W1, W2, W3 and W4 wavebands and estimated to be 0.315, 0.238, 0.307 and 0.234 mag, respectively. The spectral energy distribution (SED) of 3XMM J014528.9+610729 is shown in Figure 2.\n\nMentions:\n- The spectral energy distribution (SED) of 3XMM J014528.9+610729 is shown in Figure 2.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2. Extinction corrected observed frame spectral energy distribution of 3XMM J014528.9+610729. The Vertical error bars show 1σ uncertainties due to photometric uncertainties on the fluxes."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The fact that more and more data is available of always higher quality is also reflected in an increasing popularity of the field of stellar populations within the astronomical community (see Fig. 1.34). Brinchmann (2010) shows that many astronomers feel interested in helping to solve the problems in the field of stellar populations. I am sure that a lot of progress will come from you, the attendants of the Winter School.\n\nMentions:\n- The fact that more and more data is available of always higher quality is also reflected in an increasing popularity of the field of stellar populations within the astronomical community (see Fig. 1.34).\n\nOCR Text: \"1 3 [ \"Stulla populalioris ] ~Populalion synbhesis 1353\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1.34: The fraction of papers in A&A, AJ, ApJ and MNRAS each year that mention Stellar populations (solid line) or Population synthesis (dashed line, scaled up by a factor of 8) in their abstracts. Currently about 12% of all papers mention Stellar populations. (from Brinchmann 2010)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where H 0 is the hardness in the absence of GNDs, i.e. hardness determined in sufficiently high depth so that density of GNDs is negligible compared to the density of dislocations statistically stored in the material. The quantity h is a characteristic length depending on the shape of indenter, shear modulus of the material and H 0 . Hence, the characteristic length h depends on the density of statistically stored dislocations through its dependence on H 0 . Consequently, for a given material, h is a measure of the spacing of obstacles of dislocation slip. Fig. 13 shows (H/H 0 ) 2 plotted as a function of 1/h, so called Nix-Gao plot. It is clear that relation ( 9) is not full filled in the present case since the dependence of (H/H 0 ) 2 on 1/h exhibits at least two slopes higher slope in high indentation depths (small 1/h values) and lower slope in small indentation depths (high 1/h values). Hence, ISE in the Nb samples studied cannot be explained solely by the Nix and Gao model. There are several possible explanation of lowering of the slope of the Nix-Gao plot at low indentation depths (high 1/h values): (i) sub-surface region with enhanced hardness. The shape o the Nix-Gao plot in Fig. 13 is qualitatively similar to those reported for ion irradiated metals [67][68][69][70] consisting of harder sub-surface region containing radiation-induced defects and softer bulk. VEPAS investigations described in Section III A revealed enhanced concentration of vacancies and vacancy clusters associated with hydrogen in the sub-surface region of Nb samples studied. Hence, in analogy with ion irradiated materials, one can assume that Nb samples consist of harder sub-surface region with enhanced concentration point defects and softer bulk region. This results in two slopes in the Nix-Gao plots for Nb samples in Fig. 13. The data in Fig. 13 can be, indeed, well described using the model developed by Kasada et al. [67] for ion irradiated materials, see solid lines in in Fig. 13. Analysis of data in Fig. 13 within the Kasada's model [67] revealed that the bulk equivalent hardness in the sub-surface region is ≈ 1.5 times higher than the bulk hardness H 0 . This harder sub-surface region has depth of ≈ 0.5 µm and exhibits the characteristic length h ≈ 160 nm which is roughly 5 times shorter than the characteristics length h ≈ 800 nm in the bulk region. This indicates that the sub-surface region contains enhanced density of obstacles for motion of dislocations in accordance with enhanced concentration of v+nH complexes and vacancy clusters determined by VEPAS.\n- (ii) maximum allowable GND density. In the Nix-Gao model [64] the density of GNDs monotonically increases with decreasing indentation depth and should be very high for very small indentation depths. However, the actual GND density cannot be very large because repulsive forces between GNDs cause spread of dislocations beyond the hemisphere underneath the indentor [72]. In certain critical indentation depth h nano the density of GNDs reaches the maximum allowable value ρ GN D,max and does not increase with further reduction of indentation depth. As a consequence the slope of Nix-Gao plot decreases [71,72]. Assuming ρ GN D,max ∼ 10 16 m −2 representing typical value for metals [72], one obtains the critical indentation depth h nano ∼ 100 nm [72]. The effect of maximum allowable density of GNDs certainly reduces the slope of the Nix-Gao plot of Nb samples studied but it occurs likely at indentation depths significantly lower than 0.5 µm (1/h &gt; 2 µm −1 ) where the change of the slope of the Nix-Gao plot in Fig. 13 was observed. Hence, one can conclude that slope of the Nix-Gao plot of the Nb samples is probably reduced by both effects (i) and (ii). Enhanced hardness of the sub-surface region indicates that there is a hardening mechanism related to vacancies and hydrogen. Molecular dynamic simulations performed for Fe revealed that bare vacancies are absorbed by gliding dislocations, but vacancies associated with hydrogen are not dissociated even when meeting dislocation sinks [76]. It is likely that vacancies in Nb behave in similar way. Since hydrogen binding energy to vacancy in Nb is rather high (0.5 eV) [41] v+nH complexes are assumed to be stable against decomposition when meeting dislocation sinks and may represent obstacles for dislocation motion. Moreover, during indentation test additional vacancies are introduced into the affected region below the indent [77] and these vacancies agglomerate with v+nH already existing in the material forming vacancy clusters. Hydrogen binding to vacancy clusters is even stronger than that for monovacancies [73,74]. Hence, vacancy clusters associated with hydrogen may represent efficient obstacles for motion of dislocations. Contrary to hydrogen bound to vacancies lose hydrogen dissolved in Nb lattice is known to increase the mobility of dislocations and to enhance the plasticity of material [78,79]. Moreover, it has been reported that hydrogen dissolved in the lattice reduces the pop-in load in nanoindentation testing [80]. This phenomenon has been attributed to reduced tension of dislocation line and/or reduced shear modulus in the presence of hydrogen. Hence, on the base of the available data, it is believed that formation of vacancies and their interaction with hydrogen is responsible for enhanced hardness in the sub-surface region of Nb samples studied. Results of instrumented nanoindentation showed that mechanical properties of sub-surface region of the samples provided by Ningxia and Tokyo Denkai are similar and only very slightly affected by baking (changes in the characteristic length h are insignificant) despite the fact that VEPAS investigations showed at least partial release of hydrogen from vacancies and vacancy clusters by baking at 120 o C. As one can see in Fig. 5 baking at 120 o C resulted in a decrease of the concentration of monovacancies but not vacancy clusters. It indicates that enhanced hardness in the sub-surface region might be connected mainly with vacancy clusters. However, it is clear definite conclusion cannot be done at the present stage of knowledge and additional more detailed investigations are necessary to disclose the actual hardening mechanism in the sub-surface region of Nb samples. In summary the ISE effect in all Nb samples studied cannot be explained only by considering the Nix-Gao model of monotonically increasing density of GNDs. Point defects, namely vacancies, vacancy clusters and hydrogen impurities seem to play important role in particular in the near-surface region.\n\nMentions:\n- Consequently, for a given material, h is a measure of the spacing of obstacles of dislocation slip. Fig. 13 shows (H/H 0 ) 2 plotted as a function of 1/h, so called Nix-Gao plot.\n- The shape o the Nix-Gao plot in Fig. 13 is qualitatively similar to those reported for ion irradiated metals [67][68][69][70] consisting of harder sub-surface region containing radiation-induced defects and softer bulk.\n- This results in two slopes in the Nix-Gao plots for Nb samples in Fig. 13.\n- The data in Fig. 13 can be, indeed, well described using the model developed by Kasada et al. [67] for ion irradiated materials, see solid lines in in Fig. 13.\n- Analysis of data in Fig. 13 within the Kasada's model [67] revealed that the bulk equivalent hardness in the sub-surface region is ≈ 1.5 times higher than the bulk hardness H 0 .\n\nOCR Text: \"I6 -5 T-Rlt I2TC40h I2 caJh 4aa5 me-ened #323Jh #3 273m4\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 13. (H/H0) 2 plotted as a function of 1/h. Solid line shows results of fitting using the Kasada’s model [67]"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where δ denotes the step size that is also set by users. The pseudocode for adaptive thresholding 1-bit CS measurements is shown in Fig. 5. Following measurement, signal\n\nMentions:\n- The pseudocode for adaptive thresholding 1-bit CS measurements is shown in Fig. 5.\n\nOCR Text: \"~I nitializatjou Aanem Yed Mft\" Cmfc+ M ~Pae Ecstemneng Of M7ri*a1 i(> Vpate ~1-4 O; 3} Uprate Ain= 4 Iteraticm . X-[eu TC until %\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5. Pseudocode for adaptive thresholding of 1-bit CS measurements. Here, yk and λk for k = 1, 2, ...,M represent each element of vector y and λ, respectively. Signal reconstruction can be carried out by versatile convex optimization algorithms."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The second benchmark problem is the modeling of a local tsunami that is triggered by a near-field earthquake, which typically hits the shoreline much earlier than a tsunami triggered by a far-field earthquake. The tsunami is triggered by a hypothetical Mw 7.3 earthquake on the Seattle Fault, which cuts across Puget Sound (through Seattle and Bainbridge Island, see figure 13) and can create a tsunami that can cause significant inundation and high currents in some coastal communities around the Puget Sound. The event was designed to model an earthquake that occurred roughly 1100 years ago, and for which geologic data is available for the uplift or subsidence at several locations. Here, we focused on modeling this local tsunami and predicting its impact on Eagle Harbor at the Bainbridge island, the location of which is shown below in figure 14. The ground deformation file for generating the tsunami is obtained from PMEL, which has been used for recent comparison study of GeoClaw and MOST as part of a tsunami hazard assessment of Bainbridge Island (V. Titov et al., 2018).  surrounded by land, the tsunami waves soon get reflected by shorelines and spread out to cover the full domain very soon after the earthquake. Thus, instead of using a refinement tolerance parameter, we enforce mesh refinement everywhere in the domain regardless of wave amplitude, and never regenerate new grid patches. Four levels of refinement are used, as denoted by the rectangles in figure 14 that denote regions where refinement is enforced. Starting from the coarsest level (level 1), which has a resolution of 30 minutes, the refinement ratios are 5, 3 and 6, giving a resolution of 6 minutes on level 2, 2 minutes on level 3 and 1 3 minutes on level 4. Note that for this benchmark problem, a large proportion of the domain is dry land and the shorelines are relatively much longer and more complex. As a result, many branches occur along the execution path of solving Riemann problems of the shallow water system since more different situations arise, e.g. a Riemann problem with one state being dry initially but becoming wet, or staying dry, depending on the flow depth and velocity in the neighboring cell. For the GPU, if the 32 threads in a warp do not take the same execution path, each extra branch will be executed by the entire warp, introducing significant extra execution time. Thus the irregularity of water area in this benchmark problem is challenging for some CUDA kernels to use the GPU hardware efficiently . Red contours show uplift at levels 0.5, 1, 1.5, . . . meters, blue contours show subsidence at levels −0.05, −0.1, . . . meters.\n\nMentions:\n- The tsunami is triggered by a hypothetical Mw 7.3 earthquake on the Seattle Fault, which cuts across Puget Sound (through Seattle and Bainbridge Island, see figure 13) and can create a tsunami that can cause significant inundation and high currents in some coastal communities around the Puget Sound.\n\nOCR Text: \"\"HLuuu Huhhy _kk KH\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 13. Surface displacement for the hypothetical Seattle Fault earthquake, with Bain-"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The achievable rates of these communication schemes can be computed using the Theorem 3 in [2] and the dependency testing bound in [1], respectively. In Fig. 1, the VLD scheme is illustrated for the block-fading model described previously. Initially, the transmitter chooses a codeword from a codebook of M 1 codewords. In each block, the receiver collects information density that resolves some uncertainty about the correct codeword. After the transmission, the transmitter obtains the CSI of the previous block, and using the dependency testing bound in [1], computes the probability of errorˆ . Ifˆ &lt; , the transmitter terminates the transmission, while it sends additional incremental redundancy otherwise. This continues until the amount of information density allows the receiver to reliably decode the message. As shown in the specific realization in Fig. 1, variable-length coding with periodic feedback eventually leads to cases where the receiver collects a wasteful amount of information density.\n\nMentions:\n- In Fig. 1, the VLD scheme is illustrated for the block-fading model described previously.\n- As shown in the specific realization in Fig. 1, variable-length coding with periodic feedback eventually leads to cases where the receiver collects a wasteful amount of information density.\n\nOCR Text: \"Ffomjbd Mmt Dauc btech; [ S1=0 S= lop\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1. Variable-length transmission scheme based on delayed CSIT."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The performance of single joint SDR turbo receiver, which only runs joint MAP-SDR receiver in the initial iteration, is shown in Fig. 5 in comparison with the multi joint SDR that runs joint MAP-SDR in each iteration. We choose two Hamming radii P = 2 and 3 for single joint SDR, while that for multi joint SDR is fixed at 2. It is clear that they all perform equally good in the first iteration since the same joint MAP-SDR is invoked in that iteration. At the 3rd iteration, single joint SDR slightly degrades, especially for P = 2, but  the performance degradation is acceptable in trade for such low complexity.\n\nMentions:\n- The performance of single joint SDR turbo receiver, which only runs joint MAP-SDR receiver in the initial iteration, is shown in Fig. 5 in comparison with the multi joint SDR that runs joint MAP-SDR in each iteration.\n\nOCR Text: \"SWpP Juli_bniSoRp 2iri $ Argu Jeni SCAP-2is Sirac Jniscap 2i = Iyujniylap- Srac JniscAP ) SIH dbi\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 5: BER comparisons of multi SDR and single SDR turbo receivers at iteration = 1 and 3."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- As example Figure 4 shows the effect of moderate 2 pNnm (τ = 0.5) and 4 pNnm (τ = 1) torques on the elongation of a 60 µm DNA chain in a square channel with widths ranging from 40 − 100 µm, following the theoretical treatment of Ref. [17] and in comparison with experiments from Reisner et al. [19] on DNA in nanochannels in the absence of torsion. The shortening by thermal fluctuations was calculated using equations (48),(31) and (49). As material parameters were used P b = 57 nm (DNA with dye as in Ref. [19]) and P c = 100 nm. Applying a moderate torque induces its collapse mainly due to the lowering of the hairpin free energy. Only above 350 nm does the twist instability of confinement become important. A moderate torque of 2 pNnm reduces the elongation to less than half the value it had in the torqueless case for channels larger than 90 nm.\n\nMentions:\n- As example Figure 4 shows the effect of moderate 2 pNnm (τ = 0.5) and 4 pNnm (τ = 1) torques on the elongation of a 60 µm DNA chain in a square channel with widths ranging from 40 − 100 µm, following the theoretical treatment of Ref. [17] and in comparison with experiments from Reisner et al. [19] on DNA in nanochannels in the absence of torsion.\n\nOCR Text: \"2pn Cimtt M-riripd-\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4: Extension of a 60 µm DNA chain in square nanochannels with widths from 30-450 nm, without torque and with torques of 2 and 4 pNnm (τ = 0.5, 1), the dots are experimental data from Ref. [19]"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 1 shows how SeLFiE transfers such experienced users' know-hows to new users: when experienced users tackle with inductive problems of their own they encode their know-hows about how they use the induct tactic as SeLFiE heuristics. Each SeLFiE heuristic is an assertion that takes a triple of a proof goal, relevant constant definitions, and arguments passed to the induct tactic. A wellwritten SeLFiE assertion should return True if the arguments are likely to be useful to prove the problem with the induct tactic, whereas it should return False if the combination is not likely to be useful to prove the problem. When new users want to know if their use of the induct tactic is appropriate or not, they apply the assertion written by an expert to their own problem to know how the assertion judges their use of the induct tactic. Note that we highlighted parts of Fig. 1 to emphasize the difference from an exiting language, LiFtEr, developed for a similar purpose. In the following, we revise LiFtEr and explain why we need a reasoning framework that can take relevant definitions into account to encode reliable heuristics.\n\nMentions:\n- Fig. 1 shows how SeLFiE transfers such experienced users' know-hows to new users: when experienced users tackle with inductive problems of their own they encode their know-hows about how they use the induct tactic as SeLFiE heuristics.\n- Note that we highlighted parts of Fig. 1 to emphasize the difference from an exiting language, LiFtEr, developed for a similar purpose.\n\nOCR Text: \"exoerts\" prucf Eyincucbon : patoy Epr rekevan delintixms, soodinictcn] (nee proj goa Jem uee 5 new releoant delnitions SaUFE True caticabe induedizn| Sierlicn [c [x] ab[ysk False\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1: The overview of the SeLFiE."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Another possible solution lies in a direct measurement of the component diameters. However, they are out of reach of current optical interferometers with hectometric baseline lengths. Given the high inclination of the revised orbit, one might wonder whether Haro 1-14C might be a long period eclipsing binary. To test this idea, we fitted the spectral energy distribution with the sum of two atmospheric models (see Fig. 4). We used effective temperatures of 4400 ± 200 K for the primary and 3500 ± 200 K for the secondary (based on the spectral types from Schaefer et al. 2008). The interferometric flux ratio in the near-infrared corresponds to a ratio of 0.75 ± 0.1 for the linear radii. We then adjusted the photometry by varying the apparent diameters and the visual extinction A V , assuming that the latter is the same for the two components. The W4 photometric point at 22 µm was discarded from the fit because it is a clear outlier in the Rayleigh-Jeans regime. The best match is obtained for A V = 1.8 ± 0.7 and for φ a = 0.131 ± 0.015 mas and φ b = 0.098 ± 0.011 mas using ATLAS9 models (Castelli &amp; Kurucz 2004). Differences using NextGen models (Allard et al. 1997) are negligible. Unfortunately, Haro 1-14C cannot be an eclipsing binary because the closest approach between the two components in the apparent orbit is 0.32 mas. These diameters correspond to R a = 1.50 R and R b = 1.13 R in linear radius at the distance of the object.\n\nMentions:\n- To test this idea, we fitted the spectral energy distribution with the sum of two atmospheric models (see Fig. 4).\n\nOCR Text: \"10-- 1 L 10.0 (um) 10-13\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4. Best-fit synthetic fluxes (horizontal bars) to the spectralenergy distribution from NOMAD, 2MASS, and WISE (red circles). The corresponding synthetic spectra for the two individual components and their sum are overlaid (dashed, dotted, and solid gray lines)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The research reported in this paper is a contribution to building problem solving agents in artificial intelligence (AI) that use diagrams, much as people do, but most of AI does not, given the almost exclusive emphasis in AI on language-like or predicate-symbolic representations. Diagrammatic reasoning (DR) is an emerging area of research in a number of fields, including AI (Glasgow, Narayanan, &amp; Chandrasekaran, 1995;Chandrasekaran, Kurup, &amp; Banerjee, 2005), logic (Barwise &amp; Etchemendy, 1998;Allwein &amp; Barwise, 1999), and psychology (Tversky, 2000;Tricket &amp; Trafton, 2006). While all research in DR is in one way or other dealing with diagrams, different research issues are addressed by different researchers. The research reported in this paper considers DR as a problem solving activity in which an agent (human or artificial) makes use of two forms of representation -a spatial representation in the form of 2D diagrams and a symbolic representation that contains information in a predicate-symbolic form similar to logic and natural language. A schematic DR architecture, as proposed by Chandrasekaran et al. (2002Chandrasekaran et al. ( , 2004Chandrasekaran et al. ( , 2005, is illustrated in Figure 1.\n- 2. Any spatial relation or action that can be included in the vocabulary is solvable by our SPS. Real QCSPs are known to be computationally intractable, so a substantial part of the spatial problem solving literature concentrates on constraint satisfaction problems (CSPs). We developed a general framework for solving spatial problems specified as QCSPs. The framework bypasses the process of quantifier elimination (QE) -the computational bottleneck and a doubly exponential problem -by taking the help of previously solved similar spatial problems. We show that, if the symbolic solution to a problem can be expressed 1. The reader should keep the DR architecture in mind. As shown in Figure 1, there are two problem solvers -the main problem solver which will be always referred to as the \"problem solver\" (this might be a human) and the spatial problem solver which will be referred to as the \"SPS\" (this strictly has no human intervention). The problem solver is responsible for the entire problem solving strategy including converting domain-specific perceptions and actions into domain-independent spatial problems. The SPS is responsible only for solving the domain-independent spatial problems that it receives from the problem solver. It is important to not get confused between the roles played by the two.\n\nMentions:\n- A schematic DR architecture, as proposed by Chandrasekaran et al. (2002Chandrasekaran et al. ( , 2004Chandrasekaran et al. ( , 2005, is illustrated in Figure 1.\n- As shown in Figure 1, there are two problem solvers -the main problem solver which will be always referred to as the \"problem solver\" (this might be a human) and the spatial problem solver which will be referred to as the \"SPS\" (this strictly has no human intervention).\n\nOCR Text: \"Ymhlcl SEEill pbl | Spatal spelictun hga' Iulouhg Frbki Rules Prble Su Solr Symholc Sulios bMh: IKHAH Jptl Vyrsm Ezslilz . Sbiw pub Sher Jll\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: The diagrammatic reasoning architecture."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Staggered arrays may be defined as symmetric or asymmetric arrays. Taking the Arakawa Cgrid (Figure 9) as an example, we have a 8×8 supergrid. Scalars, at cell centres, will form a 4×4 array. A symmetric array representing the velocity component U will be of size 5×4. Quite often, though, all arrays may be defined to be 4×4, in which case, one must also specify if the U points are biased to the \"east\" or \"west\", i.e if the array value u(i,j) refers to the point U (i + 1 2 , j)\n\nMentions:\n- Taking the Arakawa Cgrid (Figure 9) as an example, we have a 8×8 supergrid.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 9. A 4×4 (not 8×8!) Arakawa C-grid."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To numerically verify the results proved in section V, we created m linearly independent random query vectors which are used to expand the bases. Figure 2 demonstrates the probability of the event that at least one of the base vectors exists in the span of l = ⌊δm⌋ vectors for δ = 0.1, 0.2, 0.3 and 0.4. Consistent with our results in section V, the probability of the event that a base exists in the span of any set of l = ⌊δm⌋ vectors goes quickly to zero.\n\nMentions:\n- Figure 2 demonstrates the probability of the event that at least one of the base vectors exists in the span of l = ⌊δm⌋ vectors for δ = 0.1, 0.2, 0.3 and 0.4.\n\nOCR Text: \"4an ?\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2. Probability of the event that at least one base exists in the span of any subset of l = ⌊δm⌋ random vectors."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Numerically is of interest to ascertain the minimal section size rate a v,L, ,α0 , for a specified L such as L = 64, for R Fig. 3. Sufficient section size rate a as a function of the signal-to-noise ratio v. The dashed curve shows a v,L at L = 64. Just below it the thin solid curve is the limit for large L. For section size B ≥ L a the error probabilities are exponentially small for all R &lt; C and any α 0 &gt; 0. The bottom curve shows the minimal section size rate for the bound on the error probability contributions to be less than e −10 , with R = 0.8C and α 0 = 0.1 at L = 64.\n- chosen to be a proscribed high fraction of C, say R = 0.8C, for α 0 a proscribed small target fraction of mistakes, say α 0 = 0.1, and for to be a small target probability, so as to obtain min{P [E ], P [Ẽ ] + P [E * ]} ≤ , taking the minimum over allowed values of t α , for every α = /L at least α 0 . For this calculation the bound from Lemma 1 is used for P [E ] and the bound from Lemma 2 is used for P [Ẽ ] + P [E * ]. This is illustrated in Figure 3 plotting the minimal section size rate as a function of v for = e −10 . With such R moderately less than C we observe substantial reduction in the required section size rate.\n\nMentions:\n- Numerically is of interest to ascertain the minimal section size rate a v,L, ,α0 , for a specified L such as L = 64, for R Fig. 3.\n- This is illustrated in Figure 3 plotting the minimal section size rate as a function of v for = e −10 .\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3. Sufficient section size rate a as a function of the signal-to-noise ratio v. The dashed curve shows av,L at L = 64. Just below it the thin solid curve is the limit for large L. For section size B ≥ La the error probabilities are exponentially small for all R < C and any α0 > 0. The bottom curve shows the minimal section size rate for the bound on the error probability contributions to be less than e−10, with R = 0.8C and α0 = 0.1 at L = 64."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Inference Model selection is based on the accuracy of the validation split. The 30 models in the run with highest average accuracy are ran against the test split. Each run is essentially the same model using the same hyperparameters but with different randomized weight initializations for the convolutional and fully connected layers. In the end the test results of the 30 models are averaged and the standard error of the results is calculated. The final results can be seen in Figure 3.\n- In Table 3 and Figure 3 we can see that the that our method outperforms the conventional 3D convolution significantly in the low data regime. However, when we have ample training data the conventional 3D convolution outperforms our method, as is to be expected. It is worth mentioning that, in general, our method uses fewer parameters and still achieves reasonable results in all settings.\n- Fig. 3. A comparison of the model using regular 3D convolutions with the model using our factorized 3D convolutions on the entire test split of Video-MNIST. An asterisk denotes if the difference between models is significant.\n\nMentions:\n- The final results can be seen in Figure 3.\n- In Table 3 and Figure 3 we can see that the that our method outperforms the conventional 3D convolution significantly in the low data regime.\n- Fig. 3.\n\nOCR Text: \"Ailimi Vt Jiua Li > ILco\"- ILetd-an Ti-i-u Jus\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3. A comparison of the model using regular 3D convolutions with the model using our factorized 3D convolutions on the entire test split of Video-MNIST. An asterisk denotes if the difference between models is significant."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 14 shows the total density of states D(ε) versus ε for Im-[Co III (corrin)]-CN + obtained by using the Gaussian program 31 with the 6-31G basis functions and the BP86 exchange potentials ? . Here, the solid and dashed vertical red lines denote the HOMO and LUMO levels obtained by DFT. According to the DFT results the energy gap is ≈ 2.2 eV, which is in agreement with the experimental results. The vertical black lines, on the other hand, denote the HOMO and LUMO levels obtained by the HF calculations shown in Fig. 3(a). Comparing the HF results on D(ε) shown in Fig. 3(a) with the DFT results of Fig. 14, we observe that the HF HOMO and LUMO levels located at −10.5 eV and −1.7 eV move to −7.8 eV and −5.6 eV, respectively. Hence, in comparison to the HF results, the DFT calculation yields additional spectral weight in the energy intervals −10.5 eV &lt; ∼ ε &lt; ∼ − 7.8 eV and −5.6 eV &lt; ∼ ε &lt; ∼ − 1.7 eV. Next, we compare the DFT results on D(ε), Fig. 14, with the single-particle spectral weight deduced from the HF+QMC data on the total electron number n T versus µ shown in Fig. 7(c) for U = 36 eV. We observe that the locations of the HOMO and LUMO levels obtained by DFT and HF+QMC are comparable. In addition, the DFT result for the total number of the electrons in the Co(3d ν ) NAO states is 7.4, which is similar to the HF+QMC result seen in Fig. 8. So, the DFT and the HF+QMC results for the energy gap and n d are comparable to each other as well as to the the experimental results. However, we will see in the following that there are important differences between the DFT and HF+QMC results when the overall distribution of the single-particle spectral weight near the semiconductor gap edges is considered.\n\nMentions:\n- Figure 14 shows the total density of states D(ε) versus ε for Im-[Co III (corrin)]-CN + obtained by using the Gaussian program 31 with the 6-31G basis functions and the BP86 exchange potentials ? .\n- Comparing the HF results on D(ε) shown in Fig. 3(a) with the DFT results of Fig. 14, we observe that the HF HOMO and LUMO levels located at −10.5 eV and −1.7 eV move to −7.8 eV and −5.6 eV, respectively.\n- Next, we compare the DFT results on D(ε), Fig. 14, with the single-particle spectral weight deduced from the HF+QMC data on the total electron number n T versus µ shown in Fig. 7(c) for U = 36 eV.\n\nOCR Text: \"3 1 VL Ml (eV) 2\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 14. (Color online) DFT results on the total density of states D(ε) of Im-[CoIII(corrin)]-CN+ obtained by using the Gaussian program with the BP86 exchange potentials and the 6-31G basis functions. The solid and dashed vertical red lines denote the HOMO and LUMO levels obtained by the DFT calculation. Similarly, the solid and dashed vertical black lines denote the HOMO and LUMO levels obtained by the Hartree-Fock calculation described in Section II. Here, we see that in the DFT calculations additional spectral weight is induced in the intervals −10.5 eV ≤ ε ≤ −7.8 eV and −5.7 eV≤ ε ≤−1.8 eV when compared with the HF results."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Indeed, consider the unit sphere S 2 ⊂ R 3 . It clearly has cx. Now take u ∈ S 2 , 0 &lt; λ &lt; 1, and set c := λu . The plane u ⊥ c will cut S 2 along a circle centered at c. For any v ∈ S 2 near u, however, the cross-cut v ⊥ c ∩ S 2 is clearly centered on the line spanned by λv (Figure 3). So for v = u, the center moves. The center cannot move in this way when cross-cuts are figure-8's, as we make precise in Lemma 3.8 shortly below, using the notion of central curve of a good tubular patch: Definition 3.7 (Central curve). Suppose Γ is a cross-cut for an immersion F : M → R n+1 relative to a hyperplane P = u ⊥ p . Let (U, ψ) denote a good tubular patch for Γ as in Definition 3.2, so that F maps ψ(Γ, h) into the hyperplane\n\nMentions:\n- For any v ∈ S 2 near u, however, the cross-cut v ⊥ c ∩ S 2 is clearly centered on the line spanned by λv (Figure 3).\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3. Cross-cuts on a sphere, via u⊥c and v ⊥ c . Both hyperplanes contain c, but only one of the cross-cuts (red) is centered at c ."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Finally, we evaluate the effectiveness of our NLC scheme when some primary sessions are implemented via multi-hop transmissions. Specifically, we consider a CCHN where the Fig. 14. The throughput performance of the NLC approach and the LLC approach, when some primary sessions are implemented via multi-hop transmissions. fourth primary session, i.e., the data transmission from Ps4 to Pd4, is implemented via multi-hop transmissions and Pr4 is the intermediate relay for the fourth primary session as shown in Fig. 13. The parameters and network settings are the same as those in Fig. 7. The only difference is that, in each frame, the transmission of the fourth primary session is completed over two subframes of length 5ms and the LLC scheme is implemented in each subframe through decode-and-forward relaying since there are two hops involved in the fourth primary session. The results are shown in Fig. 14. It is obvious that our NLC scheme can achieve a much higher throughput than that of the LLC approach, which implies that our NLC approach is effective in network throughput enhancement even if multi-hop flows are involved in the primary networks. Based on the same network topology, we further compare the completion time of primary sessions under our NLC approach and that under the LLC approach in Fig. 15. To examine the effectiveness of our NLC approach in dealing with multi-hop primary flows, we focus on the data transmission of the fourth primary session and assume it is active with probability 1.\n- Other parameters are the same as those in Fig. 14. Clearly from Fig. 15, even if primary sessions are implemented via multihop transmissions, our NLC approach can still greatly shorten the completion time of primary sessions when compared with the LLC approach and thus is more likely to motivate PUs to join the cooperation-based spectrum access processes.\n\nMentions:\n- Specifically, we consider a CCHN where the Fig. 14.\n- The results are shown in Fig. 14.\n- Other parameters are the same as those in Fig. 14.\n\nOCR Text: \"FLc M0i LLC 071.3 NLC, p-D5 Dale FalealCRU (Mbjs} P7 5\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 14. The throughput performance of the NLC approach and the LLC approach, when some primary sessions are implemented via multi-hop transmissions."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- As depicted in Figure 6, we have experimented several models to re-sample the database and create a more balanced dataset: Random over-sampling, Random Under-sampling, SMOTE (Synthetic Minority Over-sampling Technique) [15], Near Miss 1, 2 and 3, One-sided Selection, and Neighborhood Cleaning Rule.\n- Table I in Appendix A shows the results for different models and different sampling methods. A summary of the results showing F 1 score is presented in Figure 6. The ROC curves of the Random Forest Classifier are also shown in Figure 7.\n\nMentions:\n- As depicted in Figure 6, we have experimented several models to re-sample the database and create a more balanced dataset: Random over-sampling, Random Under-sampling, SMOTE (Synthetic Minority Over-sampling Technique) [15], Near Miss 1, 2 and 3, One-sided Selection, and Neighborhood Cleaning Rule.\n- A summary of the results showing F 1 score is presented in Figure 6.\n\nOCR Text: \"7uin/aru Uavc: KAMc FernnIli Kandon OTO: AL4IK 1:vrx Gtan Reuai 0ca Warcenet Tae= MR? Be} (.814341Mre Mcrnu Ranal FC MRIE Rut Layc: LJLL**14II 07G 144;rnii 01j7a Kandanni Kotes MIOSr FMt RWlun Fui Ynin_ One_Gi Ceee &e Ediencornocd Tcinin AL4u RLUBL AMMHUI4LU Osui AMlT Rondatlne Lan Sele ET LNN Ae4u\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 6. A hierarchical depiction of different sampling methods and the models used for each. The number associated with each methods indicates the F1 score. Other methods∗: Since the data is highly unbalanced, the other models mostly overfit and learn to always predict the most dominant class. These models include Naive Bayes, Naive Bayes One-Sided Selection, Naive Bayes Neighborhood Cleaning Rule, Naive Bayes Random over sampling, Naive Bayes SMOTE and its variants."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We can produce plots to see the effect of the quartic quasitopological parameter on the thermodynamics of the black hole. In figure 5, we see that the positive quartic parameter continues to act in the same manner as before (i.e. as a \"stronger\" quasitopological addition) by pulling the black hole solution further from instability. Recall further that the slope of this graph indicates the sign of the specific heat, and that in the small-r black hole (leftmost region) of the Einsteinian case, the negative slope means that the black hole is unstable.\n\nMentions:\n- In figure 5, we see that the positive quartic parameter continues to act in the same manner as before (i.e. as a \"stronger\" quasitopological addition) by pulling the black hole solution further from instability.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 5: The plot of log(T ) versus log(S) for higher-curvature black holes, where the solid line is Einsteinian, the dashed is Gauss-Bonnet, the dash-dot is 3rd order quasitopological, and the dotted is 4th order (quartic) quasitopological gravity. Here, h0 = 2.000, k = 1, z = 1, and we are in 4+1 dimensions. The other parameters were µ̂2 = 0.04, µ̂3 = −0.001, µ̂4 = 0.0004."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Looking at the right side of Fig. 9, one notices that the growthrate of the bandwidth, after an initial quiescent period, appears to approach a line, representing exponential growth of the bandwidth. We are thus led to examine the bandwidth of runs with a relatively large number of bounces, and indeed we find exponential growth of the bandwidth for these runs, shown in Fig. 10.\n- What is particularly interesting about Fig. 10 is that these runs demonstrate what appears to be a common growth rate with slope 0.04. This commonality in slope appears despite the fact that these families span different dimensions d and different forms of initial data. What they have in common is that they all form black holes after between 26 and 39 bounces. Note that the growth rate for these evolutions differs from that shown for d = 3 in Fig. 9 (with a slope of roughly 0.07) which collapses after just 21 bounces.\n\nMentions:\n- We are thus led to examine the bandwidth of runs with a relatively large number of bounces, and indeed we find exponential growth of the bandwidth for these runs, shown in Fig. 10.\n- What is particularly interesting about Fig. 10 is that these runs demonstrate what appears to be a common growth rate with slope 0.04.\n\nOCR Text: \"20 4\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 10: Bandwidth as a function of bounce for a few different evolutions."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 10 plots the performance of NETINF (break-even point) as a function of the amount of Gaussian noise added to the incubation times between infections for both an exponential incubation time model with α = 1, and a power law incubation time model with α = 2. The break-even point degrades with noise but once a high value of noise is reached, an additional increment in the amount of noise does not degrade further the performance of NETINF. Interestingly, the break-even point value for high values of noise is very similar to the break-even point achieved later in a real dataset (Figures 13(a) and 13(b)).\n\nMentions:\n- Figure 10 plots the performance of NETINF (break-even point) as a function of the amount of Gaussian noise added to the incubation times between infections for both an exponential incubation time model with α = 1, and a power law incubation time model with α = 2.\n\nOCR Text: \"ExfanEntal Moczl Ia\" Moczl Incucabon Tine Noie (sld} Pojel\"\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 10. Break-even point of NETINF as a function of the amount of additive Gaussian noise in the incubation time."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We evaluate our model, LACNN, against other baselines using the Receiver Operating Characteristic (ROC) curve on the test set and show that it achieves the best results (by a margin of 2% to 7%) among all models, as shown in Figure 6. Note that the LANN model achieves similar performance to the CNN model. This is interesting since this model does not use any convolutions for predictions and only looks at the up-  hill direction at three different distances (30, 100, and 300 meters), suggesting that alignment plays a significant role in predicting landslides.\n\nMentions:\n- We evaluate our model, LACNN, against other baselines using the Receiver Operating Characteristic (ROC) curve on the test set and show that it achieves the best results (by a margin of 2% to 7%) among all models, as shown in Figure 6.\n\nOCR Text: \"LachNHII 4uL CNN ain Auc MANN Wth WC -0 35 WNnmith WiC LLR #lh AUC _\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6: ROC curves from all models on the test set."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Clearly, the second term in the product on the lefthand side above is positive. The first term in the product is merely e − σ 2 opt , which is evidently greater than or equal to zero, since the estimation mean-square error of any filter is greater than or equal to that of the optimal filter. Hence, (8.5) is satisfied. Now, it is interesting to see how the effective quantum efficiency η eff from (8.3) varies with various parameters for our Kalman and robust filters for the uncertain system. Figure 4 shows a comparison of the effective quantum efficiencies of the filters as a function of δ for µ = 0.8. The efficiency η eff for the robust filter is unity in the worst-error case, i.e. at δ = −1, since the robust filter is optimal for this value of δ. On the other hand, η eff for the Kalman filter is far below unity in the worsterror case. Also, note that η eff = 1 for the Kalman filter at δ = 0, since the Kalman filter is optimal for the exact model, which corresponds to δ = 0. It is further interesting to note our robust filter's advantage over the Kalman filter in terms of the effective quantum efficiency in the worst-error case situation (i.e. at δ = −1), that we denote as η w eff . Figures 5-(a) and (b) show comparisons of η w eff of the filters as functions of |α| 2 and κ, respectively, for µ = 0.5. Since the robust filter is always optimal at δ = −1, its effective quantum efficiency always remains unity in the worsterror case, whereas that of the Kalman filter varies significantly and approaches η w eff = 1 of the robust filter for increasingly higher values of κ and |α| 2 . Indeed, for low values of κ or |α| 2 , the corner frequency λ 2 + 4κ|α| 2 of the Kalman filter is well beyond the worst-case corner frequency λ(1 − µ) of the uncertain phase, yielding poor estimates and therefore poor effective quantum efficiency. However, beyond the point at which 4κ|α| 2 becomes comparable to λ 2 , the value of λ (that remains fixed here) becomes increasingly less dominant and therefore the impact of any uncertainty in λ on the quality of the estimate starts diminishing, resulting in improved effective quantum efficiency. Thus, our robust filter always achieves the maximum effective quantum efficiency that is possible in the worst-case situation, unlike the Kalman filter. This is because, while the robust filter is designed to be optimal in the worst-error case, the Kalman filter is optimal for the exact model without uncertainty.\n\nMentions:\n- Figures 5-(a) and (b) show comparisons of η w eff of the filters as functions of |α| 2 and κ, respectively, for µ = 0.5.\n\nOCR Text: \"Ikh Hwh WiM- W H\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 5. Effective Quantum Efficiency: Comparison of the worsterror case effective quantum efficiencies of the Kalman and Robust filters as a function of (a) |α|2, and (b) κ. Here, we fix µ = 0.5."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Shared parameters are synchronized via the communication network. The sheer number of parameters and the iterative nature of machine learning algorithms often produce huge amounts of network traffic. Figure 1 shows that, if we randomly assign data (documents) to machines in a text classification application, the total amount of network traffic is 100 times larger than the size of training data. Specifically, almost 4 TB parameters are communicated for 300 GB training data. Given that the network bandwidth is typically much smaller than the local memory bandwidth, this traffic volume can potentially become a performance bottleneck.\n- We test the scalability of Parsa on CTRb with 10 billion edges by increasing the number of machines. We run 4 workers and 4 servers at each machine with infinite maximum delay. The results are shown in Figure 1. As can be seen, the speedup is linear with the number of machines and close to the ideal case. In particular, we obtained a 13.7x speedup by increasing the number of machines from 1 to 16.\n\nMentions:\n- Figure 1 shows that, if we randomly assign data (documents) to machines in a text classification application, the total amount of network traffic is 100 times larger than the size of training data.\n- The results are shown in Figure 1.\n\nOCR Text: \"@ Parsa random partition] J 12 2 0.3 data Size\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: The amount of (outgoing) network traffic versus the size of data in a real text dataset. The algorithm uses 16 machines to run 100 iterations. The first-order gradients are communicated."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- For next generation of mobile network, LTE, 3GPP took into account network sharing in its specification. It has defined the MOCN [17] and GWCN (Gateway Core Network) approaches for the eUTRAN sharing. The Table 5 provides a high level comparison of the two approaches. In the MOCN approach, the shared eUTRAN is connected to several CN via the S1 interface. Each mobile network operator has its own EPC. Thus the MME, the SGW and the PGW are not shared and are located in the different CN. The S1 flex allows the eNodeB to be connected to the different CN. It also allows connecting the eNodeB to several MME and SGW in a given CN. Thus, allowing load balancing to be supported between MME and SGW of a given CN. In the GWCN approach (as shown in Figure 9), contrary to the MOCN approach, the MME is also shared between the different mobile network operators.\n\nMentions:\n- In the GWCN approach (as shown in Figure 9), contrary to the MOCN approach, the MME is also shared between the different mobile network operators.\n\nOCR Text: \"Operalura OperalurB CFcralurc Wecss Wecect VEC579 RRC Hedo Ngoesnetan $ Mdua %\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 9: Configuration for GWCN sharing. In addition to RAN sharing, operators also share core network [6]"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In this section, we evaluate the performance of our proposed machine learning based beamforming codebook design solution on the scenarios described in Section V-A. The numerical results show that our proposed model can achieve performance that is comparable to EGC and adapt to different user distributions. Fig. 5 shows the evaluation results of the proposed solution in a LOS scenario. With 64 beams, the learned codebook reaches more than 90% of the upper bound, which is achieved by the EGC where the number of beams in the codebook is equal to the number of users (and assuming unquantized phase shifters). Further, with 128 beams, the developed approach achieves more than 95% of the upper bound. Besides, Fig. 5 also exhibits the performance of the codebook with quantized phase shifters. It shows that with only 3-bit phase shifters, the learned codebook can still achieve more than 85% of the upper bound with 64 beams and 90% of the upper bound with 128 beams. This is very important and interesting for cases where the resolution of the analog phase shifters is limited. Fig. 6 shows the evaluation result of the proposed solution in a NLOS scenario. This is more interesting and challenging than the LOS case. In a NLOS scenario, each user may have multiple rays reflecting off some scatters and reaching the base station. As depicted in Fig. 4, the learned codebook with the developed approach is capable of learning multi-lobe beams that capture the dominant rays and gather more energy compared to classical DFT codebooks. Fig. 6 depicts the performance of the proposed solution in this NLOS environment. The learned codebook has almost the same performance or even outperforms a 64-beams DFT codebook with only 16 beams, which is one fourth in size. With 64 beams, the learned codebook achieves almost 90% of the upper bound and noticeable gain compared to the 64-beam DFT codebook.\n\nMentions:\n- Fig. 5 shows the evaluation results of the proposed solution in a LOS scenario.\n- Besides, Fig. 5 also exhibits the performance of the codebook with quantized phase shifters.\n\nOCR Text: \"lmatn (EJC 1ena?\"! Aeltanil Xumhcielllyz\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 5. Achievable rate versus number of beams in the codebook in an outdoor LOS scenario under the receive SNR of 5 dB with different bits of quantization."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The Q-value for population of 133 Sn via the (d,p) reaction was calculated on an eventby-event basis from the measured angle and energy of the emitted proton. The energy of the beam was corrected for energy loss in the target, assuming that the reaction occurred in the center of the target, as was the energy of the proton. Three states had been previously observed in 133 Sn: the ground state, 854-and 2005-keV excited states with tentative spin assignments of 7/2 − , 3/2 − and 5/2 − , respectively. Four peaks were observed in the Q-value spectrum (Fig. 5) corresponding to these states, as well as a newly observed state. The excitation energy of this new state, interpolated from the Q-values of the other three peaks, was determined to be 1363 ± 31 keV. Figure 6 summarizes the systematics of single-particle states in N = 83 isotones, including the new 1363-keV candidate for the p 1/2 state. The smooth variation of the energy of 1/2 − states in N = 83 isotones, including the candidate in 133 Sn, supports this assignment; however, more evidence is required before the assignment can be considered firm.  [18], and present results (color online).\n- The angular distributions were extracted from the data by fitting four gaussians to the four states shown in Fig. 5 at each angle. Figure 7 from a high-spin parent. If indeed this is the expected 2f 5/2 level, the spectroscopic factor is consistent with 1. An alternative 3p 1/2 assignment is inconsistent with its observation in beta decay. On the other hand, the new 1363-keV state had not been previously observed via beta-decay, beta-delayed neutron decay [10] or in the gamma decay following fission [11].\n\nMentions:\n- Four peaks were observed in the Q-value spectrum (Fig. 5) corresponding to these states, as well as a newly observed state.\n- The angular distributions were extracted from the data by fitting four gaussians to the four states shown in Fig. 5 at each angle.\n\nOCR Text: \"8 50 -25 -15 -0.5 0,5 Q-value (MeV}\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 5. Q-value spectrum for the 132Sn(d,p)133Sn reaction with a 630 MeV 132Sn beam. The solid black line shows a fit including four peaks: the ground state (green), the 854 keV state (red), the previously unobserved 1363 keV state (blue) and the 2005 keV state (magenta) (color online)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- ProMiSH-E had a query time of 4.7 seconds and ProMiSH-A had a query time of 0.3 seconds for queries of size q=9 on a 100-dimensional dataset. ProMiSH-A is an order of magnitude faster than ProMiSH-E on datasets of all dimensions. We observed that both algorithms scale linearly with dimension d of the dataset. Figure 13 shows the query times for retrieving the top-k results for queries of varying sizes q on a 50-dimensional dataset. It reveals a linear performance of both algorithms for increasing k. ProMiSH-A is an order of magnitude better than ProMiSH-E for any result size k. All these tests show that the query time of ProMiSH scales linearly with the dataset size, the dataset dimension, the query size, and the result size.\n\nMentions:\n- Figure 13 shows the query times for retrieving the top-k results for queries of varying sizes q on a 50-dimensional dataset.\n\nOCR Text: \"ProMiSH-E, 9-3 ProMiSH-E, q-6 ProMish-A 9=3 ProMish-A 10000 2 0 1000 6 8 100 Results, 9-6 #Top\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 13. Query time analysis of ProMiSH algorithms for retrieving top-k results for queries of sizes 3 and 6 on a 50-dimensional synthetic dataset of size N=3 million. Values of t=1 and U=200 were used for the dataset."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Again the extent of the exploration for each of the above can be controlled by setting maximum values for the number of links to follow and tweets to retrieve. Once the local copy of the network has been update, the data then feeds back to the recommendation phase and another iteration of the recommendation-selection-update process is executed. A visual overview of the complete curation system process is shown in Fig. 1.  initial data collection -16 September 2011 -this list contained 128 unique users. To evaluate the robustness of the user recommendation process, we use cross validation, randomly dividing the complete Iowa user list into four disjoint datasets, each containing 32 users. As an example, the subgraph induced by the core set on the follower graph of Iowa dataset #1 is shown in Fig. 2(a)the positions of nodes were calculated using the force directed layout implementation provided by Gephi [1].\n- In our experiments we applied the workflow shown in Fig. 1 to each of the sets individually for six recommendation-selection-update iterations after the initial bootstrapping phase. Note that no information was shared between the runs. The extent of network exploration during the update phase was controlled using the following constraints:\n\nMentions:\n- A visual overview of the complete curation system process is shown in Fig. 1.  initial data collection -16 September 2011 -this list contained 128 unique users.\n- In our experiments we applied the workflow shown in Fig. 1 to each of the sets individually for six recommendation-selection-update iterations after the initial bootstrapping phase.\n\nOCR Text: \"Uwyhd Hx Uux\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Overview of curation support system, illustrating the workflow between the bootstrap, recommendation, and update phases."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The calibration on lepton energy scale has correlation with energy resolution, which is also from lepton p T cut. The lepton p T spectra from Z/γ * → ℓ + ℓ − events is shown in Fig. 11. Due to a peak structure, events with lepton p T larger than the cut value are more than events with lepton p T lower than than cut value. Even if the resolution itself is symmetrical, it smears more events from high p T to low p T . As a result, a cut applied on the reconstructed lepton p T is slightly rejecting more high p T leptons. It will cause a non linear relationship between E obs and E true in the selected sample. It is purely a statistical effect, as if the p T cut is selecting data sample with bias. Fig. 12 shows an example of the resolution effect. A 5% gaussian smearing (N (0, 0.05 2 )) is applied to the lepton energy in the Z/γ * → ℓ + ℓ − sample. A p T &gt; 25 GeV cut is then applied to the lepton p T after smearing. As we can see, the ratio of average lepton energy before and after smearing is not a constant, even if the smearing itself does not change the average of lepton energy. If the resolution is not symmetrical, such effect will be even larger. This effect has to be considered in the energy scale calibration. E(E true ) and E(E true ) must be observed with consistent resolution smearing, which requires a good modeling of the detector resolution.\n\nMentions:\n- It is purely a statistical effect, as if the p T cut is selecting data sample with bias. Fig. 12 shows an example of the resolution effect.\n\nOCR Text: \"Vem-enMIcAC\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 12: Ratio of smeared lepton energy and corresponding true energy as a function of lepton energy."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Results of GraftNet are shown in Fig 4 and Fig 5, there are totally 18 attributes in our experiment. We compute the ROC curves and corresponding AUC scores for each attribute. As can be seen, GraftNet performs well on most of the attributes and their AUC scores are between 0.95-0.99. This means the construction process of GraftNet and the network graph with dynamic data flow are effective for multi-label classification and can achieve excellent performance. But there are still four attributes of bald, youth, middle-age and pregnant which do not perform well enough, we think this is because the features of these attributes are hard to discriminate. For attributes of bald and pregnant, the appearance features are not obvious, and for attributes of youth and middle-age, it's hard to draw a clear line between them.\n\nMentions:\n- Results of GraftNet are shown in Fig 4 and Fig 5, there are totally 18 attributes in our experiment.\n\nOCR Text: \"Fll @nz Bran= RNlnd c Uraten FAion[T\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 5.ROC curves of attributes with pretrained weights on ImageNet."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We have computed the information profiles for each of the three chromosomes (Fig. 1). There are locations of low information content which are associated with DNA regions of biological interest, such as telomeric and centromere regions. We have marked with letters A, C, D, F, G and I the telomeric regions and with letters B, E and H the centromere regions. They also allow to identify the long arm (q) and short arm (p) on each chromosome. In Fig. 2, we display a zoomed view of the centromeres, revealing that their size varies inversely with the length of the respective chromosome.\n\nMentions:\n- In Fig. 2, we display a zoomed view of the centromeres, revealing that their size varies inversely with the length of the respective chromosome.\n\nOCR Text: \"WV Hw IV   ! Kim\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: Plot of the information content of the centromeres of chromosome I (C1), II (C2) and III (C3)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 1. A subgraph of NELL with Tom Cable as an emerging entity. The solid-lined circles and arrows represent the existing entities and relations. The dashed-lined circles and arrows denote an emerging entity and some of its known relationships to other existing entities. The unknown relationships that need to be inferred through inductive representation learning and explainable reasoning are shown as dotted arrows.\n\nMentions:\n- Fig. 1.\n\nOCR Text: \"Lelcib FOLIdnn ALOS4AnE Vealrad Lun RLELEBI Liciefiu Jeat AintL Ieuapuril_i [c4t Parunt\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1. A subgraph of NELL with Tom Cable as an emerging entity. The solid-lined circles and arrows represent the existing entities and relations. The dashed-lined circles and arrows denote an emerging entity and some of its known relationships to other existing entities. The unknown relationships that need to be inferred through inductive representation learning and explainable reasoning are shown as dotted arrows."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Eqs. (12) the dimensionless \"decoupling function\" D(k) is defined via the dimensionless \"decoupling parameter\" ζ(k), and describes the decoupling of the normal and superfluid velocity fluctuations, caused by the counterflow velocity. The plot of D as a function of k/k × is shown in Fig. 5, where\n\nMentions:\n- The plot of D as a function of k/k × is shown in Fig. 5, where\n\nOCR Text: \": 1/k<\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 5: (color online) Decoupling function D(k/k×) = Ens/E(0)ns vs dimensionless wave number k/k×."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The training dataset includes 70% randomly selected points of the produced dataset, and the remaining 30% is used as the test dataset to evaluate the model. The accuracy of the model is evaluated with the test dataset in 1,000 iterations as depicted in Fig. 2. In each training iteration, the model is rigorously tested with the test dataset. As observed in Fig. 4, the classification CNN layer reached an accuracy of 100% on the test dataset in less than 60 iterations. This result approves that the CNN is structured such that it can learn very fast and achieve a high accuracy of 100% on a test dataset. Thus, it is able to correctly classify all of stressed and unstressed operation states, which provides valuable and completely reliable outcomes to the utility operators.\n- Additionally, Fig. 4 shows that the performance of the CNN trained to predict the real value SIs increases with the increasing training iterations but converged to a high accuracy of 98.8% on the test dataset at the 500 th iteration. This result demonstrate that the CNN module is quite capable of predicting the status and severity of post-contingency stresses with a very high accuracy. The required training time for the stressed state classification and SI prediction models is measured to be 56 and 435.66 seconds, respectively. In real time application, the CNN module retrains periodically, every hour to capture new changes, e.g. building new transmission line or transformer and train with all historical OCs.\n\nMentions:\n- As observed in Fig. 4, the classification CNN layer reached an accuracy of 100% on the test dataset in less than 60 iterations.\n- Additionally, Fig. 4 shows that the performance of the CNN trained to predict the real value SIs increases with the increasing training iterations but converged to a high accuracy of 98.8% on the test dataset at the 500 th iteration.\n\nOCR Text: \"2 : 1 : 1 800 850 S0o 950 1000 100 [teration Security Inder prediction Binaly' Classifcation\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4. Accuracy of SI and stress status results on the test dataset versus the number of training epochs in the IEEE 118-bus case study."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 9 shows the same indicator, also taken at different times. Read10/14 measures the downloads by scientists during the years 2012 and 2013 of exactly the same sets of papers used in the Read10 measure, which covered downloads by scientists in 2000 and 2001. For the AA sample the relation is:\n\nMentions:\n- Figure 9 shows the same indicator, also taken at different times.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 9. Read 10 vs Read10/14 an identical measure over the exact same sets of papers, but measured using the 2012-2013 reads. Symbols are the same as in figure 5."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We trained the MLPClassifier 5 under different levels of ε ranging from 0.5 to 8 as plotted in Figure 7. Due to the heavy noise, the datasets with lower privacy budgets exhibited difficulty for training the MLPClassifier. However, we didn't conduct any parameter tuning to increase the performance of the MLPClassifier in order to make sure that we investigate the absolute impact of perturbation on the model. Figure 6 shows the model loss of the training process of MLPClassifier when ε = 4. As the figure shows, the model converges after around 14 epochs.\n- We recorded the accuracy of the trained MLPClassifier in the means of the weighted average of precision, recall, and f 1 -score against varying levels of privacy budget, and plotted the corresponding data as shown in Figure 7. As discussed in Section 4.8, the class, \"George W Bush\" showed a higher performance as there was a higher proportion of the input image instances related to that class. As shown in Figure 7, increasing the privacy budget increases accuracy, as higher privacy budgets impose less amount of randomization on the eigenfaces. We can see that PEEP produces reasonable accuracy for privacy budgets greater than 4 and less than or equal to 8, where 0 &lt; ε ≤ 9 is considered as an acceptable level of privacy [11]. According to the figure, only in one case out of eight have been misclassified. The parameters such as the minimum number of faces per each class, the size of the input dataset, and the hyperparameters of the MLPClassifier have a direct impact on accuracy. We can improve the accuracy of the MLPClassifier by changing the input parameters and conducting hyperparameter tuning. Moreover, the dataset has a higher number of instances for the class \"George W Bush\" compared to the other classes. A more balanced dataset would also provide better accuracy. However, in this paper, we investigate only the absolute effect of the privacy parameters on the performance of the MLPClassifier.  8: Instance of the face recognition when the images are randomized using PEEP at ε = 4 (the randomized images at ε = 4 are shown in Figure 4). The figure shows the predicted labels of the images against the original true labels.\n\nMentions:\n- We trained the MLPClassifier 5 under different levels of ε ranging from 0.5 to 8 as plotted in Figure 7.\n- We recorded the accuracy of the trained MLPClassifier in the means of the weighted average of precision, recall, and f 1 -score against varying levels of privacy budget, and plotted the corresponding data as shown in Figure 7.\n- As shown in Figure 7, increasing the privacy budget increases accuracy, as higher privacy budgets impose less amount of randomization on the eigenfaces.\n\nOCR Text: \"precisicn #-reca | f1-score 15 [12 U 15 ( 45 [ 55 & 65 privacy budzet leI\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 7: Performance of face recognition with privacy introduced by PEEP. WP refers to the instance of classification model without privacy where no randomization is applied to the input images."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To put our results in the context of other similar events, we compiled in Table 2 the properties of several bursts followed by Xray tails detected in 1E 1547.0-5408 and in other magnetars. We included, for comparison, also the three giant flares from SGR 0526-66, SGR 1806-20, andSGR 1900+14, although they involved a much larger energy budget (see Mereghetti 2008, and references therein). Based on the best available spectral and timing information reported in the literature, we estimated for each event the fluence of the tail and that of the associated initial burst and, using the distances given in Table 2, we derived the energies plotted in Fig. 10. Note that some of these estimates have relatively large uncertainties due to the poorly constrained spectra and/or instrumental saturation effects (in some cases, only lower limits could be established).\n- For the burst of 1E 1547.0-5408 we plotted two values in Fig. 10: one in which we consider the whole energy of the tail and one in which we exclude the part attributed to dust scattering. For the latter we only considered the fluence of the first part of the tail, estimated by its EPIC-pn spectrum. The fluence of the short burst corresponds to a total energy release of ∼ 2 × 10 40 d 2 4kpc erg. Therefore, this was a relatively strong burst, although not as energetic as the brighter events seen from this source on 2009 January 22 (Mereghetti et al. 2009;Savchenko et al. 2010).\n- From Fig. 10, it is clear that magnetar bursts span a large range of intensities, with several events with \"intermediate\" energy outputs, which bridge the \"standard\" bursts with the much rarer giant flares. Although there is an overall correlation between the burst and tail energetics, especially when also the giant and intermediate flares are considered, the data show a large dispersion. The energy in the tail can be either a fraction of that of the burst or exceed it. The latter case seems to require an additional energy supply released on a longer timescale after the burst, but the analysis of 1E 1547.0-5408 reported here shows that, in some cases, the tail energy might be overestimated if assuming an intrinsic NS emission only. (Mereghetti et al. 2009). Unfortunately, the fluence of these bursts in the soft X-ray band (0.5-10 keV) was not measured directly, since they were observed only in the hard X-ray range and, furthermore, the two brightest ones saturated the instruments. For this reason, Tiengo et al. (2010) could obtain only a combined information of the two factors which determine the flux of the scattered radiation, i.e. the dust optical depth and the fluence of the burst.\n\nMentions:\n- Based on the best available spectral and timing information reported in the literature, we estimated for each event the fluence of the tail and that of the associated initial burst and, using the distances given in Table 2, we derived the energies plotted in Fig. 10.\n- For the burst of 1E 1547.0-5408 we plotted two values in Fig. 10: one in which we consider the whole energy of the tail and one in which we exclude the part attributed to dust scattering.\n- From Fig. 10, it is clear that magnetar bursts span a large range of intensities, with several events with \"intermediate\" energy outputs, which bridge the \"standard\" bursts with the much rarer giant flares.\n\nOCR Text: \"SGR [o0 19654 IkIEI A JL Ma AMai\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 10. Energy of the tails versus the energy of their corresponding bursts (see Table 2). The red squares refer to the burst of 2009 February 3, uncorrected and corrected for the dust scattering contribution to the observed tail fluence."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The upper limit of the integrals (11) and ( 12), r f , must be taken large enough to guarantee that trajectories reach the free region of the effective potential, for resonances. For the case of l = 23, figure 5 shows the derivative of the winding number w (E) for negative and positive energies. There are 6 peaks for bound states with negative energy, and 2 peaks for resonance states with positive energy. The width of each resonance peak gives the lifetime of the resonance [36], for the l = 23 case, lifetimes of 1.23 × 10 −12 s and 4.29 × 10 −13 s were obtained. The peak for the lowest energy resonance is very narrow with a long lifetime, meanwhile the peak for the higher energy resonance is broader with shorter lifetime. The background shown for positive energies in figure 5 depends on the upper limit of the propagation, r f of equations (10). Although this background seems to have a periodic structure, this feature depends mainly on computational aspects, as r f takes higher values in the free region the intensity of the background increases and becomes noisier. Additional calculations were carried out for energies above the centrifugal energy barrier, there were not found resonances in the continuum.    6 shows that the peak of the first resonance is sharp and deep, as the peaks of bound states, while the peak of the second resonance is shallow. Figure 6 shows that for energy above the centrifugal barrier the arc length is a constant function of the energy.\n\nMentions:\n- Figure 6 shows that for energy above the centrifugal barrier the arc length is a constant function of the energy.\n\nOCR Text: \"-0.02 0,015 8 (Hartrcc~\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 6. Arc length as a function of the energy. The energy for the top of the centrifugal barrier is displayed as a dashed vertical line. The inset shows the resonances with more detailed. Logarithmic scale has been used for the ordinate."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- RX J1622.7-2325Nw is the SB2 component of the hierarchical quadruple system identified by Prato (2007) . The western component (Nw) is separated by ∼1 ′′ from the eastern one (Ne), which is itself a visual binary (∼0.1 ′′ ; see Figure 6). The radial velocity curve as a function of the phase is shown in Figure 7. Orbital parameters are shown in the second column of Table 5. From the orbital solution we found q = 0.98 ± 0.06 and γ = −6.75 ± 1.09 km s −1 , consistent with the values found in the Wilson plot shown in Figure 4. With a P = 3.23 day period and e = 0.30 ± 0.04, RX J1622.7-2325Nw is the shortest-period high-eccentricity PMS SB known to date.\n\nMentions:\n- From the orbital solution we found q = 0.98 ± 0.06 and γ = −6.75 ± 1.09 km s −1 , consistent with the values found in the Wilson plot shown in Figure 4.\n\nOCR Text: \"J1622./-2525Nw 6 0.01 ~6/8 (km 60 -40 -20 Uh (km 033\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4.— Linear fit to the primary vs. secondary radial velocities for RX J1622.7-2325Nw. Uncertainties in the RV are not shown because they are smaller than the symbol size."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- One caveat of our evaluation protocol is that the perplexity bounds are only estimates, which might not reflect the true value, particularly since the number of possible sequences grows exponentially with the number of words that are generated. We therefore break down perplexity per position in the generated sequences as in Eq. 6, and compare the estimated PPLs to the true enumerated PPLs at the last position, as shown in Figure 1. We find that at the final generation step, the estimated bounds agree remarkably well with the exact values, proving that our method at least gets a reasonable PPL estimate at the last generation step, and that Joint BiT-Med outperforms baselines at the last generation step for sure.\n\nMentions:\n- We therefore break down perplexity per position in the generated sequences as in Eq. 6, and compare the estimated PPLs to the true enumerated PPLs at the last position, as shown in Figure 1.\n\nOCR Text: \"M[I HEEh Fl\"t MFI An F\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Perplexity gain of Joint BiT-Med and Joint BiT-Large∗ (using Base LM-24L) at each position relative to Base LM-24L on the test set of CC-News. At each position the lower and upper bounds (Eq. 6 estimated using the method in Eq. 5, see §6.1 for more details) are estimated using 20,000 samples. The shorter the horizon (moving to the right), the tighter the estimation is but also the more limited the gains compared to base LM as un-normalized models are most useful on longer generations."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The BER convergence performance is shown in Fig. 4 for a mobile communications scenario. The channel coefficients are obtained with Clarkes model [13] and the adaptive filters are trained with 500 symbols and then switch to decision-directed mode. The results show that the proposed scheme has a significantly better performance than the existing approaches and is able to adequately track the desired signal.\n\nMentions:\n- The BER convergence performance is shown in Fig. 4 for a mobile communications scenario.\n\nOCR Text: \"Keje users, EUN e150 Ful-ha '0md GEecsman ocum 62 1 1 17079 7626- AFJ 86666409'\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4. BER performance versus number of received symbols."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We estimated the reddening of each target, comparing the observed (B − V) color from the Bellini et al. (2009) catalog with the theoretical color of a star with the same temperature and gravity. This value was determined interpolating the Kurucz (1993) grid of models with the same metallicity used in the spectra fitting procedure. The results are shown in the upper panel of Fig. 7, where seven stars deviating from the general trend are evidenced. The surface parameters of these stars are not peculiar, but we notice that half of them have very low spectroscopic mass (0.2-0.3 M ⊙ , see Fig. 13) and the three targets with E(B − V) ≥0.3 are all redder than the mean HB in Fig. 1 (V ≥18, U − V ≥ −1). After their exclusion, the mean reddening is E(B − V)=0.115±0.004, and the stars are scattered about this value with an rms of 0.037 magnitudes. These values excellently agree with previous investigations, which found E(B − V)=0.11-0.12 (Lub 2002;Bedin et al. 2004;Calamida et al. 2005;Cassisi et al. 2009). In Fig. 7, we also show the trend of the mean reddening, which was calculated by substituting to each target the mean value of the ten adjacent stars in temperature order. The 1-σ stripe was calculated from the statistical error-on-the-mean of this estimate. Moreover, E(B − V) shows only a tiny trend with temperature, which is not significative within errors. The reddening effect is expected to increase for bluer spectral types (Grebel &amp; Roberts 1995), and in fact the mean value increases from 0.110±0.005 for stars between 8 000 and 10 000 K to 0.123±0.008 in the range 20 000-30 000 K. Among stars hotter than 32 000 K, we find a mean reddening that is slightly lower (0.098±0.012), but still consistent within errors with the cooler stars. In any case, the temperature offset discussed in Sect. 4.1 cannot be related to this, because a temperature overestimate would cause a higher reddening. The great majority of these stars are helium rich (86%, see Sect. 4.4), and it could be argued that the derived lower reddening is due to this discontinuity in the surface chemical composition. However, we found no evidence that E(B − V) is on average higher for the three helium-poor objects in this temperature regime. On the other hand, it must be taken into account that the theoretical (B − V) color has been obtained from a grid of LTE models that should be inadequate for hotter stars. This could have caused the small offset observed for these targets.\n- The derived reddening estimates can be used to analyze the pattern of E(B − V) in the cluster area. The position of the target stars, with symbols proportional to the measured reddening, is shown in the lower panel of Fig. 7. The resulting map is far from uniform, because the stars in the eastern side of the cluster are on average less reddened than in the western half. The average value of E(B − V) is 0.127±0.05 and 0.133±0.04 in the fourth and third quadrant, respectively, while it is 0.11±0.02 in the first and 0.097±0.008 in the second one. The reddening is therefore uniform in the western half of the cluster, within the limits of our accuracy. On the eastern side, on the contrary, the reddening is lower, in particular, in the southeast quadrant. This observed pattern is not a consequence of an uneven distribution of the hotter stars, whose measured reddening is slightly lower than the average (see the upper panel of Fig. 7, and discussion above in this section). In fact, we have an equal number of BH candidates in both the eastern and western half, and none in the second quadrant, where the average reddening is the lowest. A more detailed map of the cluster differential reddening is prevented by the limited number of data points and by the non-uniform area sampling. Despite our limitations, our results match the pattern found by Calamida et al. (2005), who found the clumpy structure of reddening in the direction of ω Cen, with more reddened stars clustering on the west side, and in particular in the northwest quadrant.\n\nMentions:\n- The results are shown in the upper panel of Fig. 7, where seven stars deviating from the general trend are evidenced.\n- In Fig. 7, we also show the trend of the mean reddening, which was calculated by substituting to each target the mean value of the ten adjacent stars in temperature order.\n- The position of the target stars, with symbols proportional to the measured reddening, is shown in the lower panel of Fig. 7.\n- This observed pattern is not a consequence of an uneven distribution of the hotter stars, whose measured reddening is slightly lower than the average (see the upper panel of Fig. 7, and discussion above in this section).\n\nOCR Text: \"YE Hg #EUI |5 EXPECCU '€ HcCE? HO LHEY Ur6laL VP IGrbe R Hobec J6L and Aat HheTea0 Kale I6CU= Fom OLLUIUS hr 53 heezh 8Ia1 SOULD K IL220UUBmherange ZUO1U-BUHK Hmosas hok Ha82UW Kwe ixdameatedengha sshh #= rOUZ hr siccnssertwf eloswhleoe 3a3 |harcase; Hhe YE xbrh {Eex discussedin Sex | Zmhe rekaed ho is Lecause & eerahye OHerETL {\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 7. Upper panel: reddening estimates for program stars as a function of temperature. The deviant points are indicated with empty circles. The trend of the mean reddening (calculated as described in the text) with its 1-σ stripe is shown with full and dashed lines, respectively. Lower panel: spatial distribuion of the observed stars, where the size of each plotted point is proportional to the star’s reddening. The cross indicates the cluster center."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In order to compare the behavior of the IS-energies to the corresponding behavior of the MBenergies, we show the evolution of p(e M B ) for the same quench from T 0 = 0.6 to T = 0.45 for the small system in Fig. 9  most prominent differences are the better statistics and the increase in the overall relaxation time of roughly one decade in case of the small system. We have seen that the evolution of both, the IS-energies or the MB-energies again qualitatively follow the predictions of the trap model with a Gaussian DOS. Thus, the dependence of the moments on the waiting time can be understood in terms of the demarcation energy, which separates low-energy states that up to the given time are frozen from high-energy states, which at the same time have reached the new equilibrium already.\n\nMentions:\n- In order to compare the behavior of the IS-energies to the corresponding behavior of the MBenergies, we show the evolution of p(e M B ) for the same quench from T 0 = 0.6 to T = 0.45 for the small system in Fig. 9  most prominent differences are the better statistics and the increase in the overall relaxation time of roughly one decade in case of the small system.\n\nOCR Text: \"0,20 0.15 2 0.10 0,05 O,00 -305 -300 -295 -290 285\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 9: p(eMB) of the small system for different waiting times, t=4, 32, 152, 392, 1992, 15992 (from right to left) after a quench from T0 = 0.6 to T = 0.45 (black lines). Red lines: p eq T (eMB)"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- As we had seen above, in the context of sampling the presence of epsilon-transitions is not necessarily problematic. It would suffice to simply ensure that the epsilon-graph is acyclic. This can be achieved by a modification of the epsilon-removal algorithm, which we call epsilon-cycle conflation. Rather than removing all epsilon-transitions, this algorithm merely replaces every set of epsilon-paths through an epsilon-SCC with a single epsilon-transition. This is illustrated in the right-hand side of Figure 3; pseudocode appears in Figure 4.\n\nMentions:\n- This is illustrated in the right-hand side of Figure 3; pseudocode appears in Figure 4.\n\nOCR Text: \"nmxMun CONIATELASILONCYCLES r Scc + Map Irin sjille SCCrimt MAPi } Enini; IMill lu Wel EPSILONSSCCAKCFILTFRIt . Hiflwic e SHORTEMTDETANCH Aafe Qiirti AVDSTRTE > for Ir c) In Cow7oze: (ncn ADDARC Jieta\"ce'].48} sudclclc Liaie. cqwinuielit | ID jC LTSILONSCCARCHLTERMCC Lowinorsm or <cIn If Jcrlcn) Uhen DFLFtarcl else MI ttrurc } Kotthel (etlur | suh;[eclan ( it STAnT' 4 In Scit Incn STantM' s StAKTG /0 CONNECT Y | 05a|\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4: Epsilon-cycle conflation algorithm."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 7: GEMINI evaluation rules (excerpt)\n- Similarly to typing rules, each evaluation rule is a theorem. An excerpt of the set of evaluation rules is shown in Figure 7. In these rules, the character t denotes a term that may be further reduced by some evaluation rule. The character v denotes a terminal value that cannot be evaluated any further.\n\nMentions:\n- Figure 7: GEMINI evaluation rules (excerpt)\n- An excerpt of the set of evaluation rules is shown in Figure 7.\n\nOCR Text: \"(E- APPI) (E-APPZ) ViL , (Xxe Iva Xnta (E-APPARS! ten 83e (E-IFFLSFI Ehen Fan (E-IELSE-Tl cher IE [FELSE Fl then aet 35 {1\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 7: GEMINI evaluation rules (excerpt)"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- It is important to notice that the only novel aspect with respect to the standard dispersive case (treated in full detail in [22]) concerns the behavior near the universal horizon. To clarify these new aspects, we represent in Fig. 5 the global structure of the characteristics in the collapsing mass shell geometry. When tracing backwards the u-like characteristics associated with the Hawking quanta (λ &gt; 0) and their inside negative energy partners (λ &lt; 0), we see that they both originate from infalling v-like superluminal characteristics with a high and positive Killing frequency λin. The v mode which emanates from the singularity (the dashed line) returns to it after having bounced at the center of the shell (not represented), closely following the characteristic of the v mode coming from infinity which hits the shell at the same value of r.\n- This equation has two solutions, but only one is well behaved as the other one gives a trajectory along which the preferred time is not monotonic. A straightforward calculation using the dispersion relation Eq. ( 21) also shows that the sign of Ω is preserved. It should be noted that Eq. ( 27) is the dispersive version of the relativistic equation |r/(r − 2M )|λ = ω, which gives back Eq. (12) for r &gt; 2M , ω λ, and when using u rather than r. It should be also emphasized that all outgoing u-like characteristics originate from inside the shell, as in the relativistic case. This is shown in Fig. 5. Therefore, thanks to the universal horizon, the state of the field inside the shell determines the state of the u modes. In this we avoid the problem discussed in [2], namely that in the absence of a universal horizon, the u modes of a superluminal field originate from the singularity at r = 0. As discussed in footnote 3, these modes still exist, but they are now trapped inside the universal horizon.\n- For completeness, we have also represented in Fig. 5 a couple of infalling v characteristics which enter the shell for 0 &lt; r &lt; M . One comes from r = 0 (the dashed line), and one from r = ∞ (the solid line). They both reach the singularity after having bounced at r = 0 inside the shell. These characteristics, although interesting, play no role in the Hawking process.\n\nMentions:\n- To clarify these new aspects, we represent in Fig. 5 the global structure of the characteristics in the collapsing mass shell geometry.\n- This is shown in Fig. 5.\n- For completeness, we have also represented in Fig. 5 a couple of infalling v characteristics which enter the shell for 0 &lt; r &lt; M .\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 5. Characteristics crossing the infalling shell in the v−r, r plane. The Killing frequencies of the outgoing u modes and the incoming v modes is λ = ±0.5Λ. The solid (dashed) lines represent characteristics for which the value of the Killing frequency λ of the out-going u mode is positive (negative). The arrows indicate the future direction associated with the aether field. When tracing backwards the u-like characteristics associated with the Hawking quanta (λ > 0) and their inside negative energy partners (λ < 0), we see that they both originate from infalling v-like superluminal characteristics with a high and positive Killing frequency λin. The v mode which emanates from the singularity (the dashed line) returns to it after having bounced at the center of the shell (not represented), closely following the characteristic of the v mode coming from infinity which hits the shell at the same value of r."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Our observations detected 12 CO(2 − 1) and 12 CO(3 − 2) emission from the central region as well as continuum emission at all three wavelengths. We also have a global detection of 13 CO(3 − 2) when we average over the main ridge of CO emission (Figure 2). The 12 CO(2 − 1) emission generally has a higher brightness temperature than the 12 CO(3 − 2) emission, but the 12 CO(3 − 2) observations have better brightness temperature sensitivity (σ T B ∝ σ Sν /ν 2 ), leading to higher signal-to-noise for the 12 CO(3 − 2) data (Figure 4). The 12 CO(1 − 0) line was not detected. However, given the higher noise in the 12 CO(1 − 0) image (in K), this non-detection is consistent with the ratio between the 12 CO(2 − 1) and 12 CO(1 − 0) emission seen in II Zw 40 in other lower-resolution observations (0.5; Albrecht et al. 2004).\n\nMentions:\n- We also have a global detection of 13 CO(3 − 2) when we average over the main ridge of CO emission (Figure 2).\n\nOCR Text: \"13c013-2,-1 CC,32; CCO-1; Velocity Ikm 5\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2. Average 12CO(3− 2), 12CO(2− 1), and 13CO(3− 2) spectra over the main CO ridge (extraction region corresponds to the region labeled BCD in Figure 4). We clearly detect 13CO(3− 2) emission in the average spectrum, but do not detect it in the individual channel maps."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- 3) The effect of COW coverage range: We then analyze the impact of the COW coverage range on the mean UE SE in  Fig. 7. Here, we first observe that the mean SE increases with the growing relay coverage range. Second, we notice that the SE ceases to grow when the range becomes large enough to almost guarantee a COW in the UE proximity. The value of COW range after which the mean SE stagnates heavily depends on the fraction of vehicles involved in relaying. Finally, we conclude that the Aggressive strategy notably outperforms the Conservative scheme: the mean SE for the former with only 10% of COW vehicles is higher than the corresponding value for the latter when all 100% of vehicles act as COWs.\n\nMentions:\n- 3) The effect of COW coverage range: We then analyze the impact of the COW coverage range on the mean UE SE in  Fig. 7.\n\nOCR Text: \"Relsy-aiad (Haesshe} Pa = Relsy aij-d (Kazresshe} Pa =@lI Aelyy-arked (Coctrsbvel, m Relsy-Jksed (Coctrvjbvel_ Natcactonceang Oacalaal CoW covcragc rngc |u]\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 7. Mean SE increases with COW coverage range and ceases to grow when the range becomes large enough so that pC → 1."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 3 shows the XLF for LMXBs in the fields of the seven early-type galaxies in our sample, scaled by the total stellar light covered, N(LMXBs)/10 10 L K . We measure stellar light covered directly from 2MASS LGA images after carefully masking them to only include the galaxy regions covered by this study. It can be seen that for six of the seven galaxies observed, the XLFs are similar. This implies that the formation and evolution of LMXBs in these old stellar populations is similar. One galaxy, NGC 3379 has a lower number of LMXBs at a given L x , although we note that this is exacerbated in Figure 3 by small number statistics at high L x . This result was previously observed and discussed by Kim et al. (2009) and Peacock et al. (2014).\n- The dashed-black line in Figure 3 is a broken powerlaw representation of the XLF. It has the form:\n\nMentions:\n- Figure 3 shows the XLF for LMXBs in the fields of the seven early-type galaxies in our sample, scaled by the total stellar light covered, N(LMXBs)/10 10 L K .\n- One galaxy, NGC 3379 has a lower number of LMXBs at a given L x , although we note that this is exacerbated in Figure 3 by small number statistics at high L x .\n- The dashed-black line in Figure 3 is a broken powerlaw representation of the XLF.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3.— The XLFs of LMXBs in the fields of the seven galaxies studied. These XLFs are scaled by the stellar K-band light covered. The black dashed line shows a broken power-law representation of these data. The small offsets between these XLFs is indicative that these galaxies form a similar number of LMXBs per stellar mass."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The testing of the hypothesis formulated in the preceding paragraph on empirical complex networks is carried out in the following way. Two pairs of SIR model parameters (p 1 , q 1 ) and (p 2 , q 2 ) are selected. For each node of the empirical complex network and for each pair of the parameter values, the average number of infected nodes (i.e. the final size of the epidemic) for the disease starting at that very node is calculated. In particular for a node i the quantities X i p1,q1 and X i p2,q2 , the average numbers of infected nodes normalized to the total number of nodes in the network for parameters (p 1 , q 1 ) and (p 2 , q 2 ), respectively, are obtained. A plot with X i p1,q1 and X i p2,q2 on the axes is constructed. For each initially infected node a point is entered into the plot. An example of such a plot obtained for the cond-mat network is presented in Fig. 1.\n- The scattering of the points in the plot presented in Fig. 1 vividly demonstrates the dependence of the epidemic ranking on the disease spreading regime. In this plot two points A and B have been singled out to show how the ranking of two nodes is altered if the dynamical regime of disease spreading is changed. If there was no dependence of the epidemic ranking on parameters of the disease spreading model, the points in Fig. 1 would be ordered in a monotonously growing curve. Analogous plots demonstrating the dependence of the ranking on the dynamical regime have also been obtained for other studied empirical complex networks and other combinations of (p, q) pairs.\n- Figure 1. The mean number of infected nodes normalized to the total number of nodes in the network in the regime (p 1 = 0.1, q 1 = 0.9) (on the x-axis) versus the mean number of infected nodes normalized to the total number of nodes in the network in the regime (p 2 = 0.1, q 2 = 0.2) (on the y-axis) for the cond-mat complex network and the same initially infected node. For a very large number of node pairs their relative epidemic ranking changes when the first regime is changed to the second one. Points A and B marked in the plot give a clear example of such a pair. Tables Table 1. Spearman's rank correlation coefficient of epidemic ranking and ranking based on node degree or k-cores value for four studied complex networks.\n\nMentions:\n- An example of such a plot obtained for the cond-mat network is presented in Fig. 1.\n- The scattering of the points in the plot presented in Fig. 1 vividly demonstrates the dependence of the epidemic ranking on the disease spreading regime.\n- If there was no dependence of the epidemic ranking on parameters of the disease spreading model, the points in Fig. 1 would be ordered in a monotonously growing curve.\n- Figure 1.\n\nOCR Text: \"cond-mat 2003 COo 0 05 0.10 X!qi\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1. The mean number of infected nodes normalized to the total number of nodes in the network in the regime (p1 = 0.1, q1 = 0.9) (on the x-axis) versus the mean number of infected nodes normalized to the total number of nodes in the network in the regime (p2 = 0.1, q2 = 0.2) (on the y-axis) for the cond-mat complex network and the same initially infected node. For a very large number of node pairs their relative epidemic ranking changes when the first regime is changed to the second one. Points A and B marked in the plot give a clear example of such a pair."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- There are no public available benchmarks for cervical cell object detection in the community, we first establish a database consisting of 7,086 cervical microscopical images which are cropped from the whole slide images (WSIs) obtained by Pannoramic MIDI II digital slide scanner. The corresponding specimens are prepared by Thinprep methods stained with Papanicolaou stain. Conforming to TBS categories [2], 48,587 object instance bounding boxes were labeled by experienced pathologists which belong to 11 categories, namely ASC-US (ascus), ASC-H (asch), low-grade squamous intraepithelial lesion (lsil), high-grade squamous intraepithelial lesion (hsil), squamous-cell carcinoma (scc), atypical glandular cells (agc), trichomonas (trich), candida (cand), flora, herps, actinomyces (actin). Figure 5 shows some examples of each category in our database. Then we randomly divide the dataset into training set D f which contains 6,667 images, test set which contains 419 images for experiment. To verify the performance of Comparison detector on the small dataset, we randomly choose 762 images from the training dataset to form a small dataset of D s . The distribution of categories in each dataset is shown in the Fig. 6.\n\nMentions:\n- The distribution of categories in each dataset is shown in the Fig. 6.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6: The distribution of categories on different datasets."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The decision to replace CYCLOPS also provided an opportunity for upgrades compared to the original specifications. It was determined that by stripping the buffer from the fibres in the fibre slit the fibres could be more closely packed, allowing 16 fibres to fit within the inter-order separation of the UCLES echellogram instead of the original 15. It was also decided to include a 17 th fibre for simultaneous wavelength calibration, a capability that CYCLOPS/UCLES did not have before. Illumination of this fibre with a ThXe hollow cathode arc lamp during science exposures will provided simultaneous wavelength calibration at the expense of contaminating two of the science fibres with calibration light, CYCLOPS2 will therefore be able to operate either with 16 science fibres or with 14 science fibres plus simultaneous wavelength calibration. Figure 6 is a micrograph of the CYCLOPS2 fibre slit. The individual fibres have been close packed in direct contact with adjacent fibres and with the precision machined slit blocks on each side with the result that the variation in fibre core positions both along and perpedicular to the slit axis are within ±1 µm.\n\nMentions:\n- Figure 6 is a micrograph of the CYCLOPS2 fibre slit.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6. Composite micrograph of the CYCLOPS2 fibre slit."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To verify that the nitrogen ions indeed give rise to optical amplification in the backward direction, we have performed a pump-probe experiment with a 100 Hz femtosecond laser system. Similar to the setup in ref. 24, a weak probe beam with central wavelength at 390 nm generated in a BBO crystal was injected in the opposite direction of the pump laser. The pump pulse duration was 40 fs and its energy was 3 mJ. The pump was focused by a convex lens f = 400 mm in a pure nitrogen at 30 mbar. The backward-propagating seed pulse amplified by a factor of 10 at 391.4 nm is shown in Fig. 3, which demonstrates undoubtedly the optical amplification in the backward direction.\n\nMentions:\n- The backward-propagating seed pulse amplified by a factor of 10 at 391.4 nm is shown in Fig. 3, which demonstrates undoubtedly the optical amplification in the backward direction.\n\nOCR Text: \"~geedand)m) EBBJ JJSB 1 1 1 'cavelengh {rT)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3. Amplification of the backward-propagating seeding pulse in a plasma. Blue line: seeding pulse, red line: the amplified emission in presence of both the pump laser and seeding pulse. The energy of the pump laser is 7.5 mJ, it was focused in 20 mbar nitrogen gas with a convex lens f = 400 mm."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To understand the effect of external field cooling on    7 suggests an onset of FM unidirectional anisotropy at interface during field cooling. The higher magnetic fields destroy this unidirectional anisotropy as evident from increasing value of H R c while H L c remains constant. To understand the out-of-plane magnetic asymmetry we have measured field cooled M(H) for ML [20/20]3 multilayer at 5 K. The field cooled M(H) graph has been measured after cooling the multilayer in +10 kOe and -10 kOe field applied at 300 K, along out-of-plane direction. The Fig. 8 shows M(H) plot with H cool = 10 kOe and -10 kOe at 5 K. As evident in Fig. 8, the M(H) loop for H cool = 10 kOe shifts towards negative field axis and the one with H cool = -10 kOe shifts towards positive field axis. This opposite shifting of M(H) with cooling in positive and negative field, implies exchange bias effect. We have calculated H EB = -56 Oe for positive cooling field of 10 kOe, and H EB = 60 Oe for negative field cooling of -10 kOe. These values shows H EB almost similar to earlier obtained values, when magnetic field is in-plane direction.\n\nMentions:\n- The Fig. 8 shows M(H) plot with H cool = 10 kOe and -10 kOe at 5 K.\n- As evident in Fig. 8, the M(H) loop for H cool = 10 kOe shifts towards negative field axis and the one with H cool = -10 kOe shifts towards positive field axis.\n\nOCR Text: \"A10 kOe 10 kOe ML 1 [2020]3 5 K out of plane -600 -400 -200 200  400   60O H (Oc) :\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 8: (color online) Expanded view of field dependent magnetization measured at 5 K following FC protocol with out-of-plane cooling field Hcool = 10 kOe and -10 kOe for ML[20/20]3 multilayer."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Now we compare our proposed joint SDR turbo receivers with those SDR turbo receivers from [6], which we name as \"Mehran List SDR\" and \"Mehran Single SDR\", respectively. The \"Mehran List SDR\" solves SDRs in each iteration while \"Mehran Single SDR\" runs one SDR in the first iteration only. For Mehran's methods, we employ same setting as in his paper [6]: 25 randomizations, (at most) 25 preliminary elements in the list, of which 5 elements are used for enrichment. All BER curves plotted in Fig. 6 are after the 3rd iteration of turbo processing. For our joint SDR turbo receivers, Hamming radius P = 2 for list generation. The performance advantage of our receivers is clear around BER = 1e-4. Both our multi SDR and single SDR receiver outperform its counterpart, and our single SDR receiver even outperforms \"Mehran List SDR\" that solves SDRs in each iteration.\n\nMentions:\n- All BER curves plotted in Fig. 6 are after the 3rd iteration of turbo processing.\n\nOCR Text: \"Eonientecmp nnli Sda om-uf Flon Sda Pez SNP 'JBI\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 6: BER comparisons of different turbo SDR receivers."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In the more general case, a toy Monte Carlo approach can be used, by generating toy data sets with a range of true signal sizes, and selecting all observations whose discovery significance does not exceed the threshold. We construct the likelihood ratio curves R i (s) for all these observations, labelled with index i, and compute the maximum value at each signal, R envelope (s) = sup i (R i (s)). Finally, we set the threshold R + (s) to be the greatest of R envelope (s) and R max,α (s). This construction is shown in figure 3.\n\nMentions:\n- This construction is shown in figure 3.\n\nOCR Text: \"FC construction mcd fiedFC Discovery Thresnc d\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Construction of themodified FC threshold function, using curves of R′(s) formultiple toyMonte Carlo realizations, colored according to whether they exceed the 3σ discovery significance threshold (green) or not (gray). The thick blue curve shows the threshold corresponding to a 90% FC construction, while the orange curve, showing R+(s), is constructed to be equal to the discovery threshold, indicated with a dashed green line, at 0 signal, and to be greater or equal to both the likelihood-ratio curves with discovery significance less than 3σ and the FC threshold."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- As an example the magnetic susceptibility of the ten-membered ferric wheel, which was synthesized 18 years ago [53], is presented in Fig. 4. The symbols mark the experimental values [53], the solid curve shows the result of exact diagonalization [13], and the dotted curve shows the result obtained with ALPS QMC using 10 7 steps for equilibration and 10 10 steps for the Monte-Carlo sampling for each temperature. As one can see the QMC result is indistinguishable from the exact one. 6 cm −1 as well as the susceptibility data are taken from Ref. [53]. The solid curve displays the result of complete matrix diagonalization [13] whereas the dotted curve depicts the QMC result.\n\nMentions:\n- As an example the magnetic susceptibility of the ten-membered ferric wheel, which was synthesized 18 years ago [53], is presented in Fig. 4.\n\nOCR Text: \"0.28 2 3-)31 @ 0|8 Exaci 1--96 cin\"L-I: 8 QMc 0.05 0.08\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4: Susceptibility of an antiferromagnetically coupled spin ring with N = 10 and s = 5/2. The exchange parameter J = −9.6 cm−1 as well as the susceptibility data are taken from Ref. [53]. The solid curve displays the result of complete matrix diagonalization [13] whereas the dotted curve depicts the QMC result."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In order to investigate a possible formation of toroidal configurations in our analysis we selected the region where according to ETNA predictions the toroidal configuration is most pronounced in the θ f low and θ plane plane (θ plane &lt; 75 o and θ f low &gt; 20 o ). In Table I the efficiency factor values are given for experimental data and model predictions. The efficiency factor values are shown for four threshold values of the fragment charge. From Table I we notice that the EF values for experimental data are very close to the model predictions for toroidal configurations. This observation may be one of arguments in favor of the formation of toroidal/flat freeze-out configuration created in the Au + Au collisions at 23 AMeV. FIG. 13: (color online) The distributions of standard deviation of the fragment mass for non-flat events (red lines) and flat events (green lines) for experimental data (solid lines) and QMD model predictions (dashed lines).\n- Comparison of the σ A distributions (Fig. 13) for flat and non-flat events indicates that in the case of flat events this distribution is slightly shifted to larger values. This observation is in contrast with the expectation that for the flat events the enhanced similarity in the size of fragments should be visible. The corresponding distributions for QMD calculations are similar (dashed lines). Their centroids are shifted to smaller values with respect to experimental data.\n\nMentions:\n- This observation may be one of arguments in favor of the formation of toroidal/flat freeze-out configuration created in the Au + Au collisions at 23 AMeV. FIG. 13: (color online) The distributions of standard deviation of the fragment mass for non-flat events (red lines) and flat events (green lines) for experimental data (solid lines) and QMD model predictions (dashed lines).\n- Comparison of the σ A distributions (Fig. 13) for flat and non-flat events indicates that in the case of flat events this distribution is slightly shifted to larger values.\n\nOCR Text: \"eeMen b ~a Oabm: F,4;} L\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 13: (color online) The distributions of standard deviation of the fragment mass for non-flat events (red lines) and flat events (green lines) for experimental data (solid lines) and QMD model predictions (dashed lines). All the distributions presented here are constructed using the condition Zfrag ≥ 10."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Given a path p in T A we say that p is a fault-free path, if any arc in p has label with Snapshot = {C, CC}. The definition of fault-free cycle is analogous. Given a cycle F in T A the displacement of d(F ) is the sum of all the movements on the arcs in F . Essentially, a cycle has zero displacement when an agent placed at node v at the end of cycle is still in node v. A cycle has positive displacement if, after the executions of all actions in the cycle, the agents moved clockwise. The negative displacement is analogous. See Figure 2 for examples of previous definitions.\n\nMentions:\n- See Figure 2 for examples of previous definitions.\n\nOCR Text: \"{CC}-1 {CC}-1 {C,CC}:-1 {C,CC}:1 {CH1 {C}1\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: Example of cycles: we have a fault-free cycle (A,B), (B,A) with zero displacement, a cycle (B,C), (C,A), (A,B) with negative displacement equal to −3, and a cycle (A,D), (D,B), (B,A) with positive displacement equal to 3."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Our proposed solution is a generative bi-partite graph framework that models the underlying processes that are likely to generate the data. The assumption is that large amounts of data are available that can represent the true population behaviour. See Fig. 1 for an illustration of the model. First, we consider the joint distribution given as p(x, s) over the set of binary hidden random s = s 1:J ∈ {0, 1} and observed x = x 1:K ∈ R D variables. We specify a prior distribution p(s) about the hidden variables and quantify how x relates to s with the likelihood function p(x|s). Applying the Bayes' rule, we obtain the posterior distribution:\n\nMentions:\n- See Fig. 1 for an illustration of the model.\n\nOCR Text: \"Hantin s Ilids- E Jmeyia #mf Visc-le Ljwer Ccnmcama: \"Jnrkc Maulcnneu\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: The generative bi-partite framework. The visible layer represents the input discrete and continuous data. The hidden layer represents the stochastic latent variables derived from the RBM learning algorithm. Bi-directional arrows indicate information passing in both directions. The hidden layer can be used to generate new data with similar statistical properties as the input."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The impact of the call arrival rate on the energy efficiency of PVT and grid random cellular networks is evaluated in Fig. 9. When the SINR threshold is fixed, there exist thresholds for different call arrive rates in cellular scenarios. Below the threshold, the energy efficiency increases and above the threshold the energy efficiency decreases with the increase in the call arrival rate. Numerical results of PVT and grid cellular networks consistently validate that there exist maximum energy efficiency values considering different call arrive rates in cellular scenarios. Therefore, to achieve an optimal spectrum and energy efficiency of cellular networks, the call arrive rate and the SINR threshold in a cell should be considered carefully by telecommunications operators.\n\nMentions:\n- The impact of the call arrival rate on the energy efficiency of PVT and grid random cellular networks is evaluated in Fig. 9.\n\nOCR Text: \"F mdd, Enmdd Fnmde morel 6dmo e 5udio:t\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 9. Energy efficiency with respect to call arrival rateλ considering different SINR thresholdsγ0 in PVT and grid cellular networks."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- A charge reconstruction algorithm was applied as follows to digitized waveforms recorded using a sampling frequency of 1 Gsa/s. The 17 samples centered on the peak sample are determined and designated as on-pulse. Eight samples before these on-pulse samples are avoided because they lie in the transition region between on-pulse and offpulse. Furthermore, all samples after the on-pulse samples are avoided because they could include undershoot. The remainder of the samples, well before the pulse, are designated off-pulse. An example waveform with this algorithm applied is shown in Figure 12. The mean of the off-pulse samples is calculated and used as a baseline estimate. The mean of the on-pulse samples is then determined and the baseline is subtracted. The resulting quantity provides an estimate of the baseline-subtracted charge integral in the pulse and increases linearly with the pulse input amplitude. This quantity is calibrated to photoelectrons assuming a preamplifier gain of 4 mV per photoelectron. The resulting measured charge as a function of simulated input charge is shown in Figure 13.\n\nMentions:\n- An example waveform with this algorithm applied is shown in Figure 12.\n\nOCR Text: \"wayBiom Pjic : 1.1C8 pjirts Dagglne baseina 9 Mhhbi 1.0s8 1,036 Time (ns)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 12: Example electrical pulse recorded with the TARGET 5 evaluation board. The input amplitude was 10 mV, corresponding to 2.5 photoelectrons for an example preamplifier gain of 4 mV per photoelectron."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- • Altmetric publication date: the publication date as recorded by Altmetric.com, which is a mix of the journal issue date and online date (personal communication with Euan Adie and Jean Liu) as retrieved from the publisher. This is also the date Altmetric.com uses to compute the Altmetric score and provide benchmarks for papers of the same age. As shown in Figure 2, particular peaks can be observed for January 1 of each year as well as the first or last of each month. This might reflect common publishing practices, but could also be caused by aggregating data without actual day (and month) information. It was found that 15.1% of Altmetric.com records 6 did not have any publication date or they had incorrect dates (e.g. dates up to 2037). • Altmetric first seen date: the datestamp when Altmetric.com captured the first event for a particular document, which is missing for 4% of all records 7 . • First tweet date: the datestamp of the first tweet 8 captured by Altmetric.com (excluding all papers with links to arXiv IDs or ADS IDs to ensure that the tweet did not refer to a preprint). • WoS indexing date: the day when the document was indexed by WoS, which for 2012 papers was mostly during (37.7%) or in the month before (11.5%) or after (29.4%) the journal issue month.\n\nMentions:\n- As shown in Figure 2, particular peaks can be observed for January 1 of each year as well as the first or last of each month.\n\nOCR Text: \"'ncmuta IHwhTIn\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2. Number of Altmetric.com ids per Altmetric.com publication date from January 2013 to December 2014."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- It is worth noticing that the corresponding statistical mechanics model for these striped random matrices is a one-dimensional disordered model with an interaction range growing with the signal length, as in a Kac construction. Models of this kind are analytically solvable and have shown very interesting results [24]. The use of our striped sparse matrices allows for a great reduction in computational complexity. Indeed, the measurement and recovery times grow linearly with the size of the signal if sparse matrices are used, while they grow quadratically if dense matrices are used. Figure 6 shows the measurement and recovery times of a signal for different signal lengths N . For this test, we used dense blockstructured matrices and sparse striped matrices. The number of EM-BP iterations to reach the solution is roughly constant for different N . A quadratic fit for the dense case and a linear fit for the sparse one perfectly interpolate the data.\n\nMentions:\n- Figure 6 shows the measurement and recovery times of a signal for different signal lengths N .\n\nOCR Text: \"Lils? 'pans? Iulu XOu S0lu IlL\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 6. Actual time (in seconds) for recovery of a signal withdense and sparse matrices for different data lengthsN . The data are fitted respectively by a quadratic and a linear function."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Theoretical calculations of cross sections have been performed by using three nuclear reaction codes: TALYS, FLUKA and EMPIRE. For TALYS, in particular, 4 pre-equilibrium models and 6 level density models are implemented, leading to a total of 24 different model combinations. In Fig. 1, the calculated cross sections of the aforementioned reaction for the different codes are shown: the grey band represents the region between the first and the third quartile (Q1-Q3) of the 24 cross sections calculated with the different TALYS models, whereas the two dashed lines represent the minimum and maximum calculated values, and the solid line is the \"Best Theoretical Evaluation\" (BTE) defined as the center of the interquartile range [3]. Along with the theoretical calculations, also several experimental data taken from the literature are shown. Despite some differences between the models and the experimental data, a maximum of the cross section is evident between 10 and 20 MeV. Since the majority of the produced manganese isotopes (namely 48 Mn, 49 Mn, 50g/m Mn, 51 Mn and 52m Mn) are characterized by a half-life shorter than one hour, their contamination, both in terms of isotopic and radionuclidic purity, would be negligible after few hours. It is thus interesting to evaluate the amount of 53 Mn, whose half-life is of about 3.6×10 6 years, that can be produced through this reaction, compared to that of 52g Mn. In order to do that, Fig. 2 shows the ratio\n- where N represents the number of nuclei and A the activity of the nuclide. These quantities are calculated as a function of time and shown in Fig. 3. The grey band of TALYS has a meaning analogous to that described for Fig. 1, whereas for FLUKA the ratios are always equal to 1, because this code cannot calculate (p,γ) reactions, and the only long-life contaminant produced in the considered energy window is 53 Mn, through the reaction: 52 Cr(p,γ) 53 Mn. All the codes predict a IP greater than 0.8 for at least 20 days after the irradiation, and a RNP close to 1 for at least 150 days, that is a much longer time with respect to the half life of 52g Mn. This means that there is not a significant contribution to the total activity from the other produced Mn isotopes, which would be otherwise a problem for the patient health. For this reason, the (p,n) reaction is promising for the production of 52g Mn with high purity. Also the activity per unit of current at the End-of-Bombardment was calculated according to the different models, and its values in MBq/µAh are: 6.64 ± 1.7 for TALYS [3], 20.2 for EMPIRE and 23.2 for FLUKA. The procedure described for this reaction has a general validity and was applied to many other reactions, in order to identify other possible competitive channels for the production of high purity 52g Mn. The following reactions were in particular analyzed: 53   52g Mn, Nat V(α,3n) 52g Mn. Among these, the (d,2n) and (α,3n) reactions proved to be the most promising channels [4].\n\nMentions:\n- In Fig. 1, the calculated cross sections of the aforementioned reaction for the different codes are shown: the grey band represents the region between the first and the third quartile (Q1-Q3) of the 24 cross sections calculated with the different TALYS models, whereas the two dashed lines represent the minimum and maximum calculated values, and the solid line is the \"Best Theoretical Evaluation\" (BTE) defined as the center of the interquartile range [3].\n- The grey band of TALYS has a meaning analogous to that described for Fig. 1, whereas for FLUKA the ratios are always equal to 1, because this code cannot calculate (p,γ) reactions, and the only long-life contaminant produced in the considered energy window is 53 Mn, through the reaction: 52 Cr(p,γ) 53 Mn.\n\nOCR Text: \"SCrlp,njsZJMn FTars BT= [ays 01-C3 ITays mn-Iax Fuka Empra dceim Blosser 1835 Linder 1953 Wing 1922 Suvn 1 Ledui Energy 'Mel) 0\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1. Theoretical estimations and experimental data for the cross section of the 52Cr(p,n)52gMn reaction."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The trajectories for the objects were also in time parametric representation, following the same computing procedure as in the previous section. Particularly, the randomly picked starting position (using asterisk) for two objects are [−5, −3] and [−2, −3], respectively. The maximum allowed distance is set to be d = 2, and the gain is α = 1e − 8. As to the agents, they are positioned randomly near the starting position, while satisfying the distance constraint between them, which are [−4.5, −3.5] and [−3.5, −3.5] (using asterisk). For this implementation, the differential equation ( 10) is solved using the same procedure as in Section V-A. The resulting trajectories are illustrated in Figure 3, where both robots, starting from arbitrary positions succeed in tracking the moving object and keep the maximum distance within limits simultaneously.\n\nMentions:\n- The resulting trajectories are illustrated in Figure 3, where both robots, starting from arbitrary positions succeed in tracking the moving object and keep the maximum distance within limits simultaneously.\n\nOCR Text: \"Iat |Jio ue gmti\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3: Trajectories of two objects yd1(t),y d 2(t) (solid) and two agents y1,y2 (dashed). Agents succeed in tracking objects while satisfying distance constraint between them."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- After defining the optimal combinations of reservoir states to aggregate for the training, we kept them fixed, and per- formed the optimisation of the hyperparameters. Figure 9 presents the classification errors with optimal training and reservoir settings. For completeness, we also evaluated the scenario with one column, and obtained a 11.7% error with the 17-th column. The best performance of 4.42% was produced with the combination of 4 reservoir states. Compared to the results obtained with raw images, discussed in Secs. IV-A1 and IV-B, this method yields lower errors and thus presents the best approach on raw images among those considered in this study. Nevertheless, the 4.42% classification error is no match for the HOG technique (see Sec. IV-A4) or the stateof-the-art.\n\nMentions:\n- Figure 9 presents the classification errors with optimal training and reservoir settings.\n\nOCR Text: \"0.1? U Ua MUg UW MZ Nuuber ol aggiegatcd RC statcs \"sed Lo1 Uuiliile\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 9. Numerical results obtained with column-wise input data encoding and aggregation of several reservoir states for the training process, with n = 1024. In particular, training with n corresponds to optimising the readouts weights on the basis of the sole reservoir state, produced after the processing of the 17th column of the image. Conversely, training with 4n consists of aggregating four reservoir states, obtained by processing the columns 14, 16, 20, and 24 of the image, for the training process."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To better understand the results on multi-talker speech recognition, we computed the WER separately for the speech mixed with same and opposite genders. The results are shown in Table VI. It is observed that the same-gender mixed speech is much more difficult to recognize than the opposite-gender mixed speech, and the gap is even larger when the energy ratio of the two speakers is closer to 1. It is also observed that the mixed speech of two male speakers is hard to recognize than that of two female speakers. These results suggest that effective exploitation of gender information may help to further improve the multi-talker speech recognition system. We will explore this in our future work. To further understand our model, we examined the recognition results with and without using the direct PIT-CE-ASR. An example of these results on a 0db two-talker mixed speech utterance is shown in Figure 4 (using the single-speaker baseline system) and 5 (with direct PIT-CE-ASR). It is clearly seen that the results are erroneous when the single-speaker baseline system is used to recognize the two-talker mixed speech. In contrast, much more words are recognized correctly with the proposed direct PIT-CE-ASR model.\n\nMentions:\n- An example of these results on a 0db two-talker mixed speech utterance is shown in Figure 4 (using the single-speaker baseline system) and 5 (with direct PIT-CE-ASR).\n\nOCR Text: \"\"Jalzrw AWli;wH Huuf  MMWha W K' UHup Vp8 6Mwh HMMASU= VMM\" W M\"HAUK \"alzr r\" LasH V\" \"YAFK Mokhuc) MUVYA W\" xon\"k, M | AL Tns \"YX\"TMYMNMY VZXwWK\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4: Decoding results of baseline single speaker BLSTM-RNN system on 0db two-talker mixed speech sample"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- • For IF, we note that since C 9, j Nj = 10− j Nj 10 and we require C 9, j Nj ≥ 2/3 we get j N j ≤ 3. Hence IF uses a (10, 7, 9) F streaming code. In Figure 9 we plot the frame loss ratio for state-dependent symbol-wise DF, Message-wise DF and for IF. We further plot the upper bound for State-dependent symbol-wise DF derived in Section IV with R = 2/3, T ≤ 9 and N 1 = N 2 = N 3 = 2 while assuming ∀j : α j = α.\n\nMentions:\n- In Figure 9 we plot the frame loss ratio for state-dependent symbol-wise DF, Message-wise DF and for IF.\n\nOCR Text: \"v#_t6d \"Dtm- ~t kAG 4 R704>\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 9: Four-node (two relays) network loss probability for state-dependent symbol-wise DF, message-wise DF and IF with T ≤ 9, rate 2/3 and largest N1 +N2 +N3 where α denotes the erasure probability (same over all hops)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The best fit for the distance modulus of SNs is obtained adopting the Levenberg-Marquardt method (subroutine MRQMIN in [14]). The statistical parameters here adopted are the merit function or chi-square, χ 2 , the reduced chi-square, χ 2 red and the maximum probability of obtaining a better fitting, Q, see Section 2.3 in [15] for more details. Table 1 reports H 0 and Ω M for the two catalogs of SNs and Figures 1 and 2 display the best fits. .\n\nMentions:\n- Table 1 reports H 0 and Ω M for the two catalogs of SNs and Figures 1 and 2 display the best fits. .\n\nOCR Text: \"} Recshit\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1. Hubble diagram for the Union 2.1 compilation. The solid line represents the best fit for the exact distance modulus in flat cosmology as represented by Eq. (17), parameters as in first line of Table 1."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where VMO is an acronym for Visual Motion Observer. This is why the estimation mechanism is called networked visual motion observer. The block diagram of the total system of vision camera i is illustrated in Fig. 8.\n\nMentions:\n- The block diagram of the total system of vision camera i is illustrated in Fig. 8.\n\nOCR Text: \"(wiscn RRBM Cathi Imiqe Jobim Esinled] Viis\" Cunere APPRI Kcd ization Vicual Molicn Ots;fef Netwvrkad Visua| Mction Otserwer (icr wision canerai} VMcti\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 8. Networked Visual Motion Observer"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Within this setup, we de ne a standard genetic algorithm using a tness function that simply returns the aforementioned bonus for each individual. It performs well but seems to su er from premature convergence in this setup (see Figure 2 for all plots). is is the baseline approach all diversity-enabled versions of the genetic algorithm can be tested against.\n- For evaluation purposes, we provided a domain-speci c distance function. In this simple scenario, this can be de ned quickly as well and we chose to use the sum of all di erences between actions at the same position in the sequence. Figure 2 shows that this approach takes a bit longer to learn but can then evade local optima be er, showing a curve that we would expect from a more diverse genetic algorithm.\n- Finally, we implemented both genealogical distance metrics presented in this paper. We can see in Figure 2 that both approaches in fact perform comparably, even though trash bit vectors require much less computational e ort. For this experiment, we used τ = 32.\n\nMentions:\n- It performs well but seems to su er from premature convergence in this setup (see Figure 2 for all plots). is is the baseline approach all diversity-enabled versions of the genetic algorithm can be tested against.\n- Figure 2 shows that this approach takes a bit longer to learn but can then evade local optima be er, showing a curve that we would expect from a more diverse genetic algorithm.\n- We can see in Figure 2 that both approaches in fact perform comparably, even though trash bit vectors require much less computational e ort.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: Average tness achieved over time by the the nondiverse genetic algorithm (black), the domain-speci c diverse genetic algorithm (blue), the genealogically diverse algorithm based on the genealogical tree distance (green) and the genealogically diverse genetic algorithm using the trash bit distance (red). To mitigate random e ects a bit, the tness values have been averaged over 10 complete evolution runs."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In this section we describe AIPNet in details. Our approach consists of two stages: pre-training and fine-tuning. In the pre-training stage, the model is built through adversarial training with the goal of learning accent-invariant representations. In the fine-tuning stage, we stack the pre-trained model with downstream tasks for further arXiv:1911.11935v1 [cs.CL] 27 Nov 2019 optimization. In this work, we use end-to-end ASR as a downstream application, focusing on improving accent robustness for speech recognition. The framework of AIPNet is illustrated in Fig. 1.\n- In the fine-tuning stage, the outputs of GAI which encode accentinvariant linguistic content can be plugged in as inputs of any downstream speech tasks that aim to improve accent robustness, as shown in Fig. 1. In this work, we focus on multi-accent speech recognition and adopt Listen, attend and spell (LAS), a popular attention-based encoder-decoder model [22] for sequence-to-sequence speech recognition. LAS consists of an encoder encoding an input sequence into high-level representations as well as an attention-based decoder generating a sequence of labels from the encoded representations. The encoder is typically a unidirectional or bidirectional LSTM and the decoder is a unidirectional LSTM. The label inventory for LAS modeling consists of 200 word pieces and is further augmented with two special symbols &lt;sos&gt; and &lt;eos&gt; indicating the start of a sentence and the end of a sentence respectively. LAS models the posterior probability of a label sequence y given the input feature sequence GAI (X) and the previous label history y1:j−1:\n- (b) GAI embedding from F2. proach in spirits, aiming to learn accent-invariant features through gradient reversal [16]. The gradient reversal approach keeps modules of GAI , DAI and ASR model in Fig. 1. Instead of using iterative training in Section 2.1.4, we add a gradient reversal layer between GAI and DAI to reverse the backpropagated gradient for GAI training. For more details about B4, we refer readers to [16].\n\nMentions:\n- The framework of AIPNet is illustrated in Fig. 1.\n- In the fine-tuning stage, the outputs of GAI which encode accentinvariant linguistic content can be plugged in as inputs of any downstream speech tasks that aim to improve accent robustness, as shown in Fig. 1.\n- The gradient reversal approach keeps modules of GAI , DAI and ASR model in Fig. 1.\n\nOCR Text: \"ine-tunin Piebaiite slllib Anart Krent AS? Modes Dirjie: Discrmnabcr D4 Dsxriinaicr D ,; Anont-|\"Waitrt Acen-sreclhc Cenerato\" ( Wet Generalcs 6 5\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1: The framework of AIPNet including both pre-training and fine-tuning stages."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where coefficient ϕ (0) is to be adjusted from the boundary conditions between epochs 7 . The dynamics of ϕ for BD = 0 is actually mild in all epochs since BD on the r.h.s. of Eq. (3.26) is small. However mild it might be, the dynamics of ϕ modifies both the friction and Poisson terms in Eq. (3.17), and it is therefore of pivotal importance to understand what are the changes that are induced by positive and negative values of BD on these terms during the relevant epochs of the structure formation history. An exact (numerical) solution is displayed in Fig. 4, where we can see that ϕ remains within the approximate interval 0.918 ϕ 0.932 for the entire cosmic history (starting from the RDE up to our time). This plot has been obtained from the overall numerical fit performed to the observational data used in this analysis within one of the BD-ΛCDM baseline datasets considered (cf. Sec. 8). The error band around the main curve includes the 1σ-error  3 for the values of the other parameters); Right plot: The same, but for the last term of Eq. (3.17), T 2 (Poisson term). We can clearly appreciate that negatives values of BD produce a higher Hubble friction, i.e. a higher T 1 , and a lower T 2 , w.r.t. the BD = 0 case. Both things lead to a decrease of δm. The two plots have been obtained with our modified version of CLASS. See the main text for further details.\n- One can check that this equation holds after a straightforward calculation, which makes use of the three BD-field equations (2.6)-(2.8). Although at first sight the above conservation equation can be surprising actually it is not, since it is a direct consequence of the Bianchi identity. Let us now assume that the effective BD-fluid can be described by an equation of state like p ϕ = w ϕ ρ ϕ , so The contribution from those terms containing derivatives of the BD-field are subdominant for the whole cosmic history. We have verified this fact numerically, see Fig. 4. While the variation of ϕ We can see that the BD model mimics quintessence with a significance of more than 3σ; Right plot: It shows once more Fig. 4 in order to ease the comparison of the EoS evolution, which is associated to the evolution of the BD-field ϕ -cf. Eq.(4.12). The shadowed bands in these plots correspond to the 1σ regions.\n- involve differentiations with respect to the slowly varying field ϕ and as before they are negligible, in absolute value, as compared toḢ∆ϕ and H 2 ∆ϕ. The effective EoS (4.12) is a time-evolving quantity which mimics dynamical DE at low redshifts. At very high redshifts z 1, well beyond the DE dominated epoch, we can neglect Λ in the denominator of the EoS and the dominant term is 3H 2 ∆ϕ. Similarly, in the numerator the dominant term is always −2Ḣ∆ϕ. Therefore, at high redshifts the effective EoS (4.12) behaves as (4.11) since ∆ϕ cancels out: w eff (z) w ϕ (z) (z 1), which means that it just reproduces the standard EoS of the GR-ΛCDM. The exact EoS (4.12), however, must be computed numerically, and it is displayed in Fig. 8, together with the numerical plot of ϕ (which we have already shown in Fig. 4). We have used the Baseline+H 0 dataset defined in Sec. 7. For a semi-qualitative discussion of the combined EoS it will suffice an analytical approximation, as we did before with w ϕ . The most relevant part of w eff (z) as to the possibility of disentangling the dynamical DE effects triggered by the underlying BD model is near the present time (z &lt; 1). Thus, neglecting the contribution from the functions f 1,2 , but now keeping the Λ-term in the denominator of (4.12) we can use the Hubble function of the concordance model and we find the following result at linear order in ∆ϕ:\n\nMentions:\n- An exact (numerical) solution is displayed in Fig. 4, where we can see that ϕ remains within the approximate interval 0.918 ϕ 0.932 for the entire cosmic history (starting from the RDE up to our time).\n- We have verified this fact numerically, see Fig. 4.\n- While the variation of ϕ We can see that the BD model mimics quintessence with a significance of more than 3σ; Right plot: It shows once more Fig. 4 in order to ease the comparison of the EoS evolution, which is associated to the evolution of the BD-field ϕ -cf. Eq.(4.12).\n- The exact EoS (4.12), however, must be computed numerically, and it is displayed in Fig. 8, together with the numerical plot of ϕ (which we have already shown in Fig. 4).\n- Our analysis of the BD theory with a CC, taken at face value, suggests that the reason for the enhancement of H 0 in the BD model is because the effective gravitational coupling acting at cosmological scales, G eff ∼ G N /ϕ, is higher than the one measured on Earth (see Fig. 4).\n\nOCR Text: \"0,98 Baselde /lc 0.96h 0.86F Q,O1 100\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4: Exact numerical analysis of the evolution of ϕ as a function of the redshift across the entire cosmic history, starting from the radiation-dominated epoch up to our time. We use here the values of the BD-ΛCDM Baseline+H0 dataset indicated in the figure itself (cf. Secs. 7 and 8). In particular, BD = −0.00199+0.00142−0.00147. The band around the central (dotted) curve shows the computed 1σ uncertainty from the Markov chains of our statistical analysis."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The general n-loop amplitude n i=1 Z(β i ) conn has been computed in [54] and the result for the two-loop correlator reads Here we have set β 1 = β 2 = β for simplicity. In Fig. 5 we show the plot of the two- We notice that t plateau depends on β. This is in contrast to the situation in the Gaussian matrix model before taking the double scaling limit where t plateau is independent of β [57].\n\nMentions:\n- In Fig. 5 we show the plot of the two- We notice that t plateau depends on β.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5. Plot of two-loop correlator in the Airy case for ~ = 1/10. The blue curve represents the disconnected part 〈Z(β)2〉dis while the orange curve represents the connected part 〈Z(β)2〉conn."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In this section, we discuss extensions of standard DDFT. By an \"extension of standard DDFT\", we denote any form of DDFT that allows to model aspects not covered by the theories presented in Section 3 (deterministic and stochastic DDFT for simple and colloidal fluids and polymer DDFT). Such extensions exist in a large variety of forms. An overview is given in Table 1. As visualized in Fig. 4 on page 40, these extensions can be distinguished by the way in which they modify the standard DDFT that is obtained by deriving an approximate transport equation for the one-body density ρ( r, t) based on the microscopic dynamics of simple or colloidal fluids. Some extensions modify the microscopic dynamics, e.g., by replacing the Langevin equations for passive by those for active Brownian particles (see Section 4.6.3) or by considering quantum rather than classical systems (see Section 4.11). Other extensions reduce the number of approximations in the derivation of the transport equation for ρ( r, t), e.g., by not making an adiabatic approximation (see Section 5.2) or by not using an approximate grand-canonical free energy functional (see Section 4.9). This type of extensions also involves formally exact approaches, such as power functional theory (PFT) [26], which are discussed in Section 5.\n\nMentions:\n- As visualized in Fig. 4 on page 40, these extensions can be distinguished by the way in which they modify the standard DDFT that is obtained by deriving an approximate transport equation for the one-body density ρ( r, t) based on the microscopic dynamics of simple or colloidal fluids.\n\nOCR Text: \"~- FiI spgcicinltcn; Tmr- : Hctc-nian Amem [ il \"JIT. JFMFM Lo 5 Muttcr 1j?W 401B 0H#10 7eee Aeendo Ri ToEET ti 7 eE[ Jiti 4etn tclrr= Tn [3114 TCCI Mntitn\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4. Classification of extensions of standard DDFT."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- V. NUMERICAL RESULT For a general G-CIFC (with a = 0) it is challenging to analytically show that the outer bound region in Th.1 meets the general inner bound region in [2, Sec. IV] due to the numerous parameters involved in determining the points on the convex closure of the inner and outer bounds. In Fig. 3 we show the result of the numerical optimization of the outer bound region in Th.1 and of the general inner bound region in [2, Sec. IV] for a = 0.01, |b| = 10, P 1 = P 2 = 5. We see that the inner and outer bounds coincides up to Matlab numerical precision. Although this does not constitute a formal proof of capacity, it shows that our outer bound region in Th.1 could be capacity for a more general class of C-IFC than that identified by Th.3.\n\nMentions:\n- In Fig. 3 we show the result of the numerical optimization of the outer bound region in Th.1 and of the general inner bound region in [2, Sec. IV] for a = 0.01, |b| = 10, P 1 = P 2 = 5.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3. The outer bound in Th.1 and the general achievable region of [2, Sec. IV] for the G-CIFC with a = 0.01, |b| = 10, P1 = P2 = 5. Notice the the vertical axis starts at 1.5."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In order to make evolution of our SMFs more apparent, we show our maximum likelihood Schechter fits (continuous lines) and binned space densities (data points) in Figures 23 to 25 with all redshift bins on a single plot.\n\nMentions:\n- In order to make evolution of our SMFs more apparent, we show our maximum likelihood Schechter fits (continuous lines) and binned space densities (data points) in Figures 23 to 25 with all redshift bins on a single plot.\n\nOCR Text: \"all LJ< C\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 23.— Evolution of the SMFs for all galaxies based on K-band M/L ratios, showing all redshift bins in one panel. Maximum likelihood fits to the (unbinned) data are shown by the continuous curves. The circles denote comoving space densities for the various mass bins. Filled circles denote the mass range used to perform the maximum likelihood fits. Open circles denote data for very low mass galaxies which are expected to be reliable on the basis of apparent I and [3.6µm] magnitudes, but which are not represented adequately by a Schechter function. The error bars show 1 −σ Poisson errors for the numbers in each bin."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Les quelques irrégularités observées sur les courbes des figure 41 et 43 semblent confirmer l'hypothèse avancée lors de l'étude de la courbure du pli : la ligne brisée considérée doit «sauter» d'un filament à l'autre, deux filaments côte à côte n'ayant pas précisément la même courbure. Filament plié À t = 2 (i.e. sur le filament plié T 2 (x 1 ), . . . T 2 (x 1000 )), les deux plus grandes valeurs propres de la différentielle de T 2 sont représentées figure 43. Elle est légèrement supérieure à 2 au voisinage de T 2 (x 690 ), mais il n'y pas d'expansion tout au long du filament. Celle-ci est en effet de module inférieur à 1 entre T 2 (x 1 ) et T 2 (x 444 ) où elle traverse le cercle unité, puis croît lorsqu'on se déplace le long du filament plié vers x 1000 où elle dépasse 4. Il n'y a donc pas uniforme hyperbolicité sur l'attracteur.\n\nMentions:\n- T 2 (x 1000 )), les deux plus grandes valeurs propres de la différentielle de T 2 sont représentées figure 43.\n\nOCR Text: \"valeuspoeic Jlereliliell AMMIYHAM c M 'e TO) Pien #r2 iemi propre Cauxiem ? w:leur prcpre\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 43 – Différentielle de T 2 sur le filament plié : premières valeurs propres."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 11, 12 show projection of beam and the boundary ellipses are calculated by Equation (7). Figure 12 is for x direction, and Figure11 is for y direction. From Figure 11, 12, one can found that the Footprints of the beam in x direction and in y direction are surrounded by the boundary ellipses perfectly. It verifies that the functions of the boundary ellipses obtained from Equation (2) are correct, and the coupled beam's boundary in phase plane can be expressed by the Riken's lattice function successfully.\n\nMentions:\n- Figure 12 is for x direction, and Figure11 is for y direction.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 12. Projection of 1000 particles for 2000 turns in x direction. The yellow ellipse is the matched emittance cycle for initial beam. Red points are the initial beam. The green ellipse is the boundary cycle."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We focus on WASP-17b (0.49M J , T eq = 1770 K, Anderson et al. 2010Anderson et al. , 2011, one of the lowest mass hot jupiters that is also heavily irradiated. These two factors should combine to produce the most pronounced reinflation. 6 Since this is a non-equilibrium phenomenon, we employ the MESA package (Paxton et al. 2011), which has the option of a user-supplied routine for additional heating sources. The heating source we adopt has an energy generation rate per unit mass that scales quadratically with pressure above z wind , dropping by a factor of (R/z wind ) 2 at z wind , and scales as p −1.3 below the wind zone until p = 10 10 dyne/ cm 2 , below which the high conductivity leads to a diminishingly small heat-Fig. 6.-The re-inflation of WASP-17b (the data point at the top right) by Ohmic heating ( = 3%) and stellar insolation (Teq = 1770 K). Different tracks of radius evolution (solid curves with points) correspond to initial models that have cooled in isolation for 10 6 , 10 7 , 10 8 and 10 9 yrs, respectively (from top to bottom). Once the core entropy has been reduced to a low level, heating in the atmosphere only has a limited effect on the radius. The dotted curves show the same evolutionary models with stellar irradiation only.\n- ing that scales as p −4 . Such a profile (the integrated heating is shown in Fig. 3) closely approximates the actual Ohmic heating profile we calculate for WASP-17b and agrees with our analytical model in Appendix A. The results on radius expansion are shown in Fig. 6 where planet models are exposed to both Ohmic heating (3%) and surface irradiation (with T = T eq ), after having cooled in isolation for a period ranging from 10 6 to 10 9 yrs.\n- The MESA calculations show that re-inflation is ineffective: the planet can at best be re-inflated by ∼ 10% if it has already cooled (Fig. 6). In Fig. 7, we illustrate in detail why this is so. Ohmic heating is intrinsically shallow and most of the energy is deposited at p &lt; 10 bar = 10 7 dyn/cm 2 , whereupon most of it is quickly transported outward by the newly generated surface convection zone. The interior of the planet experiences the heating only when a temperature inversion is built up and heat diffuses gradually inward. 7 By the time heat has diffused downward to p = 10 11 dyne/ cm 2 , the temperature inversion is minimal and the radiative diffusion has nearly ground to a halt. As a result, only layers shallower than this experience appreciable heating. Since the pressure scale height is ∼ c 2 s /g ∼ 4%R J (T /7000 K) 1/2 , the planet can at most expand a few times this value. This result has significance for the survival of hot jupiters. -Evolution of the temperature structure in a WASP-17b like planet, under Ohmic heating (3%) and stellar irradiation, after the planet has cooled in isolation for 1 Gyrs (the lowest trajectory in Fig. 6). The outer most point lies at an optical depth of 100. The individual curves are snapshots taken at different epochs after the heating has commenced. The planet is eventually mildly reinflated to 1.25R J after 5 Gyrs. With time, a thicker and thicker surface layer of the planet is lifted from the original adiabat to a higher adiabat. Ohmic heating below p ≈ 10 7 dyne/cm 2 is typically so small that it does not contribute much to the temperature rise there. Instead, heat diffuses downward gradually with the help of a temperature inversion. Red color indicates convective regions and green radiative regions.The wind zone is convective.\n\nMentions:\n- The heating source we adopt has an energy generation rate per unit mass that scales quadratically with pressure above z wind , dropping by a factor of (R/z wind ) 2 at z wind , and scales as p −1.3 below the wind zone until p = 10 10 dyne/ cm 2 , below which the high conductivity leads to a diminishingly small heat-Fig. 6.-The re-inflation of WASP-17b (the data point at the top right) by Ohmic heating ( = 3%) and stellar insolation (Teq = 1770 K).\n- The results on radius expansion are shown in Fig. 6 where planet models are exposed to both Ohmic heating (3%) and surface irradiation (with T = T eq ), after having cooled in isolation for a period ranging from 10 6 to 10 9 yrs.\n- The MESA calculations show that re-inflation is ineffective: the planet can at best be re-inflated by ∼ 10% if it has already cooled (Fig. 6).\n- -Evolution of the temperature structure in a WASP-17b like planet, under Ohmic heating (3%) and stellar irradiation, after the planet has cooled in isolation for 1 Gyrs (the lowest trajectory in Fig. 6).\n- We illustrate this using WASP-17b, a low mass planet that has one of the highest irradiation fluxes, yet still cannot be significantly inflated after cooling (Fig. 6).\n\nOCR Text: \"2 \" - (r)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 6.— The re-inflation of WASP-17b (the data point at the top right) by Ohmic heating ( = 3%) and stellar insolation (Teq = 1770 K). Different tracks of radius evolution (solid curves with points) correspond to initial models that have cooled in isolation for 106, 107, 108 and 109 yrs, respectively (from top to bottom). Once the core entropy has been reduced to a low level, heating in the atmosphere only has a limited effect on the radius. The dotted curves show the same evolutionary models with stellar irradiation only."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 3 summarizes radial distribution functions (RDFs) computed between oxygen atoms belonging to water and oxygen atoms belonging to the carboxyl group of the serine residue. The serine residue was chosen because it is polar and, therefore, its strong interaction with polar solvent is expected. Remember that both our cosolvents are polar. Indeed, the first peak is located at 0.28 nm and also the second peak is distinguishable (although small by height) at 0.48 nm. The height of the first peak is smaller in the case of pure water (system 7)\n\nMentions:\n- Figure 3 summarizes radial distribution functions (RDFs) computed between oxygen atoms belonging to water and oxygen atoms belonging to the carboxyl group of the serine residue.\n\nOCR Text: \"nS#IFAMIALAI SIrEn IEHAMMRJ\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3. Radial distribution functions (RDF) featuring spatial correlation between carboxyl oxygen atoms of the serine residue (mini-protein) and oxygen atoms (water) in systems 2, 4, and 6."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Table 7 and Figure 9 present the experimental results of our approach com-pared with baseline 1 (Pattern-based), and baseline 2 (Random classifier). For our approach, we consider the highest F-measure score to compare against the other two baselines. Our approach provides an improvement over the comment patterns, outperforming it by 1.53 times and 1.45 times for two-class and multiclass respectively. We can see from Table 7 that our approach outperforms the simple Random baseline by 1.84 times and by 22.14 times respectively for two-class and multiclass classifications.\n\nMentions:\n- Table 7 and Figure 9 present the experimental results of our approach com-pared with baseline 1 (Pattern-based), and baseline 2 (Random classifier).\n\nOCR Text: \"W hnd Ie; &= Egueach e2 :etinh; cWjeicz dCSEC 6FT the %Iif I sak;#th:-at JslIS;, Mi sioz| ; Pmrasue e V5'5 %In Sentiing SAR cummta 'an 3T4%\" imeup m\"nkok ] ,all8xet 7 \" stak ct Uhe /~ emunewhiss|. eId ; Pmrasur? ok 985 wh:n 'Semtifing Ih: OCIIc qui7 izooxmkt 'a 4[43' imTEHNI IU O 1 45 =;Md 2M4xmwe Ih: sal: a thv ; xliticly, ce awprcauh ilctilos Wot? [aL: 'erus that tmpzm HL thv list o/ Iukura_, iwilo; $7 SAF zl ts O4_ Apube|_\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 9: Visualization of the F-measure for Different Approaches."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- BChls is shown in Fig. 1.\n\nMentions:\n- BChls is shown in Fig. 1.\n\nOCR Text: \"753 S18 515 516 514 513 [ 4512 S10 S11\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1. (Color) The top view (from cytoplasmic to periplasmic side) of 2.0 Å resolution LH2 structure of Rhodopseudomonas acidophila. The 9 B800 and 18 B850 BChls are shown in green and cyan respectively, and the purple spheres represent Magnesium of the BChls. The 18 BChls are labelled clockwise from S1 to S18, and the transition dipole of QY excitation for each BChl is represented by red arrows."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- For example, Figure 4 of Mamajek &amp; Hillenbrand (2008) shows the color dependence of R ′ HK for different clusters. One can see for a given color of the same cluster, there are many outliers, i.e., two stars with the same age and color may have much different activities. It challenges the activity-age relation and is hard to explain by the same dynamo. This assumption also can explain the different activities of a binary (Donahue 1998;Mamajek &amp; Hillenbrand 2008).  2017) found over 600 flaring giants in quarter 15 of Kepler data, whose flaring incidence is similar to the main sequence star (see Table 3). However, as we mentioned above, most of them are pulsating stars. In our sample, 76 flaring stars are with logg &lt; 3.5, and 358 flaring stars are with logg &lt; 4. Although the incidence is much smaller than that of the main sequence star, some giants or subgiants show relatively high activities (see Figure 4).\n\nMentions:\n- For example, Figure 4 of Mamajek &amp; Hillenbrand (2008) shows the color dependence of R ′ HK for different clusters.\n- Although the incidence is much smaller than that of the main sequence star, some giants or subgiants show relatively high activities (see Figure 4).\n\nOCR Text: \"Zlare &lin & JCCCD BCD) EOjI 32C0 JCC) 4CC) Ec20 V\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4.— The flare activity across the H-R diagram. Black dots denote over 200,000 Kepler stars. Colorful dots are the 3420 flaring stars. The color changes from blue to red along with the increase of the flare activity. The luminosity of stars are estimated by temperatures and radii given by KIC. Note that the parameters of DR 25 are calibrated by the Dartmouth isochrones (Dotter et al. 2008; Huber et al. 2014; Mathur et al. 2017), so that one can see regular tracks on the H-R diagram. Some previous versions of KIC may adopt Yansei-Yale isochrones for the calibration."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The overview of the proposed model is shown in Figure 1. In our paper, the ASR is viewed as a noisy channel (with transfer function H), and we learn a model of this channel, H −1 (estimate of inverse transfer function H −1 ) by using the corrupted ASR outputs (equivalent to signal corrupted by H) and their reference transcripts. Later on, we use this model to correct the errors of the ASR.\n\nMentions:\n- The overview of the proposed model is shown in Figure 1.\n\nOCR Text: \"Wb #xats ToCG 0 Jk} LEI VI: Ma: EII] LE Abta Training\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1.: Overview of NCPCM"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- From Figure 2, Pairwise Scatterplots of the eight variables showed in each case the smooth relationship of the vertical variable with respect to the horizontal one, the lower triangle gave the correlation coefficients, with size of numbers proportional to their absolute value. Representation of ecological data set in a three dimensional space. The figure appear to be three groups, which we have represented using different colors. In Figure 3 we showed the relationship between the depth, pollution and temperature settings used to separate the class. Result from heat map and parallel coordinate in Figure 4 &amp; 5 respectively perform on the ecological data set. The two figures are not clearly separate the data set and so these figures are fail to identify the actual class sizes. Each panel of Figure 6 is a diagram for a variables whose identities are given by the corresponding scatter plots, densities, correlations and box plots. A classification data set involving three groups. Each group is shown using different colored symbol. The three groups are almost separated. In this setting, a classification approach is successfully identify the three classes. The classification tree that results from using decision tree methodology with the ecological data is shown in Figure 7 &amp; 8. The splitting rules are indicated in Figures. It is clear that the data set contain three classes. We see that the variables do a very good job of distinguishing the three classification. Moreover, error rate of random forest are shown in Figure 9 for the ecological data set. It is clean from the figure that the ecological variables separate the three groups quite well. Table 3 presents mean decrease Gini. Figure 10 &amp; 11 have a supervising characteristics. A two dimensional classification figures in which the tree decision boundary is linear. Here, the model is unable to separate the classification, although it is not clear where as a decision tree is successful. The network diagram for the 205 steps neural network. The input, hidden, and the output variables are represented by nodes, and the weight parameters are represented by links between the nodes. Which is notify in Figure 12. Table 4 shows the result of SVM classification, it was observed that the algorithm revealed three classes in the dataset Table 5 shows the LDA results. The separation of the three groups is fully exhibited in the two dimensional discriminant space. The scatter of the observations in the discriminant coordinate system are shown in Figure 13. The separation is quite good. The plot in Figure 14 indicates that LD1 and LD2 are the best choice for the dimension of the final configuration. A representation of the ecological data set followed by partition plot is perfectly classify that indicated the three classes. A K-near neighbors (KNN) algorithm applied to the ecological data gives the two dimensional representation shown in Figure 15 &amp; 16. Both figures representation are very similar. It is clear that the graphs are almost three classes. Using the K-NN algorithm, the software choose K values and the exhibited the training error and test error which are very close to each other. It is clear that misclassification error is very low here. A plot of error for the ecological variables is shown in Figure 17. A plot of prediction using logistic regression is shown in Figure 18. Here the separation on the basis of the prediction values is not good. Figure 19, which shows the three normal densities for the classification explains this phenomenon. Use of the midpoint among the three densities does not make the three misclassification probabilities equal. It is clearly penalizes the population with the three classification.\n\nMentions:\n- A plot of error for the ecological variables is shown in Figure 17.\n\nOCR Text: \"Traininn Cmzi ieem KVans\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 17. Training and test accuracy using KNN with different K’s"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Figure 5, it can be seen why the differences in the separation of involved groups are important from the point of view of mean tilt angles. The effect of the increase of mean separation on the mean tilt angle and the effect of various filters can be estimated based on tilt angle TSDDupo where the information on umbral polarity is taken into account in the calculation. It can be seen that the mean tilt angle is increasing with increasing separation that is in agreement with the result of Howard (1993). This means that the mean tilt angle depends on the frequencies of the sunspot groups with various separations. It can be also seen that the mean tilt angle increases when we increase the number of more unambiguous cases by using some filters for the difference between the tilt angles calculated with different methods. The figure shows that these filters also affect the mean tilt angle of the selected sample in a different extent.\n- Figure 5 may imply that the groups with separation larger than about 10 • have a special effect on the mean tilt angle thus the large complex groups or long trains of inseparable nearby groups may be also mentioned as possible reasons of the plateau-like deviation. To reveal any effects of them on the Joy's law, we plotted the medians in three separation ranges in Figure 11. This figure shows that the curve of groups with separation of SSDDupo &gt; 10 • explicitly deviates from the two other ones but the plateau is not their result because it can be see well in the ranges of smaller separation. The time series of this latter subset of data was divided into several subintervals of the solar cycle to investigate the temporal variation of the Joy's law. Figure 12 shows that the plateau is recognizable in each subinterval although the median values have quite large differences because of the decreased sample size. It seems that the plateau is narrower in the years around the solar maximum than in the years of lower activity.\n\nMentions:\n- In Figure 5, it can be seen why the differences in the separation of involved groups are important from the point of view of mean tilt angles.\n- Figure 5 may imply that the groups with separation larger than about 10 • have a special effect on the mean tilt angle thus the large complex groups or long trains of inseparable nearby groups may be also mentioned as possible reasons of the plateau-like deviation.\n\nOCR Text: \"vi ua:ut J: Jiijusililt Vaee Ao: Seja0atcn Ice 70.3b\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5. Mean of TSDDupo versus separation of sunspot groups SSDDupo calculated for separation bins of 3 ◦ by using various selection criteria."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 2 shows the comparison of the FERs of the equal FER rule, the outage rule, and the simulation-based best bit-channels rule. We observe that the FER of the equal FER rule is 1 dB worse than the other two rules and the FER of the best bit-channels rule is approximately the same as that of the outage rule. This shows that the simulation-based rule can predict the FERs correctly. For the rest of the paper, we use the simulation-based rule to determine the code rates.\n\nMentions:\n- Fig. 2 shows the comparison of the FERs of the equal FER rule, the outage rule, and the simulation-based best bit-channels rule.\n\nOCR Text: \"33 FERNL Qubns Uoame Me KN Iab]\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2: FER comparison of polar code level rate determining algorithms, all for the Alamouti code with 2 bpcu and Ntot = 1024 in a 2 × 2 antenna configuration."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In addition to the formalized model, the parser needs an input to parse. This input is an annotation of a subset of the units of the model. Units of this subset (they can be either pattern or alternatives) are said to be detectable. Their annotation can originate from manual annotation or third party detectors. These detectable units appear in red in figures 4 and 3. The parser is able to command the external detectors as it runs. In this mode, it does not receive the input annotation a priori, but works interactively with the detectors. This allows to inform the detectors of the context and therefore to reduce their search spaces. On the example, the parser asks to the \"Buoy-Marker\" detector \"is there something between 201 and 212?\". This allows to reduce the time interval the detector will process.\n- The internal representation of the model in the parser is an AND/OR graph. This representation is called the implicit graph. Our work extends the ideas of Mahanti (Mahanti et al., 2003) for the parsing. A unit identifying a pattern gives an AND node and one identifying an alternative gives an OR node. In the implicit graph, nodes represent patterns or alternatives but not instances. Figure 4 gives an example of an implicit graph for the example model. The implicit graph is used to generate an explicit graph. In this last graph, nodes represent instances.\n- The parser is currently top-down. It builds the solution graphs starting from a set of given roots. This set can be, for example, a set of pre-detected lexical unit occurrences resulting of a first pass of lexical recognition. It is how the parser process dependency-based models. It then builds trees top-down from each root and merges the trees when possible. It is therefore obvious than solution graphs can have multiple connected components. This occurs, for example, when a signer is interrupted by a question, answers quickly and then continues his/her speech. In the case of constituency-based models, the top-down parsing requires to introduce a detectable root. It is the function of the \"Signing\" unit in figure 4 which is detected with an activity detector.\n\nMentions:\n- These detectable units appear in red in figures 4 and 3.\n- Figure 4 gives an example of an implicit graph for the example model.\n- It is the function of the \"Signing\" unit in figure 4 which is detected with an activity detector.\n\nOCR Text: \"Cu% Yat cuxLep Eymn DMk U.d LSp\" WR Sign Iex KI uxshxn QKader nrozees | c\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4: Schematic view of the implicit graph associated to the example model"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The τ candidate decays at a flight length of 960 ± 30µm into one charged particle which interacts after crossing 22 plates and can thus be unambiguously identified as a hadron. The interaction of the daughter particle produces four charged particles and a photon. Figure 2 shows the display of the event as reconstructed in the brick.\n\nMentions:\n- Figure 2 shows the display of the event as reconstructed in the brick.\n\nOCR Text: \"Eacehk\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 2: Event display of the fifth ντ candidate event in the horizontal projection longitudinal to the neutrino direction. The primary and secondary vertices are indicated as V0 and V1, respectively. The black stubs represent the track segments as measured in the films."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Approximately 9% of all subjects posted a reciprocal grade to the avatars' profile pictures. The overall distribution of the reciprocally posted grades (the response grades) R out is shown in Figure 2. The distribution is sharply bimodal: the most popular ratings are \"5\" (\"strong like\") and \"1\" (\"strong dislike\"). Since the rating of \"5+\" has an associated monetary price and is not available to non-paying MOSN members, we treat it as indistinguishable from the vanilla \"5\" for the rest of the paper. The frequencies of the intermediate stops (\"2\", \"3\", and \"4\") and the differences between them are insignificant.\n\nMentions:\n- The overall distribution of the reciprocally posted grades (the response grades) R out is shown in Figure 2.\n\nOCR Text: \"Response\" Grade\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2. Overall distribution of reciprocally posted grades."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- This contribution presents a nonlinear Wiener model RLS type adaptive filter (IM2RLS) with exponential forgetting factor which is suitable for highly frequency selective duplexer stopband frequency responses like indicated in Fig. 2. It targets the digital IMD2 cancellation for high performance cellular base stations and mobile phones. The Wiener model uses a static nonlinearity at the output of the adaptive filter which has the advantage that less coefficients are needed in the estimation process compared to a Volterra kernel based adaptive filter [13].\n- Assuming an IIP2 of 60 dBm, the resulting BB IMD2 power with a full allocated LTE10 QPSK modulated transmission and the determined correction factor of CF = 13.4 dB is [6]. In an LTE10 reference sensitivity case, the wanted signal power at the antenna can be as low as -97 dBm [18]. The thermal noise power within 10 MHz bandwidth is -104.5 dBm and the assumed receiver noise figure (NF) is 4.5 dB which results in a receiver noise floor at -100 dBm. After amplification with 20 dB LNA gain, the wanted signal power is -77 dBm and the noise floor at -80 dBm corresponding to an Rx SNR of 3 dB. The SNR drops from 3 dB to an SINR of 2.27 dB due to the IMD2 interference assuming an IIP2 of +60 dBm. In case of an reduced IIP2 of 55 dBm / 50 dBm, the SINR drops even further to 1 dB / -1.4 dB, respectively. Fig. 2\n- In the first case, the receiver and the IMD2 replica generation of the IM2RLS do not use a DC cancellation. In this hypothetical example it is assumed that the IMD2 interference is the only DC source. In the second case, the receiver uses a DC suppression, and the IM2RLS the DC-notch filter. Both cases are compared within an FDD scenario with full allocated LTE signals using 10 MHz bandwidth, QPSK modulation, short cyclic prefix, and an OSF of 2. The frequency-selective duplexer stop-band impulse response shown in Fig. 6 is used in (7) for the IMD2 interference generation. It is modeled with an FIR system which has 15 complex valued coefficients (on the native LTE10 sampling rate of 15.36 MHz) and a mean Tx-to-Rx isolation of 50 dB [1]. The resulting TxL signal has a strong frequency-selectivity like indicated in Fig. 2.\n\nMentions:\n- This contribution presents a nonlinear Wiener model RLS type adaptive filter (IM2RLS) with exponential forgetting factor which is suitable for highly frequency selective duplexer stopband frequency responses like indicated in Fig. 2.\n- Fig. 2\n- The resulting TxL signal has a strong frequency-selectivity like indicated in Fig. 2.\n\nOCR Text: \"2 \" [ IMHz\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2. Equivalent BB spectrum of the frequency-selective Tx leakage signal yTxLBB (the corresponding passband signal is located at fTx) and the total received signal yTotBB after amplification with 20 dB LNA gain. The wanted Rx signal with SNR = 3 dB, and the receiver noise floor after amplification with 20 dB LNA gain are at -77 dBm and -80 dBm =̂ -108.2 dBm/15 kHz respectively. The total received signal contains the DC-, and channel-select filtered IMD2 interference with PTx = 23 dBm at an assumed IIP2 of 50 dBm."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 4 shows another case with pure gas, this time for Q g &gt; 1, which is normally stable. Here the destabilizing effect of the dissipation term, δqS, can be seen. The solid blue curves are a sequence of increasing Q g = 1, 1.2, and 1.4, from left to right, with dissipation δ = 1. The dashed blue curve is the same as the Q g = 1 curve but with smaller dissipation, δ = 0.5, and so smaller growth rate. The solid red curve has the same Q g = 1 and δ = 1 as the top blue curve, but it has a larger scale height obtained by setting S = 0.4 instead of 0.2 in the other curves.\n- Figure 4 indicates that although the dissipative disk is unstable for some Q g &gt; 1, the growth decreases quickly as Q g increases. Eventually the disk becomes stable at large enough Q g if the scale height term is included. Recall in Section 2 we said that infinitesimally thin dissipative disks are unstable for any Q g as long as q &gt; Q g /2S. With finite thickness, there can be stability at high Q g . Setting s = 0 in equation ( 24) and using only the first term on the right, which is for gas only, we see that if 2qS/ Q g 1 + qĤ g &gt; 1, then that equation admits unstable solutions with s 2 &gt; 0. This condition rearranges to\n- Fig. 4.-Dispersion relations for pure gas disks are shown in cases formerly thought to be stable, Q g ≥ 1. The destabilizing effect of the dissipation term involving δ can be seen. Thickness effects are included, and the stability at low q is from the dilution of in-plane self-gravity by disk thickness. 5, and S = 0.5, and with dissipation parameter δ = 0.5 on the top and δ = 0 on the bottom. The middle panel shows a range of values for Q s and Q g at fixed S = 0.5 and δ = 0.5. The sequence of blue curves with decreasing height has Q s = 0.5, 1, 1.5, and 2 for Q g = 1, and the sequence of red curves with decreasing height has Q g = 0.5, 1, and 1.5 for Q s = 1. The right hand panel shows the effect of dissipation as a sequence of blue curves (δ = 0, 0.1, 0.3, 0.5, and 1 with S = 0.5 from bottom to top) and the effect of the velocity dispersion ratio as a sequence of red curves (S = 0.1, 0.2, 0.3, 0.5, and 1 with δ = 0.5 from right to left). On the right, all have Q s = Q g = 1. In the top four rows, blue curves use an expression for Q tot in which thickness effects are not considered, even though they are considered in the dispersion relation; red curves use expressions for Q tot that include thickness corrections, and are therefore self-consistent. The top four panels differ in their value of the dissipation parameter δ. The bottom row solves the dispersion relation for a disk with zero thickness, which makes the red curves the same as the blue curves. The top four rows show how increasing dissipation always lowers 1/Q tot at the threshold for instability, s peak = 0.\n\nMentions:\n- Figure 4 shows another case with pure gas, this time for Q g &gt; 1, which is normally stable.\n- Figure 4 indicates that although the dissipative disk is unstable for some Q g &gt; 1, the growth decreases quickly as Q g increases.\n- Fig. 4.-Dispersion relations for pure gas disks are shown in cases formerly thought to be stable, Q g ≥ 1.\n\nOCR Text: \"12,14, 0,.15 0.05 8 10 12 14 16 18 20 Dimensionless Wavenumber, 46-\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4.— Dispersion relations for pure gas disks are shown in cases formerly thought to be stable, Qg ≥ 1. The destabilizing effect of the dissipation term involving δ can be seen. Thickness effects are included, and the stability at low q is from the dilution of in-plane self-gravity by disk thickness."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We used Eq. (10) to calculate the fusion cross sections. In Fig. 3, we display the theoretical barrier heights V theor B (MeV) verses the experimental barrier heights V expt B (MeV) using all 16 different potentials. The experimental barrier heights V expt B are taken directly from the literature directly [19][20][21][22][23][24][25]. The limited numbers of reactions in certain cases are caused by the restrictions posed on different potentials [3,9,[14][15][16]. The lines are the fits over the points. These fitted equations distinguish tell the deviation from the experimental data. Very interestingly, we see that, all models can reproduce the experimental barrier heights for symmetric colliding nuclei within ±8% on average. We also notice that, on average, barriers formed using EDF Exact, EDF Par, Bass 77, Bass 80, Denisov DP, and the different versions by Winther and collaborators are close to the experimental data. On the other hand, barriers formed within Bass 73, Ngô 80 and Prox 77 potentials deviate by ± 7% from the experimental values. The revised versions by Bass improve the barrier heights drastically. Now Bass 77 and Bass 80 reproduce the experimental data within 1.5%. A newer version of the Bass potential (Bass 80) shows slight improvement over Bass 77. Strangely, the Ngô 80 version deviates more than 7%, whereas its first version, was able to reproduce the barrier heights within 2%. For Ngô 75, only two systems fall within its parametrization limits.\n\nMentions:\n- In Fig. 3, we display the theoretical barrier heights V theor B (MeV) verses the experimental barrier heights V expt B (MeV) using all 16 different potentials.\n\nOCR Text: \"/: 22 A:: \"ra\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 3: Comparison of theoretical fusion barrier heights V theorB (MeV) using different proximity potentials with experimental values V exptB (MeV) [19–25]. The solid lines represent the straight line least squares fit created over different points."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- During the short time lapse monitored with HST, numerical simulations predict a variation of the accretion rate by a factor of ∼ 5 (see Fig. 2) however, the mass flow proceeds along the CS spiral structures without significantly altering the overall molecular gas distribution. This is shown in Fig. 3, where the variation of the geometric cross section of the CS matter to Lyα photons within the disk cavity (inner 3A) is plotted. This cross section Ω is calculated as,\n\nMentions:\n- This is shown in Fig. 3, where the variation of the geometric cross section of the CS matter to Lyα photons within the disk cavity (inner 3A) is plotted.\n\nOCR Text: \"65 F'q\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3.— Variation of geometrical cross section of the CS matter, Ω, to Lyα photons, for η = 1, 0.5 and 0.1. The vertical solid lines mark the limits of the phase interval monitored with HST."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- When the full system is used-i.e. we seed the knowledge graph, pre-train QA, then train the source task game, then the target task game using the augmented reward function-we see a significant gain in performance, up to an 80% gain in terms of completion steps in some cases. The bottleneck at reward 7 is still difficult to pass, however, as seen in Fig. 6, in which we can see that the agent spends a relatively long time around this reward level unless the full transfer technique is used. We further see in Figures 5, 6 that transferring knowledge results in the agent learning this higher quality policy much faster. In fact, we note that training a full system is more efficient than just training the agent on a single task, i.e. training a QA system then a source task game for 50 episodes then transferring and training a seeded target task game for 50 episodes is more effective than just training the target task game by itself for even 150+ episodes.\n\nMentions:\n- The bottleneck at reward 7 is still difficult to pass, however, as seen in Fig. 6, in which we can see that the agent spends a relatively long time around this reward level unless the full transfer technique is used.\n\nOCR Text: \"Tm Um/ F6n-05\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6: Reward curve for select experiments in the horror domain."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Now we investigate the impact of different sizes of labeled data added per iteration. It was mentioned earlier that noise harms the benefits of labeled data. Here we use experiments in Figure 8 to illustrate how it happened. In the experiment shown in Figure 8, 315 labeled wafer defect maps and 2835 unlabeled wafer defect maps are used to implement semi-supervised classification. This experiment has not yet added self-supervised technology, but only selects  The accuracy of the initial model trained with the labeled data is only 75.3%, and we add unlabeled data according to different sizes. Given symbol α is the ratio that controls the size, and N is the labeled data amount. In each round, α × N unlabeled data is selected in each class to be labeled data. When the α is large (α = 1), the accuracy increases slightly when trained with some pseudo-label data, and then decreases with more pseudo-label data due to noise. When the alpha is small (α &lt;= 0.5), the growth of accuracy is more stable. After growing to a certain level, the accuracy still declines due to the increased noise. To summarize that small unlabeled data size can make the model grow more steadily, and we should use the validation set to select a batter model before the accuracy drops.\n\nMentions:\n- Here we use experiments in Figure 8 to illustrate how it happened.\n\nOCR Text: \"0.25 8 8 78 9 1 10 15 20 Adding label (%)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 8. Pseudo-representation labeling accuracy under different batch size α. We use 315 labeled wafer defect maps and 2835 unlabeled wafer defect maps to implement iterative pseudo labeling semi-supervised classification. When the α is too large (α = 1), the accuracy initially increases slightly, and then the accuracy decreases due to the noise. When the alpha is small (α <= 0.5), the growth of accuracy is more stable. After growing to a certain level, the accuracy still declines due to the increased noise."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- As before, F (σ) = F (σ; ∅) is a well-defined function. The result of calculating F (σ; Ξ), for Ξ := − −−− → X i : ρ i , is the following: If for some i, ρ i =: (Γ i ⊢ R i ) ≤ σ (i. e., σ = Γ ⊢ R i and Γ i ≤ Γ), then F (σ; Ξ) = X σ i , where i is taken to be the biggest such index; otherwise, the specification is found in Fig. 8 (and, for space reasons, in Appendix A.7), where Ξ ′ (occurring once in the first clause and twice in the last clause) is given resp. by:\n\nMentions:\n- The result of calculating F (σ; Ξ), for Ξ := − −−− → X i : ρ i , is the following: If for some i, ρ i =: (Γ i ⊢ R i ) ≤ σ (i. e., σ = Γ ⊢ R i and Γ i ≤ Γ), then F (σ; Ξ) = X σ i , where i is taken to be the biggest such index; otherwise, the specification is found in Fig. 8 (and, for space reasons, in Appendix A.7), where Ξ ′ (occurring once in the first clause and twice in the last clause) is given resp. by:\n\nOCR Text: \"TTsbka TH48 IHaw7 : gEMeW=lety l MHJ: _VEa lEk Wugi HRas THA = FW#hha: 4 Mw4*K2, SL, WEl Sa FTAMLET '#DAu\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 8: Some interesting cases of finitary representation in LJTR"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In the above W is the work-function (eV) and A FN 1.541434 µA eV V −2 and B FN 6.830890 eV −3/2 V nm −1 are the conventional Fowler-Nordheim constants [26][27][28][29][30] . The expression for G is obtained using the generalized cosine law 31,32 of field variation near the apex, a result found to be true in the present scenario as shown in Fig. 4.\n\nMentions:\n- The expression for G is obtained using the generalized cosine law 31,32 of field variation near the apex, a result found to be true in the present scenario as shown in Fig. 4.\n\nOCR Text: \"3 1,45 147 1,48 1,49 Kum\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 4. Variation of the electric field on the emitter surface around the apex follows the generalized cosine law"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Figure 4, we show the tip displacement of the same cantilever, load and material parameters from the previous Section as ν → 1/2. Notice the locking behavior when using full integration as well as one point integration with stabilization compared to one point integration of the volumetric term and one point integration with stabilization of the isochoric term. In our numerical test we were able to get stable solutions with ν reaching up to ν = 0.499.\n\nMentions:\n- In Figure 4, we show the tip displacement of the same cantilever, load and material parameters from the previous Section as ν → 1/2.\n\nOCR Text: \"FuJ Jilcglallljii J-Ful U-rNEM ~(.IS N-Sul: U-SNEAEUU 1-Su4 ~(IS 03 fs Pcico <\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4. Locking behavior with Full integration / one point integration with stabilization versus the locking free solution using reduced integration / one point integration of the isochoric term."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The toy project has a refactoring of the test code. One of the refactorings was improperly done: a fault was introduced in salaryManagerTest. Here we compute the three metrics that are generally used to evaluated test quality: percentage of passing tests, statement and branch coverage and mutation coverage. The results are presented in figure 5. Here we can see:\n\nMentions:\n- The results are presented in figure 5.\n\nOCR Text: \"Imjctt ((uubj DRvzh Ewnj: TPe TsFe rn #Rurinw Pes Heyut )\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: Percentage of passing tests, statement and branch coverage and mutation coverage for all classes of the toy project"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 5.3 shows us the structure of DatasetObject, with its most important functions and variables exposed. Dataset is initialized in Code 5.10 by reading a SegmentsData.dump file, however it stores data in its own four internal arrays. These are: list of urls to images, labels marking the scores, OSM vectors if we have loaded a marked Segments file and unique IDs of original segments. Note that we are not directly loading all of the images yet, rather we are keeping only their filenames. We can access the images when necessary, saving us from wasteful memory allocation.\n\nMentions:\n- Figure 5.3 shows us the structure of DatasetObject, with its most important functions and variables exposed.\n\nOCR Text: \"Latas lCbjeck #ihalzahon Latzset , rikjon secrents\" it |mm Isill shuue_Fy 5ez\"en6U ibabstGW Fistcgr:mu Fyetters Ky xaklyveJecet magsslpltl Icc Mans5 ^del Jes\" YvaU stal Je: OSM Wrh Ubes' U IcccsmMoj9 Ky O5M !Vi,Val, tsn yllejka1 dalslr' I6 723v Mo99 1~ I1n%5 / ~5075 MS1 - \"euhkoan: /no; :f - MOi\"sU\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5.3. Dataset object with important functions highlighted."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- This circuit, which senses the phase error as well as the threshold error, is used in a clock clock data recovery circuit as shown in Fig. 9. This circuit was simulated to verify TABLE I: Possible sequences and threshold locations for data with 1 bit ISI (refer Fig. 7) It can be shown that the same logic circuit recovers the optimum sampling threshold even for high bandwidth channels discussed in Case 1 in Section III-A. It may be noted that this analysis and the results can also be used to design low jitter clock recovery circuits for PAM4 signals.\n\nMentions:\n- This circuit, which senses the phase error as well as the threshold error, is used in a clock clock data recovery circuit as shown in Fig. 9.\n\nOCR Text: \"DN detector fron Fig: Data prlmp DA\" Clck VCO Churge Phas pwmp Churge\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 9: Clock data recovery circuit that recovers sampling clock as well as optimum sampling threshold."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The first case, for values of m 2 &gt; q 2 , presents two apparent horizons R + and R − and an initial singularity at S + . The corresponding causal diagram is shown in Fig. 12. The initial singularity at t = 0 is hidden to observers in the regular region (+, −) by the two apparent horizons, while observers in the anti-trapped region (+, +) are not. The second case is for m 2 &lt; q 2 and sufficiently far from the line σ = q 2 /m 2 = 1, the overcharged case, with one apparent horizon R − covering the singularity at R = 0. Notice that, unlike the ΛCDM cases, this case presents a lightlike future infinity. The corresponding causal diagram is shown in Fig. 13. The singularity is hidden to observers in the regular region (+, −) by the two appar-ent horizons.\n- The third case emerges when considering values q/m 1, a case with three apparent horizons at intermediate times. Given that this is a very peculiar case that, asymptotically, tends to the case with only one horizon shown in Fig. 13, we chose to not explore it further.\n\nMentions:\n- The corresponding causal diagram is shown in Fig. 13.\n- Given that this is a very peculiar case that, asymptotically, tends to the case with only one horizon shown in Fig. 13, we chose to not explore it further.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 13. Causal structure for SV spacetime in the case a(t) = t2/3, and with m2 < q2."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- It is important to note some details about these relations. Given a set of C composers as a time-sequence S, the average state at time i is defined. The opposite state is defined as the \"counterpoint\" of a musical state v i , considering its average state: everything running along the opposite direction of v i are understood as opposition. In other words, any displacement from v i along the direction r i is a contrary move, and any displacement from v i along the direction − r i is an emphasis move. Given a musical state v i and its opposite state r i , we can define the opposition vector D i . These details are better understood analyzing Figure 1.\n\nMentions:\n- These details are better understood analyzing Figure 1.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 1. Graphical representation of the measures derived from a musical move1."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- A comment is in order. In our calculations we will consider the Color Singlet Model at leading order (LO) as given above. As demonstrated in Refs. [37,38] (See also [39]) the calculation of photoproduction cross section to next-toleading order (NLO) proved that these corrections are large, increasing towards large transverse momentum of the J/Ψ meson. The corresponding NLO predictions describes the shape of the z and p T differential J/Ψ cross sections but the normalizations are a factor three below of data [30], with large uncertainties associated to the choice of the charm quark mass and the factorization and renormalization scales, which indicates that beyond NLO corrections should be included or that contributions of color octet states may be sizeable. In contrast, the total cross section can be described at LO by adjusting the charm quark mass and factorization and renormalization scales and introducing a multiplicative K -factor, which takes in account higher-order corrections. It is demonstrated in the Fig. 2 where we compare our predictions, obtained using Q 2 = p 2 T , m c = 1.5 GeV and two different parametrizations for the gluon distribution, with the experimental data from H1 Collaboration [30]. We have that, considering distinct values for the K -factor, both predictions reasonably describe the data. As the main input for the calculations of the inelastic J/Ψ photoproduction in coherent pp interactions is the energy dependence of the γ + p → J/Ψ + X cross section, we believe that the use of Color Singlet Model at leading order, with parameters constrained by the HERA data, can be considered a reasonable first approximation for the total cross section and rapidity distribution. Certainly this subject deserves more detailed studies in the future. In what follows we will assume that these same values of Q 2 and K-factor are valid for J/Ψ production in pP b and P bP b collisions and also for Υ production with m b = 4.5 GeV.\n- (2) and (3), have support at small values of ω, decreasing exponentially at large ω, the first term on the right-hand side of the Eq. (8) peaks at positive rapidities while the second term peaks at negative rapidities. Consequently, given the photon flux, the study of the rapidity distribution can be used to constrain the photoproduction cross section for a given energy. Moreover, in contrast to the total rapidity distributions for pp and P bP b collisions, which will be symmetric about midrapidity (Y = 0), dσ/dY will be asymmetric in pP b collisions due to the differences between the fluxes and process cross sections. Following Ref. [30] we will integrate the fraction of the photon energy carried away by the quarkonium in the range 0.3 &lt; ∼ z &lt; ∼ 0.9. Our predictions do not include resolved photoproduction, which contributes appreciably only at z &lt; ∼ 0.3 and diffractive production, which is confined to the quasielastic domain at z ≈ 1 and p T ≈ 0. These contributions are in general excluded from experimental measurements (See Ref. [30] and Fig. 2) in order to make a meaningful comparison. In order to estimate the dependence of our results on the inferior limit of integration z min , in Fig. 3 we present our predictions for the rapidity distribution for J/Ψ production in coherent pp collisions at √ s = 14 TeV obtained using the CTEQ6L parametrization and different values of z min . We have that our predictions increase by ≈ 23 % at midrapidity if we assume z min = 0. Similar behaviour is observed for other energies and for Υ production. In Fig. 4 we present our predictions for the inelastic J/Ψ and Υ photoproduction in coherent pp collisions at LHC energies. As expected, the total rapidity distributions are symmetric about the midrapidity. We calculate dσ/dY considering different parametrizations for the gluon distribution in the proton. It is important to emphasize that the rapidity distribution at LHC probes a large interval of photon-proton center of mass energy since W 2 γh ≃ M V √ s exp(±Y ), which corresponds to very small x ≃ M V e −|Y | / √ s. The MRSTLO and CTEQ6L predictions differ by ≈ 30 % at Y = 0, with the CTEQ6L one being a lower bound. In Table I we present our estimates for the total cross sections and production rates assuming the design luminosity L pp LHC = 10 7 mb −1 s −1 and a run time of 10 7 seconds. We predict large values for the events rate and cross sections of the order of units of nb, in contrast with values of the order of 100 µb for the inclusive quarkonium hadroproduction [27]. Despite their much smaller cross sections, the clean topology of coherent processes implies a larger signal to background ratio. Therefore, the experimental detection is in principle feasible. However, as already emphasized in the Section I the signal is expected to be reduced due to the event pileup and an alternative to measure coherent events at the LHC is by tagging the intact hadron in the final state.\n\nMentions:\n- It is demonstrated in the Fig. 2 where we compare our predictions, obtained using Q 2 = p 2 T , m c = 1.5 GeV and two different parametrizations for the gluon distribution, with the experimental data from H1 Collaboration [30].\n- These contributions are in general excluded from experimental measurements (See Ref. [30] and Fig. 2) in order to make a meaningful comparison.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 2: Energy dependence of the inelastic J/Ψ cross section considering two different parameterizations for the gluon distribution. Data from H1 Collaboration [30] for 0.3 < z < 0.9 and pT > 1 GeV."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Results are presented in Fig. 5. For the isolated teacher we see a rapid increase of the average energy around the origin. The same happens for solutions discovered by SA, which we can classify as isolated as well. Also, SA was the slowest algorithm to reach a solution (unsurprisingly, since it is known to scale badly with the system size [2,16,17]).\n- In Table I we report the probability of correctly classifying a new example generated by the teacher, usually called generalization accuracy, for each of the algorithms investigated. We note a clear correlation between the flatness of the basin as seen in Fig. 5 and the ability to classify correctly new examples, with SA having the worst performances and BP+R the best ones.\n\nMentions:\n- Results are presented in Fig. 5.\n- We note a clear correlation between the flatness of the basin as seen in Fig. 5 and the ability to classify correctly new examples, with SA having the worst performances and BP+R the best ones.\n\nOCR Text: \"reacher CP-S BP-R 0.02 0,04 0,06 0 06\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5. The energy of Eq. 45 as a function of the Hamming distance dN from the teacher and from solutions found by different algorithms. N = 1001 and α = 0.4 in all simulations. Curves are averaged over 40 samples."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Based on the technique for extracting equilibrium phonon numbers described above, the effect of cooling us- the characteristic initial increase in the cyclotron amplitude due to the presence of the axialization drive is observed. This effect is absent in figure (a) where no axialization is present. Common parameters: P 0 = 1 µW, δ = Γ/2, w 0 = 40 µm, y 0 = 40 µm, ν − = 25 kHz ing axialization without an offset beam was investigated. Figure 4 plots various cooling limits as a function of the axialization voltage for different beam waists. First, note that at V ax = 0 the magnetron motion is actively heated to very large orbits while the modified cyclotron motion remains relatively cold, consistent with the theoretical prediction that both modes cannot be cooled simultaneously. Furthermore, the larger scattering rate for smaller beam waists at the trap center leads to more efficient modified cyclotron cooling. As the axialization amplitude is increased, the magnetron mean phonon numbers start decreasing. The modified cyclotron mean phonon numbers initially go up, but with sufficient axialization the two values get closer to each other and approach a low total phonon number. It is clear that lower total mean phonon numbers can be obtained for larger beam waists, indicating that the broader beam waist contributes a larger net decrease in magnetron heating compared to the decrease in modified cyclotron cooling. Phonon numbers below 100 for both modes can be obtained by using around V ax &gt; 3 V for a beam waist of w 0 = 150 µm, which would be sufficient for commencing sideband cooling. The disadvantage of using voltages in this range is that driven micromotion can become significant if stray fields and potential differences between the axialization voltages on different electrodes are not carefully compensated. An alternative is to combine axialization with an offset Doppler cooling beam as shown in figure 5 for two different beam waists. In this case, the axialization voltages required to achieve similar mean phonon numbers is an order of magnitude lower. At around V ax = 0.7 V the effect of additional voltage starts to saturate and the final mean phonon numbers of both modes are almost equalized. The phonon numbers are within a factor of 3-4 larger than the modified cyclotron mean phonon number without axialization. Additionally, the benefits of a tighter beam waist are also evident at lower axialization voltages.\n- To further confirm that axialization can be used to effectively cool at large trapping frequencies where resolved sideband cooling can be effectively employed, a data set at ν − = 52 kHz and V ax = 1 V is shown in figure 7. At this frequency, cooling with just an offset beam is not possible. By fitting the full Rabi lineshape dynamics summed over a thermal distribution of phonons to 3 modified cyclotron and 4 magnetron sidebands, mean phonon numbers ofn + = 96(5) andn − = 136(8) were extracted. Once again, the mean phonon numbers approach equality, with the modified cyclotron being slightly lower, as predicted in figure 5, however they are approximately a factor of 3 larger than the simulated values. One contribution to the discrepancy is that heating due to spontaneous emission from the axial cooling laser was omitted from the simulation. Based on the analytical model of modified cyclotron Doppler cooling this effect could contribute up to 30% higher mean phonon numbers given a 1:1 scattering ratio of the radial and axial beams. The remaining discrepancy could be attributed to the uncertainty in setting the optimal laser parameters including detuning, beam waist and offset. Nonetheless, axialization allows us to cool the ion to low motional phonon numbers with high reliability and robustness against per-\n\nMentions:\n- An alternative is to combine axialization with an offset Doppler cooling beam as shown in figure 5 for two different beam waists.\n- Once again, the mean phonon numbers approach equality, with the modified cyclotron being slightly lower, as predicted in figure 5, however they are approximately a factor of 3 larger than the simulated values.\n\nOCR Text: \"HaFaHLNHHL HENuH U HUYHK JMUHIOUUHL EwH w\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 5: Average phonon numbers for the magnetron (circles) and modified cyclotron (diamonds) motions between 10–20 ms as a function of axialization voltage for different beam waist and offset combinations. The offset was chosen to minimize the mean phonon numbers given the beam waist and power based on additional simulations at Vax = 0.5V. Parameters: Trap potential 100 V, νz = 265 kHz, ν− = 52 kHz, ν+ = 677 kHz, P0 = 8 µW, δ = Γ/2."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We further investigate the contribution of each imaging map to the residual systematics in DR7. Fig. 24 presents the χ 2 vs each imaging map after the linear (grey), quadratic (purple), neural network with feature selection (red), and neural network plain (blue) treatments. The statistics before mitigation are not shown for clarity. We also plot the 50and 95-th percentiles of the same quantity observed in the null mocks in orange horizontal curves. Depth-g seems to be the main source of the residual systematics which is not mitigated even by the neural network-based methods. All methods also show residual systematics against skymag-g, skymag-z, and MJD-z. The standard treatments show some additional residual systematics against E(B-V), lnHI, and exptime-z. We perform further tests by masking out high extinction and low depth-g regions. We find that applying a more rigorous cut on depth-g (e.g., depth-g &gt; 24.95) yields a more cleaner mean density at the expense of losing 9% of the data. Although our objective was to avoid applying rigorous cuts on imaging and demonstrate the gain by using non-linear methods, our tests suggest that careful masking based on imaging, particularly depth-g, seems necessary for cosmological studies.\n\nMentions:\n- Fig. 24 presents the χ 2 vs each imaging map after the linear (grey), quadratic (purple), neural network with feature selection (red), and neural network plain (blue) treatments.\n\nOCR Text: \"Rczai 95: *ult NWale Muuta QLuIlam W\"JuhhiHu\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 24. The breakdown of χ2 values observed in the data on the mock footprint after linear (grey), quadratic (purple), neural network with feature selection (red), and neural network plain (blue) treatments. We also plot the 50- and 95-th percentiles of the same quantity observed in the null mocks in orange curves (note that χ2(s) is not a continuous quantity). Given the 5% threshold, we can argue that there exists known residual systematics against E(B-V), depth-g, skymag-g, skymag-z, and MJD-z."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Fig. 6 we compare the time dependencies of spin polarization (S(0, t)/S 0 ) for three different initial polarization configurations: the homogeneous spin polarization (initial spin polarization is selected as S x = S y = 0, S z = S 0 ), plain spin helix and radial spin helix. The selected period of radial and plane spin helices (k = k m ) corresponds to the longest spin lifetime of these structures at a fixed ηℓ = 0.1. It follows from Fig. 6 that the spin polarization in the radial spin helix is the most robust against the relaxation. We emphasize that the long-time behavior of all curves can be perfectly fitted by an exponential law given in the caption of Fig. 6. The numerically obtained increase in the spin lifetime of the radial spin helix 435/103 ∼ 4.2 is very close to the theoretically predicted value 32/7 ∼ 4.6. We also note that a slightly longer spin relaxation time increase (∼ 6) reported in Ref. 9 for the plain spin helix can be related to a large value of ηℓ = 0.3 that is beyond the linear spin drift-diffusion theory.\n\nMentions:\n- In Fig. 6 we compare the time dependencies of spin polarization (S(0, t)/S 0 ) for three different initial polarization configurations: the homogeneous spin polarization (initial spin polarization is selected as S x = S y = 0, S z = S 0 ), plain spin helix and radial spin helix.\n- It follows from Fig. 6 that the spin polarization in the radial spin helix is the most robust against the relaxation.\n- We emphasize that the long-time behavior of all curves can be perfectly fitted by an exponential law given in the caption of Fig. 6.\n\nOCR Text: \"3 3 Radlul sjin helia Pau Qn telx aemcbemeul Anarizalier 0 LWDD Tiine\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 6. (Color online) Spin polarization as a function of time for different initial spin polarization configurations. This plot was obtained using the parameters values ηℓ = 0.1 and a = 64.77ℓ (the period of helices). The straight lines are fitting curves selected as exp(−(t − t0)/t1). The parameters of the fitting curves are t0 = 0, t1 = 103τ for homogeneous spin polarization, t0 = 0τ , t1 = 435τ for plain spin helix, and t0 = 165τ , t1 = 430τ for radial spin helix."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The experimental results are given in Fig. 6, where y-axis shows the peak temperature and x-axis shows execution time. From the experimental results, we find:\n\nMentions:\n- The experimental results are given in Fig. 6, where y-axis shows the peak temperature and x-axis shows execution time.\n\nOCR Text: \"~l; M\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 6: Comparison between the four approaches within the same time interval"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 8. The cumulative percentage of female first authored Australian Scopus-indexed journal articles 1996-2018 in each top cited percentile (using NLCS values). For each dot on the graph, the x axis value is a cumulative percent of top-cited values and the y axis value is the female authored percentage. For example, point with x coordinate 10% has y coordinate 42.5%. This indicates that 42.5% of the 10% most cited articles have a female first-author, which is above average for Australia. The isolated dot on the right hand side with X coordinate 100% and y coordinate 40.7% corresponds to the female percentage for all Australian journal articles, which is 40.7% overall. The gap between this point and the rest of the graph corresponds to the percentage of uncited articles.\n\nMentions:\n- Figure 8.\n\nOCR Text: \"Kusira & Renentc(mztite: \"LS irda;\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 8. The cumulative percentage of female first authored Australian Scopus-indexed journal articles 1996-2018 in each top cited percentile (using NLCS values). For each dot on the graph, the x axis value is a cumulative percent of top-cited values and the y axis value is the female authored percentage. For example, point with x coordinate 10% has y coordinate 42.5%. This indicates that 42.5% of the 10% most cited articles have a female first-author, which is above average for Australia. The isolated dot on the right hand side with X coordinate 100% and y coordinate 40.7% corresponds to the female percentage for all Australian journal articles, which is 40.7% overall. The gap between this point and the rest of the graph corresponds to the percentage of uncited articles."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- use. Figures 13 represent some of the first modes that are derived with the POD procedure, while in Figure 14 we show the rate of decay of the eigenvalues for the velocity, pressure snapshot solutions and for the supremizer. Finally, we are interested in the behavior of the relative approximation error as a function of the number N of basis functions used at the reduced solution stage. We again let N vary in a discrete set N : for a fixed value of N ∈ N , and for each θ i , i = 1, . . . , N t , we compute both the reduced solution (u N (t i , θ i ), p N (t i , θ i )) and the corresponding high fidelity solution (u h (t i , θ i ), p h (t i , θ i )). We compute the L 2 relative error N,i u,t k for the velocity and the relative error N,i p,t k for the pressure at time t k ; by taking the time average of these relative errors we obtain the mean approximation error N,i u for u and N,i p for p, for each θ i ∈ P t . Figures 16 and 17 visualizes the approximation error for the pressure, at final time T , with supremizer enrichment as well as the velocity error; the error has been calculated in the L 2 norm. In Figure 15 the mean with respect time relative errors for various number of modes are visualized.\n\nMentions:\n- Figures 13 represent some of the first modes that are derived with the POD procedure, while in Figure 14 we show the rate of decay of the eigenvalues for the velocity, pressure snapshot solutions and for the supremizer.\n\nOCR Text: \"PUu M pod supiemDe Pod nmeel J0 \" _ 10-15 _\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 14. Evolutionary in time geometry: The POD eigenvalues decay for the fluid velocity u (black), the fluid pressure p (blue), and the fluid supremizer s (magenta), for a set of Ntrain = Nt = 400 (time instances) snapshots."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 5 displays the largest few eigenvalues for three different pairs of values for (B, l), as indicated. We immediately observe a rapid decrease of eigenvalues toward 0 with increasing index. These three pairs of values correspond to SBP, C, taking the values 0.157, 1.57, and 3.14. Note the several-orders-of-magnitude differences between successively smaller eigenvalues, those differences being more dramatic the smaller the value of C. The largest eigenvalue in each case is associated with an even eigenfunction.\n\nMentions:\n- Figure 5 displays the largest few eigenvalues for three different pairs of values for (B, l), as indicated.\n\nOCR Text: \"Alteraling Even-Ocd Eigenvalues 5=[4 S=@1 MeC 8aw\"\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 5. Plots of the largest few eigenvalues for three different values for the pair (B, l), namely (0.1, 0.5), (0.1, 5.0), and (0.2, 5.0), as indicated on the figure. The dotted lines connecting the successive discrete eigenvalues are provided merely to guide the eye."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We now turn to the study of intersectional fairness estimation on the real datasets. We first investigated the stability of the estimation of the fairness metrics versus data sparsity, by estimating from bootstrap samples of the datasets, varying the number of samples (Figure 2). For each number of data instances, we generated 10 bootstrap datasets and reported the average -DF and γ-SF for each model on the Adult and COMPAS datasets.\n- Our proposed HLR-FB model showed consistently stable behavior in all experiments, even with a very small number of instances, producing estimates of and γ which were similar to the final predictions of all models. The variance in the estimates of fairness was substantial for several models, but averaging over bootstrap samples mitigated this to some degree. To illustrate this, we randomly pick a bootstrap data sample at each data instance, instead of averaging over bootstrap samples (Figure 5). In this figure, we still average over 10 bootstrap samples for the Bayesian Ensemble method, as in Figure 2, as a reference to compare to the other models. Although all other models degrade somewhat in terms of estimation stability in this setting due to variance in their estimation, HLR-FB still enjoys relatively consistent and stable performance for both the -DF and γ-SF metrics.\n\nMentions:\n- We first investigated the stability of the estimation of the fairness metrics versus data sparsity, by estimating from bootstrap samples of the datasets, varying the number of samples (Figure 2).\n- In this figure, we still average over 10 bootstrap samples for the Bayesian Ensemble method, as in Figure 2, as a reference to compare to the other models.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: -DF (top) and γ-SF (bottom) measurement of algorithm M(x) for (a) logistic regression on the Adult dataset and (b) the COMPAS algorithm, using different PM,θ(y|s, θ) models, versus number of instances, averaged over 10 bootstrap samples. The dotted blue line indicates the Bayesian ensemble approach."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The functionP (S) is the market clearing price 6 defined as follows:P (S) = max{ k j=1 b i j , v i k+1 }. Mechanism MVCG(N ), which is defined in Figure 4, achieves a constant approximation to the optimal liquid welfare if there is a single dominant agent, and Mechanism RS Liquid achieves a constant approximation in all other cases. The main result of this section is: Theorem 7. Mechanism Liquid Div is a truthful mechanism for the allocation of a divisible item that gives a constant approximation to the optimal liquid welfare.\n- Figure 4: Online modified VCG mechanism.\n\nMentions:\n- Mechanism MVCG(N ), which is defined in Figure 4, achieves a constant approximation to the optimal liquid welfare if there is a single dominant agent, and Mechanism RS Liquid achieves a constant approximation in all other cases.\n- Figure 4: Online modified VCG mechanism.\n\nOCR Text: \"HCG' Ea?reszo rens an: |es:: ( Exgda\" aueia dlje\" , 95=i_ enp e a\"ellneclH an, SUre\" < n retg Bhxaanwongb hrqudsuesmua 2+24 0 004=' 3' Tcsafit_ lurshed he | >in'2w; a:heeri: enbaenartie Thernv\"el en\"ftolefhor: %3w\" |ii 2 p iskiad ahzwrerheep |m 'We\"n\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4: Online modified VCG mechanism."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The results for the Coupling between Objects are observed in table 36 and figure 39.\n\nMentions:\n- The results for the Coupling between Objects are observed in table 36 and figure 39.\n\nOCR Text: \"Depth of Inheritence 22 2,14 Luz Depth of Inheritence MARF GIPSY\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 39: Graphical Comparison of Depth of Inheritance for the Case Studies"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Inserting the experimental parameters from Ref. 1 into the theoretical prediction of the previous section gives the result shown in Fig. 3. As can be seen from this figure, this theory predicts a capacitance that is larger than the experimentally observed value by as much as 20 times. The experimental results also do not exhibit the sharp upturn in capacitance at small electron density that is characteristic of the theory in the previous section. In order to resolve this discrepancy, we consider the effects of the confining potential produced by the electrodes. In the experiment, a potential difference V g ≈ 0.5 − 1 V is applied between the gate electrode and the CNT in order to bring the Fermi level close to the bottom of the conduction band. This potential produces significant electric fields, as depicted in Fig. 4, which imply that the electric potential along the CNT is non-uniform.\n\nMentions:\n- Inserting the experimental parameters from Ref. 1 into the theoretical prediction of the previous section gives the result shown in Fig. 3.\n\nOCR Text: \"108| 0 ) 102 10- (m-1)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 3. Prediction for the capacitance C as a function of the electron density n using the experimental parameters of Ref. 1. The solid line is the prediction of the theory for a uniform gas (see Sec. II). The dashed line shows the actual capacitance Cm = 45.7 nm measured experimentally."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 4 illustrates the frequency of each user being scheduled in a relatively short period of 2000 slots. Each of the 20 user is selected as the destination for roughly 100 slots. That is, the scheme is fair to all users even within a small application time window. Fig. 5 presents the probability that k users have been probed until transmission. The theoretical results are from Lemma 1. The figure shows that both the Algorithm 1 and Algorithm 2 coincide with the theoretical results. We observe from the figure that the probability decreases sharply as the probing step approaches J max . Fig. 6 plots the scheduling gain of the proposed algorithms versus the number of users in the system. The simulation runs for 20,000 slots. In fact the simulation result matches the analytical result of Theorem 4 quite well. Also, we note the scheduling gain remains about the same for more than 9 users. Because at this time, the cost of user probing is dominant and the scheme always tries to carry out the user probing till the end.\n\nMentions:\n- Fig. 5 presents the probability that k users have been probed until transmission.\n\nOCR Text: \"stpt 0 Jd;e Ter atMuunauie- Mein [c:fFJdrjueu -lzin Cee=M4=eeate @n\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 5: The probability thatk users have been probed until transmission.K = 20, β = 0.1."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- For each of the EOSs within the EOS band, we construct a new EOS that includes a phase transition to SPM. We sketch our construction in Fig. 2. To obtain the EOS for SPM, we use three different calculations: AFDMC calculations using local chiral interactions to next-to-next-to-leading order (N 2 LO), many-body perturbation theory (MBPT) calculations from Krüger et al. (2015) based on chiral EFT interactions to next-to-next-to-next-to-leading order (N 3 LO), as well as Brueckner-Hartree-Fock (BHF) calculations (Vidana et al. 2002) based on the phenomenological Reid 93 and Nijmegen (Stoks et al. 1994) potentials. We show the results for SPM for these different calculations in Fig. 3.\n- As shown in Fig. 2 for given neutron-star and SPM EOSs, unpolarized matter is energetically favorable at low energy densities but becomes less favorable than spin-polarized matter at higher densities for sufficiently stiff EOSs. The reason is that interactions in SPM tend to be weak and results are close to the free Fermi gas (Krüger et al. 2015), while interactions in unpolarized matter are much stronger and become increasingly repulsive. We then identify the phase transition between unpolarized matter and SPM by a Maxwell construction, i.e., by matching pressure and Gibbs energy or chemical potential, and construct a new EOS using the unpolarized EOS below and the polarized EOS above the phase transition (blue dashed line in Fig. 2).\n- To explore the sensitivity to the exact construction of the phase transition, we have explored additional EOSs where the phase transition is smeared out, similar to a Gibbs construction. Such a transition would appear due to the formation of a mixed phase if protons and electrons were included (Glendenning 1992;Heiselberg et al. 1993). Instead of enforcing P polarized tr = P unpolarized tr = P pt , and connecting the EOSs by a segment with c s = 0, we construct EOSs with the unpolarized phase up to P pt −∆P and the polarized phase after P pt +∆P . These EOS segments are then connected by a smooth interpolation. The resulting M max as a function of ∆P is shown in the inset of Fig. 2 for the given EOS. Smearing out the phase transition lowers M max because the EOS gets softened earlier. The maximum M max is therefore found for a strong first-order phase transition resulting from the Maxwell construction. This is in good agreement with similar findings for phase transitions to quark matter of Bhattacharyya et al. (2010) and Wu &amp; Shen (2019) and holds for all EOSs in our sample. Therefore, our conclusions are robust with respect to the properties of the transition.\n\nMentions:\n- We sketch our construction in Fig. 2.\n- As shown in Fig. 2 for given neutron-star and SPM EOSs, unpolarized matter is energetically favorable at low energy densities but becomes less favorable than spin-polarized matter at higher densities for sufficiently stiff EOSs.\n- We then identify the phase transition between unpolarized matter and SPM by a Maxwell construction, i.e., by matching pressure and Gibbs energy or chemical potential, and construct a new EOS using the unpolarized EOS below and the polarized EOS above the phase transition (blue dashed line in Fig. 2).\n- The resulting M max as a function of ∆P is shown in the inset of Fig. 2 for the given EOS.\n\nOCR Text: \"J id ~oddu %\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2. Example EOS for unpolarized neutron-star matter (red solid line) and SPM (red dashed-dotted line), and the EOS that results from a strong first-order phase transition between the two phases in the Maxwell construction (blue dashed line with ∆P = 0), as well as an EOS that results when smearing out the phase transition as in a Gibbs construction (blue dotted lines with finite ∆P ). Inset: Mmax for one representative EOS with phase transition to SPM as a function of ∆P ."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 3 shows the progression of normalised wave frequency for electrostatic weakly collisional TEM mode as a function of b for different lower hybrid amplitude U/c s =1, and 3, and collisionality parameter L n ν the /v thi =0.01. The longer wave length drift waves are stabilized by the lower hybrid pump wave, while the shorter wavelength get destabilized (cf. Fig. 4)\n\nMentions:\n- The longer wave length drift waves are stabilized by the lower hybrid pump wave, while the shorter wavelength get destabilized (cf. Fig. 4)\n\nOCR Text: \"7707# F 6 : W\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 4. (Color online) Variation of normalised growth rate for weakly collisional TEM as a function of b for two different lower hybrid amplitude U/cs=1, and 3. Others parameters are ηi=5, R/Ln = 1.8, collisionality parameter Lnνthe/vthi = 0.01"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We define the typing judgment Γ | Σ e : T by the rules depicted in figure 5. The rules are mostly straightforward.\n\nMentions:\n- We define the typing judgment Γ | Σ e : T by the rules depicted in figure 5.\n\nOCR Text: \"\" ~Ta ;AAcu-*i Mt= = Ut= [ ag ` Welt 73>e 48v/ : 4[ 36: :0 81 4: Ein \":%\"a: 1 ' H 190r I3  ~ Te: 00 i=\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5. Typing rules of CPL."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- With correct choice of α the Ewald method exhibits O(N 3/2 ) computational cost. Figure 1 confirms this by plotting the time per iteration for a NaCl salt simulation against particle count N at a fixed density of 1 atom per (2.5Å) 3 . We include repulsive Lennard-Jones interactions to prevent the particle distribution from collapsing. However, for sizable particle counts the dominant computational cost are electrostatic forces: for N = 1.8 • 10 5 particles 87% of the time is spent computing Coulombic interactions. For all tests in this paper we set the error tolerance to 10 −6 and vary the parameters α and r c (which balance the work between the real-and Fourier-space) to minimise the runtime. For our framework the pair (α, r c ) takes values between (0.062, 13.5Å) for N = 1728 and (0.013, 29.2Å) for N = 1.8 • 10 5 . For DL POLY 4 [5] we choose a cutoff value of r c = 10Å. All runs are carried out on the \"Balena\" cluster; one node consists of two Intel E5-2650v2 8-core CPUs. Both implementations show better than expected scaling with N . For small particle numbers the SPME method used by DL POLY 4 is in the same ballpark as our implementation. The SPME method obviously outperforms our method for larger particle counts where it is an order of magnitude faster.\n\nMentions:\n- Figure 1 confirms this by plotting the time per iteration for a NaCl salt simulation against particle count N at a fixed density of 1 atom per (2.5Å) 3 .\n\nOCR Text: \"Frallevuuzk DL POLY | N\" 1 10' 10' 10' 8.0 * 22 12 \" Particlc coung (N)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Time per iteration against particle count for an NaCl system on a single 8 core CPU using OpenMP (our framework) or pure MPI (DL POLY 4)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Theoretical calculations have shown that netproton fluctuations reflect the singularity of the charge and baryon number susceptibility as expected at the critical point [61]. Figure 6 shows the typical uncorrected net-proton (∆N p ) distributions in Au + Au collisions for 0-5% centrality measured on event-by-event basis. The protons and antiprotons are measured within p T = 0.4 to 0.8 GeV/c and |y| &lt; 0.5. At lower energies, the net-proton distributions are dominated by the contributions from the proton distributions. The mean value of the distribution increases with decrease in collision energy. Different moments which describe the shape of the distribution are extracted from the ∆N p distribution.\n\nMentions:\n- Figure 6 shows the typical uncorrected net-proton (∆N p ) distributions in Au + Au collisions for 0-5% centrality measured on event-by-event basis.\n\nOCR Text: \"WAiIsice\"c ALtAL Cemta Ccllizicn Gev 11.5 Gev 39 Gev 373? (a 1 Net-proton bos\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 6: Uncorrected ∆Np multiplicity distributions measured on an event-by-event basis in Au + Au collisions at various √ s NN for 0–5% collision centrality measured by STAR experiment."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To check the long time behavior of the online spurious oscillations measured by var(t), a comparison between SD-ROM with and without online stabilized post-processing is performed in the time range [8π, 16π], which is four times wider with respect to the time window used for the generation of the POD basis. The corresponding results are displayed in figure 8, where we evaluate the measure var(t) for under-and overshoots at r = 30 both for SD-ROM and SD-ROM post-proc. in [8π, 16π], and compare it with the FOM one in the snapshots time range [8π, 10π]. Note that for r = 30 more than 99.99% of the system's kinetic energy is captured in this case. Both SD-ROM gives here almost similar and reliable results for long time integration, being SD-ROM post-proc. slightly superior to SD-ROM, and seems to rightly follow the trend initially given by the FOM by approaching values close to 1.1.\n\nMentions:\n- The corresponding results are displayed in figure 8, where we evaluate the measure var(t) for under-and overshoots at r = 30 both for SD-ROM and SD-ROM post-proc. in [8π, 16π], and compare it with the FOM one in the snapshots time range [8π, 10π].\n\nOCR Text: \"Hdhelhwhnb\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 8: Example 4.1.2: Long time behavior of measure var(t) for under- and overshoots for different ROM at r = 30."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Thus, finally, the total time taken to complete replication is In Fig. 4, we show the variation of replication time τ with the rate of transcription termination (i.e., exit rate β q of the P particles), for a constant transcription initiation rate (entry rate of P particles) α q . Note that the dependence of τ on α q arises in (18) from the use of the result ρ(β) = 1 − β for the HD phase of P particles where the relation between β and β q is given by (3). With the increase of β q , τ decreases, and eventually saturates, above a critical value of β q . This behavior is qualitatively reproduced by the heuristic analytical arguments which slightly overestimate the mean replication time. The steady state density profiles of the P particle are plotted in the inset of Fig. 4 for few different values of β q . The trend of variation of the profiles with β q is consistent with the transition from the HD phase to MC phase of the TASEP of the P particles. For all those values of β q , for which system is in HD phase, particle density ρ decreases with increase of β q . Therefore, the total number of encounters that a R particle can have inside segment 2 also decreases, which results the decrease in τ . Above a critical value of β q , the TASEP in the segment 2 makes a transition to the MC phase where the number density ρ of the P particles and, hence, τ , becomes independent of β q .\n\nMentions:\n- Thus, finally, the total time taken to complete replication is In Fig. 4, we show the variation of replication time τ with the rate of transcription termination (i.e., exit rate β q of the P particles), for a constant transcription initiation rate (entry rate of P particles) α q .\n- The steady state density profiles of the P particle are plotted in the inset of Fig. 4 for few different values of β q .\n\nOCR Text: \"Heette 7F Leltl Ae1a 4720 Io0 Pogijion 81 Etsh\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 4. The replication time τ is plotted as a function of βq. Our theoretical predictions, based on heuristic analytical arguments, are drawn by continuous curves and numerical data obtained from our MC-simulations are shown by discrete points. In inset, we plot the density profile of P particles along the lattice, for three different values of βq. The density profile data have been obtained only from MC simulations. The other relevant model parameters are αq = 100s −1, q = 30s−1, pco = pcontra = 30s −1."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We describe characteristics of the data to summarize it and illustrate the challenges with modeling it. The counts of events were clearly non-normal with overdispersion (Figure 2).\n- Figure 2 Figure 2 shows the histogram of cyber events by week. The x-axis is bins of cyber event counts, and the y-axis depicts the number of weeks in each bin. For example, there were 0-10 cyber events in about 90 of the 366 weeks. The tick marks on the xaxis depict the density of cyber events in each bin and their specific values within the bin; these were randomly jittered by 0.3 to create unique values.\n- To quantify the overdispersion shown in Figure 2\n\nMentions:\n- The counts of events were clearly non-normal with overdispersion (Figure 2).\n- Figure 2 Figure 2 shows the histogram of cyber events by week.\n- To quantify the overdispersion shown in Figure 2\n- The presence of bursts is strongly suggested by the overdispersion in the data (Figure 2 and the index of dispersion).\n\nOCR Text: \"Uedec\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2 Figure 2 shows the histogram of cyber events by week. The x-axis is bins of cyber event counts, and the y-axis depicts the number of weeks in each bin. For example, there were 0-10 cyber events in about 90 of the 366 weeks. The tick marks on the xaxis depict the density of cyber events in each bin and their specific values within the bin; these were randomly jittered by 0.3 to create unique values."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- 1 obtained from the 1 2 − resonance state when used in the capture cross section Eq. (18) gives negligible contribution to 14 C(n, γ) 15 C away from the resonance. Near the resonance it produces a sharp peak as we show later in Fig. 3. We determine the scaling of the remaining two p-wave EFT couplings by analyzing available 14 C(n, γ) 15 C data in the following.\n- 1 ∼ Q. Then the s-wave effective range ρ correction is a NLO effect, and the 2 P 3/2 interactions are LO. We present only the LO result where the effective range ρ contribution is neglected. In the 2 P 1/2 channel, the LO cross section is determined by the 15 C ground state binding momentum γ, and the 1 2 − state resonance energy E r and width Γ r . In Fig. 3, we compare the contribution from Fig. 2 (a) to that from Fig. 2 (b), (c). The dashed curve shows the non-resonant contribution in the 2 P 1/2 channel and the solid curve shows the 1 2 − resonant contribution (in the same 2 P 1/2 channel). As expected the resonant contribution is large near the resonance energy E r ≈ 1.885 MeV, and comparatively negligible elsewhere. More importantly we notice that the non-resonant contribution is non-negligible throughout the energy region. This implies that the interference between the resonant and non-resonant contribution in the total cross section is significant as we see later.\n- MeV differs from a simple Breit-Wigner form. This is a result of the significant interference between the non-resonant and resonant contribution in the 2 P 1/2 channel alluded to earlier in discussing Fig. 3.\n\nMentions:\n- Near the resonance it produces a sharp peak as we show later in Fig. 3.\n- In Fig. 3, we compare the contribution from Fig. 2 (a) to that from Fig. 2 (b), (c).\n- This is a result of the significant interference between the non-resonant and resonant contribution in the 2 P 1/2 channel alluded to earlier in discussing Fig. 3.\n- (1) 1 is negligible at low-energy, Fig. 3.\n\nOCR Text: \"0.5 1,0 1,5 2.0 Ecm (MeV)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 3. Resonant and non-resonant contribution to E1 capture cross section σ(Ecm) in the 2P1/2 channel. Solid (blue) curve is the resonant contribution, and dashed (red) curve is the non-resonant contribution."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- As mentioned above, one of the effects of nonzero eccentricity is a significant reduction in the merger timescale. It is illustrated in Fig. 11. Comparing with Fig. 5, we see that the initial eccentricity e 0 = 0.6 can decrease the merger timescales by one or two orders of magnitude. However, we find that &gt; 99% of all the detected BIMBHs (at any µ) are at the point where the eccentricity was reduced below 0.05. So, the initial eccentricity of ∼ 0.6 mostly affects the merger time but not the GW spectrum.\n\nMentions:\n- It is illustrated in Fig. 11.\n\nOCR Text: \"4,0M1 0.c 0.01 4,03 #o\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 11.— BIMBH coalescence timescales given initial eccentricity 0.6. Line styles are as in Fig. 6 (top panel)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where a class belongs Fig. 1. «framework», «application» and «utility» Tags The «application» tag marks application-specific classes. During the product line adaptation process, this tag may mark newly introduced classes as well. The «framework» tag marks classes and interfaces belonging to the product line. A third category of classes belongs to the utility level, as these classes are provided as basic classes by utility libraries or the runtime system. These may be tagged by «utility». Figure 1 exemplifies their usage. If a package is marked with one of these tags, then all of its classes and interfaces are implicitly marked as such. Table 1 summarizes the meaning of these UML-F tags.\n\nMentions:\n- where a class belongs Fig. 1. «framework», «application» and «utility» Tags The «application» tag marks application-specific classes.\n- Figure 1 exemplifies their usage.\n\nOCR Text: \"elrukVd: Kere $ clax Kelcngs Kajuht M; STrl: Apllam Iphu M Canpah FilOupu Shuau Nr\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1. «framework», «application» and «utility» Tags"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- due to the fact that the number of independent parameters is very much reduced in this model, and the cut on µ V V implies a cut on α. To show this, we plot in Fig. 3, µ ZZ against BR(H i → Inv). The color code is as in Fig. 2. On the left panel we see that the invisible branching ratio of the 125 GeV Higgs boson, H 2 in our model, could be as large as one but this is ruled out by LEP. Furthermore the LHC limit on µ ZZ , reduces the allowed space, and we obtain an upper bound on the invisible decay, for this simple model, of around 20% as we explained before. The corresponding plot for the lightest Higgs boson is shown on the right panel. We see that an invisible branching ratio of 100% is compatible with the LHC results for this model. The correlation between the invisible branching ratios of the two Higgs bosons is shown on the left panel of Fig. 4 with the same convention for the colors. Finally, on the right panel we plot m H 1 as function of BR(H 1 → Inv), with the same conventions. We see a strong anti-correlation among these panels, due to the simplicity of the model. In order to better illustrate this anti-correlation we plot in Fig. 5, µ ZZ as a function of µ γγ . The straight line reflects the fact the there is essentially only one parameter left, the angle α, after we fix the two Higgs boson masses. We also notice that in the model, the µ f for the channels where the final state f exists in the SM can only be less then one. This results from the reduced coupling of the SM-like Higgs boson.\n\nMentions:\n- In order to better illustrate this anti-correlation we plot in Fig. 5, µ ZZ as a function of µ γγ .\n\nOCR Text: \"\"1F10Gal\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: Correlation between µZZ and µγγ. The color code is as in Fig. 2."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In contrast to 7-scenes, where each scene is densely covered with reference images, we evaluate here the case where only four reference images per indoor scene are given. Each of them is placed in one corner of the room, oriented towards the center. We evaluate this scenario on two datasets: One is based on replica [38], where we manually placed the four reference cameras in every room and sample 500 query poses the same way as for the sparse training set. Using a second dataset, we show that this also works on images recorded from a sensor of our robotic system. Therefore, an Intel Realsense d435 sensor is used to record multiple trajectories in our kitchen setup. The ground truth poses are determined by applying ORBSLAM2 [39] to the recorded trajectories. Again, we extract four images this time from the recorded trajectories, one per corner facing towards the center, and these are used as reference images to determine the pose of a given query image. The results in table III show that image retrieval performs worst, which is reasonable due to the low number of reference images. Using Sift+5pt in the same way as with the 7-Scenes dataset leads to a big number of cases where no prediction is possible at all. Especially if the baseline between a given query reference pair is very large, the number of successful feature matches is too low to estimate the relative pose via the 5-point algorithm. By increasing Lowe's ratio, we can increase the number of feature matches we mark as correct, but at the same time also the accuracy of the predicted poses decreases due to the lower quality of feature matches. Compared to all tested classical and learning-based methods, our approach performs best. This shows that in the case of only a few reference images and therefore large baselines between query and reference images, learning-based approaches can outperform classical non-learning based approaches in scene-agnostic pose estimation. As can be seen in Fig. 5, this stays true if further additional reference images are given. Starting from 64 additional reference images the ExReNet trained on dense training data used in the last section can be used to achieve the best accuracy.\n\nMentions:\n- As can be seen in Fig. 5, this stays true if further additional reference images are given.\n\nOCR Text: \"SUFTHSpk FxReNe|pzrel ExkeN: dense|^ 01q 1 1 1 32 M I8 256 S12\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 5. This plot shows how the translational error in meters on our recorded dataset changes depending on the number of images that are used as reference images additional to the four ones used in table III. Both ExReNets are evaluated using scale and unc. information."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The proposed method is four-fold: adaptive scanning points segmentation, T-linkage-based outlier removal and Lshape points clustering, rectangle fitting using different criteria, and cost function-based best fit selection. The pipeline is summarized in Fig. 1.\n\nMentions:\n- The pipeline is summarized in Fig. 1.\n\nOCR Text: \"Dnp Mputl H1ouicins 5onizntco Chcicntnz Naicntnz] Vnirc: cieton T-huta: @xiy} Cahrton Bal lned 'cirikx\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1: The pipeline of the proposed L-shape fitting algorithm."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The derived total emission model for Comet IZ is compared with the average background-corrected observational Chandra spectrum in Figure 1. CX emission clearly dominates below 1 keV, and the resulting SW composition indicates high-speed SW. This composition indicates the presence of solar flare activity during the observations, which agrees with the GOES observations. In regards to scattering emissions, both the total intensity and spectral shape agree well with the observational data over the 1.0-1.2 keV energy range with an α = 2.5. Above 1.2 keV, the scattering model begins to diverge from the observed spectrum, albeit at a slower rate than that of the CX model.\n\nMentions:\n- The derived total emission model for Comet IZ is compared with the average background-corrected observational Chandra spectrum in Figure 1.\n\nOCR Text: \"rda UIGHMEi?\" CXWccel Scmhn 4 MAle;5,AWKW+?'AW Scmeri-g blal (0 -2.5,solt ~Ijej ctnut\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1.— Comparison of the modeled spectral intensity contributions from CX and dust/ice particle scattering to the Chandra observation of Comet Ikeya–Zhang. The modeled scattering emission includes dust contribution from all grain radii. The scattering model is calculated for both the average solar and solar flare spectrum, with the solar flare spectrum producing an excellent agreement to the observation at energies greater than 1 keV."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We now consider prescient versus conventional block-diagonalization schemes for the multiantenna UCR downlink with r u = 2. In Fig. 4 the underlay sum rate is displayed as a function of the UCT transmit power for t u = 8, K u = 4, K = r I = 2. The greatest benefit of the PBD schemes is observed at low to intermediate SNRs, while the sum rate of all three algorithms gradually converge at high SNR. This is because the diversion of transmit power to the ICRs under PBD now has a greater penalty in terms of spatial multiplexing loss to the multi-antenna UCRs.\n\nMentions:\n- In Fig. 4 the underlay sum rate is displayed as a function of the UCT transmit power for t u = 8, K u = 4, K = r I = 2.\n\nOCR Text: \"PUdm PDScP Lelmaim Jar P Idz\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4. Underlay sum rate for prescient and conventional block-diagonalization withtu = 8, Ku = 4,K = rI = ru = 2."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- For a length-lŶŶ-run, its possible parent run/runs are categorized into the following cases, as shown in Fig. 7 (in all cases we give examples corresponding to the length-lŶŶ-run being 00000):\n- Ŷ -run 00000 length lŶ = 5 single parent run sub-parent run 2t + 1 parent runs no edit one insertion one deletion 00000 00 ↓0 00 0000 00 lX = lŶ = 5 lX = lŶ − 1 = 4 lX = lŶ + 1 = 6 00000 ↓1 00 lX = 7 &gt; lŶ 00 100 20 lX = 2 &lt; lŶ X-run length t ≥ 1 Fig. 7: Given aŶ-run (00000) with length lŶ, its parent run may be a single run, a sub-run, or several runs. Because there can be no more than one edit in an extended run in the typicalized edit process, we can explicitly find the forms of the edits in different cases. If the parent run is a single run with length lX, there may be no edit (lX = lŶ); one insertion (lX = lŶ − 1); or one deletion (lX = lŶ + 1). If the parent run is a sub-run with length lX, there must be one and only one insertion in the parent run, which breaks the parent run into two runs with length lŶ and lX − lŶ. In this case, lX &gt; lŶ. If the parent runs are several runs where the length of the first run is lX, there must be 2t + 1 parent runs (t ≥ 1), where the odd-number runs are runs with symbols the same as theŶ-run, and the even-number runs are lenth-1 runs of symbols different from theŶ-run. In this case, lX &lt; lŶ.\n\nMentions:\n- For a length-lŶŶ-run, its possible parent run/runs are categorized into the following cases, as shown in Fig. 7 (in all cases we give examples corresponding to the length-lŶŶ-run being 00000):\n- Ŷ -run 00000 length lŶ = 5 single parent run sub-parent run 2t + 1 parent runs no edit one insertion one deletion 00000 00 ↓0 00 0000 00 lX = lŶ = 5 lX = lŶ − 1 = 4 lX = lŶ + 1 = 6 00000 ↓1 00 lX = 7 &gt; lŶ 00 100 20 lX = 2 &lt; lŶ X-run length t ≥ 1 Fig. 7: Given aŶ-run (00000) with length lŶ, its parent run may be a single run, a sub-run, or several runs.\n\nOCR Text: \"Fyug CU\" Luwth 14\"? [I = l utri Zui j x els Eikud Fir UWu: Eli Qiric\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 7: Given aŶ-run (00000) with length l Ŷ , its parent run may be a single run, a sub-run, or several runs. Because there can be no more than one edit in an extended run in the typicalized edit process, we can explicitly find the forms of the edits in different cases. If the parent run is a single run with lengthlX̄, there may be no edit (lX̄ = lŶ); one insertion (lX̄ = lŶ − 1); or one deletion (lX̄ = lŶ + 1). If the parent run is a sub-run with lengthlX̄, there must be one and only one insertion in the parent run, which breaks the parent run into two runs with lengthl Ŷ and lX̄ − lŶ. In this case,lX̄ > lŶ. If the parent runs are several runs where the length of the first run is lX̄, there must be2t + 1 parent runs (t ≥ 1), where the odd-number runs are runs with symbols the same asthe Ŷ-run, and the even-number runs are lenth-1 runs of symbols different from thêY-run. In this case,lX̄ < lŶ."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The calculations for the density matrix are done by scaling both interactions with the same factor k. The results for the largest and the second largest eigenvalues of the density matrix, corresponding to the exact ground state, are shown in Fig. 6. One observes that for J = 0 the largest eigenvalue slowly increases up to around k = 0.8 and then, for k &gt; 0.9, it decreases towards the non-interacting value. This decrease is not seen in the case of pure isovector interaction shown in Fig. 5a, which means that this is an effect caused by the isoscalar pairing interaction . On the other hand, as seen in Fig. 6, the largest eigenvalue for J = 1 has a shape rather similar to the case of the pure isoscalar interaction. At a closer inspection one can notice, however, that the presence of the isovector force has some effects on the behavior of the largest eigenvalue with J = 1. One can indeed see that this slowly decreases for k &lt; 1, which is not so for the pure isoscalar force. As it will be shown below, this decrease becomes significantly large in the presence of the other multipoles of the two-body interaction.\n- where the sum over J, T includes all possible pairs. The question of interest here is how the two-body density matrices for the (J = 0, T = 1) and (J = 1, T = 0) pairs are affected when, in addition to the interaction in these two channels, one considers also all the other components of the shell model interaction. In order to study this issue, we have evaluated the two-body density matrices using the exact wave functions of the shell model (SM) Hamiltonian (33). In the SM calculations, performed with the code BIGSTICK [24], we have used the USDB interaction scaled by a factor k. The results for the largest and the second largest eigenvalues of the density matrix in the channels (J = 0, T = 1) and (J = 1, T = 0) are shown in Fig. 7. For J = 0 the behavior of the largest eigenvalue is rather similar to the one shown in Fig. 6, with the difference that for k &gt; 1 the decrease is slower. The situation is completely different for the J = 1 pairing. In this case the largest eigenvalue decreases very fast up to around k = 1.1 and then it only slightly increases. This shows that the peculiar fast increase of the largest eigenvalue observed in the case of the isoscalar and the isoscalar-isovector pairing interactions is washed out by the other components of the SM interaction.\n\nMentions:\n- The results for the largest and the second largest eigenvalues of the density matrix, corresponding to the exact ground state, are shown in Fig. 6.\n- On the other hand, as seen in Fig. 6, the largest eigenvalue for J = 1 has a shape rather similar to the case of the pure isoscalar interaction.\n- For J = 0 the behavior of the largest eigenvalue is rather similar to the one shown in Fig. 6, with the difference that for k &gt; 1 the decrease is slower.\n\nOCR Text: \"\"T-Ch 1.2\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 6: The largest two eigenvalues of the two-body density matrix for T=0 and T=1 pairs obtained with the isovector plus the isoscalar pairing interactions scaled by the factor k. The results correspond to 32S."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The ASC as a function ofγ D in dB is depicted in Fig. 1 forγ E = 3, 6, 9 dB. The outputs of a Monte Carlo simulator are shown to exactly match with the analytical results, which validates our derived results. As expected, the performance of ASC improves with increasingγ D or decreasingγ E . Note that the ASC will fall to zero if the average SNR of the main channel is smaller than the eavesdropper channel (γ D &lt;γ E ), which is consistent with (7).\n\nMentions:\n- The ASC as a function ofγ D in dB is depicted in Fig. 1 forγ E = 3, 6, 9 dB.\n\nOCR Text: \"Antig SW # MIB) 326B\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1. ASC over FTR fading channels against γ̄D for different values of γ̄E (KD = 15, KE = 5, mD = 5.5,mE = 8.5, ∆D = 0.4, and ∆E = 0.35)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- For the case b = 2 and c as free parameter, by increasing (decreasing) the value c shadow sizes decreases (increases). For the third case, c = 2 and b as free parameter, the shadow size decreases (increases) by increasing (decreasing). One sample of shadow size vs parameter c is shown in figure 4. Figure 5 is similar to figure 3 but the spin parameter is −0.9. The shadow size variations is similar to positive spin value.\n\nMentions:\n- One sample of shadow size vs parameter c is shown in figure 4.\n\nOCR Text: \"i=17', 0-0,6,b-2\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 4. Shadow size variations vs parameter c for the case b = 2. The spin parameter is 0.6 and viewing angle is 17◦."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Finally, it is instructive to put our results into the context of a more conventional \"energy curve\" approach for constructing epitaxial phase sequences. This approach involves computation of the epitaxial strain dependence of the energy for relaxed structures based on selected distortions of the superlattice, plotting of the energies vs epitaxial strain, and analysis of the resulting curves to find the ground state structure and low energy structures at each strain. We performed additional calculations of the total energies of all the low-energy structures identified in our structure determination (Fig. 6) for all three values of epitaxial strain, and present the resulting set of energy vs epitaxial strain curves in Fig. 10. At -2% strain in Fig. 10, the stacking method identified the three lowest-energy configurations, P 4bm, P 4/mbm and P 4mm, the other two structures in this diagram being outside the 30 meV/5atoms energy range. For +2% strain, the stacking method identified the four lowestenergy structures (shown as three points in Fig. 10 due to the small energy differences), the other two structures being outside the energy window. In agreement with previous work 6,19 , at 0% strain, many distinct structures are close in energy, The energy curve approach is useful in understanding the evolution of symmetry-breaking distortions with strain and the resulting phase transitions, but involves energy computation for structures that are quite high in energy. In addition, the selection of distortions for the superlattice is generally not systematic. In comparison, our stacking method concentrates on the low-energy structures at each strain, and thus is more efficient for constructing the phase sequence and identifying the low-energy alternative structures at each strain.\n- The method is particularly suitable for highthroughput studies of superlattices with constituents drawn from a specified set of pure compounds. Once the database of low-energy structures for the specified FIG. 9. Total energies and space-group-symmetry analysis for relaxed structures of the epitaxially-strained 2:2 PTO/STO superlattice from a set of 20 randomly-distorted P 1 starting structures. Conventions as in Figure 2. Red, blue and green arrows point to P cGSS, P c high and CmGSS states, respectively. The Cm high state does not appear in this set. FIG. 10. The total energies as functions of epitaxial strain for configurations. All the configurations fall into five distinct structures for -2% strain, plotted in five colors. Different point types denote distinct space groups and show the revolution of them in epitaxial strain. set of compounds is constructed, the generation of starting structures is rapid and automatic, and computational effort is focused on structures that are likely to be low in energy.\n\nMentions:\n- We performed additional calculations of the total energies of all the low-energy structures identified in our structure determination (Fig. 6) for all three values of epitaxial strain, and present the resulting set of energy vs epitaxial strain curves in Fig. 10.\n- At -2% strain in Fig. 10, the stacking method identified the three lowest-energy configurations, P 4bm, P 4/mbm and P 4mm, the other two structures in this diagram being outside the 30 meV/5atoms energy range.\n- For +2% strain, the stacking method identified the four lowestenergy structures (shown as three points in Fig. 10 due to the small energy differences), the other two structures being outside the energy window.\n- The Cm high state does not appear in this set. FIG. 10.\n\nOCR Text: \"CFP: Kein CTF 6mi CTrn2 Epla-i3 STain % ;\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 10. The total energies as functions of epitaxial strain for configurations. All the configurations fall into five distinct structures for -2% strain, plotted in five colors. Different point types denote distinct space groups and show the revolution of them in epitaxial strain."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The causal structure associated with the independently distributed copies of U 1 , U 2 , U 3 and their observable children {A ij }, {B kl }, {C pq } is termed an inflation graph. The inflation graph of a correlation scenario is also a correlation scenario; as an example, Fig. 5 depicts the n = 2 inflation graph for the triangle scenario. These observable variables follow a probability distribution Q n ({A ij }, {B kl }, {C pq }) with the property\n\nMentions:\n- The inflation graph of a correlation scenario is also a correlation scenario; as an example, Fig. 5 depicts the n = 2 inflation graph for the triangle scenario.\n\nOCR Text: \"Glu i albijary Gziytij Fi-D C; tbe Glaliou technicue VMSNF Ie (eg\" M PiA,R.CI) Iu? Ile xkguctn bliagoual rargir&/ dlistrilsutinn (, thc irllatcd mrinllez stis-,irg {cn co_ditiou Iat: %8 Call the ##cciatoc_ listributicn < \" Urder inflativn) el P, Cleac:% C} duet atlmj ~rhr irllali: 4 i: (u Iln canncl hoc Kraizc] Ihc: hriangl sreraric: Deciding % che degrce-n ctPABO!i: I1Cmbft cexee dixguiil Iuryidt > livear prUk;\"&u_ tbe car program uexsible': 1 e if _u n5 iulutich exists fir Pi 1,B,('; tbeu the pUFE L iU Etd a \"rigxtililily: Such #ile Iaf lr icrm ~in _ liut liling kitina Jda\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5. The second-order inflation graph of the triangle scenario."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The training data consist of 27,310 essays while the test data contain 13,656 essays. Figure 1 illustrates the distribution of essays according to their level. The classification problem is unbalanced as there are far more training examples for the first levels (e.g., A 1 ) than for the rest. The organisers announced that they performed a stratified split with respect to the level label, so we expect similar distributions for the test data. 2 The released data consist of the essay text as well as various numerical features. The numerical features are either statistics calculated on the essay text (length, number of sentences/syllables etc.) or indexes that try to captrue the readability and complexity of the essays, like the Coleman and Flesch families of indexes.\n\nMentions:\n- Figure 1 illustrates the distribution of essays according to their level.\n\nOCR Text: \"14304 7688 5283 1338\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: The distribution of essays according to the CERF levels in the training data."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Similar results were obtained for crystals of Fe 1−x Mn x Ga 3 and FeGa 3−y Zn y regardless of the dopant type and level, with nonsaturating M (H) at T = 1.8 K, χ(T ) showing broad maxima at T 5 K for H [110] and χ(T )∼ T −α at T 5 K in case of an applied field along the c-axis with α between 0.2 and 0.6. Although the exact values of the exponent α may be affected by slight crystal misalignment and the presence of a small amount of paramagnetic impurities, they are always below 1. Such weak power laws in χ(T ) at T 5 K may hint at the emergence of critical fluctuations associated with a proximity to a quantum critical point in the presence of disorder introduced by doping. 64 Further, no hysteresis was observed in field-cooled (FC) and zero-field-cooled (ZFC) χ(T ) data for Fe 1−x Mn x Ga 3 and FeGa 3−y Zn y , that could indicate a spin glass behavior at low temperatures. The values of the magnetic susceptibility, measured at a temperature of 2 K, increase systematically with increasing doping level for both investigated directions of magnetic field, following the same straight lines for crystals of Mn and Zn-doped FeGa 3 (Fig. 11). These results need to be contrasted with previous studies on polycrystals of Fe 1−x Mn x Ga 3 which revealed that already in samples with x = 0.05 there are either two magnetic phase transitions at temperatures of ∼10 K and ∼30 K according to Ref. 65 or there is an onset of ferromagnetism with T C ≈ 160 K superimposed on a Curie-Weiss-type paramagnetic behavior with the effective magnetic moment of 4.9 µ B /Mn. 66 Finally, neutron powder diffraction experiments have been performed to inspect the magnetic and structural properties of hole-doped FeGa 3 at low temperatures. The diffraction patterns for Fe 0.95 Mn 0.05 Ga 3 measured at temperatures of 1.5 K and 15 K are basically identical (Fig. 12). There are no differences in the sizes and positions of either magnetic or nuclear peaks, that could indicate a change in the long-range magnetic order or a structural transition in the doped FeGa 3 at low temperatures. Furthermore, the magnetic peaks observed in the neutron powder diffraction patterns for Fe 0.95 Mn 0.05 Ga 3 persist at temperatures up to 300 K and, most importantly, are similar to those found for undoped FeGa 3 (Fig. 12), indicating that their magnetic structures are closely related.\n- The linear increase of the excess entropy associated with the Schottky anomalies with increasing doping levels observed for Mn-and Zn-doped FeGa 3 (Fig. 8(c)) is consistent with a growing number of doping-induced polarons. Also, the magnetic susceptibilities at the lowest temperatures increase nearly linearly with the dopant levels for both Fe 1−x Mn x Ga 3 and FeGa 3−y Zn y (Fig. 11), in-line with the scenario of magnetic polarons. Further, the field dependent magnetization M (H) curves measured at 1.85 K do not saturate up to the highest field of 70 kOe, and the maximum measured moments correspond to only ∼0.3 µ B per dopant atom (Fig. 10(b)). Since the doping with Mn and Zn falls short of closing the spin gap of 0.4 eV and essentially does not influence the antiferromagnetic ordering as revealed by our neutron diffraction measurements, the shape of the M (H) curves reflects the magnetization associated with the dopant atoms. The lack of saturation and only small values of the field-induced magnetic moment per dopant atom that are independent of the doping level, suggest the presence of strong exchange interactions between spins of carriers introduced by the doping and spins of Fe atoms in the antiferromagnetic material. This exchange coupling is presumably responsible for the formation of the spin polarons. Recent muon spin rotation study gave an evidence for the presence of anisotropic spin polarons in FeGa 3 at temperatures below ∼10 K. 44 However, we caution that spectroscopic measurements on Mn-and Zndoped FeGa 3 are needed to place our proposal on firmer footing.\n\nMentions:\n- The values of the magnetic susceptibility, measured at a temperature of 2 K, increase systematically with increasing doping level for both investigated directions of magnetic field, following the same straight lines for crystals of Mn and Zn-doped FeGa 3 (Fig. 11).\n- Also, the magnetic susceptibilities at the lowest temperatures increase nearly linearly with the dopant levels for both Fe 1−x Mn x Ga 3 and FeGa 3−y Zn y (Fig. 11), in-line with the scenario of magnetic polarons.\n\nOCR Text: \"1 [001] 8 8 [110]\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 11. (Color online) Values of the magnetic susceptibility for Fe1−xMnxGa3 (black) and FeGa3−yZny (red) measured at 2 K in a magnetic field applied either along the [110] direction (circles) or along the c–axis for H=1 kOe (triangles). Dashed lines are guides for the eye."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- When D = 0.16, a case that Hockney studied [46], the FGI growth rate from theory is much smaller than with D = 0.1. With D = 0.16, B = 0, ∆T = 0.5, C = 0.08, and N D = 20 the FGI seems to be much weaker. In Figure 13, the values of computed by CIC and by CELESTE are compared for C/D = ∆T = 0.125. For C &gt; 0.64, CIC error varies as C 3 . For all C, CELESTE error varies as C 3 . It is often stated that CELESTE, and energyconserving methods in general, conserve energy exactly only when ∆T → 0, but it should be noted the errors are small compared with those for CIC when C &lt; 0.3.\n- Figure 13:\n\nMentions:\n- In Figure 13, the values of computed by CIC and by CELESTE are compared for C/D = ∆T = 0.125.\n- Figure 13:\n\nOCR Text: \"1ooop 1@ 1 1 0,0L 0.001 0 (rac\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 13: for CELESTE (4) and CIC (2) in simulations of a warm, stationary plasma are compared for C/D = ∆T = 0.125. For C > 1, the CELESTE ( 4) is twice that for CIC (2). The CIC varies approximately as 1/C2 for C < 0.64, and as C3 for C > 0.64. The CELESTE error varies as C3 for all C."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We subsequently perform the following control actions: (i) Changing the power injections of generators 1-3 and controllable load buses 4-6 to P 1 = 0.5890, P 2 = 0.5930, P 3 = 0.5989, P 4 = −0.0333, P 5 = −0.0617, P 6 = −0.0165. From Fig. 6 and Fig 7, it can be seen that the bus angles of the post-fault dynamics converge to the equilibrium point of the controlled postfault dynamics which is the first equilibrium point δ * 1 . In Fig. 8, we can see that the generator frequencies converge to the nominal frequency, implying that the post-fault dynamics converge to the stable equilibrium point δ *\n\nMentions:\n- In Fig. 8, we can see that the generator frequencies converge to the nominal frequency, implying that the post-fault dynamics converge to the stable equilibrium point δ *\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 8. Effect of power injection control: Convergence of generators frequencies to the base value."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The wave number k of the instability developing at the field supercritical strengths was measured. For this, the mean distance λ between adjacent primary peaks was measured. The distances between the smaller secondary and tertiary peaks were not studied. Fig. 5 shows the experimental dependence of the instability supercritical wave number upon the external magnetic field strength compared with the relevant dependence for the layer on nonmagnetic substrate. It is seen that at weak fields the both dependences practically merge. But as the field grows, the dependence for the layer on magnetic substrate goes through an extremum while that for the nonmagnetic substrate grows monotonically. The observed at high strengths decrease of the magnetic substrate layer primary structure wave number may be due to the intensely forming in the strong field secondary and tertiary peaks, as if these were pushing the primary ones apart. Meanwhile, a nonmagnetic substrate has only the secondary peaks, and their formation with the stronger fields can only slow down the growth of the wave number upon field dependence.\n\nMentions:\n- Fig. 5 shows the experimental dependence of the instability supercritical wave number upon the external magnetic field strength compared with the relevant dependence for the layer on nonmagnetic substrate.\n\nOCR Text: \"XkAm\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 5. Dependences of instability supercritical wave number upon magnetic field strength for magnetizable (1) and nonmagnetic (2) substrates. Layer thickness h = 8 µm."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We focus on optimizing the TXOP duration and pattern, PPDU duration and the 11ax's two-level aggregation scheme working point first introduced in IEEE 802.11n [30] In 11ax the size of an MPDU is limited to 11454 bytes and the size of the A-MPDU frame is limited to 4,194,304 bytes. The transmission time of the PPDU (PSDU and its preamble) is limited to 5.484ms (5484µs) due to the L-SIG (one of the legacy preamble's fields) duration limit [1]. The A-MPDU frame structure in two-level aggregation is shown in Figure 1.\n\nMentions:\n- The A-MPDU frame structure in two-level aggregation is shown in Figure 1.\n\nOCR Text: \"Mmi_ S Ant= 4HSLM LIni\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: The generation of an A-MPDU frame in two-level aggregation."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Instead of strongly connected components, our algorithm works with loops. In contrast to components, loops can be nested. Let π be an acyclic path from the initial node l s and let α be a prefix of π leading to a node v. The node v on π is an entry node of a loop if there exists a path vβv such that none of the nodes on β appears in α. The entry node v on π enters the loop C that is the smallest set containing v and all nodes in β for each path vβv such that none of the nodes on β appears in α. For example, program in Figure 2 contains two acyclic complete paths: π 1 = l s bdl t and π 2 = l s abdl t . While π 1 contains only one entry node b associated with loop {a, b, c, d}, π 2 contains entry node a with loop {a, b, c, d} and entry node b with loop {b, c}. A node u is an exit node of C v if there exists w ∈ C v such that (w, u) ∈ E.\n\nMentions:\n- For example, program in Figure 2 contains two acyclic complete paths: π 1 = l s bdl t and π 2 = l s abdl t .\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: Example of nested loops."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The fundamental research problem involved in this parallel simulator design is to determine a near-optimal partitioning heuristic using a bi-layer network model-a static road network overlaid with a dynamic vehicular network-connected by the CV technology which spans across both the networks (Figure 1). Earlier research mainly focused on partitioning static road\n\nMentions:\n- The fundamental research problem involved in this parallel simulator design is to determine a near-optimal partitioning heuristic using a bi-layer network model-a static road network overlaid with a dynamic vehicular network-connected by the CV technology which spans across both the networks (Figure 1).\n\nOCR Text: \"Muk Vtal Kn ilk \"Talb Sarri Jeg Skalv Jlualghah Inbu Sulh EctIa Siljt Kalbars;\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Bi-layer complex transportation network"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- was set to be 1.  average MSE learning curves over 100 runs via alternating optimization. For comparison purpose, the red lines in Fig. 2 depict the average MSE per data stream obtained by the noniterative algorithm based on separate MSE. From the curves in the figure, it is observed that between the sum-MSE based precoding schemes RAO offers a better performance with a lower average MSE than UAON, but its convergence rate is always slower than the simpler UAON. Also note that the MSE performance of the separate MSE based precoding scheme obtained from the non-iterative algorithm is inferior to that of RAO. Moreover, the performance curves in Fig. 2 reveal that when imperfect CSI is utilized, the MSE differences between the separate MSE based and Sum-MSE based precoding are more pronounced relative to those in the perfect CSI case, for all the three proposed schemes, and the convergence of the robust UAON and robust RAO appears to be slower than their perfect CSI counterpart. Efforts were made to investigate how the average MSE is related to the transmission power. With a fixed P SC = 24 (dBm), Fig. 3 shows that the average MSE for MUEs of iterative algorithms decreases gradually as the transmit power at the BS increases with perfect CSI, while the average MSE for SUEs increases slightly due to the increased inter-cell interferences. Furthermore, the sum-MSE based RAO offers the smallest MSE gap between MUE and SUE, indicating better user fairness, while the separate MSE based precoding has the largest one under the low transmit power. As for the separate MSE based non-iterative precoding scheme, the   average MSE for MUEs approaches to that of RAO as the transmit power at BS increases, while the average MSE curve for SUEs goes up gradually. Subsequently, Fig. 4 illustrates the corresponding BER performance of the proposed schemes, indicating the same relationships as those of the average MSE performance revealed in Fig. 3.\n\nMentions:\n- Subsequently, Fig. 4 illustrates the corresponding BER performance of the proposed schemes, indicating the same relationships as those of the average MSE performance revealed in Fig. 3.\n\nOCR Text: \"sun-VEELae]FrEbuiy (RAO, Peileci CSI} Cuk VZE base] Prebudn mnon Penecl Csi Selaral MS2 Lased Prec Jq(petled C5 Sun-VfE based Rrecednq @RAO,Impenec[ CSI' Fim VFF hascnorano nnn Imrac - Csin Soan MS: tisod Prcoirarimaerc Tho NntaneMfS\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4: The BER per data stream for MUE/SUE versus transmit power at BS (K = 8, Perfect CSI)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Fig. 2, the non-perturbative beta function defined by Eq. ( 6) is given by the red line (NP β 4 ). The dashed lines represent the perturbative beta functions obtained in each order.\n\nMentions:\n- In Fig. 2, the non-perturbative beta function defined by Eq.\n\nOCR Text: \"Uixi} 0q) Qix ) IT' %2 Qixl) Vix\") Oixi\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 2: The non-perturbative beta function defined by Eq. (6) is given by the red line (NP β4). The dashed lines represent the perturbative beta functions obtained in each order."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The DAQ and electronics for the shutter movement have been tested separately and have shown to be reproducible to a level of better than 10 ms (Fig. 4) resulting in a relative systematic uncertainty on the level of 10 −5 . The difference in calibration counts points towards a different performance of the UCN turbine or feeding line, as the reactor power was within 0.5% the same as in April.\n\nMentions:\n- The DAQ and electronics for the shutter movement have been tested separately and have shown to be reproducible to a level of better than 10 ms (Fig. 4) resulting in a relative systematic uncertainty on the level of 10 −5 .\n\nOCR Text: \"1035 1040 1045 IOe0 3.0e5  3.0w0 Wme &\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4: Histogram of the time difference between the closed signal from shutter 1 and the open signal from shutter 2. The red line shows a Gaussian fit with a standard deviation of 3.2 ms."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In the present study, the forces introduced by the actuators are included in the reported lift and drag on the airfoil. To quantify the amount of force exerted on the airfoil by an actuator with wall-normal injection, we consider evaluating the lift and drag induced by the actuators in otherwise quiescent condition (U ∞ = 0). These forces are found to be relatively small as shown in Fig. 3. The largest C µ of 2.2% seen in the figure introduces a negative lift that is less than 1.5% in magnitude of the baseline lift. Moreover, we see essentially no drag being induced solely by the actuation. The changes in the lift and drag achieved with flow control as we report later are significantly larger in magnitude compared to the values seen in Fig. 3.\n\nMentions:\n- These forces are found to be relatively small as shown in Fig. 3.\n- The changes in the lift and drag achieved with flow control as we report later are significantly larger in magnitude compared to the values seen in Fig. 3.\n\nOCR Text: \"XI C\" [%]\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Lift (◦) and drag (×) forces exerted on the airfoil by injecting wall-normal momentum in otherwise quiescent flow (U∞ = 0). Values are reported per actuator."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We use a WIMP sensitivity calculation [34] that takes into account the kinematic details of WIMP scattering off heavy target nuclei, including nuclear form factors and effects1 of the Earth's motion [35]. For simplicity we assume a constant 50% nuclear recoil acceptance at all energies, though an energy-dependent acceptance can be used to take advantage of the strong energy-dependence of the PSD. In Figure 6 we show experimental limits from previous argon-based experiments and the current best limit at high masses from XENON1T [6]. We also show the projected sensitivity reach of the liquid argon DarkSide-20k [5] experiment, along with that of the ARGO experiment [5], which is the planned follow-on to DarkSide-20k. We present three potential sensitivities from our large DUNE-like detector after running our 1 kt fiducial volume for 3 years. Such a strategy could produce a physics result on the timescale of the DarkSide-20k result.\n- The backgrounds at each threshold are summarized in Table 1 and the sensitivity results for the three thresholds studied are shown in Figure 6. We judge the background scenarios and thresholds as progressively more ambitious, but achievable if light collection and radiopurity standards are increased as required.\n- We see from Figure 6 that due to the higher threshold (compared to dedicated dark matter detectors) this experiment is mostly sensitive to WIMP masses larger than 30 GeV. Though a 50 keV r threshold for a DUNE-like detector module will be very difficult to achieve, given the sheer size of the detector and industrial materials that must be employed, we believe a 100 keV r (and even a 75 keV r ) threshold is achievable with some reasonable radiopurity controls. Further, we have  1. Backgrounds considered and estimated count rates in the inner 1 kt fiducial volume over 3 years for different energy thresholds. These estimates presume 40 cm water tanks between the rock wall and outer cryostat, the use of underground argon with an 39 Ar rate of 7.3 × 10 −4 Bq/kg Ar , multi-site rejection with 20 mm resolution, and a 50% nuclear recoil acceptance. Electron recoil backgrounds are shown before (bPSD) and after (aPSD) application of PSD. *The numbers listed for a 50 keV r threshold require a change in the detector configuration; see text. some confidence based on our own toy Monte Carlo and [36] that 100 photons at 100 keV r can be achieved with upgraded dense optical coverage, as we suggest here. The resulting sensitivity curves show an interesting coverage with respect to DarkSide-20k, allowing a confirmation or refutation of a high mass signal between the two experiments.\n\nMentions:\n- In Figure 6 we show experimental limits from previous argon-based experiments and the current best limit at high masses from XENON1T [6].\n- The backgrounds at each threshold are summarized in Table 1 and the sensitivity results for the three thresholds studied are shown in Figure 6.\n- We see from Figure 6 that due to the higher threshold (compared to dedicated dark matter detectors) this experiment is mostly sensitive to WIMP masses larger than 30 GeV.\n\nOCR Text: \"AG? WF M\";M Dotod5 7eP38A AoET LYEONOT\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6. Shown are the achieved 90% c.l. sensitivities in solid colored lines for various experiments for WIMP DarkMatter searches [6, 12, 17]. Dashed lines for proposed argon [5] and xenon [21, 22] experiments are overlaid. The DarkSide-20k exposure is 20 tons over a planned 10 years for a total of 200 ton-yrs, while our proposal for a DUNE-like module is 3 years of a 1 kt mass for a 3 kt-yr exposure. The ARGO exposure is a planned 300 tons over ten years for a total of 3 kt-yr. Two possibilities for a DUNE-like module 4 for two thresholds and expected backgrounds, as discussed in the text, are shown in black lines. A third, aspirational, DUNE module 4 sensitivity for a 50 keV threshold is also drawn as a thinner line."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In this case, our Decision Tree achieves 82.8% accuracy for classifying the preferential path, which performs better than Linear Regression and Bayes. The accuracy can be greatly improved by using the Random Forest model which votes between multiple trees. In our experiment, by using a Random Forest model whose tree number is 200, the accuracy can As shown in Fig. 5, we can know exactly what happened to the classified procedures by analyzing the partition nodes from root to a certain leaf. Because the learning model of Decision Tree in scikit-learn package is a local greedy algorithm, which confirms the splitting order and thresholds through comparing the entropy. RSSI is chosen as the root node of decision tree in Fig. 5 as RSSI W iF i has the largest information gain ratio during the startup. Meanwhile, the nodes in the remaining layers are determined by using the information gain ratio when combining their upper layer nodes. This combined information gain ratio used to purify the samples in each node differs from the overall information gain of single attribute in Table II. This is why SIN R LT E only have the 6th CIG but appear as the third layer of decision tree. In other words, the splitting order of SIN R LT E right after RSSI LT E has a better branch gains than other explorations.\n\nMentions:\n- In our experiment, by using a Random Forest model whose tree number is 200, the accuracy can As shown in Fig. 5, we can know exactly what happened to the classified procedures by analyzing the partition nodes from root to a certain leaf.\n- RSSI is chosen as the root node of decision tree in Fig. 5 as RSSI W iF i has the largest information gain ratio during the startup.\n\nOCR Text: \"~y 4\"\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 5. Pruned Decision tree for classifying metrics into WF and LF."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 4 shows the convergence time of the proposed algorithm, as the number of SCBSs grows. This figure compares the number of iterations for two different values of rate demands R d k = 100 Mbits/s, and R d k = 50 Mbits/s. The number of iterations of the proposed algorithm is expected to increase with the network size, as clearly seen in Fig. 4. However, this figure shows that the convergence of the proposed distributed algorithm does not exceed 400 iterations for a very dense SCBN with 20 SCBSs. Indeed, the convergence time increases linearly as the network size grows, thus, yielding a reasonable scalability and complexity. In addition, the convergence time for R d k = 50 Mbits/s is almost half of the this value when R d k = 100 Mbits/s. Thus, Fig. 4 shows that the traffic at the radio access network substantially affects the convergence time of the backhaul resource allocation.\n\nMentions:\n- Fig. 4 shows the convergence time of the proposed algorithm, as the number of SCBSs grows.\n- The number of iterations of the proposed algorithm is expected to increase with the network size, as clearly seen in Fig. 4.\n- Thus, Fig. 4 shows that the traffic at the radio access network substantially affects the convergence time of the backhaul resource allocation.\n\nOCR Text: \"ummefMemimts Ro=ICCMbt5 eaamaint9 culhel Nmtercis pSe\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4. The number of iterations with95% confidence vsK with K1 = 2, and for two different rate demands,Rd k = 100 Mbits/s andRd k = 50 Mbits/s. The pk1 = (0.1, 10) for both A-BSs. The budget of D-BSs is limited to60."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Next, we discuss the performance of UAV-UEs under HARP association. Fig. 5 verifies the accuracy of the geometry-based approximation of Corollary 2. Fig. 5 presents the exact and the obtained approximation of the static UAV-UE coverage probability versus the SIR threshold ϑ. Clearly, the adopted approximation is relatively tight. Fig. 5 also shows that the coverage probability of static UAV-UEs under practical antenna patterns is much reduced compared to the achievable coverage probability from a simple antenna model. In Fig. 6, we plot the coverage probability of mobile UAV-UEs versus the number of antenna elements N t , at different penalty costs β. Fig. 6 shows that the UAV-UE coverage probability monotonically decreases as both N t and β increase. This can be interpreted by the fact that as long as N t increases, the mobile UAV-UE becomes more susceptible to handovers, which are penalized by the cost β.\n\nMentions:\n- In Fig. 6, we plot the coverage probability of mobile UAV-UEs versus the number of antenna elements N t , at different penalty costs β. Fig. 6 shows that the UAV-UE coverage probability monotonically decreases as both N t and β increase.\n\nOCR Text: \"Number of antenna elemtents _\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 6. Coverage probability of mobile UAV-UEs under HARP association (hd = 100 m, h1 = 80 m, h2 = 120 m)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- ds , that is, for trees with more than 6 nodes, a partition in two equally sized communities always has a larger modularity density than the trivial partition. More generally, partitions with a larger number of equally sized communities tend to have a larger score, as can be seen in figure 9.\n\nMentions:\n- More generally, partitions with a larger number of equally sized communities tend to have a larger score, as can be seen in figure 9.\n\nOCR Text: \"\"cmrut Vecmmintes J ecmmuntes -Aecmmuntes j cmmuites\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 9. Modularity density for random trees partitioned into equally sized communities. The values indicate that trees constitute a limitation to the application of modularity density, since, for more than 6 nodes, partitions with more than one community always have larger modularity density than the trivial one."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Whereas the shortcomings may be well-known to producers (group 1) and bibliometricians (groups 2), they are frequently not sufficiently known among managers (group 3) and the scientists under study (group 4). Table 1, for example, lists 24 name variants in the address information of 141 documents retrieved from WoS with the search string \"og= INST SCI TECH INFORMAT CHINA\", 29 June 2016. Unlike the search tag \"oo=\", \"og=\" refers to institutional information indexed and consolidated by Thomson Reuters. Nevertheless, different institutions and different representations of the same institution are indicated. Among the thousands of authors with \"Singh\" as a family name, 45 are distinguished in WoS with the first initial \"A.\" Twenty-one of these authors have more than ten publications, among which 9,266 publications for the \"author\" listed as \"A. Singh\". One cannot expect these publications to be sorted unambiguously to individual authors without substantial investments in name disambiguation. Scopus' Author Name Identifier disaggregates the set into 1,908 authors with an exact match for the search \"Singh\" as family name and \"A.\" as first initial. The top-10 of these are shown in Figure 1. The same search-\"A Singh\"-provided \"about 125,000 results\" when using GS, obviously as a cumulative result of more than one thousand authors with this name.\n\nMentions:\n- The top-10 of these are shown in Figure 1.\n\nOCR Text: \"7d 3 : tcdl *? 37a Iud: Oda {78'- Jtuee Lel JatF !Ii 17t 0 84 16t\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1. Numbers of documents of ten top authors with “Singh” as family name and “A” as first initial in Scopus (June 29, 2016). Main names attributed by the Scopus Author Identifier are added to the legend of the abscissa."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Bunch length. It looks more reasonable to increase the electron bunch length, as in this case L γγ ∝ θ d . However, to keep the conversion probability constant one would need to increase the laser flash energy A. Its dependence on the bunch length is shown in Fig. 6. In order to decrease the disruption angle by the desired factor of 1.67, the bunch length would have to be increased by a factor of (1.67) 2 = 2.8, which requires a factor 3 greater laser flash energy (for p = const). Laser power is a leading challenge for the photon collider, and thus such a big increase is excluded. Laser wavelength. Last but not least, one can increase the laser wavelength. According to Eq. 3.3, the disruption angle drops as 1/ √ λ without any change in the γγ luminosity. So, if we take λ = 2 µm instead of the current λ = 1 µm, the disruption angle would be a factor 1.41 smaller. Moreover, as shown in Fig. 3, the total disruption angle consists of two parts: the disruption angle due to the collision with the opposing electron beam plus an additional deflection in the solenoidal field of the detector, which is proportional to 1/E. According to Eq. 2.2, the minimum electron energy is proportional to the laser wavelength; therefore the deflection angle due to the detector field would be reduced by a factor of 2. These effects are clearly seen in Fig. 7, where the right figure corresponds to 2E 0 = 1000 GeV, λ = 2 µm and the left one to 2E 0 = 500 GeV, λ = 1 µm. One can see a considerable shrinking of the transverse beam sizes at λ = 2 µm, sufficient to reduce the crossing angle to 20 mrad.\n\nMentions:\n- Its dependence on the bunch length is shown in Fig. 6.\n\nOCR Text: \"p-tt 82-0.15 Omir =0,017\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6. Dependence of the conversion probability on the electron bunch length and laser flash energy."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The direct photons that can be measured in the experiments can originate from two different process: either they are produced directly in the hard partonic scattering (prompt photons) or they are produced similarly as high-p T hadrons by a fragmentation process. The prompt photons are more directly related to partonic kinematics and are thus more sensitive to small-x 2 values. However, the prompt and fragmentation components cannot be separated from each other in the measurements and thus both have to be included in the inclusive photon calculations. The net x 2 sensitivity of direct photons is plotted in Figure 3 for p+Pb collisions with √ s NN = 8.8 TeV for 5 &lt; p T &lt; 20 GeV/c at pseudorapidity ranges |η| &lt; 1 and 3 &lt; η &lt; 5. Although it has been shown [12] that, at the LHC energies, the fragmentation component is dominant at low p T , the presence of the prompt component makes the cross section somewhat more sensitive to smaller x 2 than the pions at corresponding rapidities above. The fragmentation component yields a similar tail towards higher x 2 values as for hadronic observables. Even though the fragmentation component cannot be completely excluded, it can be suppressed by introducing an isolation cut for the direct photons. Here we have studied an isolation criterion which rejects photons that are accompanied by more than 2 GeV of hadronic energy inside a cone of radius 0.4 around the photon. As can be seen from Figure 3, the isolation criterion indeed cuts off part of the tail at the high-x 2 region making the isolated photon cross section more sensitive to small x 2 . The x 2 distributions are calculated here using the JETPHOX code [13][14][15] and for the fragmentation component the BFGII parton-to-photon FFs [16] are used, otherwise the setup is the same as above.\n\nMentions:\n- The net x 2 sensitivity of direct photons is plotted in Figure 3 for p+Pb collisions with √ s NN = 8.8 TeV for 5 &lt; p T &lt; 20 GeV/c at pseudorapidity ranges |η| &lt; 1 and 3 &lt; η &lt; 5.\n- As can be seen from Figure 3, the isolation criterion indeed cuts off part of the tail at the high-x 2 region making the isolated photon cross section more sensitive to small x 2 .\n\nOCR Text: \"Ieljsi ? Lulaled [ -0,[ 7*= S6 2 26 <W 9G 3<1< {<4< {\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3. The x2 distribution for direct photons in p+Pb collisions with√ sNN = 8.8 TeV for 5 < pT < 20 GeV/c at |η| < 1 (green dashed) and 3 < η < 5 (red solid), and for isolated photons at 3 < η < 5 (blue dot-dashed)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Taking the CO LDMEs of Feng2019 [10] as default choice, we show the feed-down and resolved contributions of NRQCD in Fig. 2 and Fig. 3, respectively. It is clear that most of J/ψ mesons are produced directly and only 5% of them are from decays of ψ(2S) and χ cJ . The single resolved channel is also dominated. As the reference, the p t distribution integrated from 0.1 to 10 GeV, the direct, single-resolved and double-resolved channels account for 1%, 96% and 3% of the NRQCD prediction, respectively. Fig. 4 shows the number of J/ψ events distribution as function of p t (upper) and cos θ(lower) respectively for the integrated luminosity of CEPC 5.6 ab −1 . We see that at the CEPC, the number of events are considerable to discriminate between CS and NRQCD even as the transverse momentum p t is up to 20 GeV.\n\nMentions:\n- Taking the CO LDMEs of Feng2019 [10] as default choice, we show the feed-down and resolved contributions of NRQCD in Fig. 2 and Fig. 3, respectively.\n\nOCR Text: \"Pcmci Cired Feno 03evi\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 2: The pt distributions of direct J/ψ production and that from heavier charmonia."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Such staircase defects lead to a deviation of the printed result from the input model, where necessary volumes in the input model may be removed in the printed result (red regions in Figure 2), while unnecessary volumes that do not exist in the input model may appear in the printed result (yellow regions in Figure 2). In addition, the final result lacks the smooth geometry of the virtual input model.\n\nMentions:\n- Such staircase defects lead to a deviation of the printed result from the input model, where necessary volumes in the input model may be removed in the printed result (red regions in Figure 2), while unnecessary volumes that do not exist in the input model may appear in the printed result (yellow regions in Figure 2).\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: Approximation errors of flat layer slicing in the tease: red lines are the input 3D model surfaces; rectangles are printed layers; volumes removed from the input model are colored red; volumes added to the input model are colored yellow."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The results in Figures 2 and 3 show how the speech recognition quality varies with increasing training steps for different settings. Figure 2 compares the WER for the momentum and Adam optimization methods with and without low-rank projection. Comparing the low-rank and full-rank models, Figure 2 shows that low-rank models converge slower for both momentum and Adam optimization. The latter achieved a better performance, indicating that we are able to take advantage of the benefit of Adam optimization by using it to update U and V . Figure 3 shows that the word error rate decreases faster and to a lower rate as the rank of the gradient approximation is increased. Training the model using Adam with a gradient projection matrix of rank 150 reached a word error rate of 47.1% while the baseline model converges at a word error rate about 43.8%. Similarly, Figure 4 shows that the name recall rate increases faster and to a higher rate with the higher rank models, as expected.\n\nMentions:\n- Similarly, Figure 4 shows that the name recall rate increases faster and to a higher rate with the higher rank models, as expected.\n\nOCR Text: \"Full rink rns=5 Talk=I0 Tank=15 Tak= Fnk_IOD Finlaly Training Sters\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4. Comparison of name recall rate performance for Adam optimization with different ranks."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In the most general case it is easy to obtain values of the CG coefficients leading to a value of sin θ 13 compatible with the recent fits in [4]. In fact we find several hundred possible combinations. We therefore restrict ourselves in the following to some well motivated cases. We give a graphical summary of the results at the end of this section in Fig. 1.\n- The results of the T2K and MINOS experiments [1,3] and of the global analysis of the neutrino oscillation data [4] have important implications for the Dirac CP violation in the lepton sector when the unitary mixing matrix U † ν , originating from the diagonalisation of the neutrino mass matrix, is (up to CP violating diagonal phase matrices) of BM or TBM form, while that originating from the diagonalisation of the charged lepton mass matrix  Figure 1: Graphical presentation of the results obtained. The vertical straight (dashed) lines denote the 1σ (2σ) allowed ranges of sin θ 13 taken from [4] (see also Table 7). The yellow, green, blue, red bins correspond to the results from Table 3   [4] and used in our analysis. The results quoted were obtained using the \"old\" reactorν e fluxes (see [4] for details).\n\nMentions:\n- We give a graphical summary of the results at the end of this section in Fig. 1.\n- The results of the T2K and MINOS experiments [1,3] and of the global analysis of the neutrino oscillation data [4] have important implications for the Dirac CP violation in the lepton sector when the unitary mixing matrix U † ν , originating from the diagonalisation of the neutrino mass matrix, is (up to CP violating diagonal phase matrices) of BM or TBM form, while that originating from the diagonalisation of the charged lepton mass matrix  Figure 1: Graphical presentation of the results obtained.\n\nOCR Text: \"Sin i = Q10 012 0.1 018\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Graphical presentation of the results obtained. The vertical straight (dashed) lines denote the 1σ (2σ) allowed ranges of sin θ13 taken from [4] (see also Table 7). The yellow, green, blue, red bins correspond to the results from Table 3, 4, 5, 6."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- More interesting results are shown in Table 1, where the three machine learning based approaches are directly compared against the non-linear bid shading used in production on surplus, spend, win rate and CPM metrics. Across the board we can see that each machine learning based approach outperforms the nonlinear bid shader on surplus, total spend and win rate. The CPM metric is naturally increased with the increase of spend and win rate and in production environment its value is controlled in several goal type lines such as eCPM which we will show in our online experiments. Moreover we additionally control the balance between spend and CPM increase through hyperparameters of the algorithms (in particular the γ parameter) which we discuss in Section 5.3.3. The highest surplus and win rate improvements are obtained with the factorization machine approach yielding second highest spend at a bit higher CPM increase than of the linear regression.  the platform with their prevalence on the test set are None goal type with 40.5% and five CP-X goals -CPC (click) with 12.12%, CPA (action) with 38.95%, CPCV (completed view) with 0.57%, CPViewI (viewed impressions) with 0.78% and eCPM (mille) with 3.60%. Again, we separate the evaluation of algorithms into regression and platform performance metrics and show the performance of the proposed FM model as the best performing model. Regression metrics per goal type are shown in Fig. 4. We can see that regression performance does vary per line type, where the performance is most aligned with the overall results for two dominant goal types -None and CPA. It is expected for the performance numbers to drop for goal types that have a very small yet non-negligible share of the serving opportunities due to the inherent noise of the problem. Furthermore, once we analyze the performance of the FM-based bid shading approach against the nonlinear production baseline across surplus, total spend, win rate and CPM we can clearly see that there are improvements on the key metrics. Moreover, as the highest increase in surplus we observe is for the CPA goal which typically has the highest prices, we can concur that better shading can deliver gains in practice for advertisers targeting sophisticated, rare, and high-priced goals. But also for goal types with smaller values such as eCPM, or even CPC, where shading has fewer degrees of freedom to pick and chose how to adjust the price we can clearly see that there is added value. As a result of this analysis, we can conclude that the FM model provides a clear advantage over the production model across different goal types.\n\nMentions:\n- Regression metrics per goal type are shown in Fig. 4.\n\nOCR Text: \"E Ae5 =a AeE a Meees\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4: MSE and r2 performance metrics for the Factorization Machine algorithm broken down by the goal type."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Fig. 4, we analyze how the Nakagami fading parameter m and lognormal parameters µ and ζ impact the outage performance. The SBS density λ S is set to 50 and SIR threshold is varied from 1 dB to 10 dB. It can be seen that the existance of shadowing greatly affect the MU's outage performance when it is offloaded to SCN. For an increase of shadowing standard deviation from 4 dB to 8 dB, we observe a significant deterioration in the SIR coverage.\n\nMentions:\n- In Fig. 4, we analyze how the Nakagami fading parameter m and lognormal parameters µ and ζ impact the outage performance.\n\nOCR Text: \"Eu -%k IAJ = _e MR ~abud Luxj\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4. Impact of Nakagami-lognormal shadowing on offloaded MU’s outage performance with different fading parameters."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In figure 7 we show the time derivative of the relative temperature change from Figure 6, thus the rate of temperature change as a function of time during the ion pulse, t/∆t. Here, positive values of the derivative indicate heating and negative values show net cooling. This derivative is useful in highlighting a maximum cooling rate for a ratio of cooling time to pulse duration of τ/∆t=0.3. Note that at small τ/∆t (small hole radii R) the heating balances cooling, so that the temperature follows the ion beam intensity, which is maximum at t/∆t =1/2, whereas at large τ/∆t (large hole radii R) cooling is negligible and the maximum temperature just depends on the accumulated fluence, and so the temperature is maximum at the end of the pulse t/∆t =1. In the latter case the maximum temperature is just Tmax1.\n\nMentions:\n- In figure 7 we show the time derivative of the relative temperature change from Figure 6, thus the rate of temperature change as a function of time during the ion pulse, t/∆t.\n\nOCR Text: \"tyat=L,01,.03,0.1,0.3,1,3,10,30,100|\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 7. Evolution of derivative of normalized target temperature d(∆kBT/kBTmax1) /d(t/∆t) for nine values of τ/∆t, the ration of the cooling time to the pulse duration."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In the preceding discussion we have focussed on cases where a single molecule connects the two electrodes. However in statistical STM break junction experiments such as that of Hihath et al. [26] the electrodes can also be bridged simultaneously by two (or more) molecules in parallel. Here we report theoretical results for the inelastic contributions to the conductances of some examples of systems of this kind. We show a pair of top site bonded molecules having no hydrogen atom attached to either sulfur atom in Fig 1 (c) respectively) in Fig. 3. As can be seen in Fig. 3, the calculated inelastic tunneling spectra for these pairs of molecular wires are very similar to the spectra of the individual wires apart from small frequency shifts and splittings due to the weak interactions between the molecules. It is worth noting that although one of the molecules in Fig 1(e) is poorly bonded to one of the gold electrodes and contributes little to the low bias elastic conductance of the system (as has been discussed in Section III B) there is still a significant splitting of the mode I frequency for this two-molecule junction in Fig. 3 due to the coupling between the vibrational modes of the two molecules.\n\nMentions:\n- We show a pair of top site bonded molecules having no hydrogen atom attached to either sulfur atom in Fig 1 (c) respectively) in Fig. 3.\n- As can be seen in Fig. 3, the calculated inelastic tunneling spectra for these pairs of molecular wires are very similar to the spectra of the individual wires apart from small frequency shifts and splittings due to the weak interactions between the molecules.\n- It is worth noting that although one of the molecules in Fig 1(e) is poorly bonded to one of the gold electrodes and contributes little to the low bias elastic conductance of the system (as has been discussed in Section III B) there is still a significant splitting of the mode I frequency for this two-molecule junction in Fig. 3 due to the coupling between the vibrational modes of the two molecules.\n\nOCR Text: \"1atenJ-ter':M\" 320 2*) %0 Ze0 Jj] J u 24C 261 39) 40 422 441 4 Hm-7 #xueas) Teh T0 Kut: FneCS elc\" Feli_ EtII Wccell + 2 %C Denjhme | 100=\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 3. (Color online.) Calculated inelastic tunneling spectra for one and two PDT molecules with different numbers of thiol hydrogen atoms bridging pairs of 14 atom gold clusters in the fully relaxed geometries depicted in Fig. 1. Labels (a) – (f) denote the structures (a) – (f) of Fig. 1. The atomic motions in modes I – IV are similar to those shown for modes I – IV of the single molecules in Fig. 2(a)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We compare our estimates with observations from the Kepler mission using the NASA Exoplanet Archive 2 . If the Kepler sample is mostly unbiased and terrestrial planets have the same distribution of orbital inclinations as giant ones, the ratio of stars with planet candidates within the HZ to the total number of stars with planet candidates should represent an 2 http://exoplanetarchive.ipac.caltech.edu estimate of h G (if all stars host at least one planet) or an upper limit to it. We first select all stars observed by Kepler with at least one confirmed or candidate planet and T eff ≤ 7200 K, in accordance with Sec. 2.2. After taking multiplanetary systems into account, we find 3570 such objects in the database. This corresponds to 3924 stars, after correcting for the fraction of stars with &lt; 1.5 M to the total and assuming a Kroupa (2001) IMF for simplicity. We repeat the procedure adding conditions for the planet's radius (&lt; 2R ⊕ ) and insolation (between 0.22 and 1.13 times that of Earth) consistent with Sec. 2.2, and find 44 candidates. This corresponds to a ratio of ∼1%, slightly higher but broadly consistent with our Case 2 estimate (assuming a mass of 5.2 × 10 10 M for the MW disk; Licquia &amp; Newman 2015). This is not unexpected, since our model uses parameters derived from the Kepler sample. It would however tend to validate the use of Case 2 and our assumptions for galaxy evolution, as well as our chosen combination of f T,0 and β P . On the other hand, the Case 1 prediction lies significantly below the observed ratio, and can be reconciled only if f T,0 = 1, i.e., if all stars of solar metallicity or greater host a terrestrial planet, in conflict with observations. In panels C and D of Fig. 3 we show the redshift dependence of h G , integrated over the mass range 10 9 − 3 × 10 11 M . The evolution of the integrated galactic habitability is monotonic with time for both SF and passive galaxies, and in both cases can be broadly divided into an early linear rise followed by a plateau in which h G increases by no more than 0.1 dex, with the transition happening at z &gt; 1.5 for passive galaxies and z &lt; 0.5 for SF galaxies. We note that for Case 1 the integrated habitability is very sensitive to the number ratio of low-mass, low-metallicity (and hence low-h G ) galaxies to high-mass, high-metallicity (and high-h G ) ones. Therefore, the apparent decrease in habitability at z &lt; 0.2 for SF galaxies shown in Fig. 3, panel D, is likely due to our use of different GMFs for z ∼ 0 and z &gt; 0.2 populations. On the other hand, extending our SFHs to future epochs, and assuming that the FMR holds, we find that the habitability of SF galaxies for Case 2 will asymptotically approach that of passive galaxies, increasing by just ∼0.02 dex over the next 5 Gyr. For the total galaxy population, the shift occurs at z ∼ 1, i.e., after the peak of SFR in galaxies (and thus of cosmic SFR; e.g., Madau &amp; Dickinson 2014, and references therein). Since the stellar density does not appear to have changed significantly since this epoch as well, this implies that the total number of habitable planets in a given cosmological volume has remained roughly constant since z ∼ 1, i.e., in the last 7-8 Gyr. In the local universe, most of the stellar mass is held in large passive galaxies that formed at z &gt; 1. This suggests that most of the habitable planets in the present epoch should belong to stars older than our own. Indeed, as shown in Fig. 4, the median age of stars hosting habitable planets at z = 0 is ∼9.5 Gyr, 5 Gyr older than our own Sun and close to the estimate of Behroozi &amp; Peeples (2015). On the other hand, if we only consider SF galaxies the median age drops to ∼6 Gyr, or ∼1.5 Gyr older than the Sun, which formed after ∼70% of stars in disks that host habitable planets at z = 0, in agreement with the earlier estimate of Lineweaver (2001). Unlike the work presented here, the studies referenced above did not consider stellar death. However, the shape of the IMF implies that most planets orbit long-lived subsolar mass stars, greatly mitigating its effect. On the other hand, the impact of stellar death would be more important when considering timescales longer than the Hubble time or non-standard IMFs. Finally, we note that we might be overestimating the number of presently habitable planets formed near the peak of SF in passive galaxies (z ∼ 2, or ∼10 Gyr ago) as a result of the simplistic assumptions on galaxy structure that our model uses, as stated in Sec. 2.3.1.\n\nMentions:\n- Indeed, as shown in Fig. 4, the median age of stars hosting habitable planets at z = 0 is ∼9.5 Gyr, 5 Gyr older than our own Sun and close to the estimate of Behroozi &amp; Peeples (2015).\n\nOCR Text: \"Qossius lime (6-)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4. Distribution of the ages of habitable planets per unit volume at z = 0, for SF (blue) and passive (red) galaxies and with the total distribution shown as a black curve. The dotted lines show the respective median ages, while the dashed gray line marks the formation time of the solar system."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 4 describes an analysis of three different users of the Breadcrumbs dataset (User 1 is at the top, User 2 is in the middle and User3 is at the bottom). We can see on the left side, the evolution of the spatio-temporal entropy according to the hours of one day, and on the right side, the evolution of the spatio-temporal entropy according to the days of one week. On the left side of the figure, if we look at the sub-figure of the User 2 in the middle, the entropy constantly increases and rapidly decreases after (maybe when the user reaches the campus) during the morning. We observe that there are two peaks, in this same sub-figure, that could potentially highlight lecture breaks and the end of the day is relatively calm and constant. On the right side of the figure, the three sub-figures clearly show different weekly mobility behavior, the week of User 2 in the middle is very regular compared to User 1 and User 3 for example. In addition, Figure 5 highlights the significance of the smooth co-variables for each user by looking at the p-value. Without any surprise, the co-variable hourN b is significant for all users, whereas dayN b is only significant for User 1 and User 3.\n\nMentions:\n- In addition, Figure 5 highlights the significance of the smooth co-variables for each user by looking at the p-value.\n\nOCR Text: \"Approxinate significonce of stosth terrs; edf Ref,af 3-value User s(hourHb) 12.311 15.019 11.,203 s(daydb) 5,553 5,631 1.1e-65 Approximate significence of siooth terrs: edf Ref . dF p- wolue User 2 s(hourlb} 19,57 24,51 6,38 42-16 s(daykb) 1.02 1 0.1 0,718 Approxirate 5\"gni ficance of sroot^ terms: edf Ref .df p-volue User 3 s(hourkb) 17,39 28.849 11,503 22-16 s(daylb) 5.24   5.792   3.864 0,C8808 2e-16\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 5. Significance of the smooth terms for the three users"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- According to Broy et al. Interaction between objects occurs through the exchange of messages, as shown in Figure 3. Let MSG be an element of the system model, denoting the set of all possible messages in a system. An object with identifier id ∈ ID accepts a unique set of messages. Its input interface is defined by msg id ⊆ MSG The behavior of an object is the relationship between the sequences of messages it receives and the sequences of messages it emits as a reaction to incoming messages. We allow our objects to be nondeterministic, such that more than one reaction to an input sequence is possible. According to Broy et al. (1993); Broy &amp; Stølen (1994), the set of timed communication histories over M is denoted by M ω . A communication history is basically an infinite sequence containing (possibly only a finite number of) messages and time stamps inbetween, that mark time progress. Thus the messages occurring in a communication history are in linear order. A communication history models the observable sequence of incoming or outgoing messages of one object. The behavior of a nondeterministic object id is then given by the mapping of its input stream to the set of possible ouput streams. Using relations, the behavior of an object id is given by the relation between its input and output streams behavior id ⊆ msg ω id × MSG ω Objects encapsulate data as well as processes. Encapsulation of data means that the state of an object is not directly visible to the environment, but can be accessed using explicit communication. Encapsulation of process means that the exchange of a message does not imply the exchange of control: each object can be regarded as a separate process. Note, that this view on object controls also works in conventional sequential programs. Objects get active when receiving a message and fall asleep after emitting a message. Given the set of possible states STATE of objects in a system, the function states assigns a subset of possible states to every object id:\n\nMentions:\n- According to Broy et al. Interaction between objects occurs through the exchange of messages, as shown in Figure 3.\n\nOCR Text: \"(hlic AtstPt Objcat ~mmmauon memim {Int\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Objects in the UML system model"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Here, we study the effect of measurement projection noise with a full Monte-Carlo simulation of QAOA on some example graphs, where objective function is evaluated by repeated projective measurements until its error is below a threshold. More implementation details of this numerical simulation are discussed in Appendix F. In Fig. 7, we present the Monte-Carlo simulation result for the example instance studied earlier in Fig. 6(b). Here, we simulate QAOA by starting at either level p = 1 or p = 5, and increasing to higher p using our FOURIER heuristics. The initial parameters at p = 1 and p = 5, respectively, are chosen based on known optima found for smaller size instances. We see that QAOA can find the MaxCut solution in ∼10-10 2 measurements, and starting our optimization at intermediate level (p = 5) is better than starting at the lowest level (p = 1). In comparison, Full Monte-Carlo simulation of QAOA accounting for measurement projection noise, on the example 14vertex instance studied in Fig. 6(b). For various optimization strategies, we keep track of approximation ratio ri = |Cuti|/|MaxCut| from the i-th measurement, and plot minimum fractional error 1 − ri found after M measurements, averaged over many Monte-Carlo realizations. The blue solid and red dashed lines correspond to QAOA optimized with the FOURIER strategy starting with an educated guess of ( γ, β) at p = 1 and p = 5, respectively. The orange dash-dot line corresponds to QAOA optimized starting with random guesses at p = 1. The three labelled dotted lines are results from QA with total time T = 10, 10 2 , 10 3 . random choices of initial parameters starting at p = 1 perform much worse, which fails to find the MaxCut solution until 10 3 -10 4 measurements have been made. Moreover, when we compare QAOA to QA with various annealing time, it appears that the choice of annealing time T = 100 can perform just as well as QAOA on this instance. Nevertheless, running QAOA at level p = 5 is still more advantageous than QA at T = 100, when coherence time is limited.\n- Using the approach described above, we simulate experiments of optimizing QAOA with measurement projection noise for a few example instances, with various choices of precision parameters ( , ξ, δ) and initial points. For the representative instance studied in Fig. 7, we set = 0.1, ξ = 0.05, and δ = 0.01. In each run of the simulated experiment, we start with QAOA of level either p = 1 or p = 5, and optimize increasing levels of QAOA using our FOURIER[∞, 0] heuristic strategy. The initial point of QAOA optimization is either randomly selected (when starting at p = 1) or chosen based on an educated guess using optimal parameters from small-sized instances (at p = 1 and p = 5). Specifically, the educated guess for the initial points we use are ( u 0 , v 0 ) = (1.4849, 0.5409) at level p = 1, and u 0 = (1.9212, 0.2891, 0.1601, 0.0564, 0.0292),\n\nMentions:\n- In Fig. 7, we present the Monte-Carlo simulation result for the example instance studied earlier in Fig. 6(b).\n- For the representative instance studied in Fig. 7, we set = 0.1, ξ = 0.05, and δ = 0.01.\n\nOCR Text: \"Qiu: _lxax] 3l* Q10\" etazd zuess Qhu: Ixchol extiled Siate % 1C * numk:~~ cf measutemcnt; Li 6 5\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 7. Full Monte-Carlo simulation of QAOA accounting for measurement projection noise, on the example 14- vertex instance studied in Fig. 6(b). For various optimization strategies, we keep track of approximation ratio ri = |Cuti|/|MaxCut| from the i-th measurement, and plot minimum fractional error 1 − ri found after M measurements, averaged over many Monte-Carlo realizations. The blue solid and red dashed lines correspond to QAOA optimized with the FOURIER strategy starting with an educated guess of (~γ, ~β) at p = 1 and p = 5, respectively. The orange dash-dot line corresponds to QAOA optimized starting with random guesses at p = 1. The three labelled dotted lines are results from QA with total time T = 10, 102, 103."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Cross language word recognition framework uses a lexicon matching based approach which creates a problem when our number of query words is a bit high. Here we adapt our cross language framework for lexicon free information retrieval through word spotting by HMM based scoring [19]. The word spotting procedure is shown in Fig. 11.\n\nMentions:\n- The word spotting procedure is shown in Fig. 11.\n\nOCR Text: \"Upumaxt tlx bphpnp Knlivzhkiy M] Kht Ukeyli; Kx TtJm #I]= Wa Wr\"ilru \"Ir1 sm khj\"\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 11. Word spotting procedure using zone segmentation."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- ) Fig. 2 shows the comparison between the theoretical bound and the proposed OBC in terms of the embedding capacity. It is observed that, the curve of OBC is rather close to that of the theoretical bound. Based on the above analysis, we are to construct the codewords for C = {m 1 (k) , m 2 (k) , ..., m q + (k) }. At first, we collect all binary codewords with a length of exactly log 2 q + , denoted by C q , e.g., if q + = 6, the collected codewords will be C q = {\"00\", \"01\", \"10\", \"11\"}. Then, according to Eq. (22), we randomly select n 2 /2, i.e., q + − 2 log 2 q + , codewords from C q to generate n 2 new codewords by appending \"0\" and \"1\" to the end of the selected codewords. And, the rest non-selected codewords are kept unchanged to constitute n 1 new codewords. In this way, we can finally construct the codewords for C. For example, if q + = 6, we can select {\"00\", \"01\"} out from C q , and obtain {\"000\", \"001\", \"010\", \"011\"} by appending \"0\" and \"1\" to the end of \"00\" and \"11\". We then use {\"10\", \"11\"} in C q and {\"000\", \"001\", \"010\", \"011\"} to construct the codewords as C = {\"000\", \"001\", \"010\", \"011\", \"10\", \"11\"}. Therefore, we can finally construct the required OBC codewords.\n\nMentions:\n- ) Fig. 2 shows the comparison between the theoretical bound and the proposed OBC in terms of the embedding capacity.\n\nOCR Text: \"1 1 1 Mntri Eiul * Vmchi\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2. Comparison in terms of the embedding capacity."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where the poloidal component of the equilibrium magnetic field has been neglected compared to the toroidal component. Eq. (66) clearly demonstrates the relation between radial component of the displacement vector and the radial mode structure plotted in figure 7a. Figure 8 illustrates the rate at which the frequency chirps. It is shown that the square root dependency holds for the very early stages of frequency chirping.\n\nMentions:\n- Figure 8 illustrates the rate at which the frequency chirps.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 8. The solid line represents the time evolution of the mode frequency in the hard nonlinear regime and the dashed line, plotted for comparison, corresponds to the square root dependency."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Using our ancillary data we can attempt some interesting comparisons to measurements at other wavelengths in the hope they may shed light on the origin of the gas. Perhaps the simplest is the relation between the molecular gas luminosity (L ′ CO ) and the total infrared luminosity (L IR ), since it is likely these emissions are indeed spatially co-located. Several sources of infrared imag-ing are available for this cluster: Herschel Space Observatory, Spitzer Space Telescope and the James Clerk Maxwell Telescope. We reiterate that in the highest resolution infrared imaging available (Spitzer Space Telescope MIPS) there is only one infrared source within the 25 arc second LMT beam, and therefore it is unlikely that the CO emission is coming from multiple sources separated by more than a few arc seconds. The infrared luminosity was measured in Webb et al. (2015a) by fitting a template spectral energy distribution (SED) to the 24/250/350/450/500/850µm flux measurements. In Fig. 4 we show the position of the SpARCS1049 BCG system on the L ′ CO −L IR plane, compared to several other measurements of field galaxies at z &gt; 1 and lower redshift BCGS, drawn from the literature. We see that although SpARCS1049 is extreme in its properties, in that it is relatively IR-and CO-bright, it lies within the distribution of other infrared luminous galaxies at similar redshifts, inside and outside of clusters.\n\nMentions:\n- In Fig. 4 we show the position of the SpARCS1049 BCG system on the L ′ CO −L IR plane, compared to several other measurements of field galaxies at z &gt; 1 and lower redshift BCGS, drawn from the literature.\n\nOCR Text: \"13,5- ~nd' c4u HG:<t 13,03 ~ielil<< ; 12.5 120= 11.5- 8 110 105 10.0 8,0 9,5 10.0 10,5 1L0 115 log Lcc [K km; [c] 1\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4. The relation between LCO and LFIR. Included are systems between 1 < z < 2 with published CO measurements (grey points)(Carilli & Walter 2013, and references therein) and lower redshift (z < 0.6) IR-bright BCGs (Edge 2001; O’Dea et al. 2008; Rawle et al. 2012) We take the lowest CO transition available and correct it, following Carilli & Walter (2013), to CO(1-0). The two dotted lines correspond to the best fit relations for main sequence galaxies (blue) and starburst galaxies (yellow) of Daddi et al. (2010b) and Genzel et al. (2010). The solid black star indicates the values measured for the SpARCS1049 BCG system."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Our goal is to automatically build a supply chain for a criminal forum by analyzing the posts and replies. To build a supply chain, we need a chronological record of which products the users of a forum are buying and selling. We use automated classifiers to categorize the posts and replies, then build an interaction graph and use the graph to build the supply chain (Figure 1).\n\nMentions:\n- We use automated classifiers to categorize the posts and replies, then build an interaction graph and use the graph to build the supply chain (Figure 1).\n\nOCR Text: \"(wj Hs HMFUH JL J6 (Haxu!p : |hl I Ranj k Rxu\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Our supply chain detection approach."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In fact, some candidates may not display any particularly important moments during the answer to the first question. Figures 3 and 4 describe how long and when the peak with the largest amplitude occurs during an interview. It is interesting to note that the duration of the important slices extracted by the attention mechanism follows a very similar distribution to the duration of facial expressions which typically lasts between 0.5 and 4s [23]. Moreover, it seems they occur more often at the beginning and at the end of an answer. Such cues could indicate that non verbal behaviours occurring at the beginning (turn taking) and at the end of the answer (turn giving) have a strong impact on recruiter's evaluation as in other face to face interactions [3], [14].\n\nMentions:\n- Figures 3 and 4 describe how long and when the peak with the largest amplitude occurs during an interview.\n\nOCR Text: \"1 Duration cf attention slices seconds\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3: Histogram of the duration of attention slices"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Considering the scaling up of distributed stream processing systems, the failure rate of the system increases with the number of nodes in the system. For example, in Exascale systems multiple failures are expected everyday [37,2] and MTTF is anticipated to be in minutes [31,4]. For systems such as Flink where a failure of a single node results in restarting the whole application from the previous checkpoint, the failure rate of the system is n i=1 λ i , where λ i is the failure rate of node i [28]. Fig. 13 shows how the failure rate changes with the number of nodes in the system considering the failure rate of all nodes is 0.0022 per hour. The figure also shows the percentage utilization increase we obtain using T * instead of using 30 minute checkpoint interval for R = 30s, c = 5s, δ = 50ms and n = 5. As the number of nodes increases, failure rate increases and the utilization increase we obtain also increases significantly. For example, as indicated in the figure, we can achieve 68.8% utilization increase for 1000 nodes and 226.83% for 2000 nodes. Furthermore, as the checkpointing cost increases, percentage utilization increase reduces as well. Utilization of a streaming application decreases with the increase of n. Although real-world streaming applications can be represented using a considerable low n value, for applications with a large n value, utilization will have a significant impact due to δ and n. Fig. 14 indicates how the utilization decreases for large n values. The figure shows the utilization using the model expressed in Eq. 7 for T * , R = 30s, c = 10s, δ = 5s and λ = 0.005 per minute. As indicated in the figure for large n, the utilization comes close to 0. For example, in the figure utilization is 0.0018 for n = 15000. Therefore, large-scale applications with large n and δ values can incur significant overheads due to the checkpointing process. Fig. 15 shows the values of T * given by our model, Daly's first order model [9], 2c( 1 λ + R) and Zhuang et al.'s model [39], 2c( 1 λ + R) + c 2 . For the model of Zhuang et al., we assumed maximum processing rate and the average input rate of the system is same. All the models give near similar results for smaller c and R values as shown in Fig. 15a. However, as the values of c and R get bigger, our model deviates from the other models for larger λ values. This is because the assumptions made in Daly's first order model are not accurate for large λ values as stated in Daly's paper [9]. Moreover, we calculated the utilization based on Eq. 7 for the proposed T * and the optimal values given by Daly and Zhuang et al. presented in Fig. 15b for δ = 30 seconds and n = 25. Fig. 16 shows the percentage utilization increase gained using the proposed T * compared to other models. As indicated in the figure, the utilization increase we can gain using the proposed model becomes more significant as λ increases. For example, when λ = 11, the utilization increase gained using the proposed T * compared to the values given by Daly's model and Zhuang et al.'s model is 2.3% and 3.7% respectively.\n\nMentions:\n- Although real-world streaming applications can be represented using a considerable low n value, for applications with a large n value, utilization will have a significant impact due to δ and n. Fig. 14 indicates how the utilization decreases for large n values.\n\nOCR Text: \"J0Z05\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 14: Decrease in utilization with the increase of n using T ∗."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In this section we discuss the K-band TFR of ETGs, compare it to the one derived using CO kinematics by Davis et al. (2011) and compare our results to the TFR of spiral galaxies. The relevant data used are summarised in Table 1. Figure 6 shows the H i TFR as found from our data. For comparison, the figure includes the early-type galaxies whose circular velocity is estimated using planetary nebulae by Cortesi et al. (2013). Although we do not use these galaxies in the formal TFR fits below, the figure shows that the two samples are fully consistent.\n- Another interesting exercise is to compare our data with the TFR found for later-type galaxies. Davis et al. (2011) reported an offset between their CO TFR for ETGs of the ATLAS 3D sample with the one for spiral galaxies in the sense that for a given circular velocity, ETGs are fainter. Our data also indicate such an offset (Fig. 6). However, since our H i TFR for early-type galaxies is slightly offset from the one based on CO data in the other direction, the offset from the spiral relation is somewhat smaller.\n\nMentions:\n- Figure 6 shows the H i TFR as found from our data.\n- Our data also indicate such an offset (Fig. 6).\n\nOCR Text: \"Tuli * Pieie6 kaH; (kitt\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 6. The HiTully-Fisher relation for our 16 galaxies (red circles;W = 2vcirc). The black symbols give the data based on planetary nebulaefrom Cortesi et al. (2013) for comparison. The CO TFR of Davis et al. (2011) is given by the drawn line. The dashed line gives the Tully & Pierce (2000)K-band TFR for spiral galaxies."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The average training MSE was recorded to be 0.1485 across all the applicability predictor networks. To test the applicability predictors each predictor was tested on a class from each objective subset that the predictor was not trained on. For the test data, the recorded MSE was 0.4889. We report the training and testing mean squared (MSE) error for each layer in table. III. Applicability for the Natural Known set. This set tends to be more compact as the network should be applicable to all the classes. Top Right: Applicability for the Man-Made set. We can see that the applicabilities cover a wider range than for the other classes. This indicates that the natural features are very applicable to some man-made objects. Bottom: Applicability for the Natural Unknown set. This set is more spread out compared to its known counterpart but also more compact than the man-made set. Fig. 5. Test results for the applicability predictor. The solid lines are the the average predicted image applicability across all test images for each class. The dashed lines are the actual class applicability computed in the 30v20 testing. Fig. 5 shows the layer by layer predicted applicability versus the actual applicability metric we computed for that class. We can see in the graph that our applicability predictors are able to produce an image applicability that is very close to its true value. This suggests that our applicability networks are able to distinguish if the CNN has or has not been trained on an image even if they are both highly applicable. In Table IV, we give sample results from the conv4 applicability predictor. Interestingly the predictions on the man-made dataset tend to have the most variance, but none of the predictions overshoot the target value by very much.\n\nMentions:\n- This set is more spread out compared to its known counterpart but also more compact than the man-made set. Fig. 5.\n- Fig. 5 shows the layer by layer predicted applicability versus the actual applicability metric we computed for that class.\n\nOCR Text: \"WatriluiINh Matrlltin ee WalutoB Un Vil-I3d Jed \"EuMOE Hilc cp-o\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 5. Test results for the applicability predictor. The solid lines are the the average predicted image applicability across all test images for each class. The dashed lines are the actual class applicability computed in the 30v20 testing."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 2 compares the performance of Anytime WA* (AWA*) to the performance of A* with Partial Expansion and Enhanced A* in aligning five sequences from a set of dissimilar (and thus difficult to align) sequences used in earlier experiments (Kobayashi &amp; Imai, 1998). The cost function is Dayhoff's PAM-250 matrix with a linear gap cost of 8. The admissible heuristic is the standard pairwise alignment heuristic, and the (almost negligible) time needed to compute the heuristic is included in the running time of the search.\n- All three algorithms require much less memory than standard A* in solving this problem. We found that a good weight for Anytime WA* in solving this test problem is 100 99 , that is, the g-cost is weighted by 99 and the h-cost is weighted by 100. (Because the cost function for multiple sequence alignment is integer-valued, we use a weighting scheme that preserves Figure 2: Average performance of search algorithms in aligning sets of 5 sequences from Kobayashi and Imai (1998).\n- Figure 2 shows that Anytime WA* runs more than seven times faster and stores about the same number of nodes as PEA* with a cutoff of C = 0. When PEA* uses a cutoff of C = 50, it stores 44% more nodes than Anytime WA* and still runs 65% slower on average. Although Enhanced A* runs about as fast as Anytime WA*, it stores 36% more nodes. Anytime WA* stores fewer nodes because continuation of weighted search results in discovery of improved solutions that provide tighter upper bounds for pruning the Open list. In summary, Anytime WA* not only outperforms standard A* in solving this problem, it performs better than two state-of-the-art enhancements of A* that were specifically designed for this problem.\n\nMentions:\n- Figure 2 compares the performance of Anytime WA* (AWA*) to the performance of A* with Partial Expansion and Enhanced A* in aligning five sequences from a set of dissimilar (and thus difficult to align) sequences used in earlier experiments (Kobayashi &amp; Imai, 1998).\n- (Because the cost function for multiple sequence alignment is integer-valued, we use a weighting scheme that preserves Figure 2: Average performance of search algorithms in aligning sets of 5 sequences from Kobayashi and Imai (1998).\n- Figure 2 shows that Anytime WA* runs more than seven times faster and stores about the same number of nodes as PEA* with a cutoff of C = 0.\n\nOCR Text: \"[ae Wutigto: IC Pj\"ca6 Rcal P56\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: Average performance of search algorithms in aligning sets of 5 sequences from Kobayashi and Imai (1998)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In a portfolio, individual capital gain values and individual capital loss values are usually distinct. Though rare, identical capital gains and capital losses are possible. Identical capital gains or losses are possible for portfolios built using options. We are ignoring option premiums. That is, asset purchases may be done via the exercise of cash-covered American-style put options. Also asset sales may be done via the exercise of American-style covered-call options. In these cases with options that become inthe-money, a portfolio manager has no control of the asset sales or purchases or timing of such trades. See Figure 1.\n\nMentions:\n- See Figure 1.\n\nOCR Text: \"H dui; dui JAk ;; '#L4zl |; 3 .Md  WZJi@1 \"tket sHErix  @u) k Ireflni   wluc JU   4vall zigeq 134 #Ltjol Futzzoi;:  sda ALyz% #JsLjMj bukeu' xULsu:  4424:: HLan i \"etllund   Wtjc JU   zivdwl  zigso   )lai ni:] @I? Wf Ths Ixjtsc TuqIsl: xaf Fsdsrl\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: A Potential Wash Sale with American-style Options"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Reconfiguration of a DRT system typically involves changing one configuration -a network of communicating and concurrently executing components of the system -into another. Software components can also migrate between networked computers. The computers and their network constitute the hardware platform of the system, which does not change. We can represent the system using a simple framework (see Figure 1). The application layer consists of those components of the system and their communication connectors (i.e. objects and links) that are the focus of reconfiguration. The location layer (which does not change) is used to represent those components and connectors (i.e. nodes and channels) that are necessary in order to describe the migration of objects and links. Thus, reconfiguration can be represented as the creation and deletion of objects and links, and as changes in the mappings between objects and links, objects and nodes, links and channels, and links and nodes.\n- The formalism must be able to express the following: 1) Reconfiguration of Components: it must be possible to express the creation, deletion and replacement of objects, and also the migration of objects between nodes (see Figure 1).\n\nMentions:\n- We can represent the system using a simple framework (see Figure 1).\n- The formalism must be able to express the following: 1) Reconfiguration of Components: it must be possible to express the creation, deletion and replacement of objects, and also the migration of objects between nodes (see Figure 1).\n\nOCR Text: \"Wcdedly Cetcctets Clm Kopicaicn Cojects + Links Lajar Hardm Loczuc) Layer Ncce5 Char nels\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1. Framework for Dynamic Reconfiguration"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The estimated sensitivity for the (combination of) non-unitarity parameters |ε ee | + |ε µµ | and |ε τ τ | is shown in Fig. 1, at the 90% confidence level, by the solid yellow line. The solid blue line denotes the present constraints from LEP and the dashed blue line denotes the limit from the present theory uncertainty. The sensitivity of the CEPC to |ε ee | + |ε µµ | of ∼ 2 × 10 −4 can be translated via eq. ( 6) into a mass for the \"sterile\" or \"right-handed\" neutrino of ∼ 12 TeV, for Yukawa couplings of order one.\n\nMentions:\n- The estimated sensitivity for the (combination of) non-unitarity parameters |ε ee | + |ε µµ | and |ε τ τ | is shown in Fig. 1, at the 90% confidence level, by the solid yellow line.\n\nOCR Text: \"[O\"- Cte Jort 10 + 10-6 10- 10 [U . I0- Fce-F%ul 10-'\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1. Exclusion limits on the |εττ | and |εee|+ |εµµ| at 90% confidence level (updated version of the figure from reference2). The solid blue line denotes the present experimental precision of the EWPOs from LEP, while the dashed blue line denotes the present theory uncertainty. The solid yellow line corresponds to the estimated precision of the CEPC."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The resulting trajectories we proposed are illustrated in Figure 2, with a = 10, b = 1.5. It can be observed that the robot successfully tracks the first object up to time t s = 5s, gradually switching to the second object until t f = 15s, and track the second object until simulation stops. Particularly, the randomly picked starting positions (highlighted using asterisk) for the two objects are [−5, −5] and [5, −3] and the agent is positioned randomly near the starting position, which is [−5, 4]. We set t 0 = 0s and the total simulation time is 20s. For this implementation, the differential equation ( 10) is solved based on an explicit Runge-Kutta (4, 5) formula, the Dormand-Prince pair. Fig. 2: Trajectory of the optimal solution y * (t) (dashed green), the robot (solid red) and the objects (black for 1 and blue for 2). The robot converge to the target trajectory, which is to track the first object from [0s, 5s] and gradually switch to track the second object in [5s, 15s].\n\nMentions:\n- The resulting trajectories we proposed are illustrated in Figure 2, with a = 10, b = 1.5.\n- For this implementation, the differential equation ( 10) is solved based on an explicit Runge-Kutta (4, 5) formula, the Dormand-Prince pair. Fig. 2: Trajectory of the optimal solution y * (t) (dashed green), the robot (solid red) and the objects (black for 1 and blue for 2).\n\nOCR Text: \"Mi\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2: Trajectory of the optimal solution y∗(t) (dashed green), the robot (solid red) and the objects (black for 1 and blue for 2). The robot converge to the target trajectory, which is to track the first object from [0s, 5s] and gradually switch to track the second object in [5s, 15s]."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where τ e is the effective lifetime, τ r is the radiative lifetime, and τ −1 nr = N pv σ. Here, N p is the number density of the buffer gas,v is the average velocities of the colliding Na * 2 -Ar atoms over the Maxwell-Boltzmann distribution of relative velocities at the heat-pipe oven temperature, and σ is the collisional cross section between the excited dimer and argon atoms. The effect of argon gas collisions is eliminated with the Stern-Volmer extrapolation. The radiative lifetime was extracted from the zero pressure extrapolation of the Stern-Volmer plot. Figure 5 shows the inverse lifetimes (ns) as a function of argon pressure at 290 • C in the range of 200 mTorr -450 mTorr. The radiative lifetime at the collision-free limit (intercept of the Stern-Volmer plot) was extracted from a linear fit to the data. The radiative lifetime of the 6 1 Σ + g (7,31) state is found to be 39.56 (± 2.23) ns with a correlation coefficient of 0.99 and residual sum square of 7.86x10 −9 .\n\nMentions:\n- Figure 5 shows the inverse lifetimes (ns) as a function of argon pressure at 290 • C in the range of 200 mTorr -450 mTorr.\n\nOCR Text: \"6'5ajii AEIV1 3364231 E0.uS5 Pnssn I / ort '\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 5: (Color online) Plot showing the variation in the inverse lifetime as a function of argon pressure at 290 ◦C. Each data point is an average of 10 to 20 independent lifetime measurements, with the error bar equal one standard deviation. Red filled square data point is an outlier and was excluded in the fitting procedure. Red line shows a weighted linear fit to the data with a correlation coefficient of R2=0.98 and residual sum square of 7.86x10−9 ."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The potential considered here for the inter-ring torsion angle S-C-C-S has been borrowed from the Dreiding FF 69,74 and is shown by the black curve in the Alberga et al. 43 potential by 1.07 kcal mol -1 . This is consistent with recent studies 40,75 on the torsion potential of PT and P3HT, which suggest that the presence of hexyl chain in P3HT lowers the cis-trans barrier relative to PT due to the non-bonded repulsive interaction in the near-planar conformations. Also shown in Figure 2 is the torsion potential used by Pizzirusso et al. 44 whose form appears to be slightly different from that of Dreiding in that it exhibits a shallow energy surface around the angle o 0   On the other hand, the two potentials exhibit their local minimum and maximum values at the same values of the angle .\n\nMentions:\n- Also shown in Figure 2 is the torsion potential used by Pizzirusso et al. 44 whose form appears to be slightly different from that of Dreiding in that it exhibits a shallow energy surface around the angle o 0   On the other hand, the two potentials exhibit their local minimum and maximum values at the same values of the angle .\n\nOCR Text: \"Lxeidicg lunrlicd, =% Fizzus.:\". 14 E 5 2 2 4c o ( Nhp\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: Thiophene-thiophene inter-ring (S-C-C-S) torsion angle potential according to the Dreiding forcefield 74 (black curve) used in this work. For comparison we also show the corresponding torsion angle potentials proposed by Alberga et al. 43 (green line) and Pizzirusso et al. 44 (red line). Only the half of the horizontal axis is shown due to the symmetry of the curves."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Under the proposed paradigm, as mentioned in Section 1, the discriminator can be fine-tuned using the outputs generated from the re-ranking process, to match the new generation distribution. Inspired by the GAN paradigm, we iteratively retrain the discriminator given the new predictions until convergence. We detail the full procedure in Figure 1, where the discriminator is retrained iteratively following equation 2 at each step. The set of generated summaries G used in equation 2 to train the discriminator at step t+1 corresponds to outputs from our DAS process using the discriminator at step t. This allows to consider at each step a discriminator that attempts to correct output distributions from the previous step, in order to incrementally converge to realistic distributions (w.r.t. human summaries), without requiring any retraining of the generator.\n\nMentions:\n- We detail the full procedure in Figure 1, where the discriminator is retrained iteratively following equation 2 at each step.\n\nOCR Text: \"Dischcinztil' Nrn] MmD _ Szzh Ttmig Gtkttt 8068 Djvl_t @uco %0iD5 fyapc Tuunitn\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1. DAS self-training procedure: the generated examples are improved by the discriminator, and then fed back to the discriminator in a self-training loop."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- After properly integrating the four algorithms of the proposed SD-FFR mechanism, Fig. 4 depicts the flowchart of the SD-FFR. Specifically, when the network starts, the SD-FFR mechanism initializes the link weights by Alg. 2. In addition, the Alg. 2 is executed to manage the link weights in the network. Following the Alg. 2, information of residual link bandwidth is updated by Alg. 1. Simultaneously, the routing decisions can be made through Alg. 3, when a traffic flow joins the network. When the failure occurs, SD-FFR performs Alg. 4 for failure recovery processing. In this section, we evaluate the behavior of data transmission in SDN assisted dual-band WiFi networks by commercial APs, and apply the measurement for Mininet simulation setup in Section V aiming to effectively validate the performance of the proposed SD-FFR in real network environment. As a result, following Fig. 5, each commercial AP has Ethernet connectivity to the Internet and the SDN controller, 5 GHz wireless mesh connectivity to neighboring APs and a 2.4 GHz connectivity to mobile devices. Further, we enable OpenFlow1.3 [18] protocol on the commercial APs (i.e., TP-Link Archer C5 AC175) by installing both OpenWrt and OpenvSwitch (OVS) on them. Then, the OVS running on the APs can communicate with the ONOS controller. We use iPerf and Ping to measure the link bandwidth the data transmission latency under different network conditions. Specifically, we collect measurements by using wired and wireless link individually which can be further applied for the experimental setup in Mininet.\n- To evaluate the performance of the proposed SD-FFR, we use Mininet to create a network topology present in Fig. 5. Specifically, the Mininet runs on the computer with Intel i7-8705G CPU and 16 GB RAM. In the network, five APs not only directly connect to the GW through wired connection but also connect with neighboring APs via 5 GHz wireless link. Each AP serves provides end devices wireless connectivity operated at the 2.4 GHz frequency band. Specifically, we adopt the parameters measured in the commercial WiFi APs shown in TABLE II and the 44.6 ms link failure detection latency described in Section IV-A to configure the network.\n\nMentions:\n- As a result, following Fig. 5, each commercial AP has Ethernet connectivity to the Internet and the SDN controller, 5 GHz wireless mesh connectivity to neighboring APs and a 2.4 GHz connectivity to mobile devices.\n- To evaluate the performance of the proposed SD-FFR, we use Mininet to create a network topology present in Fig. 5.\n\nOCR Text: \"Eht Cce GH: GRa%n-Fi Edd*e \"cless M~;n link;\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 5: Network environment."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- First, we perform measurements on stationary vehicles to study the joint effect of blockage and limited mobile transmit power. We put two Android phones into two cars (with windows and doors closed), one running WiFi tethering to broadcast beacons and the other scanning for beacons. Figure 8 plots the WiFi beacon strength at different separation distances. We see that the above artifacts make the signal strength drop to -100 dBm before the distance reaches 250 meters. In the same figure, we also plot the probability of successful beacon decoding (thus challenge completion) across 400 attempts within 2 minutes. It remains 100% when the two cars are separated by &lt;80 meters, and drops to zero at 160 meters.\n\nMentions:\n- Figure 8 plots the WiFi beacon strength at different separation distances.\n\nOCR Text: \"Scan SucdcssFae Sgra S engh 06 02 4 W 6 8 W 420 Z40 1M IE 20 Lsuargc Oehezh To Denoes r\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 8: WiFi signal strength and scan success rate with respect to car distance in static scenarios."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Performance on stabilized videos. In order to show the potentiality of MFM approach in dealing with source device identification problem on real videos, we apply the proposed method to the stabi-   [8], i.e., the area-under-the-curve AUC and TPR @0.01 of ROC curves, averaged over all devices. Precisely, TPR @0.01 corresponds to the rate of correct attributions evaluated when the false positive attribution rate is equal to 0.01. We show the attribution results achieved by testing 10 random I-frames per video query and picking the maximum value among the computed PCEs. Specifically, we test different values for the number of used frequency samples (i.e., ∆ρ) and always report results achieved by [8] over the same dataset. Fig. 3 draws the ROC curves and Table 1 depicts the achieved AUC and TPR @0.01 as a function of ∆ρ. Moreover, last row of Table 1 reports the average required computational time [seconds] for testing one query frame according to the chosen strategy, considering matching cases as well as non-matching ones. It is worth noticing that the proposed approach can overcome results of [8], provided that a sufficient amount of frequency samples is selected. Furthermore, the MFM strategy enables fast computations as well, at the expense of a slightly reduced accuracy, but still acceptable.\n\nMentions:\n- Fig. 3 draws the ROC curves and Table 1 depicts the achieved AUC and TPR @0.01 as a function of ∆ρ.\n\nOCR Text: \"-Mawllk WIU 6 MFH Jw ~MM MFM 06 UO EPR\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3: ROC curves obtained testing 10 I-frames with the proposed strategy, as a function of the number of used frequency samples, i.e., ∆ρ. Results are compared to those of [8] evaluated using 10 I-frames."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The price in BTC-D is denoted by USDT, while the prices of BTC-G come from U.S. dollars trades. Fig. 2 shows the realized sample trajectory of BTC-D and BTC-G evolving closely in an extremely volatile day. The correlation between the daily logarithmic returns of the two datasets is around 98.2%.\n\nMentions:\n- Fig. 2 shows the realized sample trajectory of BTC-D and BTC-G evolving closely in an extremely volatile day.\n\nOCR Text: \"Tr;71\"5\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2: Price trajectory of BTC-D (dash line) and BTC-G (solid line) in the highest accumulated logreturn day, 20th July, 2017. RiskBTC"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- which leads to approximately linear decay in log ψ(t) (i.e., exponential decay). This behavior can again be seen in Figure 9. By looking at ψ rather than log ψ, however, we actually see that the power-law rise can be easily approximated by a linear rise because it dominates such a small portion of a typical MS galaxy's SFH (see Figure 10). We examine the evolution in sSFR and find that MS galaxies seem to become progressively less efficient (i.e. decreasing fractional mass growth) at forming stars over the course of their lifetime (see Figure 11). This is a natural extension of the sub-linear nature of the log M *log ψ relation, and decreases in a log-linear manner for the majority of the MS galaxy's lifetime (i.e. during the \"slow growth\" phase). By extension, this leads to an approximately linear fractional mass growth rate for the majority of a galaxy's lifetime, minus initial upturns at early times (where gas is plentiful and the \"fast growth\" to \"slow growth\" transition is still taking place) and downturns at late times (where the gas supply is being exhausted and the mass loss from old stars is significant).\n- Chen et al. ( 2009) (C09). C09 observes ∼ 5 × 10 5 and ∼ 3000 galaxies in the SDSS Data Release Four (DR4; Adelman-McCarthy et al. 2006) and a combination of the Deep Evolutionary Exploratory Probe 2 (DEEP2; Davis et al. 2007) survey and Palomar Observatory Wide Infrared (POWIR) survey at 0.005 &lt; z &lt; 0.22 and 0.75 &lt; z &lt; 1.0, respectively. Although C09 do not provide a MS relationship for their fits (which we will term C09(1) and C09(2), respectively) in the paper itself, the fit they do provide in their Figure 11 does not seem to include all data points (excluding the ones on the low mass end), and is calculated without assuming extinction. This fit is does not utilize an effective cut to exclusively analyze SFGs.\n\nMentions:\n- We examine the evolution in sSFR and find that MS galaxies seem to become progressively less efficient (i.e. decreasing fractional mass growth) at forming stars over the course of their lifetime (see Figure 11).\n- Although C09 do not provide a MS relationship for their fits (which we will term C09(1) and C09(2), respectively) in the paper itself, the fit they do provide in their Figure 11 does not seem to include all data points (excluding the ones on the low mass end), and is calculated without assuming extinction.\n\nOCR Text: \"GR-misz; {i-kimh >t Maah-nilA - dl Ira cMer WtdaLktAWc Coeud Hech neusmkcr Jnchenamirses: =fmer Kidmeu; 'Jeel\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 11.— As Figure 9, but for the sSFR. As can be seen, the sSFR (i.e. the inverse growth time scale of a galaxy) decreases over time to the present day values in an approximately linear fashion after an initially steep decline. Including stellar mass loss prescriptions leads to a more gradual decline in the sSFR (in log space) compared to the direct SFR-to-mass growth."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Finally we consider the refined pilot reuse scheme proposed in [14] where pilot reuse is only applied at the cell edges. This reuse policy selects B = K(β f + (1 − β f )β) pilot sequences, and assign the same β f K pilot sequences to the β f K users that are closest to their serving BSs. The other K(1 − β f )β pilots are assigned to the remaining users according to the same symmetric reuse policy as before. The refined scheme reduces to the symmetric reuse policy for β f = 0, while β f = 1 means that the same pilots are used in all cells. The idea of this scheme is to mitigate pilot contamination while limiting the pilot overhead. We apply this scheme to our M-MMSE precoder, and show the performance in Fig. 4 for M = 200 and different K. For each K and β f , we choose the highest SE from what we obtained from β ∈ {1, 3, 4, 7}. As seen from the figure, β f = 0 is preferred when K is small, which means that pilot reuse should also be applied for everyone. However, β f &gt; 0 is beneficial for larger K values since there will likely be cell center users that are not sensitive to pilot contamination, which can be exploited as in [14] to reduce the pilot overhead. However, if we look at the highest SE for each β f among the considered K and β, the increase of β f is not as obvious as it is for M-ZF shown in [14], thanks to the strong ability of our M-MMSE to actively suppress inter-cell interference.\n\nMentions:\n- We apply this scheme to our M-MMSE precoder, and show the performance in Fig. 4 for M = 200 and different K.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4. Achievable sum SE as a function ofβf with M = 200."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The dependences of the DM relic density and the lifetime of the co-annihilation partner on θ are more complicated, and shown in Fig. 9. Here we plot Ω DM h 2 (solid lines) and τ φ (dashed lines) as functions of θ by fixing m χ = 300 GeV and varying ∆M = 1.2, 1.4 and 1.6 GeV. We see that Ω DM h 2 is globally minimized at θ = 0 and π (φ = φ L ) due to the relatively large SU(2) L coupling. Another local minimum is found at θ = π/2 (φ = φ R ). The relic density has two local maxima implying that there is a cancellation in σ eff v among g L and g R terms in Eq. (5.4). The interference between g L and g R terms can also be observed in the lifetime of φ. Unlike Ω DM h 2 , τ φ is minimized (maximized) at θ 3π 8 ( 7π 8 ).\n\nMentions:\n- The dependences of the DM relic density and the lifetime of the co-annihilation partner on θ are more complicated, and shown in Fig. 9.\n\nOCR Text: \"0.12 {3m A46 S-1458 0.11 0 10 Jn 07 34;\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 9: The lifetime of φ± (dashed) and the DM relic density Ωh2 (solid) as functions of the L-R mixing parameter θ. The DM mass is fixed at 300 GeV and ∆M is varied as 1.2 (blue), 1.4 (red) and 1.6 (green) GeV."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Since PBLLA is similar to SPBLLA, we skip the performance analysis for PBLLA, but focus on SPBLLA only. We set m = 0.03 and τ from 0.01 to 0.03. Fig. 6 and Fig. 9 present the performance average SNR and the coverage size of the whole UAV ad-hoc network. The interference from the environment causes fluctuation. When the scenario dynamic degree is getting lower, the network is capable of approaching to a better outcome, and the SNR and coverage size is getting larger. In the slower dynamic scenario (τ = 0.01), the UAV ad-hoc networks can cover 95% post-disaster area. Even though the interference from the environment is serious (τ = 0.03), the coverage proportion can be up to 80%.\n\nMentions:\n- Fig. 6 and Fig. 9 present the performance average SNR and the coverage size of the whole UAV ad-hoc network.\n\nOCR Text: \"~Qd F00l} F0m\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 9: Performance of Coverage size. Lower dynamic degree index increases Coverage size. When τ = 0.01, coverage proportion is up to 95%."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We constructed a second model in an attempt to achieve better performance than the three PC model. We used all 31 engineered features, shown in Table 2, to train a second model and the performance results are shown in Figure 5. Figure 5 shows an improvement in model performance when all 31 features are used to train our model. The second model achieved an AUROC of 1.00 on our test set. This indicates that it was able to predict which digit was written without any errors. Class break downs and full feature model predictions by class are shown in the confusion matrix in Table 8. Although the top three PC explain the great majority of the variance, the other components contain the HICSS '20, January 7-10, 2020, Maui, Hawaii Lambert T. Leong and Sean Wiere (a) information needed to create a good decision boundary. Separability is often easier to observe in higher dimensions and this seemed to be the case for our classification problem. An AUROC value of 1.00 does not suggest the need for more feature engineering and suggests that our current features are sufficient. We are confident in our model's generalizability due to its good performance on the test set and have no reason to suspect any significant overfitting.\n\nMentions:\n- We used all 31 engineered features, shown in Table 2, to train a second model and the performance results are shown in Figure 5.\n- Figure 5 shows an improvement in model performance when all 31 features are used to train our model.\n\nOCR Text: \"Recewver operatirg claracterishc, Iinal mojel testsel Aoeciny laret Fal;c Fpstw: Naic\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: ROC curve and AUROC values calculated as a result of running the test set through the model trained on all 31 feature. The dash line indicates random guess performance."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The full set of fixed and controlled orbital parameters are given in Table 5, where the given errors are those formal to the WD solution (the true errors are probably larger, in particular considering that the eclipses are only partial and not total). We caution that the formal errors can be optimistic when there are strong correlations between parameters, so should be con- The orbital solution is overplotted to the observed light curve in Fig. 3 . For our modeling, we adopted temperature of the primary star as derived by the NLTE analysis, and the bolometric albedos and gravity brightening exponents appropriate for radiative atmospheres (Claret 2001(Claret , 1998, and limb darkening coefficients from Van Hamme (1993). The final solution shows a negligible dependence on the way limb darkening is accounted for, but it is very sensitive to the strength of the third light.\n- In this context, SZ Cam stands out as a very interesting member of the β Cep group of stars, because it has the longest known pulsation period, it is eclipsing, it is a quadruple system, and it is also member of a young open cluster. The bottom panel of Figure 3 suggests that neither at primary nor at secondary eclipse is the 'noise' in the light curve due to the β Cep pulsation reduced. This seems to indicate that the β Cep variable is the primary component of the non-eclipsing pair in the quadruple system SZ Cam. The spectral type of this star (inferred from the other observable properties) should be B1V, making it a perfect match to the average spectral type of known β Cep variables.\n\nMentions:\n- We caution that the formal errors can be optimistic when there are strong correlations between parameters, so should be con- The orbital solution is overplotted to the observed light curve in Fig. 3 .\n- The bottom panel of Figure 3 suggests that neither at primary nor at secondary eclipse is the 'noise' in the light curve due to the β Cep pulsation reduced.\n\nOCR Text: \"\"WWYUHHWHHHTYHAF Uh\"L Eumhu \"hHkWa HhWYYHHYHUHNYHSU\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3. The orbital solution as given in Table 5 is overplotted on our photometric data from Table 3. The middle panel shows the IC band light curve and the other three panels presents the residuals in (B − V), (V − IC) andIC (top to bottom, respectively."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- - Bringing the experimental errors to the shear displacement, Figure 5 shows the histogram of all calculated shear displacements\n\nMentions:\n- - Bringing the experimental errors to the shear displacement, Figure 5 shows the histogram of all calculated shear displacements\n\nOCR Text: \"Maein € &0N 10z0; Jtin dam0 demato\" Weeet Shear dsclace mentMum\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: Histogram of the shear displacement between the center positions of the two spots. The average separation distance is 8.840 µm, and the standard deviation is 0.064 µm."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Here, we propose two methods to estimate non-parametriĉ Q π (s t , a t ): use K Monte Carlo rollouts inspired by [17,28] and use inference algorithm (max-probability rollout) inspired by [22]. These processes are illustrated as Fig. 1. Since Q π (s t , a t ) is an expected accumulated reward, K Monte Carlo is more stable and precise than maxprobability rollout to estimate Q π (s t , a t ) with additional computation cost of K − 1 rollouts. In 1-step reformulated advantage function, the model rollouts every steps, while in n-step reformulated advantage function, the model rollouts every n steps. Therefore, the model adopts self-critical training [22], which uses rollouts to estimate value functions as a critic.\n\nMentions:\n- These processes are illustrated as Fig. 1.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1. Each state-action value is estimated by the average rewards of K rollouts sequences (K = 1) or a reward of a maxprobability rollout. The advantage function in our method is estimated by the current state-action value minus the preceding n-step state-action value. The tokens in green and yellow are the special tokens BOS and EOS. The tokens in white are the Monte Carlo trajectory and the tokens in blue are continuation rollout tokens for state-action value estimation. The n steps in the figure means that the model performs rollouts every n steps in n-step reformulated advantage function."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- By way of illustration of the use of the dataBoM gateway, consider a simple software application which serves to provide a 'traffic congestion score' for a fixed location, e.g., Hyde Park Corner, depending on how much traffic the application determines is currently at the location. This simple process has a single assembly, Traffic Scene Analysis, an input data source Location Photo, an ML model artifact Congestion Model and an output data source Congestion Score (Figure 3). In defining the BoM for the Hyde Park Corner (HPC) congestion rating process, the scientist should give each element a name and an optional description, and declare static elements, such as the URL to be used to retrieve a live photo from the location of interest. Encoding this simple single assembly process as a BoM through the gateways's API gives a data model as shown in Listing 1, which is the result of a GraphQL query on the BoM's entry.\n\nMentions:\n- This simple process has a single assembly, Traffic Scene Analysis, an input data source Location Photo, an ML model artifact Congestion Model and an output data source Congestion Score (Figure 3).\n\nOCR Text: \"Data Arlilact Source Location Congestion Photo Model Assembly Data Source Tratlic Scene Congestion Analysis Score\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3. The components of a simple traffic congestion system"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We present the generative process of the iM 3 F model, depicted in Figure 2. For the purposes of comparison, the schematic view of the finite, parametric case is presented in Figure 1, which corresponds to the original M 3 F model. First, the hyperparameters for the latent user and item factors are generated:\n\nMentions:\n- For the purposes of comparison, the schematic view of the finite, parametric case is presented in Figure 1, which corresponds to the original M 3 F model.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1: The original M3F model, as proposed in [1]."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The MWIR imager measurements are shown in Figure 8. The uncertainty in measurement is lower than that for the LWIR imager, and there is a distinct difference between the two angles of incidence which becomes more apparent as the temperature increases. Figure 9 shows the temperature difference between the two angular positions as measured by the MWIR thermal imager. The high emissivity vents display zero temperature dependence, in addition the low emissivity vents demonstrate an emissivity enhancement at shallow angles.\n- • Non-uniform emissivity -the uncertainty due to vent-to-vent emissivity variation was modelled to be Figure 9: From the MWIR measurements at the two angles of incidence, the difference in apparent temperature between the two angles (T 0°− T −50°) , for each coating, is depicted. The high emissivity vents appear to be approximately independent of temperature, whereas the low emissivity vents display a temperature dependence.\n\nMentions:\n- Figure 9 shows the temperature difference between the two angular positions as measured by the MWIR thermal imager.\n- • Non-uniform emissivity -the uncertainty due to vent-to-vent emissivity variation was modelled to be Figure 9: From the MWIR measurements at the two angles of incidence, the difference in apparent temperature between the two angles (T 0°− T −50°) , for each coating, is depicted.\n\nOCR Text: \"2-/nn FeT  rrortnc\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 9: From the MWIR measurements at the two angles of incidence, the difference in apparent temperature between the two angles (T 0°−T−50°), for each coating, is depicted. The high emissivity vents appear to be approximately independent of temperature, whereas the low emissivity vents display a temperature dependence. The enhancement of emissivity due to the coating is small."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We also consider the following values for the constants L, T , h c, N x and N t:   The energy E n decays like an exponential function exp(−µt n ) for µ &gt; 0, in the full damping case the discrete counterpart of the Timoshenko system is exponentially stable. Fig. 5 presents the graph of log(E n ) as a function of t n which is in good agreement with the theoretical result. More precisely, we have (3.9) log(\n\nMentions:\n- Fig. 5 presents the graph of log(E n ) as a function of t n which is in good agreement with the theoretical result.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIGURE 5. Exponential decay rate of the discrete energy."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The undulator taper could be simply implemented as a step taper from one undulator segment to the next. The magnetic field tapering is provided by changing the undulator gap. A further increase in power is achievable by starting the FEL process from a monochromatic seed, rather than from noise. The reason is the higher degree of coherence of the radiation in the Fig. 2. The combination of high power and low charge mode of operation. This scheme holds a great promise as a source of X-ray radiation for such application as single biomolecule imaging.\n- We can also apply this scheme to low-charge mode (0.025 nC) to obtain fully-coherent TW-level X-ray pulses with duration less than 5 f (see Fig. 2). Theoretical studies and simulations [4] (Chapter 6 and references therein) predict that such pulses may allow structural studies on large biomolecules before the radiation damage destroys the sample.\n\nMentions:\n- The reason is the higher degree of coherence of the radiation in the Fig. 2.\n- We can also apply this scheme to low-charge mode (0.025 nC) to obtain fully-coherent TW-level X-ray pulses with duration less than 5 f (see Fig. 2).\n\nOCR Text: \"bsplinkkot IcT lirawl hdig (Ir un cwlnt Ikr Ul Aa Ipk: 'Wue j;HR riht 9kex dm\"W TX\"SL m'aslX IE [I44IFLHE(E::\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2. The combination of high power and low charge mode of operation. This scheme holds a great promise as a source of X-ray radiation for such application as single biomolecule imaging."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- T Qv is included in the penalty factor to normalize the two objective functions in (opt*) to the same scale. Finally, we compare the closed-loop performance of the iterative Algorithm 3 (where both w and I are free) with Algorithm 1 in Fig. 8. The comparison is shown for Q 1 and only r ≤ 6 as the error ratio already becomes under 1% after r = 6. For this example, it is worth mentioning that Algorithm 3 turns out to be surprisingly efficient as it converges right after the first iteration. In this sense, the iterative process reduces to a single weight design after the clustering. Fig. 8 verifies that Algorithm 3 achieves better matching between g(s) and g(s) than Algorithm 1.\n\nMentions:\n- Finally, we compare the closed-loop performance of the iterative Algorithm 3 (where both w and I are free) with Algorithm 1 in Fig. 8.\n- Fig. 8 verifies that Algorithm 3 achieves better matching between g(s) and g(s) than Algorithm 1.\n\nOCR Text: \"t Mlt aatshea\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 8: Performance loss by weight refinement"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We simulate the take-off maneuver for three different aircrafts, whose effective areas matches those considered in section 3. The model and control parameters employed for the simulations are shown in Tables 3 and 4, respectively. In addition, the values ρ = 1.2 kg/m 3 , g = 9.81 m/s 2 and  the aerodynamic coefficients shown in Figure 10 have been used. The initial conditions (36) with l 0 = 2 m and x g,0 = 0 were used for all three aircrafts. The number and size of the propellers, required to compute the related power according to equation ( 10), are the same as those considered in section 3, i.e. 2 propellers with efficiency 0.7 and 0.25 m, 0.5 m, 1 m of diameter, respectively, for the three aircraft sizes. Examples of simulation results for the aircraft with d = 10 m are shown in Figures 11-14. In Figure 11, it can be noted that the total travel distance of the slide is equal to 15 m, and that the aircraft starts the ascend after 12.4 m, i.e. when the take-off speed of 15.7 m/s has been reached. As shown in Figure 12, the motor M 2 exploits the full rated torque to accelerate and then to brake the slide, while M 1 employs a relatively small fraction of its available torque for the acceleration and then settles to a constant torque corresponding to the viscous friction at the aircraft's velocity. We remark that the power required to accelerate the drum, although substantial, does not give rise to additional costs, since the machine M 1 is already present and the power required for take-off is a small fraction of the one that occurs during power generation. The propeller is engaged only af-   ter take-off and, after a short transient, it settles to a steady value sufficient to achieve the desired vertical velocity. The behavior of the latter quantity as compared with its reference is reported in Figure 13. As shown in Figure 14, the peak power for the motors is reached at the instant when the aircraft takes off. The results obtained with the other two aircrafts (d = 5 and 20 m) are qualitatively similar to those shown in Figures 11-14. In all cases, the total travel distance of the slide was about 15 m.\n\nMentions:\n- As shown in Figure 14, the peak power for the motors is reached at the instant when the aircraft takes off.\n\nOCR Text: \"Side M-oi Ltomali Pmec\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 14: Simulation results with the 10-m wingspan aircraft. Courses of the motors’ and propeller’s power."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Arabic varieties: Arabic is one of the six official languages of the United Nations, and the mother tongue of about 300 million people in 22 different countries, 4 including standard Arabic and dialects. Arabic has three main varieties illustrated in Figure 9. The classical Arabic, also termed Quranic Arabic, is used in religious texts and many old Arabic manuscripts [184]. MSA is the formal language of communication understood by the majority of Arabic speaking people, as it is commonly used in radio, newspapers, and television. Dialectical or colloquial Arabic is used in daily conversation and recently used on both TV and radio. There are mainly five groups of Arabic dialects: Maghrebi, which is spoken in North Africa including Tunisia, Algeria, Morocco, Libya, and Mauritania; Egyptian, which is spoken in Egypt and Sudan; Levantine, which is spoken primarily in the Levant region; Iraqi, which is spoken in Iraq, and Gulf which is spoken primarily in Saudi Arabia, UAE, Kuwait, Bahrain, Qatar, Oman and Yemen. This classification is general and based on the geographical location where nearest countries tend to have more common dialectical words and expressions. Dialects vary from one Arab country to another and from one region of the same country to another. For example, the Saudi dialect includes Najdi (Central) dialect, Hejazi (Western) dialect, and Southern dialect.\n\nMentions:\n- Arabic has three main varieties illustrated in Figure 9.\n\nOCR Text: \"[Latdat Trhrthe Rlote AmIcLr e Uecem ujrdanit Mtcti Etm Mstie Icati\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 9: Arabic language varieties"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- A graphical example of an ROC curve is shown as the solid black line in Figure 2.1.\n- The Neyman-Pearson lemma (Neyman and Pearson [1933]) shows that for a fixed experiment (P , Q), the likelihood ratio τ (o) = dP /dQ (o) is the most powerful test statistic for each choice of threshold τ 0 . This guarantees that the ROC curve for the likelihood ratio τ = dP /dQ will lie above, or dominate, that of any other test statistic τ as shown in Figure 2.1. This is an immediate consequence of the likelihood ratio being the most powerful test since for each false positive rate (or size) α it will have the largest true positive rate (or power) β of all tests (Eguchi and Copas [2001]). Thus ROC(dP /dQ) is the maximal ROC curve.\n\nMentions:\n- A graphical example of an ROC curve is shown as the solid black line in Figure 2.1.\n- This guarantees that the ROC curve for the likelihood ratio τ = dP /dQ will lie above, or dominate, that of any other test statistic τ as shown in Figure 2.1.\n\nOCR Text: \"E 1 False Positive Rate (FP\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2.1: ROC curve for (a) an arbitrary statistical test τ (middle, black curve), (b) an optimal statistical test τ? (top, blue curve), and (c) an uninformative statistical test (dashed red line)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The SDSS is a large international collaboration project, and it has obtained deep, multi-color images covering more than one-quarter of the celestial sphere in the North Galactic cap, as well as a small (∼ 300 deg 2 ), but much deeper survey, in the South Galactic hemisphere (York et al. 2000). The SDSS used a dedicated 2.5-meter telescope at Apache Point Observatory, New Mexico (Gunn et al. 2006). The flux densities are measured in five bands (u, g, r, i, z) with effective wavelengths of 3551, 4686, 6165, 7481 and 8931Å, respectively. The 95% completeness limits of the images are 22.0, 22.2, 22.2, 21.3, and 20.5 for u, g, r, i, and z, respectively (Abazajian et al. 2004). The relative photometric calibration accuracy for u, g, r, i, and z are 2%, 1%, 1%, 1% and 1%, respectively (Padmanabhan et al. 2008). Figure 1 shows the error of u-, g-, and r-band magnitude as a function of g-band magnitudes. Other technical details about SDSS can be found on the SDSS website http://www.sdss3.org/, which also provides an interface for the public data access.\n- In the conventional photometric metallicity calibration by polynomial fitting, the error mainly arises from two sources. One is from the fitting method itself (illustrated in Figure 3), and the other is from the errors in the color indexes. In SDSS, the photometric error of the u-band magnitude is relatively large (shown in Figure 1), limiting the application range of the photometric metallicity estimator. In the monte-carlo calibration method, we suppose that the spectrum-based metallicity distribution in a given u − g, g − r color bin is fairly reliable, and thus, by reproducing its distribution we effectively eliminate the errors arising from the fitting method. The error introduced by fluctuations in the monte-carlo method can be eliminated by increasing the number of desired random numbers. In addition, the third-order polynomial model is determined by only 10 coefficients, while the model (seed array) construction involves many more variables. This, to some extent, guarantees the accuracy of photometric metallicity distribution determined by the monte-carlo method. The deviation of the photometric MDF determined by the monte-carlo method mainly arises from the errors in the color indexes, especially when estimating the MDFs for faint stars. As shown in Figure 4, the calibration based on the monte-carlo method can be used to derive photometric MDFs with sufficient accuracy for most purposes.\n\nMentions:\n- Figure 1 shows the error of u-, g-, and r-band magnitude as a function of g-band magnitudes.\n- In SDSS, the photometric error of the u-band magnitude is relatively large (shown in Figure 1), limiting the application range of the photometric metallicity estimator.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1.— The average magnitude error of numerous stars as a function of g magnitude. Main-sequence stars with 0.2 < g− r < 0.8 and 0.6 < u−g < 2.2 are selected. Theu magnitude error is much larger than that ofg and r, especially at the faint end."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Using the same method as before, the three pairs of samples were compared to test whether the curves come from the same distribution. The results of these tests are given in table 4, where the p-values obtained using the asymptotic distribution and a bootstrap procedure, described below, are included. These values show that the differences are not so clear after normalization, and point to the first interval H s 1 being different from the other three, but there is no evidence of differences between H s 2 and H s 3 or H s 3 and H s 4. Figure 7 shows the normalized spectra for these intervals, and may help explain the results obtained, since the spectral density of a time series sums up its oscillatory behavior. As can be seen, the spectra for intervals H s 2, H s 3 and H s 4 are similar in dominant frequency and dispersion while the spectrum for H s 1 is clearly different in both aspects. It is important to observe, however, that the process of registration of the individual waves to a common interval losses the information about the period of the wave, and hence also about frequency. Thus it seems likely that it is the dispersion (and shape) of the spectral density rather than its location in the frequency scale which accounts for the differences observed in the three samples. Nevertheless, the relationship between spectral densities and the shape of waves is not clear and requires further exploration.\n\nMentions:\n- Figure 7 shows the normalized spectra for these intervals, and may help explain the results obtained, since the spectral density of a time series sums up its oscillatory behavior.\n\nOCR Text: \"7i\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 7: Spectral densities for intervals Hs1,Hs2 and Hs3."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- achieved by the best one in terms of accuracy. Note: a comparison based on individual classes was not possible as no information related to individual classes was provided in [10] [56] [18]. More importantly, the input feature vector size and number of neural network parameters in the proposed model are less than the best one by almost 62% and 34%, respectively, where the visually compares is shown in Figure 25 and\n\nMentions:\n- More importantly, the input feature vector size and number of neural network parameters in the proposed model are less than the best one by almost 62% and 34%, respectively, where the visually compares is shown in Figure 25 and\n\nOCR Text: \"Input Feature Vector (1D ArrayLengthy Qaecil, WL5 [:112(l2 , 1JP 1ug1 , Ie Drkes\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 25: Input Feature Vector Compared With Other Models for English Text Classification"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where a and b are functions of R X and R Y , whose expressions are given in (14.12) and (14.13) of [8], respectively. Fig. 3 and Fig. 4 show the performance gain, in terms of the variance index, of our estimator over Han and Amari's estimator for binary symmetric sources (M = 2) at two different values of the unknown parameter, θ = 0.05 and θ = 0.9, respectively. The performance difference is more noticeable at low rates. For θ = 0.05, the Slepian-Wolf sum rate is R X + R Y = 1.29 bits, while our estimator requires a sum rate of R X + R Y = 2R = 0.57 bits. For θ = 0.9, the Slepian-Wolf sum rate is 1.47 bits, while our estimator requires a sum rate of 0.94 bits. Furthermore, for Han and Amari's estimator to achieve the centralized performance, the required sum-rate is 2 bits for both cases, which is not only much larger than the sum rate required in our estimator but also much larger than the sum-rate required by conditions specified in the Slepian-Wolf rate region.\n\nMentions:\n- Fig. 3 and Fig. 4 show the performance gain, in terms of the variance index, of our estimator over Han and Amari's estimator for binary symmetric sources (M = 2) at two different values of the unknown parameter, θ = 0.05 and θ = 0.9, respectively.\n\nOCR Text: \"FsttrIor tniaiul\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4: Performance Comparison: θ = 0.9, M = 2"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- (916, 40) and (s, Ga) (2083, 58)), which one would expect if s really had a significant influence on A s for a constant Ga. Furthermore, this measurement by Iversen et al. [53] is significantly below the measurements of A s by Burr et al. [15] for similar values of s and Ga, which also cannot be explained by the density-ratio hypothesis. However, this behavior is consistent with our theoretical framework (Figure 4) because the relative magnitude of turbulent velocity fluctuations and thus the parameter α increase with δ/d for a constant Ga (Section 2.6). Increasing α means that a weaker mean flow is required for aerodynamic entrainment (i.e., lower A and thus A s ). In other words, thresholds for a given sediment size can be lower when the boundary layer is relatively thicker. Hence, semi-empirical threshold models fitted to a certain wind tunnel data set may neither be applicable to other wind tunnels with significantly different boundary layer thickness δ nor to natural atmospheric conditions even when s and Ga are the same.\n- Second, for the experiments by Burr et al. [15], saltation and rocking thresholds become indistinguishable when the dimensionless boundary layer thickness δ/( √ sd) 6.6 (Figure 2). However, when δ/( √ sd) falls below this critical value, A s &gt; A , which is particularly apparent for the three bluish-colored measurements at Ga ≈ 600 in Figure 2. These three measurements were largely responsible for the finding by Burr et al. [15] that A s is unusually large (see the three largest-diameter measurements in their Figure 2). However, for the natural atmosphere of Titan (δ ≈ 3 km [17]), δ/( √ sd) is much larger, and our theoretical framework thus would predict that A s is very close to A and thus considerably smaller than the values measured by Burr et al. [15] even if the additional effect of δ/d on A s from controlling the relative amplitude of turbulent velocity fluctuations (Figure 4) was not considered. There is an easy test that should be done by future studies to figure out whether this prediction holds true. If the Venus (Titan) Wind Tunnel was used to measure A s for standard air (i.e., atmospheric conditions on Earth) rather than Titan-like air, the value δ/( √ sd) would be even smaller because of a larger value of s, and one would thus expect an even larger deviation from semi-empirical threshold models when using the the Venus (Titan) Wind Tunnel with standard air. Finally, one must ask the question of whether wind tunnel experiments, in general, are an appropriate means to estimate Aeolian sediment transport initiation thresholds under natural atmospheric conditions. Natural atmospheres exhibit much thicker boundary layers (δ ∼ 0.1−10 km on Venus, Earth, Mars, and Titan [17][18][19][20]) and thus larger values of α (Section 2.6), which are associated with smaller initiation thresholds (cf. Figure 4). In fact, observations from the Mars rovers Opportunity and Curiosity [24][25][26] and the existence of gravel megaripples on Earth (e.g., in the Argentinean Puna [21]) suggest that the initiation of sediment transport may be much easier in the field than in wind tunnels. If this hypothesis were generally true, the problem of whether fully developed saltation can be initiated under natural atmospheric conditions would essentially reduce to the problem of whether saltation transport initiated by a large turbulent fluctuation event can be sustained by the mean turbulent flow. Answering this question requires knowledge of the dynamic saltation threshold A r (and the function C(Ga, s) discussed in Section 2.7), for which turbulence only plays a minor role and for which an analytical model exists that reproduces available measurements in Aeolian and viscous and turbulent fluvial environments despite not being fitted to these measurements [40]. Hence, we would be able to predict saltation transport initiation on extraterrestrial planetary bodies much more reliably than we currently do. Another consequence would be that the saltation initiation threshold A s in the field, in contrast to the one in wind tunnels, would, like A r , significantly decrease with the density ratio s for a constant Galileo number Ga [40]. Clearly, controlled field measurements are needed to test the hypothesis of whether sediment transport initiation in the field is truly much easier than in wind tunnels and thus to test whether all of these potential consequences hold true. Likewise, wind tunnel experiments in the spirit of Williams et al. [11] with the purpose to study the influences of the boundary layer thickness δ and turbulence frequency spectra on the different initiation thresholds and related quantities (e.g., bed particle removal rates, particle removal PDFs) can also shed more light on these questions.\n\nMentions:\n- However, this behavior is consistent with our theoretical framework (Figure 4) because the relative magnitude of turbulent velocity fluctuations and thus the parameter α increase with δ/d for a constant Ga (Section 2.6).\n- However, for the natural atmosphere of Titan (δ ≈ 3 km [17]), δ/( √ sd) is much larger, and our theoretical framework thus would predict that A s is very close to A and thus considerably smaller than the values measured by Burr et al. [15] even if the additional effect of δ/d on A s from controlling the relative amplitude of turbulent velocity fluctuations (Figure 4) was not considered.\n- Natural atmospheres exhibit much thicker boundary layers (δ ∼ 0.1−10 km on Venus, Earth, Mars, and Titan [17][18][19][20]) and thus larger values of α (Section 2.6), which are associated with smaller initiation thresholds (cf. Figure 4).\n\nOCR Text: \"Lur tiluaILX Il Eirl: tucelec: & Tnkatainiici jn IiF0e Jiin Galilew Dueg, G = Mle\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4. Measurements of the rocking (A′), rolling (A), and saltation (As) threshold parameters versus the Galileo number Ga. Only those conditions from Figure 2 that obey A′ ' A ' As [15,53] are shown. The color indicates the the thickness of the boundary layer δ relative to the particle diameter d (different from Figure 2), which controls the relative amplitude of turbulent fluid velocity fluctuations for a constant Ga."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- As a final note, one question that arises is whether correlation with energy is present after the circuit has been compiled (and thus fully routed) to insert any necessary SWAPs. A test of this is shown in Figure 7, where we take a sample of 1000 allocations from an anneal run for a 14-qubit circuit (cm42a 207) with 1005 single-qubit gates and 771 two-qubit gates, using IBM Melbourne as a hardware graph. Each allocation is given to Qiskit's compiler (v0.20.0) using level 0 optimization and the 'basic' routing method, and the SWAP counts of the compiled circuits are plotted. As can be seen visually, there is no strong correlation between QUBO allocation energies and compiled circuit SWAP counts. This means that we cannot predict which QUBO initial allocation will produce the best compiled circuit.\n\nMentions:\n- A test of this is shown in Figure 7, where we take a sample of 1000 allocations from an anneal run for a 14-qubit circuit (cm42a 207) with 1005 single-qubit gates and 771 two-qubit gates, using IBM Melbourne as a hardware graph.\n\nOCR Text: \"Compiled Swaps %5, Energy 293Jc0] 294J0C) -Z9201C0 Z9201C0 -Z91CJ10 Frer\"\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 7. Distribution of Qiskit SWAP counts for each allocation in a 1000 sample anneal run for benchmark circuit cm42a 207. Compiled using Qiskit v0.20.0 using a preset passmanager with optimization level 0 and the ‘basic’ routing method. We can infer that it is not possible to predict the compiled circuit SWAP counts based on the energy of a given allocation."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The result is that as soon as an aversive stimulus is detected during forward motion, the robot reverses direction and begins to move backward instead. In our experiments, the \"aversive stimulus\" of choice occurred at an arbitrary time, but it would be simple to replace this behavior with something more reactive, such as a physical bumper to detect obstacles. The position of the robot over time during this experiment is shown in Fig 11.\n\nMentions:\n- The position of the robot over time during this experiment is shown in Fig 11.\n\nOCR Text: \"1 1 Tinc IOj;\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig 11. Reflexive reversal of the robot. The forward displacement of the three motion capture markers from their starting positions during the reversal reflex experiment."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Here C is either a closed loop or is stretched out to infinity. In the second case we don't take the trace and hence the operator depends on a choice of a weight w of the representation R modulo che action of Weyl group. In the N = 2 case, the Wilson line ( 28) is replaced by its half-BPS counterpart [55] which, to preserve half supersymmetries should be stretched along a straight line L; we still denote this operator as W w (L). 29 What are the quantum numbers carried by W w (L)? This class of UV line operators is labelled by (the Weyl orbit of) the weight w, so gauge weights are useful quantum numbers. However, these numbers do not correspond to conserved quantities in a general gauge theory. For instance, consider pure (super-)Yang-Mills theory and let R be the adjoint representation. Since an adjoint Wilson line may terminate at the location of a colored particle transforming in the adjoint representation, a gluon (gluino) particle-antiparticle pair may be dynamically created out of the vacuum, breaking the line, see figure 2. If our gauge theory is in the confined phase, breaking the line L is energetically favorable, so the line label w does not correspond to a conserved quantity. On the contrary, a Wilson line in the fundamental representation cannot break in pure SU (N ) (S)YM, since there is no dynamical particle which can be created out of the vacuum where it can terminate. The obstruction to breaking the line is the center Z(SU (N )) ∼ = Z N of the gauge group under which all local degrees of freedom are inert while the fundamental Wilson line is charged. Stated differently, the gluons may screen all color degrees of freedom of a physical state but the center of the gauge group. The conclusion is that the conserved quantum numbers of the line operators W R (L) consist of the representation R seen as a representation of the center of the gauge group, Z(G), which take value in the dual group Z(G) ∨ ∼ = Z(G). On the other hand, in SU (N ) (S)QCD we have quarks transforming in the fundamental representation; hence a quark-antiquark pair may be created to break a fundamental Wilson line. Then, in presence of fundamental matter, Wilson lines do not carry any conserved quantum number. In general, the conserved quantum numbers of the Wilson lines of a gauge theory with gauge group G take value in the finite Abelian group π 1 (G eff ) ∨ ∼ = π 1 (G eff ), where G eff is the quotient group of G which acts effectively on the microscopic UV degrees of freedom.\n\nMentions:\n- Since an adjoint Wilson line may terminate at the location of a colored particle transforming in the adjoint representation, a gluon (gluino) particle-antiparticle pair may be dynamically created out of the vacuum, breaking the line, see figure 2.\n\nOCR Text: \"utr Wx crention\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: Left: an electric flux tube line created by an adjoint Wilson line. Right: the adjoint flux line is broken by the creation of a gluon-antigluon pair out of the vacuum."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In this section, we compare the performances of our chemical SC and ML decoders for short polar codes in terms of the number of required chemical species and reactions, and also the trade-off between computation accuracy and speed. To do so, we analyze the time evolution of proposed chemical reactions governed by associated ordinary differential equations (ODE) [18]. We consider decoding polar codes with generator matrix of (49). This code can be derived from (17) by setting first and third rows to frozen indices.  demonstrated that our chemical polar decoder works well as intended. Fig. 3 shows the result with the proposed chemical ML decoder. From (47) and (48), the expected output vector is calculated as L 0 = 0.143, L c 0 = 0.857, L 1 = 0.027, L c 1 = 0.973. We can verify that the molecular concentrations converge to the expected values in Fig. 3.\n\nMentions:\n- Fig. 3 shows the result with the proposed chemical ML decoder.\n- We can verify that the molecular concentrations converge to the expected values in Fig. 3.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3. Convergence behavior of the molecular concentration over time with chemical ML decoder. Only the molecular concentration corresponding to the probability of 1 is shown. Horizontal lines indicate the idealistic values."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- NGC 330 is a well-observed cluster with several studies of its CMD , implying a range of potential ages from approximately 20 to 40 Myr (see Table 2 and accompanying references). To some extent, this spread in age stems from the difficulty that theoretical isochrones have in simultaneously reproducing salient features in the CMD of this cluster, and in analogous clusters in the Milky Way and the LMC (essentially clusters hosting both BSG and RSG stars). These features include the main-sequence turn-off (MSTO), the main-sequence turn-on (MSTON), or pre-L. R. Patrick et al.: Multiplicity of red supergiants in NGC 330 Fig. 6. Observed fraction of detected variability of the RSGs in NGC 330 as a function of the ∆RV min parameter for the nine RSGs with HARPS data (black solid line). The three best-fitting simulated stellar populations with an empirically defined distribution from Moe &amp; Di Stefano (2017), containing between 20% and 40% binary systems with orbital periods in the range 2.3 &lt; log P[day] &lt; 4.3, are shown with dashed coloured lines. The dot-dashed grey lines show the limits of simulations containing a binary fraction of 10% and 100%. The time sampling, uncertainties, and number of targets of the simulations is chosen to match the HARPS data. Each simulated curve is an average of 10,000 iterations, but to best compare with the observations, the simulated curves are discretised to the same intervals as the observations. main-sequence stars, the mean magnitude of the BSG stars, and the mean magnitude of the RSG stars (Cignoni et al. 2016). In addition to the above approaches, Britavskiy et al. (2019) and Beasor et al. (2019) have proposed a new technique for determining the ages of such clusters that uses the luminosity distribution of their RSG stars, with the former finding good agreement with the MSTON age of the LMC cluster Hodge 301, but disagreeing with the more traditional MSTO approach. This new technique argues that the luminosity spread of RSGs in a cluster is the result of stellar mergers, recognising that the RSG population of any given cluster is likely affected by multiplicity on the main sequence. In this scenario the most luminous red straggler RSGs are the evolutionary descendants of rejuvenated massive stars on the main sequence that are the result of the merging of a binary system. It then follows that the distribution in luminosity of the faint portion of the RSG branch is an age indicator for a cluster. We used the age-luminosity diagram shown in Figure 7 to estimate the age of NGC 330 with this new method.\n- We estimated the age of NGC 330 using the 10th percentile of the luminosity distribution, resulting in an age of 45 ±5 Myr (see Table 2 for a summary of NGC 330 cluster properties). We note that there is a clear distinction (one magnitude in the Gaia G band, see Figure 2) between the least luminous star shown in Figure  7 and the population of less massive red stars in the region. Beasor et al. (2019) demonstrated that the lowest luminosity Fig. 7. Luminosity-age diagram for all RSGs in NGC 330, stars that have RV measurements from the present study are marked with a cross. Luminosities are derived the J-band magnitude calibration of Davies et al. (2013). Using the least luminous RSGs, we estimate the age of the NGC 330 to be 45 ±5 Myr. SMC-like evolutionary tracks from Brott et al. (2011) are used to estimate ages, and Bressan et al. (2012) tracks are shown to highlight differences between models.\n- We note that recent work by Milone et al. (2018) argued for two stellar populations in this cluster, one bi-modal population of 40 Myr, composed of a sub-population of stars rotating at close to critical rotational velocity, and a sub-population with low rotational velocity, and a second younger population of 32 Myr with low rotational velocities. Figure 7 shows that this age spread cannot explain the observed spread in apparent ages of the RSGs in the cluster, which range from 15 to 50 Myr. However, spectroscopic studies of the brightest turn-off stars in NGC 330 consistently find them to be blue stragglers, which are B-type giants beyond the end of the classical main sequence (Grebel et al. 1996;Lennon et al. 2003). As Britavskiy et al. (2019) discussed, these may be the evolutionary ancestors of the bright RSGs. et al. (2017) studied the extended main-sequence turn-off in four young massive clusters in the Magellanic Clouds (including NGC 330). Using HST photometry and synthetic CMDs, they estimated a main-sequence binary fraction of f MS = 0.54 ± 0.05 using the method of Milone et al. (2012) and taking into account all mass ratios assuming a flat mass-ratio distribution. This is significantly larger than our estimated f RSG , as expected. However, we caution that these estimates used very different approaches and therefore have very different biases. A more appropriate comparison would be with a binary fraction estimated using RV variations.\n\nMentions:\n- We used the age-luminosity diagram shown in Figure 7 to estimate the age of NGC 330 with this new method.\n- We note that there is a clear distinction (one magnitude in the Gaia G band, see Figure 2) between the least luminous star shown in Figure  7 and the population of less massive red stars in the region.\n- Beasor et al. (2019) demonstrated that the lowest luminosity Fig. 7.\n- Figure 7 shows that this age spread cannot explain the observed spread in apparent ages of the RSGs in the cluster, which range from 15 to 50 Myr.\n\nOCR Text: \"5cgeieal 50c Inax >exr etal %\"%1 Emda Age Ir)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 7. Luminosity-age diagram for all RSGs in NGC 330, stars that have RV measurements from the present study are marked with a cross. Luminosities are derived the J-band magnitude calibration of Davies et al. (2013). Using the least luminous RSGs, we estimate the age of the NGC 330 to be 45±5 Myr. SMC-like evolutionary tracks from Brott et al. (2011) are used to estimate ages, and Bressan et al. (2012) tracks are shown to highlight differences between models."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In the case of ill-conditioned channel matrix, linear precoding leads to degradation in the receive SNR. To overcome this problem, THP includes the non-linear modulo operation (MOD) as shown in Figure 1. The modulo operation is defined by:\n\nMentions:\n- To overcome this problem, THP includes the non-linear modulo operation (MOD) as shown in Figure 1.\n\nOCR Text: \"HWA| Vo\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1. Block diagram of Tomlinson-Harashima precoding."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- P e r t . In Fig. 2 we show the results for the effective potential V eff at N = 2 in the cases of both the OPT and LN. Contrary to the results obtained in perturbation theory and shown in Fig. 1, we see from the results in Fig. 2 that the OPT and LN both produce results with better convergence properties, with the OPT at order δ 2 already agreeing quite well with the exact result. This agreement remains even when the coupling is much larger, while the LN results, at increasing orders in 1/N , tend to oscillate around the exact solution.\n- In Fig. 5 we verify how the dependence on N influences the results for the effective potential. Three cases are considered, N = 2, N = 4 and N = 10 and both OPT and LN are contrasted with the exact solution. In this case, as we have adopted in Fig. 2, we have chosen to optimize Σ using the PMS so to get the optimum solutionη and this solution is then used back in V eff . Once again, the PMS applied on the self-energy is found to be the best optimization scheme. The results presented in Fig. 5 also show that in general the OPT presents more robust results than the LN approximation, with the OPT converging faster to the exact result.\n\nMentions:\n- In Fig. 2 we show the results for the effective potential V eff at N = 2 in the cases of both the OPT and LN.\n- Contrary to the results obtained in perturbation theory and shown in Fig. 1, we see from the results in Fig. 2 that the OPT and LN both produce results with better convergence properties, with the OPT at order δ 2 already agreeing quite well with the exact result.\n- In this case, as we have adopted in Fig. 2, we have chosen to optimize Σ using the PMS so to get the optimum solutionη and this solution is then used back in V eff .\n\nOCR Text: \"1.6\" 3xcr PMs # -17 3i 211; Mi | M;N\" -18= Ni; Aim? Lmz- Lva\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: The effective potential Veff for N = 2. The exact result (solid line), the LN results (dashed-dotted, dotted and dashed lines) and the OPT results (circles), where we optimize Σ by using the PMS condition."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The footstep plan generated for this scenario is shown in Fig. 4. The plan's length is N = 18 steps. It is evident that the plan satisfies the discrete-timed specification, \"the 7 th to the 15 th footstep must be in Region 2\", while finally reaching its goal state -(x: 1.5, y: 2.2, θ: 3π/4). The solve time for this example was around 2s.\n\nMentions:\n- The footstep plan generated for this scenario is shown in Fig. 4.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4: Scenario 3. Encoded LTL Specification: 7≤k≤15 pR2 . Right-leg footsteps are shown in red stars and left-leg footsteps are shown in blue circles. The orientation of each footstep is depicted by its protruding black arrow. Each footstep is numbered in brown. Goal footstep location and orientation: [x: 1.5, y: 2.2, θ: 3π/4]"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In order to simplify the understanding of the different cases that will be discussed in the following, we have adopted a diagrammatic description of the collisions. As shown in Figure 2, standard branes will be described by solid lines. By standard branes, it is understood ρ = 0. Dotted lines will represent nil-branes while solid-wavy lines will represent branes with ρ = 0 but with a non zero matter density (i.e. ρ matter = 0).\n\nMentions:\n- As shown in Figure 2, standard branes will be described by solid lines.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: Solid lines describe standard branes, dotted lines describe nil-branes and solid-wavy describe branes with ρ′ = 0 but with a non zero matter density."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where GW is the transfer function weight from the merger number ratio and radio is the transfer function weight from the radio number ratio; recall that is the fraction of slowmerging binaries produced at birth (Eq. 1). Figure 4 shows the birth DNS distribution for slow mass and the resulting radio and gravitational-wave distributions.\n\nMentions:\n- Figure 4 shows the birth DNS distribution for slow mass and the resulting radio and gravitational-wave distributions.\n\nOCR Text: \"Einimal-waw Eli-ii:\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4. The mass distribution of slow neutron stars at birth (blue), merging (magenta), and in radio (yellow). The high-mass peak is suppressed in radio by a factor of 𝜁radio = 196. Here, we have assumed that fast-merging binaries merge in 𝑡∗ = 10Myr."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In our prototype, each data owner encrypts its data with AES and OPE and uploads it to the delegate which then stores it in a MySQL server. We let data owners outsource all of their data to the cloud, causing larger data query overhead than when parts of the data are stored locally. The encrypted datasets were as much as 23 times larger in size than the original, unencrypted ones (for the x10 splice dataset). We quantify the costs for database encryption at the party and database loading at the delegate, which incur only once at the beginning, in terms of the time taken to complete the operations. Figure 2[a] illustrates these costs with varying datasets for the NaiveBayes algorithm. It can be seen that both encryption and loading time are proportional to the data size, and they remain below 8s even for the largest dataset. Figure 2[b] shows the loading time at the delegates when delegates are running on different types of EC2 instances. Across all datasets, using small instances results in longer loading time. As explained in Section 4, a data mining application built using CloudMine consists of two iterative, interleaving processes: database query and secure sum. Figure 3 shows the breakdown costs of these processes -measured as the time taken to complete the process -for Apriori algorithm. One important observation is that database query time is always greater than secure sum time. For x50 mushroom dataset, the former takes more than an order of magnitude longer to complete. The longest experiment (with x10 splice dataset) took 12 minutes to complete, of which secure sum operation accounted for only 2 minutes. This suggests that when used in real data mining algorithms, the cost of the secure sum service has small effect on the overall performance. Figure 4 shows the effect Fig. 4: Database query time for ds = mushroom and ds = x50 mushroom datasets of increasing data size to the database query time for different algorithms. It can be observed that query time scales differently for different algorithms. Particularly, Apriori demonstrates the sharpest growth as compared to NaiveBayes and K-Mode. We attribute this to the intrinsic properties of the data mining algorithm. Specifically, we observe that in our experiments with Apriori, larger datasets led to more queries being performed by the delegate (from 132 with mushroom to 4214 with the x50 mushroom dataset). Finally, we investigated the cost of the secure sum service as being used in data mining algorithms. Figure 5 shows this cost varies with n for the Apriori algorithm. As the number of parties gets larger, the secure sum cost also increases, albeit at a sub-linear rate. This is consistent to what has been observed in Figure 1[b]. Recall that the cost of a secure sum operation comprises the encryption/decryption cost at the party and the computation and communication cost at the delegates. The former is shown in Figure 5 to be almost constant, meaning that the overhead incurred when n increases can be attributed to the overhead at the delegate. Firsts, each delegate needs to perform more multiplications when n increases. Second, each will have to wait longer to receive all the messages from other delegates when n becomes bigger.\n\nMentions:\n- Figure 4 shows the effect Fig. 4: Database query time for ds = mushroom and ds = x50 mushroom datasets of increasing data size to the database query time for different algorithms.\n\nOCR Text: \"Hat Msnicon '50 musnrcom LI 1oJ000 IjCQ 61 0\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4: Database query time for ds = mushroom and ds = x50 mushroom datasets"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The initial velocity of the relaxion has to be large enough to overcome the barriers in the periodic potential. One could think that an initially small velocity is allowed for a generic initial position of φ, if the height of the barriers is small enough compared with the average slope of the potential, as it is shown Fig. 4. In the following we esti-mate under which condition the assumption of large initial velocity is not necessary, meaning that the field is in an 'easy rolling' regime. If the field starts rolling from a generic point φ 0 , that without loss of generality we assume to be in the interval 0 − 2πf , it will be able to overcome the first barrier at 2πf and consequently start the rolling phase only if V (φ 0 ) &gt; V (2πf ). To quantify the requirement that this condition is generic, we impose that it holds for all values of φ 0 between 0 and πf , implying V (πf ) ≥ V (2πf ), where\n\nMentions:\n- One could think that an initially small velocity is allowed for a generic initial position of φ, if the height of the barriers is small enough compared with the average slope of the potential, as it is shown Fig. 4.\n\nOCR Text: \"2/'\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4. Sketch of the relaxion potential assuming a coincidence of scales that may avoid the requirement of initial large velocity. Here for convenience we set V (φ0) = 0 and φ0 = 0."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- As operated in this experiment, q 0 , q 1 , and q 2 have T 1 values of 48.1, 23.7, and 43.0 µs, and T * 2 times of 1.3, 1.6, and 0.8 µs, respectively. Figure 4 shows the pulses for one iteration of the PEA experiment; Figure 7 shows an example of the measurement results for one value of R. The parameters at each R are given in Table I. For reference, included in this section are the implementations of all the terms of the Bravyi-Kitaev Hamiltonian for molecular hydrogen. In the following diagrams, α is the ancilla qubit (q 0 in the experiment), and 0 and 1 are the register qubits (q 1 and q 2 in the experiment). We must always be aware that representing our terms in terms of these gates, and then in terms of the actual basis, is not necessarily the most efficient approach.\n\nMentions:\n- Figure 4 shows the pulses for one iteration of the PEA experiment; Figure 7 shows an example of the measurement results for one value of R.\n\nOCR Text: \"RmNumhor\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 7. Example data for a single PEA experiment, run at R = 1.55 Å. The results are shown without phase kickback from the measurements of the previous bit. The line at P|1〉 = 0.5 discriminates a measurement of 1 from 0."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To minimize the transition stress on each IGBT, a minimum ON and OFF time of 2 µs was selected. All of the cells will be interleaved every 1 µs, repeated every 12 µs, giving an 83.3 kHz repetition rate for each cell. This gives a minimum of 2 and maximum of 10 cells on at any time, giving a total of 8 cells, or 7.4 kV of adjustable range. The minimum and maximum pulse widths are illustrated in Fig. 4. The PWM switching cells, which are only active for 200 µs during the flattop region, give a junction temperature rise of less than 6 °C during each pulse, enabling long life for the IGBT modules. These cells are then filtered with a second order low pass passive network with a bandwidth of 100 kHz to reduce the ripple of the PWM regulating cells to less than ±25 V [4]. The timing accuracy to set both the start time and width of the pulses is determined by the 100 MHz clock speed of the field programmable gate array (FPGA), an Altera Cyclone V SE SoC. To regulate both the switching and regulating cells, the FPGA will be used to program these cell timings with pulse to pulse learning. The PWM waveform will have the predominate ripple at 1 MHz, given by the interleaving, which will be filtered by the lowpass network on the output of the 12 regulating cells. It was also determined that the time delay between the fire command to the switching of the IGBT can vary, due to different fiber-optic cable length and turn on delay of each IGBT. According to simulations, greater than 20 ns of variation can cause ripple larger than the specification, which will be studied in more detail in the next prototype.\n\nMentions:\n- The minimum and maximum pulse widths are illustrated in Fig. 4.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4: Regulating cells PWM waveforms."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The distance constraint used by Ilyas et al. [2019] can be viewed as a ball around the input. We showed that choosing the size of that ball can be challenging. If the size of ball is too small, their optimization problem becomes infeasible. If the size of ball is large, they do not find the adversarial example closest to the input. Their problem is non-convex, like ours. The examples above demonstrated that our approach finds a closer adversarial example compared to their approach. The computation times for our method and theirs are quite similar. Moreover, Figure 12 shows that the closest distance to the decision boundary can have a large variation among the images in a dataset. Therefore, tuning the distance constraint for one image may not be insightful for most of the other images in a dataset.\n\nMentions:\n- Moreover, Figure 12 shows that the closest distance to the decision boundary can have a large variation among the images in a dataset.\n\nOCR Text: \"#irehe xfp wT;\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 12: Distance to the closest flip point has large variation among images in the training set, which shows that a single distance constraint would not be able to reveal the vulnerabilities of a model for all images. For example, a distance constraint of 0.5 cannot yield an adversarial example for the large fraction of images that are farther than 0.5 from the decision boundaries. It also would not reveal the weakest vulnerabilities for images which are much closer than 0.5 to the decision boundaries."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- These lectures were interspersed with clicker questions that prompted students to debate the implications of each experiment, and which provided an opportunity for them to distinguish between a collection of data points and an interpretation of what they signify. It is important to emphasize that our interpretation-themed clicker questions generally did not have a single \"correct\" answer, such as the example shown in Fig. 8 (which does contain at least one incorrect response). The purpose of this question was to promote in-class discussion, and to elicit some of the ways students might interpret a mathematical representation of the photon's quantum state after encountering a beam splitter. As instructors, we advocated for option (B) in this question, but we did not tell students who disagreed that their preferred perspective was necessarily incorrect. As can be seen in this figure, one of the ways we made this topic more accessible to introductory students was to represent the state of the photon after the beam splitter as a superposition of the reflected and transmitted states, rather than the more technically correct description as entangled with the vacuum [56].\n\nMentions:\n- It is important to emphasize that our interpretation-themed clicker questions generally did not have a single \"correct\" answer, such as the example shown in Fig. 8 (which does contain at least one incorrect response).\n\nOCR Text: \"Exp 2; Photon Solrce Tivz Seamsplitters Ry+Iw;) What hanpcns whcn & single phxcon enccunters the bearn splitler? Eamnprob LLnI (MlucIk a6rann Julw Xiln a raim 515potatilly Eacnp130n $ belh reteczd andtrrsn ted sruimnecuri (CIEvery phclor spils IroWwpROT: each Cn na6 IEnQy 'Ci ccan} onbals}.1 p7olon @hLEc ctscrred p8 M na5s05 Tnicucn De ;F(rtans Srele\"\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 8. Clicker question involving a Mach-Zehnder interferometer with a single-photon source, used during lecture to generate in-class discussion about physically interpreting experimental data and mathematical representations."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- which it wants. To do this we consider the construction of the bipartite graph shown in Algorithm 2. An example of this bipartite graph corresponding to user group G 1 in Example 2 appears in Fig. 2. The bottom nodes in the graph represent the useful intermediate values received by each node in G. The top nodes represent the servers in G.\n\nMentions:\n- An example of this bipartite graph corresponding to user group G 1 in Example 2 appears in Fig. 2.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2. Bipartite graph corresponding to the user group G1 in Example 2."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where k = 2πn/T with n = 0, • • • T − 1 is the momentum conjugate to t along the cylinder and we have included the disconnected contribution c 0 . We can determine c 0 by fitting C 0 (k) for k = 0 and subtracting a smooth extrapolation of lim k→0 C 0 (k). We found that our data required parameterizing the lowest eigenvalue plus at least two higher eigenvalues. We got excellent fits with χ 2 /dof 1 and estimates of the lowest eigenvalues which are essentially free of contamination. The result is shown in Fig. 2. A linear extrapolation in 1/s indicates that the disconnected contribution vanishes at finite s ≈ 64. Then, β = 0.160987 &lt; β c , in apparent contradiction to the Binder cumulant result.\n- Figure 2: The disconnected contribution to the correlation function as described in the text.\n\nMentions:\n- The result is shown in Fig. 2.\n- Figure 2: The disconnected contribution to the correlation function as described in the text.\n\nOCR Text: \"Ucdiaed4 Eine 2*10 ~ >\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: The disconnected contribution to the correlation function as described in the text."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- As an academic exercise, Fig. 3 shows the G-dwarf metallicity distribution which is obtained when the standard model SD and DD channels (from Fig. 1) are combined, but multiplied by a factor of 2.5 over the entire time range 1 . In that case, the DTD perfectly matches the observations. When the standard two-infall model is used, this results in a G-dwarf metallicity distribution that peaks in the correct location, but with much less spread than the observations. On the other hand, when a constant SFR is assumed, the predicted G-dwarf metallicity distribution shows a good match with the observed one. As a side note, it is remarkable that the one obtained with the twoinfall model shows a better resemblance with the observational distribution obtained for a spherical solar neighborhood as is used for comparison by Kobayashi &amp; Nakasato (2011).\n- As a final step, the fraction of Fe in the solar neighborhood originating from SNe Ia is investigated. Fig. 13 shows its evolution as a function of time, for the constant SFR, respectively two-infall model calculations. Between 65 and 70% of the Fe present in the solar neighborhood today was created by SNe Ia. Donder &amp; Vanbeveren (2004) were the first to conclude that the predicted G-dwarf metallicity distribution best matches the observed one when both SD and DD events contribute. However, this was before the time that a reliable observational DTD was available, and did thus not allow a comparison in that respect. Moreover, the extent of the parameter studies was more limited, and galactic observational constraints and SN Ia progenitor assumptions have strongly evolved over the last decade. In the meantime, Matteucci et al. (2009) came to the same conclusion of SD+DD \"probably being the most realistic one\". However, this was achieved on the one hand with DTDs not calculated self-consistently from first-principle assumptions but instead imported either from observations or very simple models, and on the other hand with an artifical multiplication factor ensuring a match between theory and observation (a bit like our illustrative academic exercice in Fig. 3). This is particularly troublesome given the earlier mentioned critical sensitivity of DTDs to the metallicity. If one, Z-independent DTD is imported into the chemical evolution model and used throughout, this chemical evolution model will obviously not account for the (very important) dependence of SN Ia rates on metallicity.\n\nMentions:\n- As an academic exercise, Fig. 3 shows the G-dwarf metallicity distribution which is obtained when the standard model SD and DD channels (from Fig. 1) are combined, but multiplied by a factor of 2.5 over the entire time range 1 .\n- However, this was achieved on the one hand with DTDs not calculated self-consistently from first-principle assumptions but instead imported either from observations or very simple models, and on the other hand with an artifical multiplication factor ensuring a match between theory and observation (a bit like our illustrative academic exercice in Fig. 3).\n\nOCR Text: \"J 4 ( {2 00 [7-TI\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: G-dwarf metallicity distributions obtained withthe combined SD (withc1 = 1) + DD model, with SN Ia rates multiplied by a factor of 2.5, for a constant SFR (solid) and for the two-infall model (dotted). Observational data for a spherical (gray histogram) and cilindrical (white histogram) solar neighborhood by Holmberg et al. (2007)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The overall architecture of the PCIM system framework is illustrated in Figure 2. As it can be seen, it consists of Ethereum and IPFS networks. The Ethereum network is comprised of PCAC-SC and of a blockchain ledger to manage identity and access control within the network. The resulting encrypted medical images are stored in the IPFS network. We discuss the participant interactions with system module in the following subsections (See Figure .3).\n- Figure . 3 illustrates the process of how a patient and a radiologist interact between each other in the part of the proposed PCIM system, where medical image storage and sharing are performed. First, the patient undergoes the medical image examination performed by the radiologist. A medical image P I of the patient is produced. The patient seeks to have it protected and to maintain the ownership of this image. Consequently, to address this issue, the radiologist encrypts the initial medical image and obtains encrypted image. Thereafter, the radiologist obtains the hash of the encrypted image () p hI from the IPFS network and provides the patient with () p hI for the reference purpose. () p hI is stored in the blockchain, while the encrypted medical image P I is stored in IPFS. Owing to the fact that the image was encrypted, the patient medical image P I is accessible only to those who have the decryption key and thereby, it is protected from unauthorized access.\n- As presented in Figure .3, the exact protocol for this interaction is explained in detail as follows:\n\nMentions:\n- We discuss the participant interactions with system module in the following subsections (See Figure .3).\n- Figure . 3 illustrates the process of how a patient and a radiologist interact between each other in the part of the proposed PCIM system, where medical image storage and sharing are performed.\n- As presented in Figure .3, the exact protocol for this interaction is explained in detail as follows:\n\nOCR Text: \"Patiem Radiologist PCAC: SC Frotzcol IPFS Blockchain Ledgr\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIGURE 3. Interaction model of the PCIM system."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- [4] (Proce=Process _ start, Beco=Becoming _ aware) Figure 3: A sentence of full-text annotation. If this sentence object is stored under the variable sent, its frame annotation with respect to the target revenge is accessed as sent.annotationSet [2]. (The ? under looking indicates that there is no corresponding LU defined in the SEEKING frame; in some cases the full-text annotators marked but did not define out-ofvocabulary LUs which fit an existing frame. Also, some full-text annotation sets annotate an LU without its FEs-these are shown with ! to reflect the annotation set's status code of UNANN.) via main method frame _ relation _ types().\n- Annotation sets are organized by sentence. Corpus sentences appear in two kinds of annotation: exemplars() retrieves sentences with lexicographic annotation (where a single target has been selected for annotation to serve as an example of an LU); the optional argument allows for filtering the set of LUs. ft _ sents() retrieves sentences from documents selected for full-text annotation (as many targets in the document as possible have been annotated); the optional argument allows for filtering by document name. sents() can be used to iterate over all sentences. Technically, each sentence object contains multiple annotation sets: the first is for sentence-level annotations, including the part-of-speech tagging and in some cases named entity labels; subsequent annotation sets are for frame annotations. As lexicographic annotations have only one frame annotation set, it is visualized in the sentence display: figure 2 shows the display for f.lexUnit ['revenge.n'].exemplars [20]. Full-text annotations display target information only, allowing the user to drill down to see each annotation set, as in figure 3.\n\nMentions:\n- [4] (Proce=Process _ start, Beco=Becoming _ aware) Figure 3: A sentence of full-text annotation.\n- Full-text annotations display target information only, allowing the user to drill down to see each annotation set, as in figure 3.\n\nOCR Text: \"e5t Sectencp 14.4328 foro' (PCS ] [PCS_tazset 231 [text] lanrctacionSet] Lookio all zhe treir \"@verge e unly Lhal chey have Kinu Fin pcnce Mpro e-P 0cee Keco-heoim a ra| Cebllnq Rl\"et978\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: A sentence of full-text annotation. If this sentence object is stored under the variable sent, its frame annotation with respect to the target revenge is accessed as sent.annotationSet[2]. (The ? under looking indicates that there is no corresponding LU defined in the SEEKING frame; in some cases the full-text annotators marked but did not define out-ofvocabulary LUs which fit an existing frame. Also, some full-text annotation sets annotate an LU without its FEs—these are shown with ! to reflect the annotation set’s status code of UNANN.)"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Maximum Model Size Figure 6 shows the largest trainable model by enabling different ZeRO optimizations for a fixed batch size and MP of 16. The model size increase from 40B to 60B when trained with C1 vs C2 due to a 16x (MP degree) reduction in activation memory from using P a , while the jump to 140B using C4 is from enabling P os+g which halves the memory requirement by the model states compared to P os in C2. The increase to 150B using C5 is solely due to further reduction in activation memory from offloading the partitioned activation checkpoints to the CPU memory.\n\nMentions:\n- Maximum Model Size Figure 6 shows the largest trainable model by enabling different ZeRO optimizations for a fixed batch size and MP of 16.\n\nOCR Text: \"Fixed batch size 16 200 @ 150 100 1 ZeRO Config Number 8\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6: Max model size"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Our experiments are implemented using the LibSvm [20]. The whole speech emotion recognition is shown in Fig. 2.\n\nMentions:\n- The whole speech emotion recognition is shown in Fig. 2.\n\nOCR Text: \"\"4 GZ Konaa Chcle Faig Ferot\" RncI Arthc Lnra Lotx MtRuba plix SGe INCc CIL Ra-dcit Monin Tar Jnts'\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2. UBM-SVM based speech emotion recognition."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Threshold. Figure 19 shows the effect of the location estimation threshold (ℓ thr ) on Hapi's location accuracy. Increasing ℓ thr leads to removing uncertain areas when calculating the user location and consequently improving the system's location accuracy. However, as ℓ thr increases, it may lead to removing areas where the user can be located and decreasing the location accuracy. Thus, we set ℓ thr to 0.98. [22] and Locus [18].\n\nMentions:\n- Figure 19 shows the effect of the location estimation threshold (ℓ thr ) on Hapi's location accuracy.\n\nOCR Text: \"E65 1 5.5 45 1 045 06 075 09 Q Threshold\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 19: Effect of the location estimation ℓthr on Hapi’s 2D location accuracy."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Network Discovery Clients under observation were nearstationary throughout the experiment except during the induc-  tion of the cause Low RSSI. Therefore, the APs discovered at the clients remain to be the same. Our modified scanning strategy deliberately enables active scanning on Connection Establishment, so there is a possibility of inherent switching delay that occurs due to the transition from passive to active scanning. Thus, we analyze the delay to connection setup by analyzing the time elapsed between disconnection and connection. We refer to this as Time To Connect (TTC). Time To Connect We calculate TTC by analyzing the timestamps recorded for various events at the clients. Typically, when a disconnection is triggered series of events as recorded at the client-end are disconnected, scanning, and connected. In our case, the scanning can be either active or passive. The difference of timestamps recorded for disconnected and connected events give us the value of TTC. Figure 17 presents the CDF of TTC. We observe that for the modified clients TTC is much lesser than the unmodified clients. In fact, at the 50 th percentile the TTC is mostly constant at approximately 0.25 seconds for modified clients, whereas for unmodified clients it goes up to 4.35 seconds. The reason for much lower TTC is that the modified clients are not involved in unnecessary active scans and hence, its transmission queues are relatively less loaded. Connection Persistence Once the client is connected, transmitting lesser PReqs does not deter the performance of WiFi connection. We confirm this with our next metric that measured the duration of connection persistence. We measure this duration with the time elapsed between 2 successful connection establishments as recorded at the client. For the entire length of experiment, modified clients persistently showed better connection persistence, as shown in Figure 18. We investigate the probabilities for 1, 5, and 15 minutes of association. While the unmodified clients demonstrate probabilities as -0.20, 0.43, and 0.73, the modified clients remained associated with probabilities -0.37, 0.55, and 0.80. Clearly, the modified clients demonstrate higher chances of staying associated than the unmodified clients.\n\nMentions:\n- Figure 17 presents the CDF of TTC.\n\nOCR Text: \"Urmdite: Ulens] (Mdite: (lans] 0.001 0E01 Ta Contort Isernki\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 17: Time to connect. Median time to connect for unmodified clients is 4.35 seconds, whereas for modified clients it is 0.25 seconds. Even 75th percentile for unmodified clients is 43 seconds, whereas for modified clients it is 5.7 seconds."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We used quadri-variate time series signal, which is summation of sinusoids (with combination of frequencies f 1 = 4Hz, f 2 = 8Hz, f 3 = 16Hz, f 4 = 32Hz) and Gaussian white noise of zero mean and standard deviation of 0.2., i.e. x j (t) = i∈ [1,4] sin(2πf i t) + n j (t) (for j = 1, • • • , 4). A 32Hz sinusoid is present to first, second and fourth channels; a 16Hz sinusoid is present to first, second and third channels; a 8Hz sinusoid is common to all channels; a 4Hz sinusoid is present to first, third and fourth channels. On the same machine, computation time for MFDM is 0.45 sec. and for MEMD is 69.5 sec. in this simulation. The MFDM algorithm, similar to MEMD, generating perfectly aligned intrinsic bands, as shown in Figure 1, in all the four channels, whereas, MFDM is computationally more efficient.\n\nMentions:\n- The MFDM algorithm, similar to MEMD, generating perfectly aligned intrinsic bands, as shown in Figure 1, in all the four channels, whereas, MFDM is computationally more efficient.\n\nOCR Text: \"W M\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1: MFDM applied to a quadri-variate tone-noise mixture, g nerating perfectly aligned intrinsic modes in all the four channels."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- For small values of k 0 L n , Mjølhus' formula for the conversion efficiency as a function of the injection angle 21 is no longer valid. This is nicely illustrated in Fig. 5, where simulations for three different values of k 0 L n are compared with the corresponding solution from Mjølhus' equation. For these 1D simulations, the density profiles were taken to be of parabolic shape.\n- In the case considered here, the variation between the analytical solution in the WKB limit and the full-wave solution is not very strong for k 0 L n ≥ 5. However, at small density gradient length, for example such that k 0 L n = 2, the WKB limit and Mjølhus' formula are no longer valid. This results in the discrepancy between the values obtained from the simulations and Mjølhus' formula that can be seen in Fig. 5. The general trend that for steeper profiles the conversion efficiency becomes less sensitive to an angular mismatch can also be clearly seen.\n- The highest conversion efficiencies of 75% are also achieved for profile #3, in which the distance between the mode conversion layer and the antenna is the shortest. Density profile #2 and #3 have shorter gradient lengths in the mode conversion region, and, thus, smaller values of k 0 L n , which results in the observed broader angular window (c. f. Fig. 5).\n\nMentions:\n- This is nicely illustrated in Fig. 5, where simulations for three different values of k 0 L n are compared with the corresponding solution from Mjølhus' equation.\n- This results in the discrepancy between the values obtained from the simulations and Mjølhus' formula that can be seen in Fig. 5.\n- Density profile #2 and #3 have shorter gradient lengths in the mode conversion region, and, thus, smaller values of k 0 L n , which results in the observed broader angular window (c. f. Fig. 5).\n\nOCR Text: \"0.8 kLF2 kL-5 8 0.2 kL-20 0.8 0.9 1,0 1.1 1.2 pal;,01 9 DaJN\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 5. Conversion efficiency as a function of the parallel component of the normalized refractive index, where parallel refers to the direction of the magnetic field. The different symbols correspond to results obtained from full-wave simulations for different values of the density length scale normalized to the vacuum wavelength, k0Ln, as labeled in the plot. The solid lines indicate the corresponding solution from the Mjølhus equation21."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- As mentioned previously, in his work Gramsch investigated a network composed of 68 actors, as well as the political relations and interactions between them over a period of ten years (from 1225 to 1235). These form the so-called sociomatrices, which can be identified with adjacency matrix, albeit with negative entries. As in [4], we use one socio-matrix for each year (unless otherwise stated), i.e., the temporal dimension here is one year. Such socio-matrices can be depicted as a network. For instance, Fig. 1 is a reproduction of Fig. 6 in [5]. It shows friendly, hostile and neutral (ambivalent) relationships in black, red, and yellow respectively. In order to cluster actors, Gramsch imposes that within a cluster there should be no negative connections (conflicts), without exception. For the year 1125 this then translates into the clustering shown in Fig. 2 (reproduction of Fig. 7 in [5]). He is also able to identify particular groups of actors such as the group of North Germany or the Lower Rhine.\n\nMentions:\n- For instance, Fig. 1 is a reproduction of Fig. 6 in [5].\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: The German political network from the socio-matrix of March 1225 (reproduction from Gramsch, 2013). For the meaning of some names, see below."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In this section, we provide a numerical evaluation of the performance of the power control algorithm proposed in Section IV. Note that the exact expressions of SEs are used to compute the SEs. The simulation setup consists of a multicell network with 9 cells as illustrated in Fig. 1. In addition, we use a wrap-around technique to avoid edge effects. We assume that the BSs are located in a 1 km 2 area. Each BS serves two CUs which are randomly distributed with uniform distribution in the BS's coverage area. The network contains 10 D2D pairs randomly distributed in the coverage area with uniform distribution. Since the pairs are distributed randomly, some cells will be more affected by D2D interference than others. We further assume that the distance between the transmitter and receiver of each D2D pair is 10 meters. We use the three-slope path-loss model from [17] to model the large-scale fading in the network and the parameters of this model is chosen from [18]. This model contains two reference distance d 0 and d 1 , that are chosen to be 10 and 50 meters, respectively. The carrier frequency is 2 GHz, the bandwidth is 20 MHz and the coherence interval contains 200 symbols. The total number of pilots assigned for D2D pairs is 5, where each D2D pair picks one of them randomly. In addition, the noise variance is set to −94 dBm and the maximum transmit power of the CUs and D2D transmitters are chosen to be 200 mW for both pilot and data transmission. All users transmit their pilot with maximum power and we perform power control for data transmission. The performance of the proposed power control scheme is compared with the case without D2D pairs for ZF processing at all BSs. In addition, we provide result for the case of maximum power transmission at both D2D transmitters and CUs. Fig. 2 shows simulation results in terms of the cumulative distribution function (CDF) of the SE of a typical CU k. It can be seen that the performance of our proposed algorithm is almost the same as in the case without D2D underlaying, thus the power control can efficiently mitigate the D2D interference. Comparing these results with the case of maximum power transmission, max-min power control improves the performance of the 10% weakest users which is the goal of max-min fairness power control. Note that some fortunate users can get very high SE with maximum power transmission, but we did not show the results for values above 6 b/s/Hz, since current standards do not support higher SEs than that. The gain of our proposed power control for D2D underlay is further shown in Fig. 3, where we compare the sum SE of the network for the D2D underlay case with the case of only cellular communication without D2D underlay. As we can see in this figure, the sum SE is higher with D2D underlaying. This is due to the fact that we have more active users in the network. We can conclude that D2D underlaying multi-cell massive MIMO enhances the SE compared to conventional massive MIMO systems.\n\nMentions:\n- The gain of our proposed power control for D2D underlay is further shown in Fig. 3, where we compare the sum SE of the network for the D2D underlay case with the case of only cellular communication without D2D underlay.\n\nOCR Text: \"Mela-Mminnnsen Mi;-M -wuvai um Sum SE [ns H-]\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3. Sum SE of network"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- For completeness in Fig. 5 we also present our calculations of unpolarized pp → (jet π + ) + X cross-section, see Eq. (1), at √ s = 500 GeV as a function of the pion momentum j ⊥ for z h = 0.37. As expected, calculations without TMD evolution follow simple gaussian shape that is much narrower compared to calculation with TMD evolution. One can see from Fig. 5 that effects of evolution are much more dramatic in unpolarized distributions compared to asymmetries. An experimental study of the cross-section as function of j ⊥ will be very helpful in order to study evolution effects. √ s = 500 GeV as a function of the pion momentum j ⊥ for z h = 0.37. Solid lines correspond to calculations that take into account TMD evolution using the extracted TMDs of Ref. [17] and the dashed lines correspond to extraction of TMDs without TMD evolution of Ref. [18]. We have p T = 31 GeV for the average jet transverse momentum and the jet rapidity is integrated over the range 0 &lt; η &lt; 1.\n\nMentions:\n- For completeness in Fig. 5 we also present our calculations of unpolarized pp → (jet π + ) + X cross-section, see Eq. (1), at √ s = 500 GeV as a function of the pion momentum j ⊥ for z h = 0.37.\n- One can see from Fig. 5 that effects of evolution are much more dramatic in unpolarized distributions compared to asymmetries.\n\nOCR Text: \"3 :ath} 6-8 Cei # Ncem #Tii] 1,003, 131C\" c<q<1 (Gev;\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: Unpolarized pp → (jet π+) + X cross-section at √s = 500 GeV as a function of the pion momentum j⊥ for 〈zh〉 = 0.37. Solid lines correspond to calculations that take into account TMD evolution using the extracted TMDs of Ref. [17] and the dashed lines correspond to extraction of TMDs without TMD evolution of Ref. [18]. We have 〈pT 〉 = 31 GeV for the average jet transverse momentum and the jet rapidity is integrated over the range 0 < η < 1."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Then for our world knowledge dependent network, we use the network schema shown in Figure 7 to represent the data. The network contains multiple entity types: document D, word W, named entities {E I } T I=1 , and a few relation types connecting the entity types. Notice that, we use \"entity type\" to represent the node type in HIN, as Definition 4.1 showed. We use \"named entity type\" to represent the type of the name mentioned in text (widely used in NLP community), e.g., person, location, and organization names. The entities in HIN do not have to be named entities, e.g., the categories of animals or diseases. We denote the document set as D = {d 1 , d 2 , . . . , d M }, where M is the size of D, the word set as W = {w 1 , w 2 , . . . , w N }, where N is the size of W, and the Fig. 8: Document based heterogeneous information network example. The network G contains five entity types: document, word, date, person and location, which are represented with gray rectangle, gray round, green square, blue round, and yellow triangle, respectively. entity set as E t = {e t\n- In Figure 8, we show a real example of the world knowledge dependent network. From the figure, we can see that two documents represented as gray rectangles are modelled in the HIN. Besides the two documents, we have words represented as gray rounds. The link between a word and the corresponding document indicates that the document contains the word. We also have named entities that associated with certain types (e.g., date, person, location), relevant to the documents specified from the knowledge base. The link between a document and a named entity means that the document contains the named entity. The link between two named entities represents the relation between the named entities in the knowledge base generated by world knowledge specification. In summary, after performing world knowledge specification for documents, world knowledge representation aims to construct a document based HIN that contains the link and type information that are useful to understand the documents, thus lead to better text mining performance.\n\nMentions:\n- We denote the document set as D = {d 1 , d 2 , . . . , d M }, where M is the size of D, the word set as W = {w 1 , w 2 , . . . , w N }, where N is the size of W, and the Fig. 8: Document based heterogeneous information network example.\n- In Figure 8, we show a real example of the world knowledge dependent network.\n\nOCR Text: \"hxx\"Jcttexh:] ufr |letin cleFs zt023V) ?te35; |tce rte Vilei esaltsioxil\"o DycI;h _n); Iitrstco ; Jnelokgezi Iieicpt iaussile-J- Jusito n\"zh15\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 8: Document based heterogeneous information network example. The network G contains five entity types: document, word, date, person and location, which are represented with gray rectangle, gray round, green square, blue round, and yellow triangle, respectively."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Another example is provided by Fig. 19, which shows deuteron-induced reactions on 209 Bi. The shapes of the cross sections are globally reproduced fairly well, though some details are missed. The decrease of the (d, 2n) cross section on the right of the peak is somehow too slow. The precise threshold behavior for (d, n) and (d, p) reactions is not exactly reproduced, in spite of the use of exact Q-values. Actually, the theoretical (d, n) and (d, p) cross sections are roughly the same. However, the experimental (d, n) cross section is roughly 4 times smaller than the (d, p) cross section in the region of the maximum (∼10 MeV). This discrepancy might be due to the Coulomb Refs. [95,99] for 209 Bi(α, n) and from Refs. [99,100] for the two other reactions. In the first panel, the data of Ref. [99] are singled out by the large error bars.\n- Another possible interpretation calls for the Coulomb dissociation of the projectile. One can imagine that, at low energy, below or slightly above the Coulomb barrier, the 3 He is dissociated, say in d + p, by the Coulomb field before it really hits the nucleus. If the deuteron escapes, the accompanying proton \"sees\" a Coulomb barrier which is twice as small as the one seen by the 3 He, but its kinetic energy is three times as small.The proton is thus repelled. This phenomena would shift a part of the ( 3 He, n) and ( 3 He, 2n) cross section to other channels. An argument in favour of this interpretation, quoted in Ref. [104], comes from the fact that the sum of the experimental 181 T a( 3 He, xn) cross sections does not exhaust the total reaction cross section. This effect, based on Coulomb dissociation, is expected to be roughly controlled by a parameter that can be losely defined as η = (Zr) 2 /B, where Z, r and B are the charge, the radius and the binding energy of the cluster, i.e. increasing with the electric polarisability of the projectile and inversely proportional to its \"stability\" [35,[105][106][107]. On these grounds, one expects that the effects encountered for 3 He are largely reduced for α-particles, as one may indeed observe above. However,the effect should be even more pronounced for deuterons. But this does not seem to be the case for the example of Fig. 19. Of course, the effect is expectedly reduced for light targets. We now turn to illustrative results for light targets. Fig. 24 shows a comparison of our predictions with experiment concerning (α, n) reactions on 56 F e and 60 N i targets. The agreement for the 56 F e(α, n) cross section is quite good. There are two sets of data points for the reaction 60 N i(α, n). Our predictions are close to the data set of Ref. [110]. In addition, one can notice that our model is able to respect the difference between the cross sections for the two targets, that are in fact rather similar to each other. The reason is that our model properly takes account of the different Coulomb penetrabilities and, especially, of the real Q-values. The latter are -5.053 and -7.938 MeV, for 56 F e(α, n) and 60 N i(α, n) reactions, respectively.\n- The last case refers to the 60 N i(d, n) reaction, illustrated in Fig. 26. Although the opening of the channel is well described in our model, the cross section is noticeably underestimated, in similarity with the 209 Bi case (see Fig. 19). This observation may be consistent with the explanation in terms of the Coulomb dissociation. However, this effect should in principle be much smaller 60 N i, as stated above.\n\nMentions:\n- Another example is provided by Fig. 19, which shows deuteron-induced reactions on 209 Bi.\n- But this does not seem to be the case for the example of Fig. 19.\n- Although the opening of the channel is well described in our model, the cross section is noticeably underestimated, in similarity with the 209 Bi case (see Fig. 19).\n\nOCR Text: \"d+Bi INCL4.6 CAM chinnci Lu= Jpd\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 19: Deuteron-induced reactions on 209Bi. The predictions of the INCL4.6-ABLA07v5 model (full red lines) are compared with experimental data (symbols) relative to 209 Bi(d, n) (squares), 209Bi(d, 2n) (full dots) and 209Bi(d, p) (triangles) reactions. Cross sections are given as functions of the deuteron incident kinetic energy. Data from Refs. [101– 103]."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To verify the proposed scaling law in eq. (3), we approximate the number of nodes N by the population of an area, since generation data are not available for all synchronous areas and population and generation tend to be approximately proportional [42]. We utilise the population size as a proxy for size of the grid N . Indeed, we note that the aggregated noise amplitude does approximately decay with the inverse square root of the population size, as predicted, see Fig. 3. At a certain size, the noise saturates. The deviations from the prediction, such as by South Africa (ZA) and Iceland (IS) are likely caused by different local control mechanisms, or non-Gaussian noise distributions, which we focus on in the next section. Interestingly, while Faroe Islands (FO) and Mallorca (ES-PM) do display non-Gaussian probability density functions, they follow the proposed scaling law. Why this is the case and how a fully non-Gaussian scaling law could capture this even better remain open questions for future work. Still, we observe a decay of the noise, approximately following the prediction over four orders of magnitude.\n- We summarise that smaller regions tend to display more intermittency in their increments than larger regions, again consistent with findings on the scaling of the aggregated noise amplitude (Fig. 3). Furthermore, we observe that increment distributions tend to approach Gaussian distributions for larger increments, as expected [9], but with distinct time horizons that depend on the grid area. For most of the islands the excess kurtosis remains high even for lags of ten seconds. Contrary, in most areas of continental size the excess kurtosis is very small already for lags larger than one second. Very interesting is also the following observation: Non-Gaussian distributions in the aggregated frequency statistics (Fig. 2) are not necessarily linked with non-Gaussian increments. For example in Continental Europe (DE) we observe Gaussian increments but a non-Gaussian aggregated distribution. The deviation from Gaussianity in the aggregated distribution, e.g. in terms of frequent extreme events, is likely explained by the external drivers, such as market activities [47]. Finally, this analysis presented here extends previous increment analyses [22,48], which only considered increments of less than a second (τ &lt; 1 s), while we observe relevant non-Gaussian behaviour for larger increments (τ ≥ 1 s). We further analyse the differences between aggregated kurtosis and increment kurtosis in Supplementary Note 1 and discuss Castaing's model [49] and superstatistics [50] as more theoretical approaches towards increment analysis in Supplementary Note 3.\n- In addition to the standard deviation σ or the aggregated noise amplitude , we might also consider using a Gaussian kernel to detrend the data and only analyse the standard deviation of the detrended data. Finally, we could also consider using the standard deviation of the increments (at the smallest available time lag τ = 1) as a proxy of the actual noise. We compare these different approaches in Supplementary Fig. 2: Moving from top to bottom: Taking the unfiltered standard deviation of the data (Supplementary Fig. 2 a), does give a rough trend but areas as South Africa (ZA) or Great Britain (GB) have a much larger standard deviation than we would expect from the scaling law. When we introduce the de-trending, the overall standard deviation drops (Supplementary Fig. 2 b) but the problems with GB and ZA persist. Likely this is caused by control regulations that allow larger deviations than they are allowed for example in Continental Europe. Next, we extract the aggregated noise amplitude (Supplementary Fig. 2 c) and observe a decay, well-approximating the conjectured scaling law. Finally, we note that using the standard deviation of the increments at lag τ = 1s is almost identical to the aggregated noise amplitude (Supplementary Fig. 2 d). Comparing all four plots, in particularly in terms of the standard deviation of the best fit (shaded area), leads to the conclusion that panels c and d are the best descriptors. Both the extracted noise as well as the increments on the one second scale ∆f 1 are good proxies of the diffusive forces acting on the synchronous area. Supplementary Figure 3. Increment analysis reveals non-Gaussian characteristics, dominantly in islands. We plot the Castaing parameter λ 2 τ , given by Supplementary Eq. ( 14), for the different examined power grid frequency recordings. We observe a non-vanishing intermittency in Gran Canaria (ES-GC), Iceland (IS), Faroe Islands (FO), Mallorca (ES-PM), Britain (GB), Texas (US-TX) , and South Africa (ZA). In contrast, the increments' distribution of the Baltic (EE), Continental Europe (DE), Nordic (SE), Russia (RU) synchronous areas and the Western Interconnection (US-UT) approach a Gaussian distribution.\n\nMentions:\n- Indeed, we note that the aggregated noise amplitude does approximately decay with the inverse square root of the population size, as predicted, see Fig. 3.\n- We summarise that smaller regions tend to display more intermittency in their increments than larger regions, again consistent with findings on the scaling of the aggregated noise amplitude (Fig. 3).\n- Supplementary Figure 3.\n- Computing the Castaing parameter λ 2 τ for our data, we observe very heterogeneous results between the various synchronous areas, see Supplementary Fig. 3.\n\nOCR Text: \"## KC LSCT USIE \"oIlalon\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3. The noise tends to decrease with an increasing size of the synchronous area until it saturates. We plot the extracted noise amplitude compared to the logarithm of the population in a given synchronous area. The population size serves as a proxy for the total generation and consumption of that area, as data on the size of the power grids is not commonly available. The shaded area is the standard deviation of the estimation."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In order to assess whether the motor specification fit these requirements, we analyzed a simple 1 degree of freedom model shown in Figure 3. The mallet was modeled as a combination of a ball and a rod. The ball had radius R and mass m b , and the rod had length L and mass ms. The mallet rotates with the pivot point at the center of the motor with angle θ. For downstroke, the net torque on the striker M d with motor torque output Mm is,\n\nMentions:\n- In order to assess whether the motor specification fit these requirements, we analyzed a simple 1 degree of freedom model shown in Figure 3.\n\nOCR Text: \"M\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Shimon’s new striker consists of a mallet and a motor shaft. The mallet is modeled as a ball with radius R and a rod with length L"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The experimental results for evaporating droplets (initial volume 5 µl) of pure water (E 0% + W 100%), binary mixture of varying compositions and pure ethanol (E 100% + W 0%) deposited on a heated substrate are discussed in this section. The substrate is maintained at T s = 60 • C and the experiments are conducted for seven compositions of ethanol-water binary mixtures. These compositions and the corresponding lifetimes of the droplet are presented in Table II. It can be seen that the droplet lifetimes, t e range from 12 seconds for a pure ethanol droplet to 190 seconds for a pure water droplet, which are much smaller than the corresponding values of t e at T s = 25 • C (Table I). The variation of t e with the percentage volumetric concentration of ethanol (E %) in the binary mixture at T s = 60 • C is shown in Figure 9. It can be seen that there is a rapid linear decrease in the lifetime of the droplet with an increase in the concentration of ethanol till E = 50%, after which the lifetime of the droplet is seen to vary only slightly with further increase in ethanol concentration till E=80%. We believe that this behaviour can be attributed to the non-ideal vapour pressure phase diagram of water-ethanol binary mixtures [37] which is discussed further in Section IV B.\n\nMentions:\n- The variation of t e with the percentage volumetric concentration of ethanol (E %) in the binary mixture at T s = 60 • C is shown in Figure 9.\n\nOCR Text: \"(%)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 9: The variation of the lifetime time, te (in s) of a droplet with the percentage concentration of ethanol (E) in the binary mixture at Ts = 60 ◦C."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Fig. 3, we integrate the network for T = 10 5 and we plot the three degree distributions. As expected, the out-community ρ(k o ) and the total ρ(k) degree distributions falls as power laws with exponent −ν. On the other hand, the in-community degree ρ(k c ) saturate to the community size distribution P (s), as all the nodes reach their maximum in-community degree value (s − 1), being that the modules' size is far smaller than the network size (s max = √ N N ). On the contrary, the out-community degree takes longer times to saturate to its maximum value N − s s. It is worth stressing that the results presented in this section apply to the networks obtained integrating links over time. A process unfolding on such networks, in general, will be affected by the time-aggregated features of the graph. The extent to which this is true, is function of the interplay between the time-scale describing its evolution, τ P , and the various τ (a, s). In the limit τ p τ (a, s) the process would effectively evolve on the instantaneous, annealed networks that are characterized by a small average degree and modularity. In the opposite limit instead, the process would effectively unfold on static networks obtained integrating links over longer time characterized by high average degree and low modularity. Indeed, the average degree is this regime will be dominated by out-community links that make the connections between different communities increasingly stronger, thus increasingly destroying the identity of communities. In the limit τ p ∼ τ (a, s) the process would effectively evolve on maximally modular networks (for a given set of parameters). Arguably, this is the most interesting regime that we will consider in the following.\n\nMentions:\n- In Fig. 3, we integrate the network for T = 10 5 and we plot the three degree distributions.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 3. Plot of the three degree distributions and the theoretical prediction, given in Eq. 4. Parameters used are: N = 105, ω = 2.1, ν = 2.1, m = 1, smin = 10, µ = 0.9 and T = 105 evolution steps."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Consider a steady uniform flow U ∞ over a thin flat plate of length L and infinite width. Inside the two-dimensional (2D) (Blasius) boundary layer that develops over the plate, we place a small localized disturbance (denoted by d in Fig. 1) of simple Gaussian shape; the set-up is the same as in [35] and the simulation is performed using a spectral code [36]. Fig. 2 summarizes the spatio-temporal evolution of the disturbance. It shows a contour plot of the stream-wise component of the perturbation velocity at a wall normal position Y = δ * (0), where δ * (X) is the displacement thickness of the boundary layer. The temporal growth of this disturbance is determined by classical linear stability theory (i.e. eigenvalue analysis of the linearized Navier-Stokes equations). Such an analysis reveals that asymptotically a compact wave-packet emerges -a TS wave-packet -that grows in time at an exponential rate while travelling downstream at group velocity of approximately U ∞ /3. This disturbance behaviour is observed as long as the amplitude is below a critical value (usually a few percent of U ∞ ) [5]. Above the critical value, nonlinear effects have to be taken into account; they eventually result in a break down of the disturbance to smaller scales and finally to transition from a laminar to a turbulent flow [5]. However, the key point -that enables the use of linear theory for transition control -is that the disturbance may grow several orders of magnitude before it breaks down.\n- Using a spatially localized forcing (denoted by u in Fig. 1) downstream of the disturbance, one may modify the conditions in order to reduce the amplitude of the wavepacket and thus delay the transition to turbulence. Physically this forcing is provided by devices called actuators. An example of an actuator is a loudspeaker that generates short pulses through a small orifice in the plate. The volume of the loudspeaker and the shape of the orifice determines the type of actuation. Another example is plasma actuators, where a plasma arch is used to induce a forcing on the flow [37].\n- In closed-loop control, a sensor (denoted by y in Fig. 1) is used to measure the disturbance that is meant to be cancelled by the actuator (u): based on these measurements one computes the actuator action in order to effectively reduce the amplitude of the perturbation. Examples of sensors include pressure measurements using a small microphone membrane mounted flush to the wall, velocity measurements using hot-wire anemometry near the wall or shearstress measurements using thermal sensors (wall wires). Finally, we place a second sensor (denoted by z in Fig. 1) downstream of the actuator to measure the amplitude of the perturbation after the actuator action. The minimization of this output signal may serve as an objective of our control design, but the measurements also provide a means to assess the performance of the controller.\n\nMentions:\n- Inside the two-dimensional (2D) (Blasius) boundary layer that develops over the plate, we place a small localized disturbance (denoted by d in Fig. 1) of simple Gaussian shape; the set-up is the same as in [35] and the simulation is performed using a spectral code [36].\n- Using a spatially localized forcing (denoted by u in Fig. 1) downstream of the disturbance, one may modify the conditions in order to reduce the amplitude of the wavepacket and thus delay the transition to turbulence.\n- In closed-loop control, a sensor (denoted by y in Fig. 1) is used to measure the disturbance that is meant to be cancelled by the actuator (u): based on these measurements one computes the actuator action in order to effectively reduce the amplitude of the perturbation.\n- Finally, we place a second sensor (denoted by z in Fig. 1) downstream of the actuator to measure the amplitude of the perturbation after the actuator action.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1. Scheme of a Blasius boundary-layer flow developing over a flat plate. A disturbance modelled by d grows exponentially while convected downstream. The actuator u is used to attenuate the disturbance before it triggers transition to turbulence; the actuation signal is computed based on the measurements provided by the sensor y. The output z, located downstream of the actuator, estimates the efficiency of the control action."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We found the solution for p = L 3 , r = 0 corresponding to the first excited state of Hamiltonian. To make sure that this really is the lowest lying excited state, we compared to the results of a numerical diagonalization of the Hamiltonian for finite lattice lengths. This state has an equal density of particles A, B and vacancies of 1 3 . The roots all lie in the complex plane. A typical pattern of roots for the first excited state of the Hamiltonian is shown for L = 36 in Fig. 2. The solid line is the unit circle that is given as a guide to the eye.\n\nMentions:\n- A typical pattern of roots for the first excited state of the Hamiltonian is shown for L = 36 in Fig. 2.\n\nOCR Text: \"Ficam\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2. Complex roots λk, k = 1, . . . , 12 for the Bethe ansatz equations with p = L 3 , r = 0 and L = 36."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Store coefficients in LUT Figure 19 Training process for repairing de-blocking algorithm\n\nMentions:\n- Store coefficients in LUT Figure 19 Training process for repairing de-blocking algorithm\n\nOCR Text: \"TeTeyl Ohu} Cez) GErBl X} \"h\"o] (Elclc LaS Szitljic Ieclit\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 19 Training process for repairing de-blocking algorithm"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Finally, for J 1 and J 2 antiferromagnetic, we fix J 1 + J 2 = 1 and change J 1 − J 2 as a parameter. We observe that NNN pairings dominate for J 1 − J 2 &lt; −0.1 and NN pairings dominate for J 1 − J 2 &gt; 0.2 (Fig. 4). In the intermediate range, there is only weak B-type pairing. A schematic phase diagram within the range 0 &lt; J 1 , J 2 &lt; 1 is shown in Fig. 5.\n\nMentions:\n- A schematic phase diagram within the range 0 &lt; J 1 , J 2 &lt; 1 is shown in Fig. 5.\n\nOCR Text: \"NNN S-wiave NNs-wave NNN $-wave NN 5-wave NN d-wave NN $-wzve\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 5. (color online) A schematic phase diagram for the model (6) within 0 < J1, J2 < 1."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The second prototype (V2) consists also of four cassettes but we operate them at 5 • . At this inclination the expected efficiency at 2.5Å is about 43% if we employ the sputtered coating of the version V1 [14]. The cassettes are the ones shown in Figure 18, conceived to study the single converter layer option. A rigid substrate is directly coated with the converter material. The cassette width in the version V1 was about 12 mm, in the version V2 we reduce their actual size to 6 mm. Consequently the MWPC gap, between the converter and the cathodes, is 4 mm. With respect to the version V1 the wire plane is closer to the converter. The prototype active area, considering the cassette overlap, is about 3.2 × 9 cm 2 . The read-out PCBs are those used in the version V1. Figure 19 shows a cassette substrate both coated and un-coated. Since the efficiency is saturated as the thickness of the layer exceeds 3 µm, we study the possibility to use different converters. We can deposit a painting containing 10 B grains and make a coating a few hundreds of microns thick. A thick layer also functions as an integrated collimator. Any neutron that comes from the sides of the detector has less probability to be detected and is more likely absorbed in the outer layers. Hence, only neutrons which impinge the detector from the front have a serious chance to generate a signal. Neutron background is then decreased. The uniformity of the coating, even in the single layer configuration, is an important aspect to guarantee the converter flatness. The latter has to ensure the precision of the neutron incidence angle, in fact if it varies slightly the efficiency changes widely. Furthermore, a deviation from the converter flatness also induces the variation of the electric field and then the local gain of the detector changes. The roughness of the converter should be below the neutron capture fragment ranges, which is of the order of a few µm for 10 B. In fact, the gain in efficiency due to an inclination comes from the fact that the neutron path travels close under the surface. If the surface is irregular (on the µm scale or more), that can be seen as equivalent for a neutron to hit a surface perpendicularly, there is not much gain in efficiency. It is crucial that the size of our grains, in the painting, is less than the particles ranges, i.e. their size should be below the micron scale for 10 B. The conductivity of the painting can be an issue. If the resistivity is too large the charge evacuation is not guaranteed and consequently the actual electric field is affected. We mix a glue with 10 B grains of sizes ∼ 10µm. We did not have access to a finer-grained 10 B powder: our grinding technique resulted in ∼ 10 µm grain size. We used this powder. As the grain size is not smaller than the fragment ranges we know that there can be an efficiency issue. The layer resistivity was measured to be about 50 MΩ • m and for a 0.5 mm thick layer. Figure 20 shows two PHS: one is taken with a 3 µm thick 10 B 4 C layer [14] and the other with the 10 B painting both installed in a MWPC, at normal incidence and with a 2.5Å neutron beam. The difference in gain on the two spectra is due to the difference in the gas gap between the wire plane and the converter, since the painting is a few mm closer to the wires than the sputtered layer. The painting efficiency is 1.5% lower than the sputtered coating. In order to investigate if the resistivity of the painting is not to large to avoid the evacuation of the charges, we place the painting layer on a very intense beam of 560 KHz and we measure the counting rate as a function of time. There are no losses after several hours. The resistivity of the 10 B painting seems to be acceptable.\n\nMentions:\n- Figure 20 shows two PHS: one is taken with a 3 µm thick 10 B 4 C layer [14] and the other with the 10 B painting both installed in a MWPC, at normal incidence and with a 2.5Å neutron beam.\n\nOCR Text: \"{cennq coamad ~caii Cuse helnhihni 3cam\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 20. Comparison between the PHS of the sputtered coated layers [14] and the10B painting."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where λ &gt; 0 is a relaxation parameter. However, the ill-posed nature of the tomography problem limits the SIRT strategy to exhibit only semi-convergence [30,31]. Accordingly, improvements seen in the first few SIRT iterations start to deteriorate as the number of iterations increases due to noise propagation. In our analysis, we show a locally converged SIRT result obtained after 10 iterations with λ = 0.25 and W set as the identity matrix. The MBIR reconstruction is obtained by iteratively minimizing the surrogate cost function of eq. 22 which, in turn, is achieved by following Algorithm 1. The q-GGMRF parameters used in this implementation were p = 1.1, T = 0.001 and σ x = 0.8. We used a 3 × 3 non-causal weighting matrix, b ij , to incorporate the influence of two nearest neighbors for any given pixel. The first nearest neighbors are weighted by a factor of 1/6 while the second nearest nearest neighbors are weighted by a factor of 1/12. The decreasing trend of the surrogate cost function over several iterations is shown in Fig. 8. An important point to note here is that the decreasing trend of the surrogate cost is a direct consequence of its strictly convex nature. Convergence after the cost minimization means that the reconstruction is globally minimal and unique. Hence, the coarseness seen along the inner region of the locally converged SIRT result is no longer visible in the MBIR result. Also, the superiority of the MBIR approach over any of the other tomography methods can be discerned from the RMSE analysis which reveals an error of only 0.0213.\n\nMentions:\n- The decreasing trend of the surrogate cost function over several iterations is shown in Fig. 8.\n\nOCR Text: \"1 1 1 1 No. cf iterations 1\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 8: Illustration of minimization of surrogate cost function using the ICD technique to deduce the MBIR estimate of reconstructed Shepp-Logan phantom in Fig. 7(e)"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Noise Level Estimator Estimating the noise level would not be an easy task due to the complex noise model and in-camera pipeline. In our experiment, we find that previous simple noise level estimators [19,7], which consist of five convolutions, could not accurately estimate the noise level. The main reason is that the previous estimators have a Figure 3: Illustration of the proposed AIN-ResBlock with corresponding kernel size (k), feature scale (s), and number of features (n). Note that n is linearly increasing according to s. Leaky ReLU is employed for an activation function. The Norm (red) block denotes channel-wise spatial normalization block. Average-pool scales the size ofσ(y) to be the same as that of h. small receptive field so that it could not fully capture complex noise information. From this observation, we design a new noise level estimator with a larger receptive field by employing down/up-sampling and multi-scale estimations. Specifically, estimator produces down-scaled estimation mapσ 4 (y) ∈ R H/4×W/4×3 and original-sized estimation mapσ 1 (y) ∈ R H×W ×3 . Then, these two outputs are weight averaged to feed reconstruction network:\n- Adaptive Instance Normalization The proposed AIN-ResBlock plays two crucial roles in the proposed denoising scheme. One is regularizing the network not to be overfitted to SN images, and the other is adapting SN denoiser to RN denoiser. For this, we build AIN-ResBlcok with two convolutions and two AIN modules, which is presented in Fig. 3. The AIN module affine transforms normalized feature map h ∈ R H ×W ×C of convolution by taking a conditional inputσ(y) where H × W denotes the spatial size of feature map at each scale s, and C is the number of channels. Specifically, the AIN module produces affine transform parameters such as scale (γ) and shift (β) for each pixel. Thus, every feature map is channel-wisely normalized and pixel-wisely affine transformed according to the noise level. The update process of feature map in AIN module at site (p ∈ R H , q ∈ R W , c ∈ R C ) is formally repre- sented as\n\nMentions:\n- The main reason is that the previous estimators have a Figure 3: Illustration of the proposed AIN-ResBlock with corresponding kernel size (k), feature scale (s), and number of features (n).\n- For this, we build AIN-ResBlcok with two convolutions and two AIN modules, which is presented in Fig. 3.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Illustration of the proposed AIN-ResBlock with corresponding kernel size (k), feature scale (s), and number of features (n). Note that n is linearly increasing according to s. Leaky ReLU is employed for an activation function. The Norm (red) block denotes channel-wise spatial normalization block. Average-pool scales the size of σ̂(y) to be the same as that of h."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Fig. 2, the QSL time τ QSL (τ ∞ ) (blue solid curve) together with τ 2 (red dot-dashed curve) and τ 1 (green dashed curve) and non-Markovianity N (Λ) (black dotted curve) are compared to parameter ξ in the case α = π/4 and τ = 2π/(∆ω∆n) with ∆ω = ω 2 − ω 1 , where the related parameters are all selected according to experimental data with ∆n = 0.01, σ = 1.8 THz, ω 1 = 2.676 PHz (≃ 704.5 nm), and ω 2 = 2.692 PHz (≃ 700.3 nm) [26]. The most remarkable feature appeared in Fig. 2 is that the non-Markovian effect will slow down the quantum evolution, for the monotonicity of N (Λ) is in agreement with τ QSL in the non-Markovian region ξ ∈ [ξ 1 , ξ 2 ]. By controlling the environment parameter ξ (related to the tilted angle of the FP cavity), QSL time can be well controlled. The above phenomenon that the stronger the non-Markovianity, the longer time required to reach the target state is just the opposite side illustrated in Ref. [22], where the non-Markovian effect will speed up the evolution for corresponding model. To our common wisdom, the non-Markovianity reflects the memory effect of the environment, which is usually thought as beneficial in quantum tasks [44]. The above phenomenon, however, implies that for a photon, the non-Markovian effect may slow down its quantum evolution which may do harm to quantum computational processes with photon systems [45].\n\nMentions:\n- In Fig. 2, the QSL time τ QSL (τ ∞ ) (blue solid curve) together with τ 2 (red dot-dashed curve) and τ 1 (green dashed curve) and non-Markovianity N (Λ) (black dotted curve) are compared to parameter ξ in the case α = π/4 and τ = 2π/(∆ω∆n) with ∆ω = ω 2 − ω 1 , where the related parameters are all selected according to experimental data with ∆n = 0.01, σ = 1.8 THz, ω 1 = 2.676 PHz (≃ 704.5 nm), and ω 2 = 2.692 PHz (≃ 700.3 nm) [26].\n- The most remarkable feature appeared in Fig. 2 is that the non-Markovian effect will slow down the quantum evolution, for the monotonicity of N (Λ) is in agreement with τ QSL in the non-Markovian region ξ ∈ [ξ 1 , ξ 2 ].\n\nOCR Text: \"I5o \" \"3T7imt ~m  lir 0045 { 3 ~,330 ; @o15\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 2: Non-Markovian effect on quantum speed limit (QSL) of a photon under dephasing nosie. QSL time τ∞ (blue solid curve), τ2 (red dot-dashed curve), τ1 (green dashed curve), and N (Λ) (black dotted curve) as a function of parameter ξ controlling the relative height of two peaks of frequency distribution. The initial state with α = π/4 evolves during an actual driving time τ = 2π/(∆ω∆n) ≃ 0.39 ps with ∆n = 0.01, σ = 1.8 THz, ω1 = 2.676 PHz (≃ 704.5 nm), and ω2 = 2.692 PHz (≃ 700.3 nm) [26]."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The estimators proposed in the classical and the Bayesian framework can also be compared with each other based on their MSE performance as the system noise decreases. The aim here is to show that the latter approaches the former as σ → 0. Fig. 7 depicts the MSE for the cases of Gaussian, exponential and log-normal distribution for the likelihoods with N = 25. In the plot, the horizontal lines represent the MSEs in the classical framework, obtained with the MLEs, as shown in ( 86) and (87) in Appendix C. It can be observed that, for all the three considered distributions, the MSE obtained by using the FGE for estimating θ approaches the MSE of the MLEs as σ &lt; 10 −3 .\n\nMentions:\n- Fig. 7 depicts the MSE for the cases of Gaussian, exponential and log-normal distribution for the likelihoods with N = 25.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 7. MSE in estimating of θN vs σ. Solid lines: FGE; dashed lines: MLE (classical framework)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 2 shows the probability distribution function of the order parameter q 6 for different packing fractions corresponding to the onset of crystallization (top panel) and dense crystallized packings (bottom panel). Cumulative distributions Q 6 (x) ≡ x −∞ n(q 6 )dq 6 are also shown for each PDF. The relative numbers of spheres in hcp-like (n hcp ) and fcc-like (n fcc ) state are plotted in Fig. 3 as functions of φ. The total number of spheres involved in crystalline arrangements (n tot = n hcp + n fcc ) is also shown. For φ φ c the relatively slow increase of n tot with φ is associated with emergence of single hcp-like spheres, which are nearly randomly distribute over the system volume. The increase has a power-like character (n tot ∝ φ α with α ≃ 10). At φ ≃ φ c the slopes of n tot (φ) and n fcc (φ) drastically increase. The number of fcc-like spheres reaches that of the hcp-like spheres (n fcc ≈ n hcp ) at φ ≃ 0.66, where transition from hcp-dominated to fccdominated packing occurs. Oscillations of n fcc and n hcp , clearly seen for φ 0.68 correspond to the property of dense HS packings discussed above (see Figs. 1 and 2) and also observed in Ref. [19] with the help of Delaunay simplexes. Figure 4 shows the probability distribution of the ratio n fcc /(n fcc + n hcp ) for the dense (φ ≥ 0.68) HS systems. Two peaks of the distribution indicate that dense HS systems prefer to be dominated by either fcc phase (with somewhat higher probability) or by hcp phase (with somewhat lower probability). Equal distributions between fcc and hcp phases are considerably less probable. However, the lack of statistics (only 35 packings were available for the analysis) and specifics of the JT algorithm may also affect this observation. For this reason we do not elaborate further on this point here.\n\nMentions:\n- The relative numbers of spheres in hcp-like (n hcp ) and fcc-like (n fcc ) state are plotted in Fig. 3 as functions of φ.\n\nOCR Text: \"C=lo Mo-5 033 Ya:10 Packing fract on\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 3: (Color). Relative density of hcp-like and fcc-like spheres, nhcp (green) and nfcc (red), as well as the total density of crystalline spheres ntot ≡ nhcp + nfcc (blue), versus the packing fraction φ. For moderate packing fractions (φ ≤ 0.645), the increase of ntot with φ is due to the appearance of single hcp-like spheres. The increase has a powerlaw character (ntot ∝ φ α, where α ≃ 10). Much more rapid power-law growth of both ntot and nfcc takes place for φ & 0.645 (ntot ∝ φ β, nfcc ∝ φ γ , where β ≃ 50 and γ ≃ 100). Transition from hcp- to fcc-dominated packing (nhcp . nfcc) occurs at φ ≃ 0.66. Oscillations of nfcc and nhcp which are clearly seen for sufficiently dense systems (φ & 0.68) reflect two preferable possibilities of HS packing (cf. Figs. 1 and 2). The dependence of the order parameter whh6 on φ is also plotted (dashed black line) to indicate the onset of crystallization."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- 3) Certifying robustness on datasets: Here we compute the certificate derived above on real world data sets, including MNIST and others. It is expected that the certificate can be computed in reasonable time on well structured data like MNIST, while it may not be efficiently computed or provide no useful lower bound for other less structured data sets. Fig. 4: Certificate results on MNIST for the codewords constructed by Algorithm 1, compared to the empirical result without defense (nat_pre), and the empirical result with defense (disc_adv_pre). We plot two certificate results: estimated certificate on test imagesŝ, and global certificateŝ * , as defined in Proposition 2. Also see Table IV for the detailed numbers.\n- Our results are provided in Table IV, and exact values of s andŝ * are compared with experimental results in Figure 4. There exists some methods to compute estimated certificate robustness, like [24], [25], [26]. The state-of-art estimated certificate robustness on MNIST under ∞ perturbations with ε = 0.1 is 94.18% [25], with a fairly sophisticated method. Our discretization defense, which is much simpler and more efficient, gives a better estimated certificate robustness of 96.43%. This demonstrate the effectiveness of this simple certificate method. It also verifies the analysis for our idealized generative model of images, providing positive support for the conclusion that in the presence of well separated clusters of pixels, the pixel defense method on a good base model can successfully defend the adversarial attacks.\n\nMentions:\n- Fig. 4: Certificate results on MNIST for the codewords constructed by Algorithm 1, compared to the empirical result without defense (nat_pre), and the empirical result with defense (disc_adv_pre).\n- Our results are provided in Table IV, and exact values of s andŝ * are compared with experimental results in Figure 4.\n\nOCR Text: \"Cerincate M :e Me Aarnmie Abal cerulca:e HanceTncidennas Maae\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4: Certificate results on MNIST for the codewords constructed by Algorithm 1, compared to the empirical result without defense (nat_pre), and the empirical result with defense (disc_adv_pre). We plot two certificate results: estimated certificate on test images ŝ, and global certificate ŝ∗, as defined in Proposition 2. Also see Table IV for the detailed numbers."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 5 graphically depicts the frequency of evaluation domains. Some publications used more than one domain for evaluation purposes. Domains confined to only one study are being skipped here for the sake of brevity.\n\nMentions:\n- Fig. 5 graphically depicts the frequency of evaluation domains.\n\nOCR Text: \"Fowere d Muded Ked (a1 Buscess Ppelra AlElo: 0t RAac te MiuN hmis Cea Hemoli e\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: RQ4: Studies classification based on evaluation domains"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- An example of the droplets reaching the non-resolvability limit on a different grid is given in this subsection. Using a slightly more refined, uniform grid with 32 3 cubic elements for the exact same simulation (keeping r = 1.63cm), we arrive at the amplitude decay graph which differs from Fig. 8a as the flow is much better resolved. Inspecting the Figure 9, we see that, to begin with, the amplitude decay actually takes place, as the peaks' decrease obeys -albeit roughly -the analytic decay curve (31) plotted for the dominating second mode. This may be attributed to the viscous terms in governing Navier-Stokes equation being much more resolved (using this grid, we get r = 0.0163m ≈ 5.21∆x). There is no perceptible difference  between the 'no model' and ADM-τ curves, as (owing to the criterion ( 24)) the model is almost striclty off for this droplet radius. Once the radius is made smaller, ( 24) is fulfilled, e.g. for half the above radius, that is r = 0.81cm the model contributes at around 22% ratio when ǫ is set to its standard value of κ max /5, similar to the third row in Table 1. However, the Σdf ≈ 8 • 10 −3 for that simulation, and the model influence is substantially smaller. This is visible in Figure 10a, with barely legible shift visible towards t = 5, albeit the model still delivers the actual increase of otherwise underpredicted frequency 12 . This seems to be caused by the different flow physics stemming from decreased droplet radius and overall frequency increase compared to r = 1.63cm droplet. In this flow, the amplitude peaks (Figure 10b) decrease more regularly than in 16 3 simulation (Fig. 8a) although they no longer conform to (31).\n\nMentions:\n- Inspecting the Figure 9, we see that, to begin with, the amplitude decay actually takes place, as the peaks' decrease obeys -albeit roughly -the analytic decay curve (31) plotted for the dominating second mode.\n\nOCR Text: \"Elgzcmt Mneni acl ar ord WlEEE ed gecg pui OC\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 9: Droplet oscillation amplitude of a droplet with radius r = 1.63cm using a 323 grid. Series for ADM-τ (squares), simulation without a model (solid), analytic decay curve a2(t) using (31)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The WAIC values contain Monte Carlos variation; to overcome this, we fit a cubic spline model to Ω. Let ζ be the standard deviation of the residuals from this model fit. We select the smallest number of expected crossings e c where the corresponding ω c is within ζ of min(Ω). This approach encourages smoothing while maintaining performance. Figure 6 gives a visualization of this tuning. Other methods such as cross-validation could easily be used for tuning the expected number of crossings; we find this data-driven approach to be efficient while still yielding good results.\n\nMentions:\n- Figure 6 gives a visualization of this tuning.\n\nOCR Text: \"Opima creliu Cuy:mVS Fote 4laeh Gi Ktuz-Iiccxru\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6: Example of tuning the expected number of crossings. The jagged dashed line illustrates the Monte Carlo variation present in WAIC estimates. The solid line shows the spline fit. The horizontal dotted line shows the minimum WAIC value plus one standard deviation from the spline fit. The solid vertical line gives the minimum candidate expected number of crossings value where there is a WAIC value less than one plus the standard deviation."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- A set of 5 points sampled using the TPLHD technique is considered as initial observation points. One realization is represented in Figures 18c and 18d. The adaptive sampling technique is evaluated after reaching 115 sample points. Samples needed to generate the metamodel are shown in Figure 18c. The resulting metamodel is represented in Figure 18d. It can be seen that the holes inside the red domain are not detected by the surrogate model. This reduces the percentage of correctly classified points. However an accurate prediction of these holes with kriging would require a significantly larger number of samples. Furthermore identifying a point inside the regular motion subdomain might lead to a perceived reliability of the system dynamics around the observation point which might not be desired from an engineering point of view. The convergence of the error metrics are displayed in Figure 19, which shows an efficient convergence behavior. The results of the MiVor metamodel after 115 samples are very proficient with values of a p LLE≥0 = 97.12 % and a p LLE&lt;0 = 97.32 %. Considering the very complex form of the chaotic region including notched boundary and holes as shown in Figure 18b, these percentages of accurately identified points computing only 115 observation points are highly promising for detecting non-stable dynamic parametric domains at low cost.\n- Fig. 19 -Convergence of correctly classified points for 2D LLE problem P 4 .\n\nMentions:\n- The convergence of the error metrics are displayed in Figure 19, which shows an efficient convergence behavior.\n- Fig. 19 -Convergence of correctly classified points for 2D LLE problem P 4 .\n\nOCR Text: \"5 2 $ 4 % 0 % 3 % 10:/15 Sup?\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 19 – Convergence of correctly classified points for 2D LLE problem P4."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- For this example the initial DOA is θ = 100 • with the DOA increasing by 1 • at each time snapshot, with the signal value remaining constant at -1. The performance of the three methods is summarised in Table III, with the RM SE values illustrated in Figure 4.\n\nMentions:\n- The performance of the three methods is summarised in Table III, with the RM SE values illustrated in Figure 4.\n\nOCR Text: \"Muuuae: Sieea\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4. RMSE values for the non-endfire region example."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Since the degradation of DSLR camera is complex and contains multiple distortions such as blur and noise, we need to improve the network representation ability. After HFE, we hence propose the main body with three same branches to handle each frequency feature map separately. Inspired by [38] and [17], we introduce channel attention mechanism into the grouped residual dense block (GRDB) termed as CA-GRDB to adaptively aggregate those residual dense blocks (RDBs) [40] with different attention weights. We cascade four CA-GRDBs in each branch where each CA-GRDB consists of three RDBs, resulting 12 RDBs of each branch shown in Fig. 2.\n\nMentions:\n- We cascade four CA-GRDBs in each branch where each CA-GRDB consists of three RDBs, resulting 12 RDBs of each branch shown in Fig. 2.\n\nOCR Text: \"#altnier) 6u\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 2. The proposed network architecture: FAN"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In the following we present the results we obtained in the ITS scenario and in a mixed LMS environment (MIX scenario). For such scenario, far more challenging than the maritime one, we used real channel measurements obtained for ITS and MIX scenarios in the context of ESA MIMOSA project. Note that Figure 6. Normalized throughput versus transmit power for polarization-multiplexing scheme and Soft-MMSE-SIC receiver at edge of coverage. certain throughput gain with respect to the SISO case can be achieved even for the considered channels if the loss in terms of C/N is tolerable. Note also that no interleaver is included in the considered bearers a .\n\nMentions:\n- Note that Figure 6.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 6. Normalized throughput versus transmit power for polarization-multiplexing scheme and Soft-MMSE-SIC receiver at edge of coverage."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Muon induced fast neutrons underground have been reported by LVD experiment [28] at Gran Sasso depth (3.1 km.w.e). The observed proton recoil spectra is up to 300 MeV, corresponding to neutron energy up to a GeV depending on the scattering angle, with much higher statistics due to a much bigger detector. data from this work is more flat than that of Gran Sasso from LVD. This difference is attributed to the difference of the depth between two sites. Note that the simulated nuclear recoil curve in FIG. 18 and 19 takes a normalization factor of 1.21 from the muon measurement (see FIG. 16). Fitted with the same function to the simulation curve gives the parameter A = 1.10 • 10 −10 and α = 0.76. As can be seen in Fig. 19, the difference between the fitted curve with the measured neutron data and the fitted curve from the adjusted simulation results, using muon data, is about 30%. We integrate the energy region above the 4 MeV energy threshold, the measured neutron flux is determined to be 2.23×10 −9 cm −2 s −1 . Considering the 4 MeV en- FIG. 17: The nuclear recoil events obtained from the NR band in FIG. 15. The solid dots are the visible energies \"seen\" by the PMTs. The solid blue and red lines stand for the nuclear recoils caused by gamma rays and neutrons, respectively. The gray line is the simulated total NR response to the showers. A factor of 1/1.21 is also applied to the simulated NR curves here in order to match the normalization assumed for the ER response.\n\nMentions:\n- As can be seen in Fig. 19, the difference between the fitted curve with the measured neutron data and the fitted curve from the adjusted simulation results, using muon data, is about 30%.\n\nOCR Text: \"4o\" Dala {Uhis MDikI Fil Ip Dala \"[mis KOr 4 E Sinualion (Ihis wiExk) Fil Io Siulaban (Uis VDn) LVD (scaled; Enersy (Mev)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 19: The measured nuclear recoil spectrum at Soudan Mine (black solid dots) is compared with the one measured by LVD experiment at Gran Sasso (red open dots). The total flux of the LVD measurement is normalized by a factor of 1.66× 10−5 in the plot. The gray line is the simulation result for the Soudan measurement. For fitting results, black line for Soudan data, dashed line for the simulation and red line for LVD measurement, are also presented, respectively."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- b) Number of Times of Iterative Refinement: We also conduct experiments to investigate the time of recursive refinement. The results in Fig. 4 show that when the time of refinement exceeds two, the performance becomes saturated. This may suggest that the gain of modeling even higher-order interactions more than two is marginal.\n\nMentions:\n- The results in Fig. 4 show that when the time of refinement exceeds two, the performance becomes saturated.\n\nOCR Text: \"Tmec nt teratiur Retinement\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4. Results on the Argoverse validation set under the different time of iterative refinement."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In practice, the probability of classifying an input image at a side branch is a parameter related to aspects inherent of input data that depends on numerous factors. One of these factors is image quality. To show that image quality affects the probability, and thus the partitioning decision, we train a B-AlexNet for image classification, using a cat-and-dog dataset [8]. This dataset is composed of images of dogs and cats in different environments without any distortion. Once trained, we apply a batch with 48 samples with different levels of Gaussian blur. This experiment implements Gaussian filters with dimensions 5, 15, 65 to represent images with low, intermediate, and high distortion, respectively. We use these images as samples to perform inferences to classify if an animal is a dog or a cat. Figure 6 shows the probability of classifying an input image according to the entropy threshold. This figure shows that as distortion level increases, the probability that a sample is classified at a side branch decreases. This is true since images with higher distortion levels tend to have a higher uncertainty in the inference, resulting in a lower probability that it is classified at a side branch.\n\nMentions:\n- Figure 6 shows the probability of classifying an input image according to the entropy threshold.\n\nOCR Text: \"1,0 608 2 204 @0.2 0,0 0.0 0.1 0,2 0,3 0,4 0,5 Entropy Threshold\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 6. Probability of side branch classification under different distortion levels in B-AlexNet."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The minimum and maximum value and the range of X as a function of N are given in Figure 9 and Table 4. As it is required to observe N ≃ 100 photons at the energy threshold, the range turns out to be about 5σ. X &gt;-min X &lt; &gt; X -&lt; X max range Figure 9. The minimum and maximum values and the range of X as a function of N for a Gamma distribution with parameters corresponding to the reference EAS (Table 1).\n- As expected, the results show that, due to the Gamma-like shape of the Gaisser-Hillas function, the range of the detected photon distribution (that is the observed image length) does not change by a large amount as a function of the number of detected photons. In fact, as Figure 9 shows, the range is roughly a logarithmic function of the number of detected photons.\n\nMentions:\n- The minimum and maximum value and the range of X as a function of N are given in Figure 9 and Table 4.\n- X &gt;-min X &lt; &gt; X -&lt; X max range Figure 9.\n- In fact, as Figure 9 shows, the range is roughly a logarithmic function of the number of detected photons.\n\nOCR Text: \"4>-i} mac? < rng?\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 9. The minimum and maximum values and the range of X̂ as a function of N for a Gamma distribution with parameters corresponding to the reference EAS (Table 1)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 8 shows the impact of the adopted clump size on the bolometric light curve. The larger are the clumps, the stronger is the impact on the light curve, with a great boost at early times, a shorter optically-thick phase duration (earlier transition to the nebular phase), and greater fluctuations. What drives this behaviour is that the number of clumps drops as their size is increased, facilitating the escape of radiation between fewer and larger clumps. For smaller clumps, the lines of sight are more evenly covered by clumps, preventing the escape of radiation. For an infinitely small clump size, the radial optical depth converges towards that for the smooth case and the trapping efficiency of the material is unchanged. This is the case described in Dessart et al. (2018a), in which clumping acts primarily through its influence on the recombination rate.\n\nMentions:\n- Figure 8 shows the impact of the adopted clump size on the bolometric light curve.\n\nOCR Text: \"~ 3 Win TAXAL 3 Jin TAXEAC 3 WtTA*an 3 Jn TAXEh\" Jars -ceAcc-IL\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 8. Comparison of bolometric light curves for 2D 2P ejecta models with ξ0 = 0.1, Vcl = 4000 km s−1, and with different choices for the clump lateral size σcl, which covers from 0.005 to 0.04 times the local radius (see label as well as Eq. 11 for details)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Initially, in order to select the best predict model, we generate two models by using both algorithms that yielded dependency network by both models has been shown in figures 4 and 5. These two figures show performance difference between both algorithms. Of course, by observing rise procedure of lift chart, it may be imagined that these two algorithms have no sensible difference in terms of measurement criteria and model yielded by decision tree has had relatively better performance. Even, scores inserted in Figure 4 emphasize on relatively better performance of yielded model by decision tree model in comparison with Association Rules model. However, it should be noted that privilege value of such a model is negligible. But, why we chose latter model to predict admission possibility of candidates in next course? We believe that, according to yielded total information by Figure 6, Association Rules model had had better performance.\n\nMentions:\n- Initially, in order to select the best predict model, we generate two models by using both algorithms that yielded dependency network by both models has been shown in figures 4 and 5.\n- Even, scores inserted in Figure 4 emphasize on relatively better performance of yielded model by decision tree model in comparison with Association Rules model.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4. Above figure shows dependency Network of Decision Three"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To evaluate which solver would provide maximum efficiency in practical scenarios, one thousand random vectors for each problem dimensionality n ∈ {2 2 , . . . , 2 30 } were sampled and the corresponding sparseness-enforcing projections for σ * := 0.90 were computed using all four solvers. We have used the very same random vectors as input for all solvers, and counted the number of times the auxiliary function needed to be evaluated until the solution was found. The results of this experiment are depicted in Fig. 4.\n\nMentions:\n- The results of this experiment are depicted in Fig. 4.\n\nOCR Text: \"Bisaclion Neton Cn\" Halley Cn Newton Cn\" I0 12 14 16 |8 20 22 24 26 26 31 J0g2 Dimensionalityi\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 4 Number of auxiliary function evaluations needed to find the final interval for the projection onto T using four different solvers. The error bars indicate±1 standard deviation distance from the mean value. Since Newton’s method applied to Ψ̃ is consistently outperforming the other solvers, it is the method of choice for all practical applications."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- As shown in Fig. 5, only the critic is trained at the first stage (first 1000 iterations in our experiments) under π exploration and reaches a high target-oriented grasping success rate (defined as # successful target-oriented grasping # total motions (pushes and grasps) ). Then we replace the policy to be coordinator π c and start to train the coordinator to increase its prediction accuracy gradually. Note that criticcoordinator finally achieves a higher target-oriented grasping success rate in even more cluttered scenes.\n\nMentions:\n- As shown in Fig. 5, only the critic is trained at the first stage (first 1000 iterations in our experiments) under π exploration and reaches a high target-oriented grasping success rate (defined as # successful target-oriented grasping # total motions (pushes and grasps) ).\n\nOCR Text: \"Critic-Exploarion Critic- Coord natcr Coordinator Rolm 6ec U Uralnnarsesis\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 5: Training performance. The purple lines indicate the target-oriented grasping success rate of the critic-policy, and the blue line indicates the training accuracy of action classifier fa in policy πc over training steps."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Example 2. Consider an integer variable num, a pointer variable p, and a double pointer variable carsp with target type struct car (defined in Example 1). The following C code fragment declares num, p, and carsp, allocates space for an array of num consecutive cars pointed to by carsp, and makes p point to the array's third element. Using function pointers. Class CE ROUTINE, which translates C function pointers, is usable both in the Eiffel and in the C environment (see Figure 2). On the Eiffel side, its instances wrap Eiffel routines using agents-Eiffel's mechanism for function objects. A private attribute routine references objects of type ROUTINE [ANY, TUPLE], an Eiffel system class that corresponds to agents wrapping routines, with any number of arguments and argument types stored in a tuple. Thus, Eiffel code can use the agent mechanism to create instances of class ROUTINE. For example, if foo denotes a routine of the current class and fp has type CE ROUTINE, create fp.make agent (agent foo) makes fp's attribute routine point to foo. On the C side, when function pointers are directly created from C pointers (e.g., references to external C functions), CE ROUTINE behaves as a wrapper of raw C function pointers, and dynamically creates invocations to the pointed functions using the library libffi [9].\n- The Eiffel interface to CE ROUTINE will then translate calls to wrapped functions into either agent invocations or external calls with libffi according to how the class has been instantiated. Assume, for example, that fp is an object of class CE ROUTINE that wraps a procedure with one integer argument. If fp has been created with an Eiffel agent foo as above, calling fp.call ( [42]) wraps the call foo (42) (edge 1 in Figure 2); if, instead, fp only maintains a raw C function pointer, the same instruction fp.call ( [42]) creates a native C call using libffi (edge 2 in Figure 2).\n- The services of class CE ROUTINE behave as an adapter between the procedural and object-oriented representation of routines: the signatures of C functions must change when they are translated to Eiffel routines, because routines in object-oriented languages include references to a target object as implicit first argument. Calls from external C code to Eiffel routines are therefore intercepted at runtime with libffi callbacks (edge 3 in Figure 2) and dynamically converted to suitable agent invocations.\n\nMentions:\n- Class CE ROUTINE, which translates C function pointers, is usable both in the Eiffel and in the C environment (see Figure 2).\n- If fp has been created with an Eiffel agent foo as above, calling fp.call ( [42]) wraps the call foo (42) (edge 1 in Figure 2); if, instead, fp only maintains a raw C function pointer, the same instruction fp.call ( [42]) creates a native C call using libffi (edge 2 in Figure 2).\n- Calls from external C code to Eiffel routines are therefore intercepted at runtime with libffi callbacks (edge 3 in Figure 2) and dynamically converted to suitable agent invocations.\n\nOCR Text: \"libfei LAgbw El XUUbfL\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: Calls through function pointers."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 5 shows simulation results obtained from importance sampling (solid lines) and from uniform sampling (dashed lines) in the dual Forney factor graph. From Fig. 5, the estimated log partition function per site is about 2.503.\n\nMentions:\n- Fig. 5 shows simulation results obtained from importance sampling (solid lines) and from uniform sampling (dashed lines) in the dual Forney factor graph.\n- From Fig. 5, the estimated log partition function per site is about 2.503.\n\nOCR Text: \"Numhet of Samples\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 5: Estimated log partition function per site vs. the number of samples for a 30× 30 Ising model, with Jk ∼ U [1.0, 1.25] for k ∈ BA and Jk ∼ U [1.25, 1.5] for k ∈ BB . The plot shows five different sample paths obtained from importance sampling (solid black lines) and five different sample paths obtained from uniform sampling (dashed blue lines) on the dual factor graph."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Recently, multi-layer convolutional neural networks (CNNs) have been identified as a powerful technique for image recognition problems. These neural networks consist of convolutional layers of n columns of m neurons -layers of neurons that share a common set of m × n weights and a bias. The set of weights + biases is called a filter or kernel, and this filter is combined in a multiplicative sum (a convolution) with an m × n subset of input neurons to give an output value. The filter is moved along the image, each time covering a different m × n subset of input neurons, and the set of output values corresponding to a single filter is called a feature map. With this strategy, further convolutional layers can be used to analyze the higher-level features encoded in the feature maps output from previous layers. Often to reduce the amount of computation and neurons present in deeper layers, max-pooling operations are performed, in which the neuron with maximum output value in an m × n window (or \"pool\") is selected, and all others in the pool are discarded. Such an operation performed on a layer of neurons leads to a new layer of reduced size. A deep CNN may be constructed from a series of several such convolutional operations and max-pooling operations, along with more conventional fully-connected layers, in which all neurons output from the previous layer are connected to the input of each neuron in the fully-connected layer, and other operations not discussed here (see figure 7 for a general schematic).\n\nMentions:\n- A deep CNN may be constructed from a series of several such convolutional operations and max-pooling operations, along with more conventional fully-connected layers, in which all neurons output from the previous layer are connected to the input of each neuron in the fully-connected layer, and other operations not discussed here (see figure 7 for a general schematic).\n\nOCR Text: \"hse 34;, Fetded Jpt a/ Mene ISsteate:4] Vi\"&z 01| @tzut lizsed]\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 7. Schematic of a deep convolutional neural network for 2-category classification. The input layer consists of the pixel intensities of an image, possibly in multiple color channels. The hidden layers consist of several different operations performed on the input neurons - this example shows a 3 × 3 convolution followed by a 3×3 max-pooling operation, with the resulting neurons input to a fully-connected layer which feeds the two neurons in the output layer. The activation function of the two neurons in the final layer is such that the two outputs are exponentiated and normalized. The values in such a layer, called a “softmax” readout layer, can then be interpreted as probabilities of classification as signal or background."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- To study the feasibility of the different RBT schemes empirically, we use our ppCD toolkit. In our experiments, we consider the overhead required by the client and server alike. We compare the different schemes in term of their resource consumption and privacy achieved as detailed in the following experiments. Note that all of the experiments are performed on a computing machine equipped with an Intel Core 2 Quad CPU that utilizes a 32-bit data bus and operates at 2.5 GHz with 3.25 GB of RAM. Experiment 1. In this experiment, we measure the average time required for transforming different datasets according to the RBT scheme in section II.3.A. For different datasets (shown in Table 4), we execute RBT and measure the required computation. As expected, we found that, on average, the required computation time linearly depends on the number of records as shown in Table 4 and rendered in Fig. 6. Experiment 2. In this experiment, we tried to maximize the accuracy of the time measurement in the MRBT case. Because the ppCD toolkit shares the computing machine's resources with other running processes, measured time in an experiment may not be as accurate as needed. To eliminate the error in measurement that results from this scenario, we run the MRBT on s 10 500 times and measure the execution time (shown in Fig. 7). We observed that some of the measured times are greatly higher than the majority of measurements. For higher accuracy, we filtered these ambiguous measurements and replaced them with the average measurements (shown in Fig. 8). The average time required for processing is then computed over all the measurements, including the filtered measurements, which greatly matches with results in experiment 3. Experiment 3. We evaluate additional computation overhead represented as time and required for the MRBT over the RBT. As shown in Fig. 9, for a dataset of 31,250 records transformed using the MRBT at m=100, we found that the additional computation required in MRBT is slight compared to the initial overhead required in RBT. While RBT requires 19.8910 ms, the MRBT requires 20.7804 ms for the same dataset. The extra time in MRBT over the time required in RBT is 0.8894 ms, which is 4.47% of the overall overhead. Though this overhead is small in relation with merit realized in MRBT, it is even smaller for larger datasets. Experiment 4. In this experiment, we generated normally distributed synthesis data which has a statistical mean equal to its variance (μ=σ 2 =100). The dataset has 10 6 records and each record has 10 attributes (about 130 MB on a desk). We transformed the dataset using the RBT to measure its feasibility. We realized that transforming the whole dataset takes about 1.601 seconds. In the same dataset, MRBT with m=100, took 1.614 seconds, which is about 0.88% additional overhead. Experiment 5. We observed that required computation overhead for transforming data in the ARBT, as shown in steps 1 and 2 of Fig. 2, are equal to the overhead required in MRBT. Furthermore, we observed that the computational overheard required for step 3 in Fig. 2 is equal to that required in the RBT scheme for half of the dataset over which conventional clustering is to be performed. For instance, if a dataset is divided into two parts, then transformed using ARBT and retransformed to enable conventional cluster, the overall computation overhead required at the client side is one and a half times of the overheard required in the RBT or the MRBT (since both schemes require almost the same amount of overhead). However, compared to the naive scenario described in section IV.3, ARBT requires only half of the overhead required for further data transformation. Random Sequential Experiment 6. We learned the impact of initial centroids on the number of required iterations for the Lloyd algorithm used for computing final clusters. We studied the case of random against sequential centroid assignment. Figure 10 shows the number of iterations required for data subsets in Table 4. We realized that though the random selection of initial centroids does not necessarily reduce the number of iterations as shown for small data subsets, it reduces the overhead when the number of records to be clustered is large. For instance, the average number of iterations required in the random scenario is 23.2727 iterations per dataset, while it is 24.2727 per dataset for sequential scenario with the final clusters k=7. Experiment 7. The previous experiment was performed again to measure the time required for clustering datasets in both of the random and sequential assignment scenarios. We realized that the average time required for clustering a set for the random scenario is 6,620.4 ms, while it is 7,980 ms for the sequential assignment scenario. A comparison between the two scenarios for the different datasets is shown in Fig. 11.\n\nMentions:\n- Figure 10 shows the number of iterations required for data subsets in Table 4.\n\nOCR Text: \"Rardom Ylrtal InasI\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 10. Number of iterations for achieving stabilized clusters for random vs. sequential selection of initial centroids"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 11 compares the class specific action recognition accuracies of R-NKTM and NKTM [37]. The proposed R-NKTM achieves better recognition accuracy on most action classes. The easiest action to identify is jumping jack with an average accuracy of 95.4% and the hardest is phone answering with an average accuracy of 33.3%. These results are not surprising, since jumping jack is one of the activities with the most discriminative trajectories while phone answering is confused with drinking because the motion of these actions is very similar.\n\nMentions:\n- Figure 11 compares the class specific action recognition accuracies of R-NKTM and NKTM [37].\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 11: Per class recognition accuracy of the proposed R-NKTM and NKTM [37] on the UWA3D Multiview ActivityII [35] dataset."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- A MDF containing two TO-LO phonon mode pairs of E 1 symmetry and one phonon mode pair of A 1 symmetry for the DF tensor components ε ⊥ and ε , respectively, suffices to excellently match the experimental data for all samples except the one of the lowest Al content x = 0.61. For this sample, two additional low-polarity modes had to be included at ω AM1 ⊥ = 689.9 cm −1 and ω AM2 ⊥ = 710.6 cm −1 in order to achieve a good match between experimental and model data as marked by the asterisks in Fig. 2. The sensitivity for the out-of-plane high-frequency dielectric constant ε ∞, was insufficient to independently vary this parameter during the regression analysis. A constant anisotropy according to the results for the AlN film was assumed for all samples, i.e., a constant offset of 0.05 compared to the in-plane high-frequency dielectric constant ε ∞,⊥ was assumed for ε ∞, for all samples. FCC contributions were allowed in the Si-doped top layer only and in order to determine the FCC concentration N and the optical mobility parameter µ, the value of the isotropically averaged effective mass parameter for each Al x Ga 1−x N sample was varied according to the Al content x (see Fig. 9). The sensitivity for the optical mobility parameter µ along the c-axis was too low to independently determine values for µ and µ ⊥ . Therefore an isotropically averaged optical mobility parameter µ was assumed for the model calculations. The determined phonon mode parameters and FCC parameters for all Al x Ga 1−x N samples are summarized in Table I.\n- Under the assumption of a linear increase of the effective mass parameter in AlGaN alloys with increasing Al content and assuming an isotropically averaged effective mass parameter value of m * = 0.232 m 0 for electrons in GaN, 6 an isotropically averaged electron effective mass parameter value of m * = 0.364 m 0 can be extrapolated for AlN (Fig. 9). The value for the electron effective mass parameter for arbitrary values of the Al content can be calculated using the following linear trend:\n- Due to the much smaller Mueller matrix elements and resulting smaller signal-to-noise ratio, the sensitivity during the regression analysis for the FCC parameter of the DF tensor component ε was significantly decreased for the sample with Al content x = 0.82 compared to the sample with x = 0.79. Therefore, isotropically averaged effective mass and mobility parameters had to be assumed for this sample in the analysis. The results for the sample with x = 0.79 suggest this averaging procedure is feasible since no significant anisotropy is expected. The reduced FCC-induced birefringence is related to the much smaller free electron concentration parameter of N = (2.4 ± 0.1) × 10 18 cm −3 found for this sample. The determined isotropically averaged elec- tron effective mass parameter m * = (0.339 ± 0.006) m 0 matches excellently with the value determined from the linear extrapolation in Fig. 9. The regression analysis for the sample with Al content x = 0.89 did not result in stable FCC parameters even when assuming isotropically averaged values for the effective mass and mobility parameters. The best-match model in Fig. 8 was calculated by assuming an isotropically averaged electron effective mass parameter m * = 0.350 m 0 as extrapolated from the results of the sample with x = 0.79. The resulting excellent match between best-match model calculated and experimental data indicates that the assumed value for the effective mass parameter must be close to the actual material value. A free electron concentration of N = (2.3 ± 0.2) × 10 18 cm −3 and mobility parameter of µ = (12 ± 1) cm 2 /Vs are determined from the simultaneous analysis of the MIR-SE and MIR-OHE data. The significantly lower mobility parameter in the Al 0.89 Ga 0.11 N film (see Table I) explains the small off-diagonal block Mueller matrix elements in the OHE data of this sample.\n\nMentions:\n- FCC contributions were allowed in the Si-doped top layer only and in order to determine the FCC concentration N and the optical mobility parameter µ, the value of the isotropically averaged effective mass parameter for each Al x Ga 1−x N sample was varied according to the Al content x (see Fig. 9).\n- Under the assumption of a linear increase of the effective mass parameter in AlGaN alloys with increasing Al content and assuming an isotropically averaged effective mass parameter value of m * = 0.232 m 0 for electrons in GaN, 6 an isotropically averaged electron effective mass parameter value of m * = 0.364 m 0 can be extrapolated for AlN (Fig. 9).\n- The determined isotropically averaged elec- tron effective mass parameter m * = (0.339 ± 0.006) m 0 matches excellently with the value determined from the linear extrapolation in Fig. 9.\n\nOCR Text: \"038 m2\"#N-0J64 m _ (extrapolatexl) 0336 03 032 030 0.28 0.26 0.232 m Al conl:nL\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 9. Linear extrapolation of the electron effective mass parameter in AlxGa1−xN alloys as estimated for assumed values of the isotropically averaged effective mass parameter of m∗ = 0.232m0 for electrons in GaN 6 and the OHE determined isotropically averaged value for Al0.79Ga0.21N of m∗ = 0.336m0. The values as determined or assumed in the best-match model calculated OHE data in Fig. 8 are marked by symbols."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Our first experiment sets out to compare the use of a nonsequential classification algorithm versus a sequential model for dialogue act classification on our dataset. We experiment with the default Naive Bayes (NB) and Linear SVC algorithms from Scikit-Learn [24], comparing with our sequential SVM-HMM model. We test each classifier on each of our four class sets, reporting weighted F-macro for each experiment. Figure 4 shows the results of the experiments. From this experiment, we observe that our sequential SVM-HMM outperforms each non-sequential baseline, for each of the four class sets. We select the sequential SVM-HMM model for our preferred model for subsequent experiments.\n\nMentions:\n- Figure 4 shows the results of the experiments.\n\nOCR Text: \"Non-Sequential Basellnes Sequentlal SVM-HMM aun ASre 1C Hard Numter Classes 075\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4: Plot of Non-Sequential Baselines vs. Sequential SVM-HMM Model"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The line profile shows that the cloud remains detectable even after 400 Myr, though by this point the S/N level appears to be in a terminal decline (see figure 3). The S/N even increases initially, reaching a peak of about 8 after about 100 Myr and falling back to its initial value of 4 by the end of the simulation. These values are in good agreement with the observed clouds. However, this rise in S/N comes at the expense of the line width. The FWHM drops rapidly, falling below 100 km s −1 in just 50 Myr. After this the FWHM continues to drop approximately linearly, though more slowly, reaching 50 km s −1 after about 240 Myr and 35 km s −1 by the end of the simulation.\n\nMentions:\n- The line profile shows that the cloud remains detectable even after 400 Myr, though by this point the S/N level appears to be in a terminal decline (see figure 3).\n\nOCR Text: \"3Nivemta TSLed FHH iJe1lra F\" Ikiscce Ieot]\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3. Snapshots of the line profile evolution of the fiducial model (A). The thick lines indicate the average over all three viewing angles whereas the thin lines show the profiles obtained for separate viewing angles."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- η c is the lowest state of the charmonium family, its mass and width are hence critical. However, current mass measurements are not consistent, and this problem is traced by CLEO to be the distorted line-shape of η c [11] from a standard Breit-Wigner form,as shown in Fig 3.\n\nMentions:\n- However, current mass measurements are not consistent, and this problem is traced by CLEO to be the distorted line-shape of η c [11] from a standard Breit-Wigner form,as shown in Fig 3.\n\nOCR Text: \"3 3 0.04 034 Elv) [Gev]\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Fits to the photon spectrum in exclusive J/ψ → γηc decays using relativistic BreitWigner (dotted) and modified (solid) signal line shapes convolved with a 4.8 MeV wide resolution function."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 4 shows the variation of the measurements of the magnetic field produced by the coil inside the magnetic shield. The noise amplitude, typically 400 pT, is mainly determined by the coil current relative stability estimated to be of the order of 10 −5 .\n- The measurements shown in Figs. 4 and 5 do not represent the ultimate magnetometer precision. In the case of the Earth field measurement, several noise sources inside the building degrade the magnetometer performance, while in the case of operation inside a magnetic shield the precision is limited by the stability and homogeneity of the magnetic field to be measured. Since the total effective acquisition time was 0.1 s, we estimate that the data shown in Figs. 4 and 5 correspond to a sensitivity of 130 pT / √ Hz and 1.5 nT / √ Hz respectively. The limit magnetometer precision can be estimated from the uncertainty in the determination of frequency 2ω L from the atomic signal shown in Fig. 3. This uncertainty is taken as the mean square root time deviation δT of the experimental data from the best fitting curve. The signal to noise ratio is S/N = T /δT where T is the oscillation period. The uncertainty δ in determination of ω L is given by:\n\nMentions:\n- Fig. 4 shows the variation of the measurements of the magnetic field produced by the coil inside the magnetic shield.\n- The measurements shown in Figs. 4 and 5 do not represent the ultimate magnetometer precision.\n- Since the total effective acquisition time was 0.1 s, we estimate that the data shown in Figs. 4 and 5 correspond to a sensitivity of 130 pT / √ Hz and 1.5 nT / √ Hz respectively.\n\nOCR Text: \"[ Time Kmin)\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 4: Typical dispersion of magnetic field measurements inside a magnetic shield."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The majority (n=22) of the probands indicated that they do not search selectively for e-books. However 15 of the respondents said that they have used e-books in the last six months. Research literature (n=12) tops the list (see Figure 1) of the (very) frequently utilized digital literature in this period, followed by text books (n=6) and reference or advisory books (n=4).\n\nMentions:\n- Research literature (n=12) tops the list (see Figure 1) of the (very) frequently utilized digital literature in this period, followed by text books (n=6) and reference or advisory books (n=4).\n\nOCR Text: \"L Ecl D2-Teclet: $ FSairria #r;ng #t Ge Ir:s\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: Frequencies of reading e-books in the last six months"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In order to investigate the condensation regime further we introduce not one but three special diagonal elements with values -4, -4.5 and -5, and then choose the basis state corresponding to the diagonal element equal to -4 as the initial state before the quench. The results are presented in Fig. 6. We observe that two eigenstates with the lowest and the second lowest energy are almost unoccupied. The third eigenstate from below has large occupation comparable to one, i. e. we observe condensation but not into the lowest energy state. The averaged occupations of the remaining eigenstates agree very accurately with the QMC formula. In the present case, choosing the basis states corresponding to the diagonal elements -5 or -4.5 as the initial states would lead to the condensation into the lowest or the second lowest eigenstates, respectively. We note here that condensation into the second or the third lowest energy state contradicts to the QMC statistics always predicting the condensation into the lowest energy state. We discuss this disagreement in subsection IV B.\n- As shown in Fig. 6, when more than one special diagonal element is introduced, the condensation occurs not necessarily into the ground state of the system, but possibly into a higher energy eigenstate. This circumstance together with the fact that in Fig. 3(b) the deviations from the QMC statistics occur for the bin of the lowestenergy states indicates that the phenomenon of localization may be involved here. The QMC ensemble requires a fair sampling of the Hilbert space. This implies a broad participation of eigenstates ψ i in a given basis state φ k . But if some of the eigenstates are localized among very few basis states, they will not be present in most of the quench-induced ensembles considered in this paper. This localization property can be characterized by the inverse participation ratio (IPR) of basis states φ k in a given energy eigenstate ψ i :\n\nMentions:\n- The results are presented in Fig. 6.\n- As shown in Fig. 6, when more than one special diagonal element is introduced, the condensation occurs not necessarily into the ground state of the system, but possibly into a higher energy eigenstate.\n\nOCR Text: \"0,012 0.Q12\" 0Gc 0,008 _ 0.006\" 0,014 OGOL 0.002 0,029- -4 -2 2   4\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 6: Same as Fig. 4 but for the random matrix quench with three special diagonal elements -5, -4.5, -4 and Eav = −4."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In addition, the fitting technique described in [43] which uses a polynomial fit to simultaneously model the temporal evolution of the cyclotron frequency measurements of the mother and daughter nucleus was used. The result of this fit is shown in Fig. 4, where a fifth-order polynomial function was fitted to a subset of measured cyclotron frequencies for both decay partners of mass A = 21. The proportionality factor between the fit functions represents the cyclotron-frequency ratio for the whole measurement series between the two masses. Therefore, the fluctuations of the measured cyclotron frequencies ν i (t) can be described with a polynomial function f (t) and the frequency ratio of Eq. (3):\n\nMentions:\n- The result of this fit is shown in Fig. 4, where a fifth-order polynomial function was fitted to a subset of measured cyclotron frequencies for both decay partners of mass A = 21.\n\nOCR Text: \"+44u6u Eatio: |.0l1x137y7 ~iWcW COOS itixw=L70 ZNe ZLNa ZI,0 20 W0 020 0i0) 060) O) lw1 12w9 \"mn\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 4. Cyclotron-frequency-ratio determination using a simultaneous polynomial fit for a subset of measured cyclotron frequencies (incl. error bars) of 21Ne+ and 21Na+."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The transmission spectra of the resulted MOW sample are shown in Figure 4. The light guiding mechanism in this type of MOW can be described via Fabry-Perot resonances [38,80], and the condition corresponding to the maximal decoupling of the core and cladding modes can be written as follows:\n- where j is an integer ( 0,1,2 ) j=  , describing the mode order, n1 is the refractive index of a medium filling the capillaries (which is air in our case), n2 is the refractive index of the fiber glass, and d denotes the wall thickness for the first capillary layer. However, we notice that this model has restrictions. Specifically, it cannot provide the correct quantitative description for fiber losses and modal content away from Fabry-Perot resonances. Therefore, in the cases when these properties are relevant, the inhibited coupling model is more preferable [81]. Equation (1) gives the local bands of the transmission shown in Figure 4 for j = 5, 6, 7, 8. The redshift of the local transmission bands is caused by the effective increase in d owing to the buffer layer (the difference in the refractive indexes of the fiber glass and polymers is negligible [58,59,82,83]). Table 1 shows excellent agreement between local band spectral shifts that are measured and predicted by Equation (1), for a total polyelectrolyte layer thickness of 30 nm.\n\nMentions:\n- The transmission spectra of the resulted MOW sample are shown in Figure 4.\n- Equation (1) gives the local bands of the transmission shown in Figure 4 for j = 5, 6, 7, 8.\n\nOCR Text: \"KCI wdhab F Izze:\" Fcte Wawblenothanm\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4. Spectral shifts of local transmission bands induced by poly(styrenesulfonate)/poly(diallyldimethylammonium chloride) (PSS/PDDA) polyelectrolyte layers. Transmission spectra for the empty (uncoated) MOW (blue) and the MOWs coated with the buffer layer (red)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The beam currents obtained for a few ion species are shown in Table 1. In particular, the beam intensities in the order of 1 pµA for alphas and at least ten times more for 12 C gave us the idea that one can use the accelerator to study light ion-ion reactions of relevance for nuclear astrophysics. The accelerator high voltage is monitored by a generating voltmeter (GVM) that provides feedback for the Tandetron T M driver. GVM requires periodic calibration and for this work the resonant reaction 27 Al(p,γ) 28 Si was used [9,10]. The wellknown narrow resonance at E p =992 keV was scanned in 0.1 kV steps and the excitation function is given in Figure 3, which is included only to illustrate the actual performances of the pro- cedure. In order to determine the calibration curve two more cross-section maxima were measured near 1317 keV, respectively 1381 keV [11].\n\nMentions:\n- The wellknown narrow resonance at E p =992 keV was scanned in 0.1 kV steps and the excitation function is given in Figure 3, which is included only to illustrate the actual performances of the pro- cedure.\n\nOCR Text: \"{,Auxei 27AKp ;)ESi IVl\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Excitation function for the 27Al(p,γ)28Si at 992 keV resonance."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Figure 5 shows the distribution of subjects' individual f-measures. We can see that about 46% of the subjects are predicted very well (f-measure over 0.9), 29% predicted reasonably well (f-measure over 0.8), and the rest of the subjects (about 25%) are with f-measure less than 0.8.\n\nMentions:\n- Figure 5 shows the distribution of subjects' individual f-measures.\n\nOCR Text: \"1 01 02 03 0.4 05 05 07 03 0.9 Fmeasure\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: A histogram showing, for every f-measure F , how many subjects have f-measure of F ."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We have design a hybrid system in which first feature is used to find the relevant image and the other feature is used for filtering the result. If we individually use the two features then the result is found to be 60 percent which is not good in comparison 85 percent. The progress graph of the three systems i.e. using corner only, using invariant moment only and hybrid of the two is shown in figure 4.\n\nMentions:\n- The progress graph of the three systems i.e. using corner only, using invariant moment only and hybrid of the two is shown in figure 4.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4. Precision/Recall Graph of Comparative methods"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We evaluate the performance of proposed schemes with respect to their average achievable sum rate, where averaging is with respect to the channel realizations with i.i.d. coefficients h ij ∼ CN (0, 1). Also, we considered the performance of full-cooperation (i.e., vector broadcast channel with sum-power constraint (see for example [38] for an efficient algorithm to compute the vector broadcast channel sum-capacity). In Fig. 6, Scaled PCoF shows the satisfactory performance in the moderate SNRs (i.e., SNR &lt; 20 dB). Yet, this scheme suffers from the non-integer penalty at high SNRs. Remarkably, Scaled PCoF with DPC (and optimization with respect to the scaling factor β in Theorem 3) performs within a constant gap with respect to full-cooperation at any SNR. ♦\n\nMentions:\n- In Fig. 6, Scaled PCoF shows the satisfactory performance in the moderate SNRs (i.e., SNR &lt; 20 dB).\n\nOCR Text: \"[ic7 Z 7av ECe Raadtua\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 6. Average sum rate for Gaussian Network-Coded CIC with i.i.d. channel coefficients ∼ CN (0, 1)."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Example 2 : (p. 43) Consider the case of an unstable linear switched system (Fig. 5), composed of two subsystems f i (x) = A i x, i ∈ {1, 2} with :\n- Remark 3 : In the case of a divergent switched system, the product x 1 x 2 will naturally increase and be greater than 1. Therefore, the corresponding Bernstein series will be also divergent (see Fig. 5).\n\nMentions:\n- 43) Consider the case of an unstable linear switched system (Fig. 5), composed of two subsystems f i (x) = A i x, i ∈ {1, 2} with :\n- Therefore, the corresponding Bernstein series will be also divergent (see Fig. 5).\n\nOCR Text: \"Fur -Wl- Glardc ~bl NE;ul #ibh:\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 5: Comparison of the B-switched system and the standard system in the case of a non stabilizing switching signal."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where the correction for total transverse center of mass kinetic energy depends on the HO scale 1/ √ M 0 Ω and is easily removed from each solution. We have solved for the electron's anomalous magnetic moment (a e ) when confined to an external trap and we have taken the limit where the trap vanishes [12]. Here we remove the trap altogether and follow procedures presented in more detail in Ref. [24]. We adopt a Fock space of electron and electron-photon configurations consistent with a total angular momentum projection of 1/2 and total HO quanta (sum over constituents' 2n + |m|) limited by N max . The transverse infrared properties are still regulated by the finite HO basis. We also introduce a form of renormalization that sets to unity the amplitude for the electron Fock space components of the interacting (physical) electron. With this prescription, as demonstrated in Fig. 1, the extrapolation to the infinite basis reproduces well the expected Schwinger result.\n\nMentions:\n- With this prescription, as demonstrated in Fig. 1, the extrapolation to the infinite basis reproduces well the expected Schwinger result.\n\nOCR Text: \"U.1I5 Schwinger result 01.112535_ W11 133.486: = IL.MS 0LO9S Cven mas/2 cxld XJax ; Qu9 even Maz ' cld Nmaw/2 ic 0O5 0.15 (Nmnax = K-1/2}-12 Mtttt\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: (Color online) Square root of the electron anomalous magnetic moment (ae), normalized by e2, versus the inverse square root of the HO basis cutoff Nmax = K− 12 . Symbols are BLFQ results: squares (circles) for even (odd) Nmax/2, spanning Nmax = 18 . . . 258. Lines are linear extrapolations based on data points at Nmax = K − 12 > 100 and agree favorably with the Schwinger result √ 1/8π2 [23]."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We have also verified that the theoretical approximations are accurate for the expander graphs and details omitted here since they are similar. 2) Consequence: Theorem 1 and Theorem 2 have the following consequence: Remark 1. For a fixed large ARL, when M increases, the ratio M/b is bounded between 0.5 and 2. This is a property quite useful for establishing results in Section V. This is demonstrated numerically in Fig. 1  Note that e 20 is on the order of 5×10 8 , hence, it means that ARL can be very large, however, it is still bounded above (this means that the corollary holds for an non-asymptotic regime). Remark 2. As b → ∞, the first order approximation to the EDD in Theorem 2 is given by b/(∆ 2 /2), i.e., the threshold b divided by the Kullback-Leibler (K-L) divergence (see, e.g., [32] shows that ∆ 2 /2 is the K-L divergence between N (0, I M ) and N (V µ, I M )). This is consistent with our intuition since the expected increment of the detection statistics is roughly the K-L divergence of the test. For finite b, especially when the signal strength is weak and when the number of sketches M is not large enough, the other terms than b/(∆ 2 /2) will play a significant role in determining the EDD.\n\nMentions:\n- This is demonstrated numerically in Fig. 1  Note that e 20 is on the order of 5×10 8 , hence, it means that ARL can be very large, however, it is still bounded above (this means that the corollary holds for an non-asymptotic regime).\n\nOCR Text: \"200, ARL 5000\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 1. For a fixed ARL being 5000, the threshold b versus M obtained using Theorem 1, when N = 100 and w = 200. The dashed line corresponds to a tangent line of the curve at one point."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We introduce a model that clusters users into user groups and documents into document clusters, in order to generalize relevance over both groupings. Each user may have several \"attitudes,\" that is, belong to different groups during different relevance evaluations, and likewise each document may have several \"aspects.\" These are modeled as probabilistic soft assignments. Our notation is summarized in Table 1. The model generates rating triplets (user, document, rating), or (u, d, r), with binary relevances r as follows (see Figure 1):\n\nMentions:\n- The model generates rating triplets (user, document, rating), or (u, d, r), with binary relevances r as follows (see Figure 1):\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 1: A Graphical Model Representation of Our Two-Way Grouping Model. The grey circles indicate observed values. The boxes are “plates” representing replicates; the value in the corner of each plate is the number of replicates. The rightmost plate represents the repeated choice of N (user, document, rating) triplets. The plate labeled with KU represents the multinomial models of the KU different user groups. The plate labeled with KD represents the multinomial models of the KD different document clusters."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- where h(x) = H(x)/H 0 and x = ln(z + 1) −1 . The evolvement of Om-diagnostic tool versus redshift is depicted in Fig. 3. It is obvious that in the late time the trajectories for interacting and non-interacting mode present positive values conveying phantom like behavior and emphasizing on the EoS parameter result as seen in Fig. 2. However, in z + 1 &gt; 1, the trajectories demonstrate negative values which implies an universe with quintessence behavior.\n- In this paper, we studied a new holographic dark energy model with Hubble horizon as IR cutoff in fractal universe for flat FRW space. The features of fractal cosmology could remove ultraviolet divergencies and also make a better understanding of the universe in different dimensions. There are many models of holographic dark energy created by cosmologists with various cosmological constraints and explain that the size of L as the horizon length should be less than the mass of black hole with the same size. In order to prevent formation of black hole, the minus sign of the equation of state of dark energy is not enough. Phantom-like dark energy is the most appropriate condition to avoid the formation of black hole (ω D &lt; −1). In the present work, calculation of the EoS parameter for NHDE with Hubble horizon as IR cutoff demonstrated that the EoS parameter lies in phantom realm for interacting mode (b 2 = 0.035-fitted with recent observational data) as seen in Fig. 2. Plotting the deceleration parameter showed an accelerating expansion for the universe and behaves approximately similar to the cold dark matter. The plot of the Om-diagnostic tool against redshift by taking x = ln(1 + z) −1 is shown in Fig. 3. It may be noted that the choice of HDE in this work is a very particular example of generalized HDE introduced in Nojiri and Odintsov, where a phantom cosmology based approach towards unification of early and late-time universe was proposed.\n\nMentions:\n- The evolvement of Om-diagnostic tool versus redshift is depicted in Fig. 3.\n- The plot of the Om-diagnostic tool against redshift by taking x = ln(1 + z) −1 is shown in Fig. 3.\n\nOCR Text: \"0,035 Z+]\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: The evolution of Om-diagnostic tool versus z"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The periodic wave solutions of Eq.( 35) can be realized for much more values of the boundary conditions and for different amplitudes of the blood waves. Several examples of periodic solutions of Eq.( 35) obtained through the system of equation (36) are shown in Fig. 1. Because of their larger parameter regions of existence the periodic solutions can be used for construction of the wave motion in the blood in presence of irregularity of heart beats as follows. Let the heart makes a pulsation and the blood wave start to propagate in the artery. This can be modelled by a half a period of the periodic wave solution of Eq. (35). When the next pulsation comes one can stop at the corresponding values of U and its derivatives and can treat them as the new initial conditions. These new initial conditions describe slightly different periodic wave. Half a period of this wave can be used to model the blood flow wave up to the moment of the next pulsation. At this moment the reached values of U and its derivatives are again the initial conditions that describe the blood wave corresponding to the third pulsation, etc. In such a way a sequence of slightly different waves (shown in Fig. 2.) may model the slight irregularity of the heart activity. It is well known that the time intervals between the heart pulsations are long-range correlated and this is one of the numerous arising of long-range correlations in various systems [40] - [43]. Such long-range correlated pseudorandom sequences modeling heart activity may be generated by a computer program and the values in the sequence will determine the end of the corresponding wave train and the beginning of the next wave train. This was realized in Fig. 2.\n\nMentions:\n- In such a way a sequence of slightly different waves (shown in Fig. 2.) may model the slight irregularity of the heart activity.\n- This was realized in Fig. 2.\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 2: Periodic blood waves constructed by parts of periodic solutions of Eq.(35) by the algorithm described in the text. It may seem that a single wave is shown at the figure. Actually the wave is constructed by the parts of 3 different periodic solutions of Eq.(35). Namely it is assumed that the first pulsation started at η = 0 with initial conditions U(0) = V (0) = W (0) = 0.99 and µ1 = 1.43; µ3 = 1.1. The second pulsation was at η = 2.844 with initial conditions U(2.844) = 0.7253; V (2.844) = −0.5571; W (2.844) = 1.5255 and µ1 = 1.42; µ3 = 1.11. The third pulsation was at η = 6.924 with initial conditions U(6.924) = 0.6409; V (6.924) = −0.2000; W (6.924) = 1.6772 and µ1 = 1.44; µ3 = 1.08."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Fig. 2 shows the calculated IS energies for all studied state points. As expected, deeper and deeper basins are explored on cooling. The deepest IS are sampled at the lowest explored density (ρ ≈ 0.9 g/cm 3 ), revealing the optimal density for the build up of the hydrogen-bonded network. Interestingly, a region of negative curvature of E IS vs ρ is present at low T , a first indication of an energetic destabilisation of the liquid [64,65].\n\nMentions:\n- Fig. 2 shows the calculated IS energies for all studied state points.\n\nOCR Text: \"p'isrni\"\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 2. Average inherent structure energy EIS for all studies state points."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Fig. 10 the experimental distribution of the missing mass of K S , M X (K S ), is presented with the cut on −t Θ &lt; 0.45 GeV 2 . The dashed line is the result of the Monte Carlo simulation, which is a smooth distribution without any structure. To account for imperfections in the detector simulation, we allowed this distribution to vary slightly to describe the data better.\n- where SIM (φ) is the Monte Carlo simulated histogram from φ−production, and P OL 3 is a 3rd order polynomial function. All parameters of the P OL 3 function were allowed to vary. The result is the dashed-dotted line in Fig. 10; we refer to that distribution (F B ) as the null or background (B) hypothesis, i.e. assuming that experimental spectrum is fully described by the modified Monte Carlo distribution.\n- shown as the solid line in Fig. 10.\n\nMentions:\n- In Fig. 10 the experimental distribution of the missing mass of K S , M X (K S ), is presented with the cut on −t Θ &lt; 0.45 GeV 2 .\n- The result is the dashed-dotted line in Fig. 10; we refer to that distribution (F B ) as the null or background (B) hypothesis, i.e. assuming that experimental spectrum is fully described by the modified Monte Carlo distribution.\n- shown as the solid line in Fig. 10.\n- Now the resonance structure appears on top of a background with inclined shape and not in the middle of the symmetric distribution, as in Fig. 10.\n- Below, in Table I we summarize statistical information about hypotheses testing based on data presented in Fig. 10 and Fig. 11 as a result of the fits described above.\n\nOCR Text: \"150- 100 1,5 1,6 Mx (K= [Gev]\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 10: Missing mass of KS with a cut −tΘ < 0.45GeV 2. The dashed line is a result of φ MC simulation, the dasheddotted line is a modified MC distribution, and the solid line is a result of the fit with modified MC distribution plus Gaussian function."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In this section, we show how changing the memory configuration can optimize the price. Using the profiling information in Table II we were able to efficiently search over all the combinations of fusion and memory configurations the same way we search over placement of functions on edge and cloud. For each fusion solution, we search over two different memory configurations (128 MB and 256 MB) for each function. We note that for the non-fused case, we have 5 functions so there are 25 combinations but after fusing we might end up with 2 functions so we search for 4 combinations. Figure 12 shows the relationship between the execution time and the price for different solutions of fusion and memory configuration. The Figure shows that the data points are organized into clusters. Each cluster represents one fusion solution and within each cluster there are several data points that have different prices and execution times. We note that for the non-fused case, the solution that have the best price was non-trivial since it keeps the memory for the 4 functions at 128MB and it increases the memory of f 3 only to 256MB, this ended up  II). Such speedup has a positive impact on the overall price and execution time. We conclude that setting the memory configurations manually is not ideal and running all profiling configurations is very expensive. On the other hand, profiling each function separately and running a scalable algorithm such as Costless can help finding a non trivial and cheaper memory configurations. The best solution we found improved both the price and the execution time by 6% and 10% respectively.\n\nMentions:\n- Figure 12 shows the relationship between the execution time and the price for different solutions of fusion and memory configuration.\n\nOCR Text: \"140 - 120 - 100- 5,0 Fyacutiontime cpci\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 12: Price and time for different fusions and memory configurations"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- We thus can easily know which element happened-before another element. A partially ordered set on the other hand, is a set where only certain pairs of elements are comparable, i.e. one element precedes the other in the ordering, but not every pair of elements is necessarily comparable. As an example, let's look at the diagram in figure 4. The diagram shows a set S with eight timestamps. We say that an element in S \"happenedbefore\" another element, if and only if every value of element a is less than or equal to every corresponding value in element b. So for example element (1, 0, 0) happened before element (1, 1, 0), because none of the values in element (1, 0, 0) are greater than in element (1, 1, 0) -We say that (1, 0, 0) happened-before (1, 1, 0). If on the other hand we try to compare element (1, 1, 0) and (1, 0, 1), one can see that both elements have values larger than the other element at some indices. We say that this pair is not comparable. One cannot determine which element occurred before the other one. In the context of the vector clock, incomparable timestamps can occur when nodes send messages concurrently, or when nodes were not able to communicate with one another for some time.\n\nMentions:\n- As an example, let's look at the diagram in figure 4.\n\nOCR Text: \"0,1. 1 1. 0 . 1.1. 0 0.1.0 1.0,\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 4: A partially ordered diagram, also known as a join-semilattice."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- For instance, N = 249 and H ′ (N) ≃ 4.5 for the Debussy's piece in Fig. 1, and H ′ (r) for the distribution in the figure is H ′ (r) ≃ 2.0. Similarly, H ′ (N) ≃ 5.7 (resp. 4.2) and H ′ (r) ≃ 2.2 (resp. 1.4) for the Mozart's (resp. Mussorgsky's) piece in Fig. 2 (resp. Fig. 3). Thus, a reduction factor of H ′ (r)/H ′ (N) ≈ 0.44 (resp. 0.39, 0.33) is expected for the averaged following time (minus unity) for the Debussy's (resp. Mozart's, Mussorgsky's) piece, which also contributes to reducing some fraction of the estimation errors.\n\nMentions:\n- Similarly, H ′ (N) ≃ 5.7 (resp. 4.2) and H ′ (r) ≃ 2.2 (resp. 1.4) for the Mozart's (resp. Mussorgsky's) piece in Fig. 2 (resp. Fig. 3).\n\nOCR Text: \"pcsitions Resumpton 3ositions Scorc pcsiton chordl Stco\"\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Figure 3: Distribution of stop and resumption positions in the performances during practicing Mussorgsky’s “Promenade.” See caption in Fig. 1."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- The scalability of the ASP part of the system has been tested over graphs of increasing size. First of all, we measured running times of the Brain Evolution Simulation module over a set of simulated graphs possibly representing connectome; in particular, we fixed the number of nodes (84 in our tests, coherently with the technique described in Section 5.1) and we randomly generated graphs with increasing number of edges up to a complete graph. It is worth recalling that, as pointed out in Section 6.1.1, the average number of edges in graphs corresponding to real connectome is around 2000. Results are reported in Figure 19; each data point is the average running time of 10 different executions over random graphs having the same number of nodes and edges. Via this figure, it is possible to observe that all the tested ASP programs for studying structural properties are solvable with execution times always below one second on any potential graph representing a connectome. There are obviously small variations between different samples and properties; nonetheless, the figure shows that any connectome can be easily managed by our approach in order to study structural properties. As a further scalability test, we generated graphs with increasing number of nodes; as for the number of edges in these graphs, we measured the average number of edges in the graphs representing real connectome and we kept the same proportion of edges for each generated graph. Results are shown in Figure 20 for graphs up to 700 nodes; again, each data point is the average running time of 10 different executions over random graphs having the same number of nodes and edges. In this case, it is possible to observe that, while the number of nodes is around 100, running times for all the problems are reasonable; when the number of nodes grows further, the combinatorial explosion of programs including non-deterministic choice rules is reflected in rapidly increasing running times (indeed, as an example, Max Clique is reported as an hard problem in the ASP Competition series (Gebser et al. 2016;Gebser et al. 2017;)); Min Vertex-Cover is affected first by this issue. In fact, we observed that, on the machine used, it may require more than one hour of computation for determining the Min Vertex-Cover on a graph with around 150 nodes or finding the Max Clique on a graph with around 500 nodes. It is interesting to observe that, again, considering only important/unimportant edges allows to move forward the limit of computation. Indeed, the lower number of considered edges simplifies the graph and the execution time is faster; as it was expected, considering important edges (40% of the total) allows a further improvement with respect to the not important edges (60% of the total). Notably, k-hub scales very well on tested graphs.\n- The experience above calls for the need of some extra features of ASP systems, e.g., extending their solving capabilities with custom heuristics and propagators; some work in this direction is currently ongoing (see (Dodaro and Ricca 2018) and references therein). However, the applicability of these approaches in our context is not straightforward. As a matter of fact, even taking the possibility of specifying suitable propagators into account, it is not always possible to easily define model generation guiding rules; let us think, for instance, to assortativity, where it is not clear how to guide the edge selection in order to imply a decrease in the property. Moreover, the problem is even more complex if we consider that the combinatorial explosion of this problem is coupled with an optimization task. However, it is worth stressing again the potential role of important/unimportant edges in encompassing, to some extent, scalability issues in our general framework. In fact, we have first shown in Sections 6.1.4 (see specifically Figure 14) that removing even a high percentage of unimportant edges does not affect the classification quality. We have then shown in Section 6.2 (see specifically Figure 20) that limiting the computation on (a small number of) important edges only significantly reduces performance issues, thus extending the dimensions of graphs that can be managed.\n\nMentions:\n- Results are shown in Figure 20 for graphs up to 700 nodes; again, each data point is the average running time of 10 different executions over random graphs having the same number of nodes and edges.\n- We have then shown in Section 6.2 (see specifically Figure 20) that limiting the computation on (a small number of) important edges only significantly reduces performance issues, thus extending the dimensions of graphs that can be managed.\n\nOCR Text: \"Ti , IU? FtT Mor\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 20: Running time of the Brain Evolution Simulation module on simulated graphs with increasing number of nodes."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In the experiments, a sample unstructured input document is fed to the developed compiler system. It is mainly a decree issued in the Arabic language that dates back to year 2018. Figure 4 depicts the input document; while, Figure 5 shows the output code generated by the compiler after translating the input document. Obviously, the output is a structured XML file conveying the data originally presented in the input file. All in all, the proposed system delivers automation capabilities for converting hand-written juridical files into digital XML documents that can be managed by modern computers. As the original decree file became in digital format, it can now be stored and archived on computers. Moreover, being structured, it can be easily and systematically digitally manipulated including searching, editing, extending, and printing. This automation could prove to be a starting point for a future digital government platform for the Lebanese government.\n\nMentions:\n- Figure 4 depicts the input document; while, Figure 5 shows the output code generated by the compiler after translating the input document.\n\nOCR Text: \"Rctoyty 13 2 7*3 Tic ~aedelsnnalat\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig 5: Output XML Document"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Number of the Sentences In the remaining 1.6 million commit messages, we counted the number of the sentences in each commit message by using Stanford CoreNLP [12]. The majority of the commit messages have few sentences. 82% of the commit messages have only one sentence. Only 0.2% of the commit messages have more than ten sentences. Figure 3 shows the histogram of the number of the sentences in the commit messages (excluding the messages have more than ten sentences due to space limit).\n\nMentions:\n- Figure 3 shows the histogram of the number of the sentences in the commit messages (excluding the messages have more than ten sentences due to space limit).\n\nOCR Text: \"125d000 - 1 ICddooo _ 75d000 _ sodo00 _ 250000 _ 1 2 ; 9   10 Number ol senlences per commii\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 3. Histogram of number of sentences in the commit messages"}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- Generating MIR and Possible World Visualizations. Once the input example's inconsistency is repaired, Euler/X will proceed to generate all maximally informative relations (MIR; see Thau et al., 2009 [14]) among taxonomic concept pairs. The interaction of the three articulations involving disjunction (Fig. 1) form an inherently ambiguous input alignment, which results in a total of seven equally consistent \"possible world\" solutions. These possible worlds can be displayed using a simple \"Reduced Containment Graph\" -a transitively reduced directed graph in which an edge represents proper inclusion. (Fig. 4). Interactive Ambiguity Reduction. Although the seven possible worlds (Fig. 4) accurately reflect the resolving power of the input alignment (Fig. 1), the user may now have the ability and desire to reduce the inherent ambiguity by selectively eliminating certain (apparently improbable) possible worlds. This is facilitated by the Euler/X feature of ambiguity reduction. At runtime, Euler asks the user more questions (generated by a decision tree function) via pop-out interactive windows allowing the user to select the preferred answer, e.g., by specifying that the current articulation in the query instance is \"1.A &gt; 2.G\", i.e., 1.A properly includes 2.G (Fig. 5). Based on the responses Euler/X can reduce the number of possible worlds from seven to three, filtering out four possible worlds in which 1.A and 2.G and overlap. Visual clustering of similar possible worlds. We can expect some use cases with larger sized input taxonomies and multiple inherent ambiguities to yield large numbers of possible worlds. Euler/X offers a visual representation of the cumulative possible worlds \"universe\" via a distance matrix (Fig. 6). As shown in Fig. 4, our input example has seven possible worlds. We can compute pairwise distances among these by integrating the numbers of MIRs in which they differ and thereby generating a network that summarizes the similarities and differences.\n\nMentions:\n- Euler/X offers a visual representation of the cumulative possible worlds \"universe\" via a distance matrix (Fig. 6).\n\nOCR Text: \"Dlt\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 6: Distance matrix-based visualization of the seven possible worlds of the input example. See also Fig. 4. The absolute distance between two possible worlds is the shortest distance traceable in the graph; e.g. the distance between possible worlds 5 and 6 is 4 steps."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- . IV. We see that the value of A increases for large N DW , which agrees with the results of the previous study [23]. We note that there is a subtlety in the results shown in Fig. 3. The values of A plotted in this figure slightly increase with time, deviating from the exact scaling behavior (A = constant). At this stage it is not clear whether this slight increase of A continues in later times or not, because of the limitation of the dynamical range of the numerical simulations. Here, we just take account of the possibility of the deviation from the scaling solution, and fit the result of A(τ ) obtained from the simulations with N = 16384 into the model function given by Eq. (2.44). In terms of the conformal time, this model function can be rewritten as\n\nMentions:\n- We note that there is a subtlety in the results shown in Fig. 3.\n\nOCR Text: \"4c- I-{1h16 - I-t0 I=MEh0~= J6 - 1,Po= T2F \"37} I~=; Eh-| '377 ccnlonal Iime iin Ihe unit of t Jt 4a7 Re=\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "FIG. 3: Time evolution of the area parameter A for various values of NDW without the bias (Ξ = 0). For the simulations with N = 8192 and N = 16384, we plot the mean and standard deviation among 10 realizations. For those with N = 32768, we just plot the result of 1 realization."}]}
{"messages": [{"role": "system", "content": "You are a helpful scientific writing assistant. Generate a concise and accurate caption for the figure based on the provided context."}, {"role": "user", "content": "Paragraphs:\n- In Fig. 6, we plot ρ 2 as a function of ∆ for different values of P 3 for fixed value of b = 1.5.\n\nMentions:\n- In Fig. 6, we plot ρ 2 as a function of ∆ for different values of P 3 for fixed value of b = 1.5.\n\nOCR Text: \"iocB\"\n\nTask: Generate a caption."}, {"role": "assistant", "content": "Fig. 6. Variation of ρ2 as a function of the gap ∆ in bits. b = 1.5."}]}
